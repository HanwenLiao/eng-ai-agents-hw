{"video": "video_03", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video what we will do is we will see now this kind of new architecture see now this kind of new architecture that we motivated from the earlier that we motivated from the earlier discussion about simple rnn cell discussion about simple rnn cell behavior u and if you recall that other behavior u and if you recall that other video what we have seen is that in video what we have seen is that in when we actually back propagating when we actually back propagating through time that kind of w matrix was through time that kind of w matrix was really the key element that defined really the key element that defined whether or not we had going to have whether or not we had going to have explosive or diminish gradients and explosive or diminish gradients and in the case of di in kind of gradients", "image_path": "img_data/video_03_chunk_0.jpg"}
{"video": "video_03", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "explosive or diminish gradients and in the case of di in kind of gradients in the case of di in kind of gradients the simple n had a lot of problems to be the simple n had a lot of problems to be trained and as the u gradients trained and as the u gradients dictate how fast we can actually train dictate how fast we can actually train because of our stochastic grade because of our stochastic grade descent kind of relationship in that descent kind of relationship in that in this video we will see how we in this video we will see how we solve this problem we at least try to solve this problem we at least try to solve this problem with this long solve this problem with this long shortterm memory architecture and in order to start this architecture and in order to start this discussion i'm going to redraw", "image_path": "img_data/video_03_chunk_1.jpg"}
{"video": "video_03", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "architecture and in order to start this discussion i'm going to redraw discussion i'm going to redraw what we have seen earlier this is what we have seen earlier this is the concatenation of the hidden state at tus2 and the current input which is t tus2 and the current input which is t minus one we are concatenating it passing one we are concatenating it passing through the w matrix adding a through the w matrix adding a bias this is what we have seen in that", "image_path": "img_data/video_03_chunk_2.jpg"}
{"video": "video_03", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "through the w matrix adding a bias this is what we have seen in that bias this is what we have seen in that video as the architecture of a simple video as the architecture of a simple rnn layer and finally passing through rnn layer and finally passing through this kind of nonlinearity t h to this kind of nonlinearity t h to produce the ht minus one which is the produce the ht minus one which is the hidden statea t minus one let me draw hidden statea t minus one let me draw one more instance of this one more instance of this unrolled kind of architecture we concatenated now the", "image_path": "img_data/video_03_chunk_3.jpg"}
{"video": "video_03", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "architecture we concatenated now the ht minus one with xt to form the hd these are the two instances of this simpler and an architecture of this simpler and an architecture we have seen this kind of we have seen this kind of problems the first step to problems the first step to introduce the lstm architecture the introduce the lstm architecture the first change that we actually making in first change that we actually making in that architecture is the introduction", "image_path": "img_data/video_03_chunk_4.jpg"}
{"video": "video_03", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "first change that we actually making in that architecture is the introduction that architecture is the introduction of a neural network to form of a neural network to form features that are going to be features that are going to be constructed by the network itself constructed by the network itself as we have seen in another video when we as we have seen in another video when we looked at the fully connected network looked at the fully connected network architectures at that time we architectures at that time we introduce a very simple network that introduce a very simple network that was consist of just of three neurons was consist of just of three neurons three sigmoidal neurons and we have", "image_path": "img_data/video_03_chunk_5.jpg"}
{"video": "video_03", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "was consist of just of three neurons three sigmoidal neurons and we have three sigmoidal neurons and we have actually commented at that time that actually commented at that time that there is a body of the network and there is a body of the network and there is the head of the network and the is the head of the network and the job really of the body is to create the job really of the body is to create the features which is required by the head features which is required by the head in order for the head to actually do its in order for the head to actually do its task a classification task in that case task a classification task in that case but also a regression task could also be but also a regression task could also be done in the very similar kind of done in the very similar kind of fashion what we will need to fashion what we will need to change here is we are going to be intr", "image_path": "img_data/video_03_chunk_6.jpg"}
{"video": "video_03", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "fashion what we will need to change here is we are going to be intr change here is we are going to be intr producing replacing effectively this producing replacing effectively this concatenation and this matrix concatenation and this matrix multiplication and bias addition here multiplication and bias addition here with a neural network that it will sort of network that it will sort of driven by a sigmoidal nonlinearity driven by a sigmoidal nonlinearity i'm going to just replace this thing i'm going to just replace this thing here with the following i'm going to have two", "image_path": "img_data/video_03_chunk_7.jpg"}
{"video": "video_03", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "here with the following i'm going to have two following i'm going to have two inputs in this neural network i'm inputs in this neural network i'm just continue to draw it this h t just continue to draw it this h t minus one and i'll just going call it minus one and i'll just going call it input neural network and this is instead input neural network and this is instead of input neur network i'll just instead of input neur network i'll just call it input a dense layer and we'll write the functional form of this dense layer a bit later all", "image_path": "img_data/video_03_chunk_8.jpg"}
{"video": "video_03", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "layer and we'll write the functional form of this dense layer a bit later all form of this dense layer a bit later all right this is our input dense right this is our input dense layer and the second thing that we layer and the second thing that we need to introduce here in this kind of need to introduce here in this kind of new architecture in fact let me new architecture in fact let me actually i can draw it over actually i can draw it over here the second thing is this kind of line that is this kind of line that represents a new memory", "image_path": "img_data/video_03_chunk_9.jpg"}
{"video": "video_03", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "is this kind of line that represents a new memory represents a new memory that is called long-term memory for that is called long-term memory for is also called cell state let me is also called cell state let me write it as a new state we had the write it as a new state we had the hidden state hd minus one before now hidden state hd minus one before now introducing the cell state which will introducing the cell state which will symbolize with the letter symbolize with the letter s and this is the cell state line and this line will basically represents", "image_path": "img_data/video_03_chunk_10.jpg"}
{"video": "video_03", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "this is the cell state line and this line will basically represents this line will basically represents memory in a sense that we are able to memory in a sense that we are able to store a cell state over u in that on store a cell state over u in that on that line that what the line represents that line that what the line represents and retrieve it this is the and retrieve it this is the we'll see now in a moment what we'll see now in a moment what happens the input features which are happens the input features which are being generated out of the input kind being generated out of the input kind of dense layer are going to be added with the contents of the that", "image_path": "img_data/video_03_chunk_11.jpg"}
{"video": "video_03", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "added with the contents of the that memory at this specific moment in time memory at this specific moment in time we are kind of reading out of this we are kind of reading out of this memory and this arrow represents this memory and this arrow represents this kind of read operation and at that kind of read operation and at that moment in time we are at we're bringing moment in time we are at we're bringing the st minus 2 from that memory in the st minus 2 from that memory in the similar fashioners we have been the similar fashioners we have been using the hd minus 2 for the using the hd minus 2 for the -called shortterm memory the hidden -called shortterm memory the hidden state the cell state at st minus one state the cell state at st minus one is being brought in and is being added is being brought in and is being added to whatever input features the", "image_path": "img_data/video_03_chunk_12.jpg"}
{"video": "video_03", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "is being brought in and is being added to whatever input features the to whatever input features the input dense layer is giving input dense layer is giving us to form effectively the new hidden us to form effectively the new hidden state that will be calling st minus one state that will be calling st minus one at this moment in time we have st at this moment in time we have st minus one here the addition of this minus one here the addition of this and the st minus 2 is giving us s minus and the st minus 2 is giving us s minus one that we are going to store", "image_path": "img_data/video_03_chunk_13.jpg"}
{"video": "video_03", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "and the st minus 2 is giving us s minus one that we are going to store into the cell state that's this is and then state that's this is and then of course the other unrolling time of course the other unrolling time instance in another unrolling time instance in another unrolling time instance we're going to form our from instance we're going to form our from that hidden just to finish this will that hidden just to finish this will form u from the stus one will pass it form u from the stus one will pass it through the nonlinearity to through the nonlinearity to form the h t minus one as if as exactly as we were doing it", "image_path": "img_data/video_03_chunk_14.jpg"}
{"video": "video_03", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "form the h t minus one as if as exactly as we were doing it one as if as exactly as we were doing it earlier and i think you can contrast earlier and i think you can contrast these two kind of diagrams what we are these two kind of diagrams what we are doing here this thing was effectively replaced by the thing was effectively replaced by the input dense layer and we have the new cell layer and we have the new cell state we pass it through the state we pass it through the nonlinearity to form the new hidden nonlinearity to form the new hidden state ht minus one the hidden state ht minus one the hidden state we have seen earlier is a nonlinear we have seen earlier is a nonlinear function tan h function of the st", "image_path": "img_data/video_03_chunk_15.jpg"}
{"video": "video_03", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "we have seen earlier is a nonlinear function tan h function of the st function tan h function of the st minus one all right then we take this minus one all right then we take this htus one we pass it through another sort of input dense layer that's another sort of instance layer that's another sort of instance of this rnn this is at this moment in of this rnn this is at this moment in time our first stage this is now the second first stage this is now the second state starts obviously this requires an", "image_path": "img_data/video_03_chunk_16.jpg"}
{"video": "video_03", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "first stage this is now the second state starts obviously this requires an state starts obviously this requires an input xt we are summing over now we're bringing the over now we're bringing the memory at time s t minus one we memory at time s t minus one we reading from st minus one and see reading from st minus one and see what happened right now look at this what happened right now look at this line effectively we have formed a line effectively we have formed a skip over connection this is a skip over", "image_path": "img_data/video_03_chunk_17.jpg"}
{"video": "video_03", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "skip over connection this is a skip over connection this is a skip over connection we have seen the skip over connection we have seen the skip over connections in residual convolutional connections in residual convolutional networks and the rationale networks and the rationale there was to solve yet there was to solve yet another vanishing gradient problem another vanishing gradient problem and effectively allow a -call highway and effectively allow a -call highway which is represented here by this line which is represented here by this line which is the cell state to actually be which is the cell state to actually be the conveyor that will transfer this the conveyor that will transfer this kind of gradient", "image_path": "img_data/video_03_chunk_18.jpg"}
{"video": "video_03", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "the conveyor that will transfer this kind of gradient undisturbed many instances in the undisturbed many instances in the past and this is now we're getting some past and this is now we're getting some somewhere with this we are u taking somewhere with this we are u taking again the tan h from st minus one to again the tan h from st minus one to form the h of t the first changes we form the h of t the first changes we have done is the introduction of the have done is the introduction of the input dense layer and this kind of a input dense layer and this kind of a cell state and now we need to introduce cell state and now we need to introduce this by itself will actually", "image_path": "img_data/video_03_chunk_19.jpg"}
{"video": "video_03", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "cell state and now we need to introduce this by itself will actually this by itself will actually solve issues with u back propagation solve issues with u back propagation but at the same time we need to be but at the same time we need to be mindful of a couple of things the mindful of a couple of things the first thing we need to be mindful first thing we need to be mindful is that the features which mindful is that the features which have been generated by the network we have been generated by the network we need to make sure that we are need to make sure that we are able to adjust them in terms of what able to adjust them in terms of what exactly input features we are need at exactly input features we are need at this specific moment in time we", "image_path": "img_data/video_03_chunk_20.jpg"}
{"video": "video_03", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "exactly input features we are need at this specific moment in time we this specific moment in time we need to kind of filter them rather than need to kind of filter them rather than adjustment we need to filter them and adjustment we need to filter them and this filtering is going to be achieved this filtering is going to be achieved by a gate what we'll be calling an by a gate what we'll be calling an input gate and we'll see exactly how input gate and we'll see exactly how this will be added over here in this will be added over here in this diagram similarly we need to be treating diagram similarly we need to be treating the both the cell state and also the both the cell state and also the hidden state over here as resources because these are here as resources because these are basically vectors which we have prior a", "image_path": "img_data/video_03_chunk_21.jpg"}
{"video": "video_03", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "here as resources because these are basically vectors which we have prior a basically vectors which we have prior a prior kind of define in terms of the prior kind of define in terms of the vector sizes and the more we add into vector sizes and the more we add into them the we do not want to kind of them the we do not want to kind of run out of this kind of resource we run out of this kind of resource we need to treat them as a resource and need to treat them as a resource and make sure that we add only the things make sure that we add only the things that we need at the specific instance in that we need at the specific instance in time in order for us to sort of time in order for us to sort of achieve the goal and of course achieve the goal and of course this goal is going to be", "image_path": "img_data/video_03_chunk_22.jpg"}
{"video": "video_03", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "achieve the goal and of course this goal is going to be this goal is going to be produced as if we exactly in this in produced as if we exactly in this in the way that we have seen earlier from the way that we have seen earlier from a specific head that we need to a specific head that we need to attach to u to process these kind of attach to u to process these kind of states we'll see that in a states we'll see that in a moment let's try to introduce moment let's try to introduce first our first gate which i'll call it first our first gate which i'll call it the input gate is going to be gate the input gate is going to be added over here", "image_path": "img_data/video_03_chunk_23.jpg"}
{"video": "video_03", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "gate the input gate is going to be added over here and we'll take the form of a and we'll take the form of a multiplier this multiplier is not multiplier this multiplier is not the orig the multiply it's called the orig the multiply it's called hadamar multiplication and in english hadamar multiplication and in english basically stands for element by element basically stands for element by element multiplication we have if you a multiplication we have if you a vector that it is going to be vector that it is going to be produced at the input from the input produced at the input from the input kind of dense layer and we are element kind of dense layer and we are element by element applying a factor between", "image_path": "img_data/video_03_chunk_24.jpg"}
{"video": "video_03", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "kind of dense layer and we are element by element applying a factor between by element applying a factor between zero and one this input gate is going zero and one this input gate is going to end up being a neural network itself that it will as we will see be represented by the output of by the represented by the output of by the sigmoid that adjustment is going sigmoid that adjustment is going to happen because we are going to be to happen because we are going to be forgetting or not", "image_path": "img_data/video_03_chunk_25.jpg"}
{"video": "video_03", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "to happen because we are going to be forgetting or not considering inputs elements the considering inputs elements the inputs that are not necessarily relevant inputs that are not necessarily relevant to the task we are effectively to the task we are effectively adjusting the contents of this adjusting the contents of this long-term memory and this is long-term memory and this is and of course this input will also be and of course this input will also be driven by the exactly the driven by the exactly the same instances in time of our inputs the same instances in time of our inputs the hidden state and the input xt", "image_path": "img_data/video_03_chunk_26.jpg"}
{"video": "video_03", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "same instances in time of our inputs the hidden state and the input xt minus1 and then of course we have to do another adjustment this to do another adjustment this adjustment will make sure that we are adjustment will make sure that we are forgetting the elements of the cell forgetting the elements of the cell state that need to be forgotten and state that need to be forgotten and this will also be called now the forget this will also be called now the forget gate for that how we're going to gate for that how we're going to introduce this forget gate we will introduce this forget gate we will introduce a", "image_path": "img_data/video_03_chunk_27.jpg"}
{"video": "video_03", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "introduce this forget gate we will introduce a multiplier that it will sort of be multiplier that it will sort of be right here and this line that takes right here and this line that takes this st minus one and adds it to the this st minus one and adds it to the input dense layer to the output of this input dense layer to the output of this input dense layer we will add a multiplier this multiplier is going to multiplier is going to be the forget", "image_path": "img_data/video_03_chunk_28.jpg"}
{"video": "video_03", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "multiplier is going to be the forget gate this is xt and this is ht minus one obviously there's going to be a multiplier right here multiplier right here too from the previous kind of stage but too from the previous kind of stage but and we can actually draw", "image_path": "img_data/video_03_chunk_29.jpg"}
{"video": "video_03", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "too from the previous kind of stage but and we can actually draw it but we are not going to decorate it with any variables at this moment we have variables at this moment we have the forget gate and finally we need the forget gate and finally we need to kind of manage also the shortterm to kind of manage also the shortterm hidden state what we will do now hidden state what we will do now is we will be adding a multiplier right is we will be adding a multiplier right here", "image_path": "img_data/video_03_chunk_30.jpg"}
{"video": "video_03", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "is we will be adding a multiplier right here hd minus one is obtained after this multiplication with again these multiplication with again these multiplications are the -called multiplications are the -called hadamar multiplications or element by hadamar multiplications or element by element multiplications this is going element multiplications this is going to be called the -called to be called the -called output gate and this will be the xt ht minus", "image_path": "img_data/video_03_chunk_31.jpg"}
{"video": "video_03", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "gate and this will be the xt ht minus one and this is basically the one and this is basically the all the changes in at least in terms of all the changes in at least in terms of block diagram we would to introduce block diagram we would to introduce this whole thing here from the moment we have and here from the moment we have and let me also draw it over let me also draw it over here we have a multiplier and this is hd which exactly the same nature as the", "image_path": "img_data/video_03_chunk_32.jpg"}
{"video": "video_03", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "hd which exactly the same nature as the output gate we just introduced and this is xt sorry this output gate is 8 htus 2 xt -1 this output gate is", "image_path": "img_data/video_03_chunk_33.jpg"}
{"video": "video_03", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "is 8 htus 2 xt -1 this output gate is xt ht minus1 these are the what we will minus1 these are the what we will be calling from now on is this thing be calling from now on is this thing over here this block will be calling it the lsdm cell and we can actually contrast it with the", "image_path": "img_data/video_03_chunk_34.jpg"}
{"video": "video_03", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "and we can actually contrast it with the simple rnn cell that we have we had earlier these are all at least in terms of block diagrams let's try terms of block diagrams let's try to bring a couple of examples to bring a couple of examples to understand the a little bit the understand the a little bit the function of the input gate and", "image_path": "img_data/video_03_chunk_35.jpg"}
{"video": "video_03", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "understand the a little bit the function of the input gate and function of the input gate and let's write it down the input gate is a learns mind you that this we have learns mind you that this we have parameters here this input gate parameters here this input gate output gate and forget gate are all output gate and forget gate are all neuron networks during the training neuron networks during the training the parameters of this gate will be the parameters of this gate will be determined it to learn to", "image_path": "img_data/video_03_chunk_36.jpg"}
{"video": "video_03", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "the parameters of this gate will be determined it to learn to determine whether or not the input xt is worth preserving for example just to", "image_path": "img_data/video_03_chunk_37.jpg"}
{"video": "video_03", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "worth preserving for example just to preserving for example just to give an example here sort of let's say the in one sentence from sort of natural one sentence from sort of natural language processing let's assume that language processing let's assume that we have a task that has to do with we have a task that has to do with text summarization in one sentence we summarization in one sentence we say that the bank introduced a new savings", "image_path": "img_data/video_03_chunk_38.jpg"}
{"video": "video_03", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "a new savings account in this case the input gate will be such that it will kind of will be such that it will kind of remember everything but in the case everything but in the case where let's say the sentence is the where let's say the sentence is the bank comma faced with criticism", "image_path": "img_data/video_03_chunk_39.jpg"}
{"video": "video_03", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "bank comma faced with criticism over interest rates comma introduced and on and a new savings account this part criticism faced account this part criticism faced with criticism over interest rates is with criticism over interest rates is certainly for the task of tx mization", "image_path": "img_data/video_03_chunk_40.jpg"}
{"video": "video_03", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "with criticism over interest rates is certainly for the task of tx mization certainly for the task of tx mization may not be needed and therefore may not be needed and therefore the u corresponding elements of the u corresponding elements of the vector which is multiplying if you vector which is multiplying if you the input features may be u such that it the input features may be u such that it will need to be close to zero and will need to be close to zero and therefore not necessarily being stored therefore not necessarily being stored in the sort of cell state that's a in the sort of cell state that's a basically simple example of basically simple example of remembering exactly what the input gate remembering exactly what the input gate will be doing the forget gate is again learns to", "image_path": "img_data/video_03_chunk_41.jpg"}
{"video": "video_03", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "will be doing the forget gate is again learns to the forget gate is again learns to learns after the training process to learns after the training process to remember exactly what is needed to be remember exactly what is needed to be preserved in multiple instances in time preserved in multiple instances in time in terms of cell state and the output in terms of cell state and the output gate is doing that for the hidden gate is doing that for the hidden state i hope that this is a kind of state i hope that this is a kind of clear what kind of changes we have clear what kind of changes we have introduced in terms of block diagram and introduced in terms of block diagram and in terms of equations now let's write in terms of equations now let's write down the equation we can actually get", "image_path": "img_data/video_03_chunk_42.jpg"}
{"video": "video_03", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "in terms of equations now let's write down the equation we can actually get down the equation we can actually get to appreciate the parameters that we to appreciate the parameters that we have introduced and there are plenty of have introduced and there are plenty of parameters and there for lstms are kind parameters and there for lstms are kind of complex structures because of complex structures because of that reason the hope is that in the back that reason the hope is that in the back propagation we'll be able to preserve propagation we'll be able to preserve better the gradient if you look at a better the gradient if you look at a back propagation you actually start with propagation you actually start with and imagine we also have a loss and", "image_path": "img_data/video_03_chunk_43.jpg"}
{"video": "video_03", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "propagation you actually start with and imagine we also have a loss and imagine we also have a loss and this loss it is effectively obtained by this loss it is effectively obtained by attaching a head on this hidden state attaching a head on this hidden state h oft that loss is going to result into the a is going to result into the a downstream gradient but this downstream gradient but this downstream gradient as it enters over here it finds gradient as it enters over here it finds that skip over connection and this that skip over connection and this skover connection passes through skover connection passes through uninterrupted of course we have this multiplier over", "image_path": "img_data/video_03_chunk_44.jpg"}
{"video": "video_03", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "uninterrupted of course we have this multiplier over of course we have this multiplier over here and this uninterrupted sort of path is the uninterrupted sort of path is the one that we are going to be using to one that we are going to be using to go back in time we will go over here go back in time we will go over here we will see another tage function sorry another summer tage function sorry another summer and we are going to pass through that and we are going to pass through that pass through the input kind of dense pass through the input kind of dense layer but there are the other path is to", "image_path": "img_data/video_03_chunk_45.jpg"}
{"video": "video_03", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "pass through the input kind of dense layer but there are the other path is to layer but there are the other path is to go over again over another skip go over again over another skip connection if you unroll the skip connection if you unroll the skip over connection diagram just in the over connection diagram just in the same way as we have done in residual same way as we have done in residual kind of networks then you will actually see networks then you will actually see that we are have a multiplicity of paths that we are have a multiplicity of paths again to go over the various kind again to go over the various kind of stages we the diversity of stages we the diversity of gradient flow is preserved that is", "image_path": "img_data/video_03_chunk_46.jpg"}
{"video": "video_03", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "of stages we the diversity of gradient flow is preserved that is gradient flow is preserved that is the sort of rationale of introducing the sort of rationale of introducing if you this highway and that's the if you this highway and that's the reason why these architectures are reason why these architectures are instances of what we call highway instances of what we call highway networks that's that is a networks that's that is a block diagram and now we can actually go block diagram and now we can actually go ahead and see how we can write down the ahead and see how we can write down the equations are equations the equations are presented to you in the site in presented to you in the site in the", "image_path": "img_data/video_03_chunk_47.jpg"}
{"video": "video_03", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "presented to you in the site to understand the back propagation site to understand the back propagation through time for this lstms and how through time for this lstms and how it contrasts what we have seen earlier it contrasts what we have seen earlier with the simple rnn and with the simple rnn and demonstrate that we can definitely have demonstrate that we can definitely have some hope to remember kind of a longer some hope to remember kind of a longer periods of time in the past and avoid periods of time in the past and avoid this soal diminishing gradient kind of this soal diminishing gradient kind of problem let's kind of draw a diagram problem let's kind of draw a diagram sort of showcasing the skip over sort of showcasing the skip over connection in this kind of diagram we have", "image_path": "img_data/video_03_chunk_48.jpg"}
{"video": "video_03", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "connection in this kind of diagram we have in this kind of diagram we have the cell this lstm cell was the one that we have lstm cell was the one that we have just drew earlier with all this kind of just drew earlier with all this kind of forget gates but for now what i will do forget gates but for now what i will do is i'll just leave all this kind of is i'll just leave all this kind of multiplication had multipliers with the multiplication had multipliers with the gates kind of behind and go back to this gates kind of behind and go back to this original diagram we have seen where if original diagram we have seen where if you remember we have some state st", "image_path": "img_data/video_03_chunk_49.jpg"}
{"video": "video_03", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "original diagram we have seen where if you remember we have some state st you remember we have some state st minus one that was going through the minus one that was going through the tan h function and we had this kind of a function and we had this kind of a input dense layer and what we were doing is we layer and what we were doing is we were taking this std minus one with a skip over this std minus one with a skip over connection or we adding into the connection or we adding into the input and then we had another", "image_path": "img_data/video_03_chunk_50.jpg"}
{"video": "video_03", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "connection or we adding into the input and then we had another block that consists exactly of this function and this effectively what function and this effectively what reminds us this is st right now we reminds us this is st right now we are doing this in order to add it and are doing this in order to add it and form the new cell state which is st + one and on this is exactly what we have seen earlier in exactly what we have seen earlier in another video in the set of residual", "image_path": "img_data/video_03_chunk_51.jpg"}
{"video": "video_03", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "exactly what we have seen earlier in another video in the set of residual another video in the set of residual networks actually we can actually go networks actually we can actually go ahead and unroll this diagram and create ahead and unroll this diagram and create yet another diagram we have done already yet another diagram we have done already exactly the same process in the residual exactly the same process in the residual kind of network video for those who kind of network video for those who have missed it you can consult that have missed it you can consult that kind of video to see exactly how the kind of video to see exactly how the unrolling process was demonstrating unrolling process was demonstrating the fact that in the during back the fact that in the during back propagation where we have some kind propagation where we have some kind of gradient that it is flowing backwards of gradient that it is flowing backwards then that architecture with the skip", "image_path": "img_data/video_03_chunk_52.jpg"}
{"video": "video_03", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "of gradient that it is flowing backwards then that architecture with the skip then that architecture with the skip over connections was allowing the over connections was allowing the gradient to flow over this path in fact a flow over this path in fact a whole diverse set of paths and one of whole diverse set of paths and one of them was a direct skip over all them was a direct skip over all of the stag that we have seen here of the stag that we have seen here there is a in good faith we there is a in good faith we have the ability to maintain this have the ability to maintain this kind of gradient flow over many st kind of gradient flow over many st stages now in practice and empirically", "image_path": "img_data/video_03_chunk_53.jpg"}
{"video": "video_03", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "kind of gradient flow over many st stages now in practice and empirically stages now in practice and empirically because we have introduced you because we have introduced the gates the inputs the gates know the gates the inputs the gates which are associated with the gates which are associated with the forgate gates and on as well the forgate gates and on as well also other practical kind of also other practical kind of considerations we are going to be unable considerations we are going to be unable to maintain this kind of grading over to maintain this kind of grading over long periods of time empirically what long periods of time empirically what have sh shown is that although lstms have sh shown is that although lstms improved what we have seen earlier from improved what we have seen earlier from simple rnns finally themselves are", "image_path": "img_data/video_03_chunk_54.jpg"}
{"video": "video_03", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "improved what we have seen earlier from simple rnns finally themselves are simple rnns finally themselves are also suffering from finite memory also suffering from finite memory we don't expect miracles here but at we don't expect miracles here but at least we have some better situation least we have some better situation compared to the simple rnn and then from compared to the simple rnn and then from this reason lsdm are used instead of this reason lsdm are used instead of simple ns for any use case that requires simple ns for any use case that requires them back to the textbook the them back to the textbook the core side that is we are going to core side that is we are going to we're actually seeing here the diagram we're actually seeing here the diagram of the lstm cell and i think the picture", "image_path": "img_data/video_03_chunk_55.jpg"}
{"video": "video_03", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "we're actually seeing here the diagram of the lstm cell and i think the picture of the lstm cell and i think the picture is slightly different from what we have is slightly different from what we have seen just now and but it does show seen just now and but it does show the all the trainable parameters in this the all the trainable parameters in this full version we have the full version we have the input dense layer that we have input dense layer that we have seen this involves a dage seen this involves a dage nonlinearity typically we have the nonlinearity typically we have the forget gates this is the input for get forget gates this is the input for get gate with all the stainable parameters", "image_path": "img_data/video_03_chunk_56.jpg"}
{"video": "video_03", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "forget gates this is the input for get gate with all the stainable parameters gate with all the stainable parameters the cell state u sorry this is the input the cell state u sorry this is the input for getgate this is the cell state for getgate this is the cell state for getgate with another set of trainable getgate with another set of trainable parameters and the output for get gate parameters and the output for get gate with yet another set and this is this with yet another set and this is this actually diagram kind of shows the actually diagram kind of shows the cell as we will need to implement it cell as we will need to implement it in software and this is effectively what in software and this is effectively what the apis of both pytor in terms of low the apis of both pytor in terms of low are implementing evidently we have fairly implementing evidently we have fairly complex structure and but i hope you", "image_path": "img_data/video_03_chunk_57.jpg"}
{"video": "video_03", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "implementing evidently we have fairly complex structure and but i hope you complex structure and but i hope you recognize how if you break it recognize how if you break it down into its parts and just treat the down into its parts and just treat the essentials how the structure kind of essentials how the structure kind of serves the purpose of having us to sort serves the purpose of having us to sort of surviving in a longer periods of time of surviving in a longer periods of time that kind of gradient now the in that kind of gradient now the in terms of equations effectively the terms of equations effectively the diagram just shows this equations but diagram just shows this equations but there is another reference unfortunately there is another reference unfortunately we are not really able to do the we are not really able to do the animation right now in kind of this kind animation right now in kind of this kind of format of writing but there is a", "image_path": "img_data/video_03_chunk_58.jpg"}
{"video": "video_03", "start": "0:29:30", "end": "0:29:53.800000", "timestamp": "0:29:30 - 0:29:53.800000", "text": "animation right now in kind of this kind of format of writing but there is a of format of writing but there is a youtube video that is the reference for which i video that is the reference for which i believe it's a kind of a useful video to believe it's a kind of a useful video to go through because it just shows a go through because it just shows a very simple use case and kind very simple use case and kind of demonstrates how the gates are f of demonstrates how the gates are f helping us in that kind of simple use helping us in that kind of simple use case it has", "image_path": "img_data/video_03_chunk_59.jpg"}
{"video": "video_04", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now that we have seen in an earlier video the simple rnn kind of video the simple rnn kind of architecture and we went through a architecture and we went through a simple example of time serious simple example of time serious prediction i think it's worthwhile prediction i think it's worthwhile trying to understand what are the trying to understand what are the performance limitations of rnn of simple performance limitations of rnn of simple rnn architectures and that is going to rnn architectures and that is going to motivate us to developing a slightly motivate us to developing a slightly more advanced or considerably more advanced or considerably more advanced kind of architecture out of advanced kind of architecture out of this kind of discussion the question this kind of discussion the question that i think we it's worthwhile kind of that i think we it's worthwhile kind of trying to answer is the following", "image_path": "img_data/video_04_chunk_0.jpg"}
{"video": "video_04", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "that i think we it's worthwhile kind of trying to answer is the following trying to answer is the following can simple rnn capture long-term time dependencies and the approaches will actually follow here is we will unroll actually follow here is we will unroll the simpol architecture in it's u the simpol architecture in it's u assuming let's say", "image_path": "img_data/video_04_chunk_1.jpg"}
{"video": "video_04", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "the simpol architecture in it's u assuming let's say a classification kind of use case and classification kind of use case and look at how the network kind of learns look at how the network kind of learns by looking at the gradient flow by looking at the gradient flow during the training and of procedure during the training and of procedure this will actually reveal to us how this will actually reveal to us how the limitation of as we will see the limitation of as we will see the answer is a negative answer but we the answer is a negative answer but we need to have the block diag in order for need to have the block diag in order for us to sort of capture that limitation in us to sort of capture that limitation in our minds what i'm going to do here our minds what i'm going to do here is i'm going to draw this kind of", "image_path": "img_data/video_04_chunk_2.jpg"}
{"video": "video_04", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "our minds what i'm going to do here is i'm going to draw this kind of is i'm going to draw this kind of architecture we have u some gates architecture we have u some gates that are associated with the matrix w as we have seen it with the matrix w as we have seen it multiplying a hidden state hd minus 2 multiplying a hidden state hd minus 2 which is itself is coming in hd minus 2 which is itself is coming in from the previous unrolling instance then we have", "image_path": "img_data/video_04_chunk_3.jpg"}
{"video": "video_04", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "instance then we have a m a bias addition and of course we receive the input which is itself as receive the input which is itself as we discussed earlier multiplied by a matrix earlier multiplied by a matrix u this is my input ht minus u this is my input ht minus one then this will give us some form", "image_path": "img_data/video_04_chunk_4.jpg"}
{"video": "video_04", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "u this is my input ht minus one then this will give us some form one then this will give us some form of an activation ht minus one will'll of an activation ht minus one will'll pass that through that t h nonlinear unit to obtain an hd minus one and we will also then take the one and we will also then take the hidden state i just have only one layer hidden state i just have only one layer at this moment it doesn't really matter at this moment it doesn't really matter for that discussion to have one or more for that discussion to have one or more we're going to have a tensor v which is", "image_path": "img_data/video_04_chunk_5.jpg"}
{"video": "video_04", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "for that discussion to have one or more we're going to have a tensor v which is we're going to have a tensor v which is feeding this gate v ht minus1 we have the c the bias and then as we discuss we're going bias and then as we discuss we're going to assume some form of to assume some form of a classification example we need a soft max to example we need a soft max to produce my y of t minus one and then we produce my y of t minus one and then we have evidently a cross entropy loss to", "image_path": "img_data/video_04_chunk_6.jpg"}
{"video": "video_04", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "have evidently a cross entropy loss to evidently a cross entropy loss to obtain an lt minus one at the top and obtain an lt minus one at the top and this cross entropy loss is requiring the this cross entropy loss is requiring the ground truth yt minus one in general ground truth yt minus one in general these two will be vectors i went ahead and actually drew another instance of this next to it drew another instance of this next to it and this previous instance was lt giving and this previous instance was lt giving it lt minus one the t minus one instance it lt minus one the t minus one instance this is a t instance at this moment in this is a t instance at this moment in time the network is here and", "image_path": "img_data/video_04_chunk_7.jpg"}
{"video": "video_04", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "this is a t instance at this moment in time the network is here and time the network is here and evidently there's going to be this very important line that connects these two and for line that connects these two and for those who do not see it then that's those who do not see it then that's basically this line which i just threw basically this line which i just threw over here that is the really the what here that is the really the what important kind of connection between important kind of connection between the two instances the hidden state is the two instances the hidden state is caring forward in order to create this", "image_path": "img_data/video_04_chunk_8.jpg"}
{"video": "video_04", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "the two instances the hidden state is caring forward in order to create this caring forward in order to create this dependency with the previous hidden dependency with the previous hidden state to the output h oft as we state to the output h oft as we discussed let's see what happens discussed let's see what happens now during back propagation and now during back propagation and we've seen back propagation the first we've seen back propagation the first principles of back propagation and in principles of back propagation and in another video for those who have another video for those who have missed it but the back missed it but the back propagation as we discuss will actually propagation as we discuss will actually involve some form of local", "image_path": "img_data/video_04_chunk_9.jpg"}
{"video": "video_04", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "propagation as we discuss will actually involve some form of local involve some form of local gradient calculations upstream and downstream calculations upstream and downstream gradient calculations involving local gradient calculations involving local gradients at each kind of gate we see gradients at each kind of gate we see the gradient that is floating from the gradient that is floating from the let's say the loss function going through this function going through this whole sequence of gates and this whole sequence of gates and this sequence of gates have nothing different sequence of gates have nothing different than the sequence of gates we have seen than the sequence of gates we have seen in fully connected layers because that's in fully connected layers because that's really a fully connected layer followed really a fully connected layer followed by cross entropy and soft max and we by cross entropy and soft max and we have already we already know how toate", "image_path": "img_data/video_04_chunk_10.jpg"}
{"video": "video_04", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "by cross entropy and soft max and we have already we already know how toate have already we already know how toate fully connected layers and arrives over fully connected layers and arrives over here this is my d ht minus one i'm here this is my d ht minus one i'm following the notation that we have i'm following the notation that we have introduced earlier where we proceed with introduced earlier where we proceed with d the derivative d4 derivative d the derivative d4 derivative the gradient we see here this is this gradient", "image_path": "img_data/video_04_chunk_11.jpg"}
{"video": "video_04", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "the gradient we see here this is this gradient here this is this gradient will actually flow over the tan will actually flow over the tan h will actually go into this kind of adder and actually go into this kind of adder and it will be broken down into the parts which are needed for us into the parts which are needed for us to sort of calculate exactly how they to sort of calculate exactly how they actually i forgot to draw the tensor u then it will arrive over", "image_path": "img_data/video_04_chunk_12.jpg"}
{"video": "video_04", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "draw the tensor u then it will arrive over u then it will arrive over there here this gate which is really there here this gate which is really very essential and it will go over very essential and it will go over there and go backwards and this is the reason why backwards and this is the reason why this process is called in the this process is called in the literature back propagation back prop propagation through time", "image_path": "img_data/video_04_chunk_13.jpg"}
{"video": "video_04", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "propagation back prop propagation through time it will reach this point and if you followed along this kind of that other followed along this kind of that other video on the introduction of back video on the introduction of back propagation it sees this kind of a propagation it sees this kind of a hidden gate there's a hidden gate over hidden gate there's a hidden gate over here that effectively this is the here that effectively this is the copy gate or the for gate whatever we copy gate or the for gate whatever we call it back then and it will", "image_path": "img_data/video_04_chunk_14.jpg"}
{"video": "video_04", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "copy gate or the for gate whatever we call it back then and it will call it back then and it will definitely receive this gate also definitely receive this gate also another gradient coming that it is called the coming that it is called the dht d dht minus one of made a mistake here this is dht one of made a mistake here this is dht because the tensor is ht this will be because the tensor is ht this will be the dht minus one and the this dht minus one and the other version of this dht", "image_path": "img_data/video_04_chunk_15.jpg"}
{"video": "video_04", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "one and the this dht minus one and the other version of this dht one and the other version of this dht minus one the other component of dht minus one the other component of dht minus one will see will be sumed over to form this will see will be sumed over to form this kind of gradient that was the effect of that gradient that was the effect of that copy gate and the whole process will gate and the whole process will repeat itself and will go back in time repeat itself and will go back in time and another stage back in time and another stage back in time and as we have seen in a back and as we have seen in a back propagation discussion the there are propagation discussion the there are two possibilities that actually can", "image_path": "img_data/video_04_chunk_16.jpg"}
{"video": "video_04", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "propagation discussion the there are two possibilities that actually can two possibilities that actually can happen typically the gradient kind of happen typically the gradient kind of as it goes through the grade is adjusted as it goes through the grade is adjusted as we had also formed this kind of as we had also formed this kind of analogy of a water valve and there are two valve and there are two possibilities that are going to happen possibilities that are going to happen and this possibilities depend on and this possibilities depend on matrix w let me write this down the matrix w let me write this down the matrix w you can show that the matrix w", "image_path": "img_data/video_04_chunk_17.jpg"}
{"video": "video_04", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "w let me write this down the matrix w you can show that the matrix w is responsible for the diminishing or explosive nature of d ht which is by definition the partial ht which is by definition the partial derivative of", "image_path": "img_data/video_04_chunk_18.jpg"}
{"video": "video_04", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "ht which is by definition the partial derivative of let's say of the loss with respect to hd and you can actually see that and show it's the proof is kind of outside show it's the proof is kind of outside of the of this discussion but the of this discussion but the structure of this matrix structure of this matrix w and it all depends because it is w and it all depends because it is actually been involved in u in this actually been involved in u in this back propagation process if i can say back propagation process if i can say that if the igen values", "image_path": "img_data/video_04_chunk_19.jpg"}
{"video": "video_04", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "back propagation process if i can say that if the igen values of w are less than one then effectively we w are less than one then effectively we have a diminishing situation gradient flow diminishes means lots of problems in terms of means lots of problems in terms of training because there is training because there is the effectively the adjustment of the effectively the adjustment of parameters effectively stops and you parameters effectively stops and you can think about that when you consider can think about that when you consider the stock hasic gradient kind of", "image_path": "img_data/video_04_chunk_20.jpg"}
{"video": "video_04", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "can think about that when you consider the stock hasic gradient kind of the stock hasic gradient kind of equation that depends on that kind of equation that depends on that kind of gradient on how the parameters are gradient on how the parameters are adjusted and on the other hand adjusted and on the other hand also this is a this is b if the igen values of w are larger than one then we have an explosive explosion of the gradient and explosive explosion of the gradient and this is a very problematic situation too", "image_path": "img_data/video_04_chunk_21.jpg"}
{"video": "video_04", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "explosive explosion of the gradient and this is a very problematic situation too this is a very problematic situation too but we can actually clip the gradient but we can actually clip the gradient and we can do certain tricks to and we can do certain tricks to adjust if you that explosion and manage it but explosion and manage it but the diminishing nature is kind of very the diminishing nature is kind of very problematic because effectively training problematic because effectively training stops and the gradient typically stops and the gradient typically is not empirically has been shown that is not empirically has been shown that is not able to survive for more than a is not able to survive for more than a few of these instances back in time", "image_path": "img_data/video_04_chunk_22.jpg"}
{"video": "video_04", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "is not able to survive for more than a few of these instances back in time few of these instances back in time and we also are going to introduce and we also are going to introduce another capability in the back another capability in the back propagation through time we're going to propagation through time we're going to have a window over which we are back have a window over which we are back propagating evidently for applications propagating evidently for applications that require many steps we are going to that require many steps we are going to the process of back propagation is the process of back propagation is going to be very long and coupled going to be very long and coupled with the fact that the gradient may not with the fact that the gradient may not even survive for a long period of time even survive for a long period of time what we decided to do is we decided to", "image_path": "img_data/video_04_chunk_23.jpg"}
{"video": "video_04", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "even survive for a long period of time what we decided to do is we decided to what we decided to do is we decided to use a kind of a tration window of let's use a kind of a tration window of let's say 20 or whatever the number is 10 say 20 or whatever the number is 10 it's a hyper parameter and at it's a hyper parameter and at every tration window we are resetting every tration window we are resetting the back propagation process and the back propagation process and we are starting from scratching with we are starting from scratching with respect to this kind of gradient respect to this kind of gradient calculation when people we hear calculation when people we hear people saying that rnn provide people saying that rnn provide some form of memory then this is the some form of memory then this is the memory this is this connection between", "image_path": "img_data/video_04_chunk_24.jpg"}
{"video": "video_04", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "some form of memory then this is the memory this is this connection between memory this is this connection between the two instances and how much gradient the two instances and how much gradient flow you can pass back in time flow you can pass back in time heavily determines how much you can heavily determines how much you can remember and apparently empirical remember and apparently empirical observations of the rnn indicate that observations of the rnn indicate that cannot really remember the simple rns cannot really remember the simple rns cannot really remember for a long cannot really remember for a long periods of time we are need to move periods of time we are need to move on into another architecture that you on into another architecture that you probably guessed somehow this probably guessed somehow this controlling of this w matrix has to be", "image_path": "img_data/video_04_chunk_25.jpg"}
{"video": "video_04", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "probably guessed somehow this controlling of this w matrix has to be controlling of this w matrix has to be there in builtin into that kind of there in builtin into that kind of architecture and we will call this architecture and we will call this architecture long shortterm memory architecture long shortterm memory architecture and for those who want to architecture and for those who want to just remember a simple example to just remember a simple example to demonstrate this kind of exploding and demonstrate this kind of exploding and vanishing kind of gradients and without vanishing kind of gradients and without having to involve yourselves into having to involve yourselves into decomposition of matx decomposition and decomposition of matx decomposition and igen values and on i have provided igen values and on i have provided here a simple block diagram of what", "image_path": "img_data/video_04_chunk_26.jpg"}
{"video": "video_04", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "igen values and on i have provided here a simple block diagram of what here a simple block diagram of what is effectively called infin interal is effectively called infin interal response filer but it's kind of not response filer but it's kind of not necessarily u need to remember that necessarily u need to remember that where you we receive some kind of input where you we receive some kind of input x of t and you are adding from some x of t and you are adding from some scaled version of what you remember scaled version of what you remember in the previous instance of that kind in the previous instance of that kind of output of that device h oft the of output of that device h oft the equation kind of matches what you have equation kind of matches what you have seen earlier here there's no seen earlier here there's no nonlinearity here but the equation kind", "image_path": "img_data/video_04_chunk_27.jpg"}
{"video": "video_04", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "seen earlier here there's no nonlinearity here but the equation kind nonlinearity here but the equation kind of matches what we have seen earlier of matches what we have seen earlier as the trivialized equation of as the trivialized equation of an rnn where we have some dependence on an rnn where we have some dependence on the previous state and of course the previous state and of course the input and if you see what's the input and if you see what's happening and if you just do the happening and if you just do the calculations for let's say w being u calculations for let's say w being u the absolute value of w being less than the absolute value of w being less than one then you can actually see for some one then you can actually see for some impulse we send at the input delta t", "image_path": "img_data/video_04_chunk_28.jpg"}
{"video": "video_04", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "one then you can actually see for some impulse we send at the input delta t impulse we send at the input delta t then we can see exactly how the then we can see exactly how the hidden state kind of varies over time hidden state kind of varies over time and it's the envelope of this hidden and it's the envelope of this hidden state means the absolute value of the state means the absolute value of the hidden state is going to diminish in hidden state is going to diminish in fact with some kind of a exponential in fact with some kind of a exponential way and evidently when the w is way and evidently when the w is becoming larger than one then that becoming larger than one then that hidden state is going to explode and hidden state is going to explode and this is definitely an analogy that we this is definitely an analogy that we can always use to remember what is", "image_path": "img_data/video_04_chunk_29.jpg"}
{"video": "video_04", "start": "0:15:00", "end": "0:15:13.100000", "timestamp": "0:15:00 - 0:15:13.100000", "text": "this is definitely an analogy that we can always use to remember what is can always use to remember what is happening inside the sort of a simple happening inside the sort of a simple rnn", "image_path": "img_data/video_04_chunk_30.jpg"}
{"video": "video_05", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "let's try to understand now the what exactly data we need in order to do what exactly data we need in order to do this kind of a prediction of that price of a of prediction of that price of a of an asset in the following let's say an asset in the following let's say trading day and in your site you have trading day and in your site you have a notebook that is called time series a notebook that is called time series prediction used in rnns that book prediction used in rnns that book notebook was borrowed from the handsone notebook was borrowed from the handsone machine learning and psychic learn of machine learning and psychic learn of flow version two edition", "image_path": "img_data/video_05_chunk_0.jpg"}
{"video": "video_05", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "machine learning and psychic learn of flow version two edition flow version two edition 3 has a slightly kind of different 3 has a slightly kind of different notebook and the it's kind of an notebook and the it's kind of an instructive notebook because it is going instructive notebook because it is going to be dealing with a synthetic data set to be dealing with a synthetic data set we will be synthesizing the data that we will be synthesizing the data that we need in order to do the prediction we need in order to do the prediction and that's why it is a bit more and that's why it is a bit more useful for us at this stage the useful for us at this stage the important thing to understand is that important thing to understand is that the we will be needing to generate the we will be needing to generate sequences in fact thousands of sequences in fact thousands of sequences in a similar way as we had", "image_path": "img_data/video_05_chunk_1.jpg"}
{"video": "video_05", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "sequences in fact thousands of sequences in a similar way as we had sequences in a similar way as we had thousands of scalers x that we had thousands of scalers x that we had dealt when we looked at let's say linear dealt when we looked at let's say linear regression video and in a similar way regression video and in a similar way as we had thousands of images when as we had thousands of images when we're dealing with convolutions over here we will convolutions over here we will actually need to generate many actually need to generate many thousands of sequences these are thousands of sequences these are examples of these sequences and u for every sequences and u for every example", "image_path": "img_data/video_05_chunk_2.jpg"}
{"video": "video_05", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "sequences and u for every example we will need to have an example we will need to have an sequence which is the sequence of 50 sequence which is the sequence of 50 values from 0 to 49 as well also the target variable y 49 as well also the target variable y which is the x50 these are which is the x50 these are effectively our data set and now let me effectively our data set and now let me sort of write down in a kind of a sort of write down in a kind of a familiar to us a table that familiar to us a table that fact we have still have this x and y fact we have still have this x and y", "image_path": "img_data/video_05_chunk_3.jpg"}
{"video": "video_05", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "fact we have still have this x and y table for every value we have a sequence every for every value we have a sequence now i don't want to put the necessarily now i don't want to put the necessarily the under bar because that's a the under bar because that's a sequence of values rather than a vector sequence of values rather than a vector of some sort but i'll just overuse the of some sort but i'll just overuse the notation and this is the first example x notation and this is the first example x followed by another variable which followed by another variable which is a target variable x1 that's one is a target variable x1 that's one sequence followed by a tet variable", "image_path": "img_data/video_05_chunk_4.jpg"}
{"video": "video_05", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "is a target variable x1 that's one sequence followed by a tet variable sequence followed by a tet variable pair and this is another pair and this is another one and on at the end of one and on at the end of the day we have we will have m of these sequences coupled with their target v the ground truth of the target variable the ground truth of the target variable would to predict in this case would to predict in this case these are the ground truth is these are the ground truth is the price of the asset in that specific the price of the asset in that specific sequence what happens after x49", "image_path": "img_data/video_05_chunk_5.jpg"}
{"video": "video_05", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "the price of the asset in that specific sequence what happens after x49 sequence what happens after x49 that is your y1 for that sequence that is your y1 for that sequence for this sequence that is your for this sequence that is your x50 that is for the m sequence that's x50 that is for the m sequence that's also your x50 that is the nature also your x50 that is the nature of our train data set and this is of our train data set and this is actually shown over here in on the actually shown over here in on the side the initial function side the initial function generate time series is doing just generate time series is doing just that it is starting with some form", "image_path": "img_data/video_05_chunk_6.jpg"}
{"video": "video_05", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "generate time series is doing just that it is starting with some form that it is starting with some form of the number of has an of the number of has an arguments as the let's say the bad size arguments as the let's say the bad size and the number of steps and we are and the number of steps and we are generating here 10,000 of those generating here 10,000 of those sequences 3,000 sorry 7,000 will sequences 3,000 sorry 7,000 will be assigned to the training data set and be assigned to the training data set and 3,000 to the test data set and we have 3,000 to the test data set and we have 50 indices time sequence steps that 50 indices time sequence steps that each sequence", "image_path": "img_data/video_05_chunk_7.jpg"}
{"video": "video_05", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "50 indices time sequence steps that each sequence consist of and of course we're going to consist of and of course we're going to have to generate 51 because the last have to generate 51 because the last one we're going to be using as our one we're going to be using as our target variable that we are trying to target variable that we are trying to create a predictor that to predict that create a predictor that to predict that target variable i hope the code is target variable i hope the code is understood at the same time just understood at the same time just because the data set is actually because the data set is actually synthetic we actually can see here that synthetic we actually can see here that the serus itself is generated by adding the serus itself is generated by adding certain sinusoids with certain", "image_path": "img_data/video_05_chunk_8.jpg"}
{"video": "video_05", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "the serus itself is generated by adding certain sinusoids with certain sinusoids with certain frequencies frequency one and frequencies frequency one and frequency two and some kind of frequency two and some kind of offsets to control exactly how the offsets to control exactly how the additions here we have effectively additions here we have effectively three terms that are and we adding three terms that are and we adding some kind of a random noise for at some kind of a random noise for at the end of the day to form if you the end of the day to form if you that sequence that is that sequence that is how we will be need to have our data how we will be need to have our data set structured in order for us to set structured in order for us to learn the structure of this time", "image_path": "img_data/video_05_chunk_9.jpg"}
{"video": "video_05", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "set structured in order for us to learn the structure of this time learn the structure of this time series now that we have seen the series now that we have seen the structure of the neuron this is just a structure of the neuron this is just a single neuron in the same way as this single neuron in the same way as this was just a single neuron of but this was just a single neuron of but it's recurrent in nature the in it's recurrent in nature the in the same way as we have seen earlier the same way as we have seen earlier what we need to do now is now that we what we need to do now is now that we have presented the problem we will need have presented the problem we will need to understand how we will use this to understand how we will use this structure how this structure kind of structure how this structure kind of fits into solving this problem of this fits into solving this problem of this x50 the prediction of this time", "image_path": "img_data/video_05_chunk_10.jpg"}
{"video": "video_05", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "fits into solving this problem of this x50 the prediction of this time x50 the prediction of this time series the next training day series the next training day let's see i think it's worthwhile let's see i think it's worthwhile highlighting a couple of things and highlighting a couple of things and by the way earlier i actually forgot to by the way earlier i actually forgot to draw the transfer function or the of that t h transfer function or the of that t h this is my t h of u let me call this z h of u let me call this z and the tan h is a function that goes", "image_path": "img_data/video_05_chunk_11.jpg"}
{"video": "video_05", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "h of u let me call this z and the tan h is a function that goes and the tan h is a function that goes from minus1 to + one this is very similar to what we have seen earlier and to the sigmoidal function earlier and to the sigmoidal function but obviously it is different because but obviously it is different because went from 0 to one and what we went from 0 to one and what we because there's a feedback loop over because there's a feedback loop over here what we will do is we'll take here what we will do is we'll take this and unroll it in many instances in", "image_path": "img_data/video_05_chunk_12.jpg"}
{"video": "video_05", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "here what we will do is we'll take this and unroll it in many instances in this and unroll it in many instances in time in order for us to understand how time in order for us to understand how that data set will be fed into the that data set will be fed into the neuron we will be into the neuron we will be u abstracting out the contents of the u abstracting out the contents of the previous kind of block diagram into this previous kind of block diagram into this block the block this block the neuron starts with some initial neuron starts with some initial hidden state h0", "image_path": "img_data/video_05_chunk_13.jpg"}
{"video": "video_05", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "neuron starts with some initial hidden state h0 and it receives the first this is just one of the first this is just one of the sequences this is let's say sequences this is let's say this is a sequence x i don't know 1,255 all right that is it receives the zeroth element of that receives the zeroth element of that sequence in at effectively this is sequence in at effectively this is t is equal to z to make it a bit more t is equal to z to make it a bit more explicit it then it produces we'll", "image_path": "img_data/video_05_chunk_14.jpg"}
{"video": "video_05", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "t is equal to z to make it a bit more explicit it then it produces we'll explicit it then it produces we'll understand exactly what the output arrow understand exactly what the output arrow is but what we will do now is just is but what we will do now is just going to use the output of the neuron which is the output of the neuron which is the new hidden state h1 to and use that as an argument to and use that as an argument to another transfer to another function another transfer to another function u that implements the neuron but this u that implements the neuron but this function will now have as input", "image_path": "img_data/video_05_chunk_15.jpg"}
{"video": "video_05", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "u that implements the neuron but this function will now have as input x1 let me also draw the this arrow which will become important a bit later this will become important a bit later this creates h2 together with x2 we're forming h3 and on after a number of on after a number of steps we arrive at the point in time where the x49 is going to be fed", "image_path": "img_data/video_05_chunk_16.jpg"}
{"video": "video_05", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "is going to be fed in and the y 49 y50 just not y50 but x50 the h50 is going to be the x50 the h50 is going to be the hidden state this is obviously is going hidden state this is obviously is going to be my prediction why had this is my", "image_path": "img_data/video_05_chunk_17.jpg"}
{"video": "video_05", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "prediction why had this is my prediction i had which is the one prediction i had which is the one that i was asked to produce and that i was asked to produce and obviously this prediction is going to be obviously this prediction is going to be used for driving the mean square driving the mean square error and sort of calculating the mean error and sort of calculating the mean square error producing if you a square error producing if you a loss and with the usual kind of a loss and with the usual kind of a stochastic gr descent algorith adjust stochastic gr descent algorith adjust the trainable parameters of this the trainable parameters of this neuron now we need to be clear that", "image_path": "img_data/video_05_chunk_18.jpg"}
{"video": "video_05", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "the trainable parameters of this neuron now we need to be clear that neuron now we need to be clear that we understand that physically there is we understand that physically there is the architecture of the previous block the architecture of the previous block diagram but here we see a diagram but here we see a functional representation of that functional representation of that architecture u or logical representation architecture u or logical representation of what is happening over multiple of what is happening over multiple instances in time physically at any instances in time physically at any one instance in time you have this and one instance in time you have this and you have the next instance you have this you have the next instance you have this next you have this you never have you next you have this you never have all of the elements sort of", "image_path": "img_data/video_05_chunk_19.jpg"}
{"video": "video_05", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "next you have this you never have you know all of the elements sort of know all of the elements sort of that you see here this is called the that you see here this is called the unrolled architecture it's actually very unrolled architecture it's actually very useful for us to draw it this way useful for us to draw it this way because in many sense we have to reason because in many sense we have to reason about what is happening over multiple about what is happening over multiple instances in time this is instances in time this is the block diagram how the training will the block diagram how the training will function now let's look at this using function now let's look at this using the python code before we see the python code before we see the implementation of this simple rnn neuron implementation of this simple rnn neuron i think it's worth while looking at", "image_path": "img_data/video_05_chunk_20.jpg"}
{"video": "video_05", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "implementation of this simple rnn neuron i think it's worth while looking at i think it's worth while looking at what it's actually shown here which is a what it's actually shown here which is a baseline case and the baseline kind baseline case and the baseline kind of tries to answer the following kind of tries to answer the following kind of question why we had to introduce rnns question why we had to introduce rnns would the previous architectures be good would the previous architectures be good enough for our purpose and definitely enough for our purpose and definitely the answer at this point moment in the answer at this point moment in time is positive yes we could have time is positive yes we could have used a fully connected used a fully connected network and the way that we have used network and the way that we have used a fully connected network will be able", "image_path": "img_data/video_05_chunk_21.jpg"}
{"video": "video_05", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "network and the way that we have used a fully connected network will be able a fully connected network will be able to capture in one go in every to capture in one go in every sequence in we can use a sequence in we can use a dense layer of let's say 50 elements layer of let's say 50 elements 50 with an input size of 50 and an 50 with an input size of 50 and an output with just a single neuron output with just a single neuron it is without even the it is without even the sigmoid when we will be able to sigmoid when we will be able to predict the price of the asset remember predict the price of the asset remember this is a regression don't need the", "image_path": "img_data/video_05_chunk_22.jpg"}
{"video": "video_05", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "predict the price of the asset remember this is a regression don't need the this is a regression don't need the sigmoid we just had a dot product and sigmoid we just had a dot product and we could actually use the mean square we could actually use the mean square error to be able to train that error to be able to train that simple neuron of just a single neuron simple neuron of just a single neuron linear unit to obtain our prediction linear unit to obtain our prediction and this basically what is actually and this basically what is actually happening here and u that whole happening here and u that whole sequence is fed into that neuron and sequence is fed into that neuron and one and of course we have thousands", "image_path": "img_data/video_05_chunk_23.jpg"}
{"video": "video_05", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "sequence is fed into that neuron and one and of course we have thousands one and of course we have thousands of sequences that we feed all of them of sequences that we feed all of them through that kind of training process through that kind of training process over a number of epochs and yes we over a number of epochs and yes we are minimizing if you the are minimizing if you the loss and we are be able to predict loss and we are be able to predict after that model is obtained finally after that model is obtained finally when the w star is obtained our when the w star is obtained our prediction which is the u red over here prediction which is the u red over here versus the ground truth over there", "image_path": "img_data/video_05_chunk_24.jpg"}
{"video": "video_05", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "prediction which is the u red over here versus the ground truth over there versus the ground truth over there and this thing is working at this point and this thing is working at this point in time and that is one of the many in time and that is one of the many baselines even in convolutional new baselines even in convolutional new networks we have seen the we networks we have seen the we can actually use that use onedimensional convolution that use onedimensional convolution to be able to feed into that to be able to feed into that convolutional layer convolutional neuron the sequence and the advantage neuron the sequence and the advantage and of course disadvantages with this", "image_path": "img_data/video_05_chunk_25.jpg"}
{"video": "video_05", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "neuron the sequence and the advantage and of course disadvantages with this and of course disadvantages with this convolutional approach one made convolutional approach one made disadvantage is that in rnns we need to disadvantage is that in rnns we need to be modeling applications we have be modeling applications we have variable lengths both at the input variable lengths both at the input and also at the output we'll see and also at the output we'll see those in a future kind of those in a future kind of video but this is both feasible video but this is both feasible architectures at this point it will architectures at this point it will not be necessarily be feasible for much not be necessarily be feasible for much longer but at this point they are both longer but at this point they are both feasible all right the simple rnn", "image_path": "img_data/video_05_chunk_26.jpg"}
{"video": "video_05", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "longer but at this point they are both feasible all right the simple rnn feasible all right the simple rnn code is actually shown over here it code is actually shown over here it is consist of u a single neuron as we is consist of u a single neuron as we have drawn it that accepts an input have drawn it that accepts an input shape of none comma 1 this n comma shape of none comma 1 this n comma one means that we are de already one means that we are de already seeing the advantage of this seeing the advantage of this architecture to accept any number of architecture to accept any number of inputs that we require these are inputs that we require these are all scalar inputs and we are using adam here not", "image_path": "img_data/video_05_chunk_27.jpg"}
{"video": "video_05", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "all scalar inputs and we are using adam here not inputs and we are using adam here not we have not really seen adam in we have not really seen adam in detail but it is a cousin of stochastic detail but it is a cousin of stochastic gr descent you can replace it with gr descent you can replace it with stochastic gr descent and with stochastic gr descent and with at this moment you're not going at this moment you're not going to see much difference we will to see much difference we will definitely are going to train it as we definitely are going to train it as we discuss with mean square error as a loss discuss with mean square error as a loss and obtain if you some predict and obtain if you some predict prediction and this prediction is actually shown", "image_path": "img_data/video_05_chunk_28.jpg"}
{"video": "video_05", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "prediction and this prediction is actually shown and this prediction is actually shown over here it may not really look over here it may not really look quite different than the kind of an quite different than the kind of an earlier prediction but definitely earlier prediction but definitely the mean square error is worse and the mean square error is worse and maybe the scale is not doing as a maybe the scale is not doing as a favor to highlight this kind of favor to highlight this kind of difference and the reason why the difference and the reason why the at this moment in time the rnn the at this moment in time the rnn the simple rnn neuron that we have here is simple rnn neuron that we have here is performing worse than the sort of", "image_path": "img_data/video_05_chunk_29.jpg"}
{"video": "video_05", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "simple rnn neuron that we have here is performing worse than the sort of performing worse than the sort of linear unit is quite simple if you linear unit is quite simple if you consider the number of parameters that consider the number of parameters that we have in our implementation in we have in our implementation in our parameters we have we can our in our parameters we have we can actually see it over here in the actually see it over here in the kind of a block diagram which actually kind of a block diagram which actually have grown mind you this now does have grown mind you this now does not exist it is just a linear unit not exist it is just a linear unit what we have in the baseline and", "image_path": "img_data/video_05_chunk_30.jpg"}
{"video": "video_05", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "not exist it is just a linear unit what we have in the baseline and what we have in the baseline and evidently the number of parameters of evidently the number of parameters of that linear unit is as large as the that linear unit is as large as the number of inputs that we have used in number of inputs that we have used in this case we have used the whole this case we have used the whole sequence in one go we are talking sequence in one go we are talking about 50 parameters here plus the about 50 parameters here plus the bias term 51 and in this case what we bias term 51 and in this case what we have here in the symbol neuron this have here in the symbol neuron this is a scalar the one sample at the is a scalar the one sample at the time one scale at the time is", "image_path": "img_data/video_05_chunk_31.jpg"}
{"video": "video_05", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "is a scalar the one sample at the time one scale at the time is time one scale at the time is actually coming in and therefore we have actually coming in and therefore we have just one two three parameters here just one two three parameters here for the simple rnn neuron and therefore for the simple rnn neuron and therefore the number of parameters is quite the number of parameters is quite different we are effectively are different we are effectively are comparing kind of apples and oranges in comparing kind of apples and oranges in a sense to improve the performance of this sense to improve the performance of this simple architecture we need to follow a simple architecture we need to follow a similar trajectory what we have seen in similar trajectory what we have seen in the past that we have followed in the past that we have followed in the past in the", "image_path": "img_data/video_05_chunk_32.jpg"}
{"video": "video_05", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "the past that we have followed in the past in the neural fully connected architectures we neural fully connected architectures we went from a single neuron to a went from a single neuron to a multiplicity of neurons we introducing multiplicity of neurons we introducing this kind of dense matrices to represent a fully connected matrices to represent a fully connected architecture and then in the architecture and then in the convolution also we went from a single convolution also we went from a single convolutional neuron to multiple convolutional neuron to multiple convolutional neurons in other words convolutional neurons in other words multiple filters and that's multiple filters and that's exactly what we will do now here we will exactly what we will do now here we will we need to do that for yet another we need to do that for yet another reason", "image_path": "img_data/video_05_chunk_33.jpg"}
{"video": "video_05", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "we need to do that for yet another reason it's very unlikely that all our it's very unlikely that all our applications will actually require a applications will actually require a scalar output this specific scalar output this specific application linear regression sorry not application linear regression sorry not regression the regression problem was regression the regression problem was requiring if you a scalar output requiring if you a scalar output but obviously other problems will not but obviously other problems will not somehow we need to produce we need somehow we need to produce we need to accommodate two things the first is to accommodate two things the first is that kind of state that we that kind of state that we have produced the prediction out of it", "image_path": "img_data/video_05_chunk_34.jpg"}
{"video": "video_05", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "that kind of state that we have produced the prediction out of it have produced the prediction out of it in general has to be a in general has to be a vector and actually we can see that vector and actually we can see that even in this simple regression kind even in this simple regression kind of problem why itself has to be a vector of problem why itself has to be a vector we are introducing now some form of we are introducing now some form of decoupling let me write this down decoupling let me write this down we kind of have it in writing we kind of have it in writing we need to do two things we need to increase the number of parameters of", "image_path": "img_data/video_05_chunk_35.jpg"}
{"video": "video_05", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "increase the number of parameters meters associated with our model that's one thing secondly we need to make the hidden let me call it hidden make the hidden let me call it hidden state h of t a", "image_path": "img_data/video_05_chunk_36.jpg"}
{"video": "video_05", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "make the hidden let me call it hidden state h of t a vector therefore h under bar of vector therefore h under bar of t and decapo it from the dimensionality of the target variable", "image_path": "img_data/video_05_chunk_37.jpg"}
{"video": "video_05", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "dimensionality of the target variable why this is kind of quite important even in this kind of problem important even in this kind of problem we don't have necessarily in this we don't have necessarily in this problem to have just a single neuron problem to have just a single neuron which is three parameters we can which is three parameters we can actually have a as we will see a simple actually have a as we will see a simple ar layer now that consists of multiple ar layer now that consists of multiple of these neurons and we'll see exactly of these neurons and we'll see exactly how we will construct it and produce how we will construct it and produce if you a vector state if you a vector state h50 at or a vector state h t in", "image_path": "img_data/video_05_chunk_38.jpg"}
{"video": "video_05", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "if you a vector state h50 at or a vector state h t in h50 at or a vector state h t in general at every single step and out of general at every single step and out of this vector state we can do this vector state we can do dimensionality matching as we have done dimensionality matching as we have done in every head that we have seen in every head that we have seen in earlier kind of neural networks in order earlier kind of neural networks in order for us to match the dimensionality of for us to match the dimensionality of our predicted variable which is the y our predicted variable which is the y the target variable y and that's why i the target variable y and that's why i was actually drawing this kind of arrows was actually drawing this kind of arrows from each of these stages a bit later", "image_path": "img_data/video_05_chunk_39.jpg"}
{"video": "video_05", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "was actually drawing this kind of arrows from each of these stages a bit later from each of these stages a bit later a bit earlier another way of a bit earlier another way of actually thinking about it is that actually thinking about it is that vector hidden state will capture that vector hidden state will capture all of the dependencies that a all of the dependencies that a price will actually have all these price will actually have all these hidden latent factors that the price hidden latent factors that the price will depend on it could be a season will depend on it could be a season could be some sort of values that are preceding if you of values that are preceding if you this specific training day for", "image_path": "img_data/video_05_chunk_40.jpg"}
{"video": "video_05", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "of values that are preceding if you this specific training day for this specific training day for example the fact that we have a example the fact that we have a trend over here for the price of this trend over here for the price of this trading day may be dependent in trading day may be dependent in with let's say the a number of with let's say the a number of early trading days but definitely we early trading days but definitely we have not a lot of dependency on this one have not a lot of dependency on this one the size of this the dimensionality the size of this the dimensionality of this hidden state is itself our own of this hidden state is itself our own kind of designed variable it has to kind of designed variable it has to be an architectural kind of parameter be an architectural kind of parameter that we need to keep in mind and how", "image_path": "img_data/video_05_chunk_41.jpg"}
{"video": "video_05", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "be an architectural kind of parameter that we need to keep in mind and how that we need to keep in mind and how to sort of select it just to sort of select it just anything that we have seen in the past anything that we have seen in the past every single neuron network we have every single neuron network we have that kind of problem at hand how many that kind of problem at hand how many neurons we need and on neurons we need and on what we will do now is we will draw what we will do now is we will draw that kind of architecture which is the that kind of architecture which is the generalized generalization of this generalized generalization of this simple neuron that we call now the simple neuron that we call now the simple rnn", "image_path": "img_data/video_05_chunk_42.jpg"}
{"video": "video_05", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "rnn layer and i maintain the term simple layer and i maintain the term simple because we will see that this is one because we will see that this is one of the two main architectures we will be of the two main architectures we will be dealing with on that of rnn type dealing with on that of rnn type right now we are dealing with the simple right now we are dealing with the simple version let me just draw it we version let me just draw it we will in the input we're going to have will in the input we're going to have some kind of concatenation", "image_path": "img_data/video_05_chunk_43.jpg"}
{"video": "video_05", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "will in the input we're going to have some kind of concatenation of a hidden state which is now a vector and the input x of t in general this input will be a vector in our problem that we have be a vector in our problem that we have seen in time series was a scalar but in seen in time series was a scalar but in general will be a vector general will be a vector two and by now we have seen two and by now we have seen in early videos that we have this matrix in early videos that we have this matrix that we multiply with the that we multiply with the concatenated kind of vector", "image_path": "img_data/video_05_chunk_44.jpg"}
{"video": "video_05", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "that we multiply with the concatenated kind of vector w we add the bias b and we pass it through the b and we pass it through the nonlinearity than h now every single nonlinearity than h now every single nonlinearity we have seen in the past nonlinearity we have seen in the past was also vector in vector output was also vector in vector output function this does not change function this does not change anything we can just maintain that anything we can just maintain that for t h as well this is our for t h as well this is our vector h of t and similar to what we vector h of t and similar to what we have seen earlier we will", "image_path": "img_data/video_05_chunk_45.jpg"}
{"video": "video_05", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "vector h of t and similar to what we have seen earlier we will have seen earlier we will be passing it through the d and we be passing it through the d and we will be d is a memory location we will be d is a memory location we will be storing this vector and then retrieve be storing this vector and then retrieve the retrieve actually this is ht the retrieve actually this is ht minus one retrieve the earlier hidden minus one retrieve the earlier hidden state and feed it to the input as we state and feed it to the input as we will see we don't have the will see we don't have the multiplication that we explicitly drawn", "image_path": "img_data/video_05_chunk_46.jpg"}
{"video": "video_05", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "will see we don't have the multiplication that we explicitly drawn multiplication that we explicitly drawn here because there will be the here because there will be the equation will be slightly different equation will be slightly different and there will be a multiplication with and there will be a multiplication with for sure with this hidden state is just for sure with this hidden state is just not shown here and that will be not shown here and that will be the that is the what is called a the that is the what is called a simple rnn layer let's write down simple rnn layer let's write down some kind of equ the equation that some kind of equ the equation that represents that vector hidden represents that vector hidden state and see how it is also related state and see how it is also related to the fully connected layer that we to the fully connected layer that we have seen earlier the equation", "image_path": "img_data/video_05_chunk_47.jpg"}
{"video": "video_05", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "to the fully connected layer that we have seen earlier the equation have seen earlier the equation generally is going to be h of t the generally is going to be h of t the vector is going to be t h off we will be introducing here a matrix u this matrix u i'll draw it this is going to u i'll draw it this is going to have n input always n was the kind of n input always n was the kind of dimensionality of features in terms", "image_path": "img_data/video_05_chunk_48.jpg"}
{"video": "video_05", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "n input always n was the kind of dimensionality of features in terms dimensionality of features in terms of x right now this n is decorated of x right now this n is decorated with input just to distinguish that from with input just to distinguish that from and then we'll have n neurons as rows n neurons is the dimensionality of h of t it is the dimensionality of h of t it is n neons by one that is the n neons by one that is the dimensionality of h of t u will be dimensionality of h of t u will be multiplied with h x of t obviously", "image_path": "img_data/video_05_chunk_49.jpg"}
{"video": "video_05", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "dimensionality of h of t u will be multiplied with h x of t obviously multiplied with h x of t obviously because it refers to the input x because it refers to the input x oft is a column vector of n input rows let me actually write it over here the dimensionality of x of t and inut by one that's the input and this is now a plus sign", "image_path": "img_data/video_05_chunk_50.jpg"}
{"video": "video_05", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "t and inut by one that's the input and this is now a plus sign plus and we're going to have a w prime matrix this matrix w prime will be n neons by an input and we will multiply this h of t minus one state and we'll see this w prime matrix is embedded in this matrix w", "image_path": "img_data/video_05_chunk_51.jpg"}
{"video": "video_05", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "state and we'll see this w prime matrix is embedded in this matrix w matrix is embedded in this matrix w that's why the thing is missing from that's why the thing is missing from here and this is evidently here and this is evidently h t minus one has exactly the same h t minus one has exactly the same dimensions as h of t dimensions as h of t plus b which of course b is plus b which of course b is n neurons by one and this is the closing of the parenthesis only thing that remains parenthesis only thing that remains is to define this w", "image_path": "img_data/video_05_chunk_52.jpg"}
{"video": "video_05", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "parenthesis only thing that remains is to define this w matrix which is the concatenation of u matrix which is the concatenation of u transpose and w prime transpose the transpose and w prime transpose the dimensions are work out because ut dimensions are work out because ut transpose is it is n neurons by sorry it is n neurons by sorry it is n input by n neurons and the wt n input by n neurons and the wt transpose is a still an input by an transpose is a still an input by an neurons the dimensions of w", "image_path": "img_data/video_05_chunk_53.jpg"}
{"video": "video_05", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "transpose is a still an input by an neurons the dimensions of w neurons the dimensions of w is and i have done a mistake over here is and i have done a mistake over here this is n neurons by n neurons the dimensions of w prime transpose is a neurons by neurons transpose is a neurons by neurons finally the dimension w is an finally the dimension w is an input plus and neutrons by and", "image_path": "img_data/video_05_chunk_54.jpg"}
{"video": "video_05", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "neutrons by and neurons that's basically for this dimensioning effectively we can write dimensioning effectively we can write this equation is exactly what the block diagram kind of indicates tan h diagram kind of indicates tan h of with this dimensioning we have the of with this dimensioning we have the concatenation of xt and hd minus one left multiplying the matrix w one left multiplying the matrix w plus", "image_path": "img_data/video_05_chunk_55.jpg"}
{"video": "video_05", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "one left multiplying the matrix w plus b and this is basically our equation of a simple rnn layer and then equation of a simple rnn layer and then we have the contrast this against we have the contrast this against the sort of fully connected layer the sort of fully connected layer the fully connected layer is let me write fully connected layer is let me write it here is we have seen earlier is z of it here is we have seen earlier is z of ru of wx + b you can actually see how ru of wx + b you can actually see how the two are kind of contrasting this is the fully connected", "image_path": "img_data/video_05_chunk_56.jpg"}
{"video": "video_05", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "the two are kind of contrasting this is the fully connected contrasting this is the fully connected layer and this is the rnn the simple layer and this is the rnn the simple rnn layer this is now part of the story the decoupling has also to be the decoupling has also to be implemented at least we reach a point implemented at least we reach a point where we have now a vector h of t and where we have now a vector h of t and increase the number of parameters as increase the number of parameters as we'll see significantly we'll see significantly and therefore we are able to model", "image_path": "img_data/video_05_chunk_57.jpg"}
{"video": "video_05", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "we'll see significantly and therefore we are able to model and therefore we are able to model more complicated hypothesis the decoupling needs also to hypothesis the decoupling needs also to be outlined what we will actually be outlined what we will actually have here we're going to have here we're going to take we need to imp as we discuss take we need to imp as we discuss the decoupling between the hidden state the decoupling between the hidden state and the dimensionality of that sort and the dimensionality of that sort of target variable y and therefore this of target variable y and therefore this is going to be done exactly as we have is going to be done exactly as we have done in other network", "image_path": "img_data/video_05_chunk_58.jpg"}
{"video": "video_05", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "is going to be done exactly as we have done in other network done in other network heads by some kind of fully connected heads by some kind of fully connected layer v i will call it that it will layer v i will call it that it will take this h of t with the take this h of t with the dimensions as we discuss of n neurons by one and using the matrix as a transformation with some bias as a transformation with some bias c will give us the output let me call it o of", "image_path": "img_data/video_05_chunk_59.jpg"}
{"video": "video_05", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "c will give us the output let me call it o of output let me call it o of t obviously not zero just the letter o t obviously not zero just the letter o and this is going to lead us to and this is going to lead us to implement a variety of tasks depending implement a variety of tasks depending on the task we may have a soft on the task we may have a soft max as we have seen in another video max as we have seen in another video what the softmax is for classification what the softmax is for classification multiclass classification problems let multiclass classification problems let me write down as classification", "image_path": "img_data/video_05_chunk_60.jpg"}
{"video": "video_05", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "multiclass classification problems let me write down as classification and or we may actually see it in the case of regression it in the case of regression implementing some form of a linear implementing some form of a linear unit let me call this i don't know e as unit let me call this i don't know e as a vector e transpose o of t will give transpose o of t will give us a y hat of t and this will be a", "image_path": "img_data/video_05_chunk_61.jpg"}
{"video": "video_05", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "transpose o of t will give us a y hat of t and this will be a the regression case there a couple of examples well starting from our current example we starting from our current example we have right now we may have a hidden have right now we may have a hidden state which is let's say 20 or 30 or state which is let's say 20 or 30 or whatever dimensions right in terms whatever dimensions right in terms n neurons will be at 20 or 30 because we n neurons will be at 20 or 30 because we believe that 20 or 30 different lter believe that 20 or 30 different lter factors are affecting the price of the", "image_path": "img_data/video_05_chunk_62.jpg"}
{"video": "video_05", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "believe that 20 or 30 different lter factors are affecting the price of the factors are affecting the price of the asset of the commodity in the next asset of the commodity in the next trading day and then evidently we trading day and then evidently we will need to match 20 with to one will need to match 20 with to one and this is effectively this dot and this is effectively this dot product and this will implement our product and this will implement our regression that's our example for regression that's our example for regression in the case of regression in the case of classification we may have a hidden classification we may have a hidden state over here let's say of 100 or state over here let's say of 100 or 300 and we need to be able to", "image_path": "img_data/video_05_chunk_63.jpg"}
{"video": "video_05", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "state over here let's say of 100 or 300 and we need to be able to 300 and we need to be able to match it with the number of classes that match it with the number of classes that we have here this matrix v is going we have here this matrix v is going to match the size of the hidden state to match the size of the hidden state to the number of classes that we need over the number of classes that we need over here which effectively the same in terms here which effectively the same in terms of dimensions of the input of the soft of dimensions of the input of the soft max we need a posterior probability max we need a posterior probability distribution let's say of k of size k distribution let's say of k of size k sometimes this k will be small sometimes this k will be small sometimes it will be a quite substantial in a case it will be a quite substantial in a case of for example of language modeling we of for example of language modeling we have to predict the next word have to predict the next word that is actually going to happen", "image_path": "img_data/video_05_chunk_64.jpg"}
{"video": "video_05", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "have to predict the next word that is actually going to happen that is actually going to happen given if you a model that is given if you a model that is going to be of some kind of has some going to be of some kind of has some kind of form of memory and this will be kind of form of memory and this will be in potentially in the millions in potentially in the millions this matrix v is not necessarily doing this matrix v is not necessarily doing dimensionality reduction it may need to dimensionality reduction it may need to expand in terms of number of dimensions expand in terms of number of dimensions and therefore matching the and therefore matching the dimensionality of h of t to whatever dimensionality of h of t to whatever number of dimensions we require again number of dimensions we require again for multiclass classification to close a for multiclass classification to close a bit discussion about the architecture of", "image_path": "img_data/video_05_chunk_65.jpg"}
{"video": "video_05", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "for multiclass classification to close a bit discussion about the architecture of bit discussion about the architecture of a simpol rnn i think it's worthwhile a simpol rnn i think it's worthwhile discussing a little bit another discussing a little bit another enhancement that we also have seen in enhancement that we also have seen in early architecture -called de arnn in instead of having just a single layer we were going to have multiple layer we were going to have multiple stacked layers and the rationale of that stacked layers and the rationale of that let me just draw that kind of let me just draw that kind of architecture we're going to have a architecture we're going to have a simple rnn layer which i'll call rn simple rnn layer which i'll call rn n1 that is going to sort of receive a", "image_path": "img_data/video_05_chunk_66.jpg"}
{"video": "video_05", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "simple rnn layer which i'll call rn n1 that is going to sort of receive a n1 that is going to sort of receive a state x t min - one xt minus state x t min - one xt minus one that and then it's going to one that and then it's going to obviously receive from an earlier obviously receive from an earlier instance an h t minus instance an h t minus 2 which is of course a vector now and 2 which is of course a vector now and it will return the ht minus1 one for the first layer over here to", "image_path": "img_data/video_05_chunk_67.jpg"}
{"video": "video_05", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "one for the first layer over here to another unrolled of course in instance of that rnn which course in instance of that rnn which will receive an h of will receive an h of t and on this is this the t and on this is this the first layer but now instead of just first layer but now instead of just matching the h of t minus one dimension matching the h of t minus one dimension into a certain output which is happens into a certain output which is happens over here we are actually feeding this h over here we are actually feeding this h of t minus one into another", "image_path": "img_data/video_05_chunk_68.jpg"}
{"video": "video_05", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "of t minus one into another layer another simple rn layer i will call it this rnn two and this layer is actually operating in a similar fashion to the first in a similar fashion to the first layer it is itself has its own kind layer it is itself has its own kind of hidden state hd - one of hidden state hd - one of two and on and therefore", "image_path": "img_data/video_05_chunk_69.jpg"}
{"video": "video_05", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "of hidden state hd - one of two and on and therefore two and on and therefore finally we may get a number of layers and in fact what we can also layers and in fact what we can also do is we will be adjusting the do is we will be adjusting the dimensionality of this hidden state per dimensionality of this hidden state per layer now and it's again our own layer now and it's again our own responsibility of what kind of responsibility of what kind of dimensionality is each layer may have dimensionality is each layer may have its own dimensionality with respect to its own dimensionality with respect to the a hidden state let me put under the a hidden state let me put under bars here to keep that in mind and", "image_path": "img_data/video_05_chunk_70.jpg"}
{"video": "video_05", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "the a hidden state let me put under bars here to keep that in mind and bars here to keep that in mind and finally when we are all said and done we finally when we are all said and done we will be also implementing at the end be also implementing at the end the head and this head may be as we the head and this head may be as we will see in some applications will need will see in some applications will need to be implemented and exercised at every implemented and exercised at every step but in some other applications step but in some other applications it may be exercise only at the end", "image_path": "img_data/video_05_chunk_71.jpg"}
{"video": "video_05", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "step but in some other applications it may be exercise only at the end it may be exercise only at the end in let's say in sentiment analysis an in let's say in sentiment analysis an application that we will see in application that we will see in the natural language processing kind of the natural language processing kind of video we will only need to produce video we will only need to produce a prediction at the a very end of a prediction at the a very end of accepting let's say a sequence of text accepting let's say a sequence of text because it doesn't really make sense to because it doesn't really make sense to predict something when you just read a predict something when you just read a couple of words you have to read the couple of words you have to read the whole let's say product review before whole let's say product review before you determine whether it's a positive or you determine whether it's a positive or negative review that's a one", "image_path": "img_data/video_05_chunk_72.jpg"}
{"video": "video_05", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "you determine whether it's a positive or negative review that's a one negative review that's a one example of exercising this head at the example of exercising this head at the end of this kind of processing and now let's see what processing and now let's see what happens in terms of going back to this happens in terms of going back to this notebook and see one instance of this notebook and see one instance of this deep rnn specifically for the regression deep rnn specifically for the regression problem that we have dealt the time problem that we have dealt the time series prediction problem this is the series prediction problem this is the notebook we are effectively notebook we are effectively matching we have three layers first matching we have three layers first layer with 20 neurons second layer with 20 neurons second layer with another 20 neurons and then the third another 20 neurons and then the third layer with a one neuron and every layer", "image_path": "img_data/video_05_chunk_73.jpg"}
{"video": "video_05", "start": "0:37:00", "end": "0:37:30", "timestamp": "0:37:00 - 0:37:30", "text": "another 20 neurons and then the third layer with a one neuron and every layer with a one neuron and every layer is sort of accepting the dimensionality is sort of accepting the dimensionality of the hidden state of the earlier layer of the hidden state of the earlier layer the rationale of having that is the rationale of having that is evidently to gradually build someh evidently to gradually build someh how the representations that we need in how the representations that we need in order to make the final prediction in order to make the final prediction in this case we are using the simple rn this case we are using the simple rn layer itself to create the scalar layer itself to create the scalar output we could have used a slightly output we could have used a slightly different architecture where we use this different architecture where we use this kind of fully connected layers to do the kind of fully connected layers to do the dimensionality matching but in this", "image_path": "img_data/video_05_chunk_74.jpg"}
{"video": "video_05", "start": "0:37:30", "end": "0:38:00", "timestamp": "0:37:30 - 0:38:00", "text": "kind of fully connected layers to do the dimensionality matching but in this dimensionality matching but in this case the author decided to do case the author decided to do the deep rnn evidently will have a the deep rnn evidently will have a better performance than what we have better performance than what we have seen earlier in terms of pred the earlier in terms of pred the prediction ability not only we have prediction ability not only we have introduced in this case what we have introduced in this case what we have seen earlier more neurons but we also seen earlier more neurons but we also introduced multiple layers we introduced multiple layers we significantly increased the number of significantly increased the number of parameters that we need and kind of", "image_path": "img_data/video_05_chunk_75.jpg"}
{"video": "video_05", "start": "0:38:00", "end": "0:38:30", "timestamp": "0:38:00 - 0:38:30", "text": "significantly increased the number of parameters that we need and kind of parameters that we need and kind of matched the parameters we have matched the parameters we have that with those that we have seen that with those that we have seen earlier in the fully connected earlier in the fully connected architecture in the -called baseline architecture in the -called baseline case this second case is actually shown case this second case is actually shown here where we have now a matrix v here where we have now a matrix v matching the dimensionality of the matching the dimensionality of the hidden state delivered to us by the hidden state delivered to us by the previous layer the previous simple rnn previous layer the previous simple rnn layer and delivering to us evidently a layer and delivering to us evidently a scalar and this is actually the scalar and this is actually the architecture that performs very", "image_path": "img_data/video_05_chunk_76.jpg"}
{"video": "video_05", "start": "0:38:30", "end": "0:38:33.900000", "timestamp": "0:38:30 - 0:38:33.900000", "text": "scalar and this is actually the architecture that performs very architecture that performs very well in terms of our prediction", "image_path": "img_data/video_05_chunk_77.jpg"}
{"video": "video_06", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we will see a new network architecture neural architecture network architecture neural architecture that's actually called recurr neural that's actually called recurr neural network or rnn and we will start with network or rnn and we will start with a simple version of that architecture a simple version of that architecture that has been developed many decades ago that has been developed many decades ago in fact to address the following in fact to address the following problem how can we model sequences of problem how can we model sequences of data this data may arrive in some kind data this data may arrive in some kind of a time serious for", "image_path": "img_data/video_06_chunk_0.jpg"}
{"video": "video_06", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "data this data may arrive in some kind of a time serious for of a time serious for example when we try to predict let's say example when we try to predict let's say the stock price of an asset of the in the stock price of an asset of the in the next let's say trading day or may in the next let's say trading day or may arrive in some form of any sequence arrive in some form of any sequence for example even the text that we are for example even the text that we are reading from a paragraph it comes in reading from a paragraph it comes in some form of order these kind of some form of order these kind of problems require us to revisit this problems require us to revisit this original kind of neural architecture we original kind of neural architecture we need to understand why we kind of need to understand why we kind of need to revisit that original kind of need to revisit that original kind of neuron the dot product followed by", "image_path": "img_data/video_06_chunk_1.jpg"}
{"video": "video_06", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "need to revisit that original kind of neuron the dot product followed by neuron the dot product followed by nonlinearity or just a dot product if it nonlinearity or just a dot product if it is just a linear model and also the is just a linear model and also the why we also need to revisit why we also need to revisit why convolutional for example networks were convolutional for example networks were not really able to capture everything not really able to capture everything that we need in this type of problems that we need in this type of problems and then we will be using this and then we will be using this architecture later to form architecture later to form complicated prediction problems have to complicated prediction problems have to do with applications such as what i do with applications such as what i mentioned about time series prediction mentioned about time series prediction as well also language modeling we'll", "image_path": "img_data/video_06_chunk_2.jpg"}
{"video": "video_06", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "mentioned about time series prediction as well also language modeling we'll as well also language modeling we'll start with just a simple kind of start with just a simple kind of example just to everyone kind of example just to everyone kind of understands the sort of what we're understands the sort of what we're trying with to model here let's trying with to model here let's assume we have a vehicle over here and assume we have a vehicle over here and we would to sort of capture and we would to sort of capture the state kinematic state of this the state kinematic state of this vehicle the state that i'd be vehicle the state that i'd be calling let's say with s oft over calling let's say with s oft over here and similar discussion we had in here and similar discussion we had in another video when we discuss recursive", "image_path": "img_data/video_06_chunk_3.jpg"}
{"video": "video_06", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "here and similar discussion we had in another video when we discuss recursive another video when we discuss recursive state estimation could be represented by state estimation could be represented by some kind of location in the st 3d some kind of location in the st 3d coordinate system and some form of coordinate system and some form of steering direction could be given steering direction could be given with six degrees of freedom the with six degrees of freedom the roll pit and yo and some form of roll pit and yo and some form of velocity or some kind of derivative of velocity or some kind of derivative of the velocity could be the velocity could be something that this is a state", "image_path": "img_data/video_06_chunk_4.jpg"}
{"video": "video_06", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "the velocity could be something that this is a state something that this is a state and this is a now a variable that we and this is a now a variable that we would to be able to understand how would to be able to understand how we actually model its evolution over we actually model its evolution over time let me write this down time let me write this down in many applications we are tasked to model systems", "image_path": "img_data/video_06_chunk_5.jpg"}
{"video": "video_06", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "tasked to model systems where the state or any decomposable where the state or any decomposable variable for that matter can will change over time the there is the reason why we time the there is the reason why we have to introduce this kind of index have to introduce this kind of index over here we can actually capture over here we can actually capture explicitly this order of where", "image_path": "img_data/video_06_chunk_6.jpg"}
{"video": "video_06", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "over here we can actually capture explicitly this order of where explicitly this order of where we have the systems in general are we have the systems in general are called dynamical systems and these called dynamical systems and these dynamical systems are can be model in dynamical systems are can be model in general with an equation the general with an equation the general form of this system whose state is form of this system whose state is changing over time is s of t is equal changing over time is s of t is equal to f of t s of t minus one and a of t to f of t s of t minus one and a of t this is a general form where f oft is this is a general form where f oft is modeling if you the system it modeling if you the system it itself there a there is if you a", "image_path": "img_data/video_06_chunk_7.jpg"}
{"video": "video_06", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "modeling if you the system it itself there a there is if you a itself there a there is if you a function that itself is time varying function that itself is time varying it's a very complicated kind of scenario it's a very complicated kind of scenario what we are discussing here the what we are discussing here the let's say function that itself changes over time and the arguments of that kind of function is the preview state kind of function is the preview state and some kind of action that is actually taking at eant t", "image_path": "img_data/video_06_chunk_8.jpg"}
{"video": "video_06", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "and some kind of action that is actually taking at eant t action that is actually taking at eant t and in our case here we have to make and in our case here we have to make some simpli simplifications because we some simpli simplifications because we are not going to be able to model here are not going to be able to model here a system whose function is a system whose function is state evolution is itself time varying state evolution is itself time varying instead we will be assuming that the instead we will be assuming that the system is static is kind of stationary static is kind of stationary statistically and therefore we will be statistically and therefore we will be migrating to the following kind of", "image_path": "img_data/video_06_chunk_9.jpg"}
{"video": "video_06", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "statistically and therefore we will be migrating to the following kind of migrating to the following kind of scenario will be modeling instead of scenario will be modeling instead of state h of t it is sort of common state h of t it is sort of common to call this state h for to call this state h for hidden and the function will be hidden and the function will be calling that function let's say g to calling that function let's say g to distinguish from that f of t over here distinguish from that f of t over here we notice that there's no t over there we notice that there's no t over there and this function of course will depend and this function of course will depend on some previous state and some", "image_path": "img_data/video_06_chunk_10.jpg"}
{"video": "video_06", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "and this function of course will depend on some previous state and some on some previous state and some action this action for us it will be the action this action for us it will be the arrival of an input that we will always arrival of an input that we will always we will be calling x and that we will be calling x and that input is coming in some form of order input is coming in some form of order therefore that is the reason why we therefore that is the reason why we actually have an index t and our just actually have an index t and our just any model we have developed we will any model we have developed we will have this hypothesis g and have this hypothesis g and if you have for forgotten this", "image_path": "img_data/video_06_chunk_11.jpg"}
{"video": "video_06", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "have this hypothesis g and if you have for forgotten this if you have for forgotten this hypothesis was discussed in the learning hypothesis was discussed in the learning problem video this hypothesis d is problem video this hypothesis d is going to be always parameterized with a going to be always parameterized with a set of parameters theta we will set of parameters theta we will start all of these are vectors i start all of these are vectors i should have mentioned that also should have mentioned that also potentially earlier that is potentially earlier that is basically the equation that we will try basically the equation that we will try to capture with some neural netor to capture with some neural netor architecture and u now we will architecture and u now we will see the how this equation can", "image_path": "img_data/video_06_chunk_12.jpg"}
{"video": "video_06", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "architecture and u now we will see the how this equation can see the how this equation can be captured by just revisiting our be captured by just revisiting our kind of sigmoid d neuron and we will be neuron and we will be u modifying that kind of sigmoidal u modifying that kind of sigmoidal neuron to understand how this can be neuron to understand how this can be achieved in our earlier kind of achieved in our earlier kind of video we have seen this following video we have seen this following structure a dot product", "image_path": "img_data/video_06_chunk_13.jpg"}
{"video": "video_06", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "video we have seen this following structure a dot product structure a dot product followed by nonlinearity let's say to followed by nonlinearity let's say to implement some task i forgot to add over implement some task i forgot to add over here the bias addition and this is let's say the sigmoidal nonlinearity will actually sigmoidal nonlinearity will actually give us some y hat and evidently over here i mean this hat and evidently over here i mean this is effectively the simple is effectively the simple binary kind of classifier fire", "image_path": "img_data/video_06_chunk_14.jpg"}
{"video": "video_06", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "is effectively the simple binary kind of classifier fire binary kind of classifier fire that was a task that we discussed that was a task that we discussed when the sig modal unit was involved when the sig modal unit was involved and then we need to sort of see how and then we need to sort of see how this will evolve that we are able to capture evolve that we are able to capture what we have discussed over here in this what we have discussed over here in this equation we will be maintaining equation we will be maintaining the dot product is going to be maintained because i will be going to be maintained because i will be using the u w for something else i", "image_path": "img_data/video_06_chunk_15.jpg"}
{"video": "video_06", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "going to be maintained because i will be using the u w for something else i using the u w for something else i will be changing the parameter vector will be changing the parameter vector over here to u it is u transpose over here to u it is u transpose x now this x here is going to have some x now this x here is going to have some index now x of t we'll see now in a moment an application that it will able to capture application that it will able to capture this to understand exactly what this to understand exactly what x of t is this is my parameter x of t is this is my parameter vector you and this will be my", "image_path": "img_data/video_06_chunk_16.jpg"}
{"video": "video_06", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "x of t is this is my parameter vector you and this will be my vector you and this will be my bias maintaining the bias and we need a bias maintaining the bias and we need a new nonlinear function and the rationale of this nonlinear function is kind of historic nonlinear function is kind of historic in this type of architectures but an in this type of architectures but an intuitive way of understanding why we intuitive way of understanding why we moved from some something a moved from some something a sigmoidal to let's say tan h is that the sigmoidal to let's say tan h is that the sigmoid being creating values", "image_path": "img_data/video_06_chunk_17.jpg"}
{"video": "video_06", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "sigmoidal to let's say tan h is that the sigmoid being creating values sigmoid being creating values between zero and one any time that we between zero and one any time that we had something which was kind of had something which was kind of negative we would have significantly negative we would have significantly kind of negative had values very close kind of negative had values very close to zero and this will create a problem to zero and this will create a problem in the feedback loop which i'm in the feedback loop which i'm about to introduce this will be my h about to introduce this will be my h oft the output up to this point we oft the output up to this point we have captured something that it will have captured something that it will definitely be has some kind of definitely be has some kind of parameterization theta accept some kind", "image_path": "img_data/video_06_chunk_18.jpg"}
{"video": "video_06", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "definitely be has some kind of parameterization theta accept some kind parameterization theta accept some kind of sequences x oft one data point at of sequences x oft one data point at a time and produces some kind of h oft a time and produces some kind of h oft at the output of this kind of at the output of this kind of nonlinearity g right now g is nonlinearity g right now g is captured by the tan h u but we need to captured by the tan h u but we need to introduce the dependency on the previous introduce the dependency on the previous input the dependency on the previous input the dependency on the previous input is going to be captured by input is going to be captured by modeling and this is kind of a assumed", "image_path": "img_data/video_06_chunk_19.jpg"}
{"video": "video_06", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "input is going to be captured by modeling and this is kind of a assumed modeling and this is kind of a assumed in several kind of textbooks in several kind of textbooks something that is acts a memory that's a unit memory over here it just stores the earlier state and it just stores the earlier state and now this earlier state that was now this earlier state that was there just before h of t which was there just before h of t which was called ht minus one can be retrieved and called ht minus one can be retrieved and adjusted by h scalar w and added into the", "image_path": "img_data/video_06_chunk_20.jpg"}
{"video": "video_06", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "w and added into the sum over there the summer over there sum over there the summer over there that is effectively the that is effectively the if you the new neuron that if you the new neuron that we have evolved that is able to capture we have evolved that is able to capture also the previous state and if i kind of also the previous state and if i kind of write the equation of this neuron it write the equation of this neuron it will be h of t is going to be well i should also t is going to be well i should also mention that right now this moment in mention that right now this moment in time we are only able to model", "image_path": "img_data/video_06_chunk_21.jpg"}
{"video": "video_06", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "mention that right now this moment in time we are only able to model time we are only able to model scalar sort of hidden states h of t scalar sort of hidden states h of t is going to be t h let me also remove this under bar from here to avoid any confusion tan h of u transpose xt plus w ht", "image_path": "img_data/video_06_chunk_22.jpg"}
{"video": "video_06", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "xt plus w ht minus1 plus b this is the equation minus1 plus b this is the equation that we have introduced that we have introduced that effectively captures this model on effectively captures this model on the left hand side and a couple of the left hand side and a couple of observations on this kind of observations on this kind of architecture the hidden state here is a architecture the hidden state here is a scalar simply because at the output of scalar simply because at the output of this dot product we have a scalar this dot product we have a scalar obviously b is a scal scalar and obviously b is a scal scalar and everything else is a scalar and", "image_path": "img_data/video_06_chunk_23.jpg"}
{"video": "video_06", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "obviously b is a scal scalar and everything else is a scalar and everything else is a scalar and therefore this is the reason why we therefore this is the reason why we didn't put the under bar there and didn't put the under bar there and obviously theta here is vector of all obviously theta here is vector of all the trainable parameters we have the trainable parameters we have the vector u the scalar w and the bias b vector u the scalar w and the bias b this is the vector of trainable this is the vector of trainable parameters and now what we need to do is parameters and now what we need to do is to look at an application and this to look at an application and this application will also be a simple application will also be a simple application of time serious analysis but application of time serious analysis but i think it's very instructive because", "image_path": "img_data/video_06_chunk_24.jpg"}
{"video": "video_06", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "application of time serious analysis but i think it's very instructive because i think it's very instructive because it is also going to be geared to it is also going to be geared to capture the to require if you capture the to require if you initially a scalar hidden state initially a scalar hidden state the target variable is going to be the target variable is going to be scaler this application has to do scaler this application has to do with modeling a sequence of with modeling a sequence of values and let me just draw over here values and let me just draw over here the sort of an example of that we have", "image_path": "img_data/video_06_chunk_25.jpg"}
{"video": "video_06", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "the sort of an example of that we have a time series plot a time series let's say something that we will be let's say something that we will be dealing with obviously this is t and this will obviously this is t and this will actually be x and that's the reason why actually be x and that's the reason why we always had x of t as coming in we always had x of t as coming in i think it's very important for us to i think it's very important for us to understand that what type of data understand that what type of data sets we need to have in order for us to", "image_path": "img_data/video_06_chunk_26.jpg"}
{"video": "video_06", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "understand that what type of data sets we need to have in order for us to sets we need to have in order for us to capture a model of a sequence and capture a model of a sequence and before that we also need to kind and before that we also need to kind of very clearly state the problem and of very clearly state the problem and the problem is the following this is the problem is the following this is the problem predict the price x of a commodity a", "image_path": "img_data/video_06_chunk_27.jpg"}
{"video": "video_06", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "a commodity in the next trade trading day at the end of next trade trading day at the end of the closing of time of the next the closing of time of the next trading day that is let's assume that time is given in days trading days and from the sequence", "image_path": "img_data/video_06_chunk_28.jpg"}
{"video": "video_06", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "from the sequence of let's say 50 let's say capital t is equal to 50 trading day p prices that's that is the prices that's that is the problem it's a very problem it's a very simple problem and it requires simple problem and it requires also however kind of some thinking", "image_path": "img_data/video_06_chunk_29.jpg"}
{"video": "video_06", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "simple problem and it requires also however kind of some thinking also however kind of some thinking because i can tell you that we are because i can tell you that we are not going to be feeding into this not going to be feeding into this network although we'll be feeding one network although we'll be feeding one value at a time the data set that we value at a time the data set that we will need to have is quite different will need to have is quite different from the data sets that we used to in from the data sets that we used to in the earlier kind of discussions of an the earlier kind of discussions of an early discussions meaning the fully early discussions meaning the fully connected neural networks as well also the convolution in networks as well also the convolution in your networks let's write down some your networks let's write down some numbers here those this is a time numbers here those this is a time instant zero this is a time instant", "image_path": "img_data/video_06_chunk_30.jpg"}
{"video": "video_06", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "numbers here those this is a time instant zero this is a time instant zero this is a time instant 49 this is a price time instant 49 this is a price time instant 49 and where we are after some and where we are after some value this will actually be the value this will actually be the target variable here is going to be target variable here is going to be obviously always called y and this will obviously always called y and this will actually be my y is going to be x actually be my y is going to be x 50 that is it and of course 50 that is it and of course it's very evident why this neuron net it's very evident why this neuron net will need to model definitely we need to", "image_path": "img_data/video_06_chunk_31.jpg"}
{"video": "video_06", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "it's very evident why this neuron net will need to model definitely we need to will need to model definitely we need to model the time correlations the model the time correlations the correlations in time in an early correlations in time in an early architecture let's say in convolution architecture let's say in convolution networks we'll be dealing with networks we'll be dealing with correlations in space spatial correlations in space spatial correlations and we've seen various correlations and we've seen various applications that are kind of applications that are kind of originating from that kind of foundation originating from that kind of foundation over here the price is over here the price is evidently has some dependencies on evidently has some dependencies on earlier prices is not a memoryless kind earlier prices is not a memoryless kind of process otherwise will not have any", "image_path": "img_data/video_06_chunk_32.jpg"}
{"video": "video_06", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "earlier prices is not a memoryless kind of process otherwise will not have any of process otherwise will not have any possibility of predicting it and possibility of predicting it and therefore the neural network should therefore the neural network should be able to capture this time be able to capture this time correlations in time let me write this down to capture or model time correlations in this case because the sequence has a semantics of", "image_path": "img_data/video_06_chunk_33.jpg"}
{"video": "video_06", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "model time correlations in this case because the sequence has a semantics of because the sequence has a semantics of time here only here similarly to what we had seen in cnn's in terms of in that case was cnn's in terms of in that case was spatial correlations", "image_path": "img_data/video_06_chunk_34.jpg"}
{"video": "video_07", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we'll introduce two foundational probability rules the sum foundational probability rules the sum rule and the product rule and that we rule and the product rule and that we will also give raise to the important will also give raise to the important rule that's called base rule that the rule that's called base rule that the correspond to the algebraic addition correspond to the algebraic addition and multiplication if you that and multiplication if you that are quite essential and coming are quite essential and coming very frequently in many very frequently in many of the topics that we will be discussing of the topics that we will be discussing in machine learning and ai i would", "image_path": "img_data/video_07_chunk_0.jpg"}
{"video": "video_07", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "of the topics that we will be discussing in machine learning and ai i would in machine learning and ai i would to introduce those using if you to introduce those using if you a simple experiment i have here a simple experiment i have here in this experiment two in this experiment two erns or containers this is container erns or containers this is container let's say zero and container one and i will be symbolizing with the letter y these containers", "image_path": "img_data/video_07_chunk_1.jpg"}
{"video": "video_07", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "the letter y these containers and these containers contain sort of and these containers contain sort of widgets of two types one widgets of two types one is black widget and the other is type is black widget and the other is type is a white widget it doesn't really matter how many widget it doesn't really matter how many at this point we don't really count the at this point we don't really count the widgets to learn about this kind of widgets to learn about this kind of rules it just doesn't really matter rules it just doesn't really matter we have here the experiment goes this", "image_path": "img_data/video_07_chunk_2.jpg"}
{"video": "video_07", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "we have here the experiment goes this the experiment goes this the first step of the experiment this the first step of the experiment is to pick up randomly a container not to pick up but just pick a container randomly a container randomly a container and as we said that we will container and as we said that we will use the random variable name y which", "image_path": "img_data/video_07_chunk_3.jpg"}
{"video": "video_07", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "container and as we said that we will use the random variable name y which use the random variable name y which is evidently binary random variable it is evidently binary random variable it can only take value zero and can only take value zero and one and we put our hand inside that one and we put our hand inside that kind of container and we take out a kind of container and we take out a widget and we open our hand and then widget and we open our hand and then we reveal the identity of the widget and we reveal the identity of the widget and we will repl put it down put it back we will repl put it down put it back from the widget that it came from the widget that it came from we effectively sample a widget", "image_path": "img_data/video_07_chunk_4.jpg"}
{"video": "video_07", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "sample a widget with replacement the random variable for the widget is going to be all x and obviously x will be u be all x and obviously x will be u let's say zero will correspond let's say zero will correspond let's say to the black widget and one will correspond to the widget and one will correspond to the white widget", "image_path": "img_data/video_07_chunk_5.jpg"}
{"video": "video_07", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "widget and one will correspond to the white widget that's basically the mapping that we have implemented and there is a that we have implemented and there is a random variable to govern that kind of random variable to govern that kind of sampling kind of process the with sampling kind of process the with replacement is a common term in replacement is a common term in sort of sampling and sort of sampling and means put back the widget to the means put back the widget to the container we came from and actually the container we came from and actually the reason actually we need to do that is reason actually we need to do that is because we do not want to change in because we do not want to change in the statistics of the problem of over", "image_path": "img_data/video_07_chunk_6.jpg"}
{"video": "video_07", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "because we do not want to change in the statistics of the problem of over the statistics of the problem of over time by putting it back then we have time by putting it back then we have the same probability of picking up an the same probability of picking up an widget of a specific type out of each widget of a specific type out of each container the problem is kind of container the problem is kind of stationary the other thing of course stationary the other thing of course they are giving us before we start they are giving us before we start asking questions is we are giving some asking questions is we are giving some kind of what we call marginal kind of what we call marginal probability just a probability distribution of y", "image_path": "img_data/video_07_chunk_7.jpg"}
{"video": "video_07", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "probability just a probability distribution of y and this is p of y and as you might and this is p of y and as you might expect the probability distribution of y expect the probability distribution of y is a sort of discrete y is a discrete is a sort of discrete y is a discrete random variable therefore we expect random variable therefore we expect to see some kind of a plot this to see some kind of a plot this where this thing is 0.4 for when we have a container zero 0.4 for when we have a container zero and this is evidently 0.6 when we and this is evidently 0.6 when we have container one because the summation have container one because the summation of the two has to be equal to 1.0 to be", "image_path": "img_data/video_07_chunk_8.jpg"}
{"video": "video_07", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "have container one because the summation of the two has to be equal to 1.0 to be of the two has to be equal to 1.0 to be a probability distri a proper a probability distri a proper probability distribution the what probability distribution the what it basically says in english is graph it basically says in english is graph is translated is that i have some is translated is that i have some preference to select containers and preference to select containers and in fact i'm not selecting containers in fact i'm not selecting containers with equal probability i have some kind with equal probability i have some kind of a preferential bias towards container of a preferential bias towards container one could be anything all right one could be anything all right here comes the first here comes the first question is can you question the first question is can you tell me anything", "image_path": "img_data/video_07_chunk_9.jpg"}
{"video": "video_07", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "question the first question is can you tell me anything about the p of x we first of all we can definitely suggest that because we definitely suggest that because we have two types of widget and x is a have two types of widget and x is a discrete random variable we expect to discrete random variable we expect to see something that maybe this is see something that maybe this is the other way around but we definitely the other way around but we definitely expect to see something that we expect to see something that we just need to specify the heights of just need to specify the heights of these two sort of probabilities to", "image_path": "img_data/video_07_chunk_10.jpg"}
{"video": "video_07", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "just need to specify the heights of these two sort of probabilities to these two sort of probabilities to estimate the probability of course we estimate the probability of course we will be executing this experiment we will be executing this experiment we will be executing the experiment let's will be executing the experiment let's call this execution that happens n call this execution that happens n times of both of these kind of steps and times of both of these kind of steps and let me just also plot over here the diagram we have", "image_path": "img_data/video_07_chunk_11.jpg"}
{"video": "video_07", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "diagram we have definitely four possibilities here that can happen if this represents that can happen if this represents the x random variable and this the x random variable and this represents the y random variables we represents the y random variables we have a in experient involves two random have a in experient involves two random variables x and y and all the events are variables x and y and all the events are joined the this means that x joined the this means that x happens let's say sorry first the happens let's say sorry first the y happens where we have let's say select", "image_path": "img_data/video_07_chunk_12.jpg"}
{"video": "video_07", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "happens let's say sorry first the y happens where we have let's say select y happens where we have let's say select a container let's say zero and then a container let's say zero and then the x happens let's say i take a black the x happens let's say i take a black widget out of that kind of container widget out of that kind of container x can y can be zer or one and x can y can be zer or one and therefore x can also be z or one and therefore x can also be z or one and therefore what i'm actually therefore what i'm actually representing here with dots is the joint events something that", "image_path": "img_data/video_07_chunk_13.jpg"}
{"video": "video_07", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "events something that and of course what i would and of course what i would to do is i want to give you also to do is i want to give you also the counts that i have obtained here the counts that i have obtained here i will call this count n0 0 this count i will call this count n0 0 this count n01 n11 and n10 that's basically the count the counts that i obtained and count the counts that i obtained and now that i have obtained the counts i now that i have obtained the counts i will be able to start making", "image_path": "img_data/video_07_chunk_14.jpg"}
{"video": "video_07", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "now that i have obtained the counts i will be able to start making will be able to start making some kind of thoughts about the how some kind of thoughts about the how to estimate the p of x is equal to z to estimate the p of x is equal to z i'm going to write over here p of x is i'm going to write over here p of x is equal to z well x is equal to zer can equal to z well x is equal to zer can happen in when two things are happen in when two things are happening first it can happen when i happening first it can happen when i have selected a black widget can come have selected a black widget can come either from", "image_path": "img_data/video_07_chunk_15.jpg"}
{"video": "video_07", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "have selected a black widget can come either from the container zero or from the container one definitely the time the number of times that i've obtained a black widget times that i've obtained a black widget is when i add the n0 0 n n01 and i is when i add the n0 0 n n01 and i divide it by n and this is basically divide it by n and this is basically", "image_path": "img_data/video_07_chunk_16.jpg"}
{"video": "video_07", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "divide it by n and this is basically the first thing that i can say also first thing that i can say also i can say something about similarly i can say something about similarly about the p of x is equal to 1 i can say about the p of x is equal to 1 i can say that this is can come that this is can come also from the zero container or from the container one and this is kind of fairly kind of intuitive and i can", "image_path": "img_data/video_07_chunk_17.jpg"}
{"video": "video_07", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "one and this is kind of fairly kind of intuitive and i can definitely generalize these definitely generalize these two this it was n0 0 + n01 / n and two this it was n0 0 + n01 / n and then this is n 1 0 + n11 divid by n i then this is n 1 0 + n11 divid by n i have answered the question that they're have answered the question that they're asking me to answer but i would to asking me to answer but i would to generalize what i have done over here generalize what i have done over here into the following rule which i'll into the following rule which i'll come this i'll call the sum rule i would come this i'll call the sum rule i would to say that the p of x which", "image_path": "img_data/video_07_chunk_18.jpg"}
{"video": "video_07", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "come this i'll call the sum rule i would to say that the p of x which would to say that the p of x which gives you the probability distribution gives you the probability distribution of x is a summation here you see two of x is a summation here you see two terms in the summation because i have a terms in the summation because i have a random variable y which has can only random variable y which has can only take two values but in general it's a take two values but in general it's a summation over all values of y of the summation over all values of y of the joint probabbly distribution of x and y joint probabbly distribution of x and y similarly i can answer problems i similarly i can answer problems i know that right now the problem gave know that right now the problem gave me the p of y but in general i", "image_path": "img_data/video_07_chunk_19.jpg"}
{"video": "video_07", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "know that right now the problem gave me the p of y but in general i me the p of y but in general i can symmetrically do this as well if they symmetrically do this as well if they didn't and they gave me something about didn't and they gave me something about x i can definitely respond something x i can definitely respond something about i can definitely say something about i can definitely say something about p of y this is the called about p of y this is the called sum rule and it's also known as the rule and it's also known as the marginalization rule that allow me to do marginalization rule that allow me to do the following if i know i'm giving the following if i know i'm giving some information about the joint", "image_path": "img_data/video_07_chunk_20.jpg"}
{"video": "video_07", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "the following if i know i'm giving some information about the joint some information about the joint probability distribution i want to take probability distribution i want to take some of these random variables out i some of these random variables out i will sum over them and therefore this will sum over them and therefore this will allow me to obtain the -cal will allow me to obtain the -cal marginal probability distribution marginal probability distribution associated with this joint probability associated with this joint probability distribution that is the first rule distribution that is the first rule that i said is the equivalent of that i said is the equivalent of addition but in the probability addition but in the probability algebraic addition but in the algebraic addition but in the probability domain now the second probability domain now the second question comes and let me just question comes and let me just separate the second", "image_path": "img_data/video_07_chunk_21.jpg"}
{"video": "video_07", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "question comes and let me just separate the second question over here the second question is can you here the second question is can you tell us anything about the p of y given tell us anything about the p of y given x now this is a probability distribution x now this is a probability distribution that's actually called conditional that's actually called conditional probability distribution and it probability distribution and it symbolized with this symbol over here symbolized with this symbol over here this vertical line that separates a this vertical line that separates a random variable from another but this random variable from another but this another the other r available the x more", "image_path": "img_data/video_07_chunk_22.jpg"}
{"video": "video_07", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "random variable from another but this another the other r available the x more another the other r available the x more specifically in this case because it's specifically in this case because it's coming after that symbol which is coming after that symbol which is actually called given it's is a fixed quantity it's given it's not random variable given it's not random variable anymore that kind of have some anymore that kind of have some kind of implications onto how we you kind of implications onto how we count in order to come up with know we count in order to come up with the specific values of this of these the specific values of this of these conditional probabilities now conditional probabilities now definitely we have four possibilities", "image_path": "img_data/video_07_chunk_23.jpg"}
{"video": "video_07", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "conditional probabilities now definitely we have four possibilities definitely we have four possibilities in this conditional probability the first possibility and probability the first possibility and let me write down on the right side let me write down on the right side these possibilities is these possibilities is when the zero container selected when the zero container selected given that i had seen a given that i had seen a black widget the second possibility is the widget the second possibility is the container one was elected given that i", "image_path": "img_data/video_07_chunk_24.jpg"}
{"video": "video_07", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "widget the second possibility is the container one was elected given that i container one was elected given that i have seen a black widget and the same things for the white widget these are the all the four possibilities i have in this four possibilities i have in this problem i have two binary random problem i have two binary random variables evidently when i have variables evidently when i have nonbinary random variables there are nonbinary random variables there are many more possibilities but definitely i can possibilities but definitely i can actually start addressing the first one actually start addressing the first one and i can definitely say that and i can definitely say that something about it given the fact that i", "image_path": "img_data/video_07_chunk_25.jpg"}
{"video": "video_07", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "and i can definitely say that something about it given the fact that i something about it given the fact that i was given the counts now we need to be kind of counts now we need to be kind of slightly kind of careful here because slightly kind of careful here because you want to consider this given the x is you want to consider this given the x is equal to zer to effectively equal to zer to effectively constraint the space over which you constraint the space over which you are executing this experiment given are executing this experiment given that x is equal to zero you're actually that x is equal to zero you're actually instead of dividing by capital n you are instead of dividing by capital n you are dividing over the summation of n0 0 and n n0", "image_path": "img_data/video_07_chunk_26.jpg"}
{"video": "video_07", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "dividing over the summation of n0 0 and n n0 over the summation of n0 0 and n n0 0 and n01 this will act as a constraint your denominator is this because your only look for x is equal to because your only look for x is equal to z and then ob evidently the numerator z and then ob evidently the numerator is when y is equal to z well when y is equal to z well when y is equal to z you're looking at the n0 0 equal to z you're looking at the n0 0 count of events and that is basically", "image_path": "img_data/video_07_chunk_27.jpg"}
{"video": "video_07", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "equal to z you're looking at the n0 0 count of events and that is basically count of events and that is basically your response that is going to be your response that is going to be your result if you which is a number result if you which is a number similarly you with a similar kind of similarly you with a similar kind of considerations you will actually considerations you will actually do the others let me write them down quickly p of y is equal to0 given xal to 1 is", "image_path": "img_data/video_07_chunk_28.jpg"}
{"video": "video_07", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "quickly p of y is equal to0 given xal to 1 is n1 0 n 1 0+ n11 and finally p of y isal to 1 given x equal to 1 is of y isal to 1 given x equal to 1 is n 1 one ided by n10 + by n10 + n11 and now we can actually similar sly n11 and now we can actually similar sly to what we have done kind of earlier we", "image_path": "img_data/video_07_chunk_29.jpg"}
{"video": "video_07", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "n11 and now we can actually similar sly to what we have done kind of earlier we to what we have done kind of earlier we can start to understand how we can start to understand how we can potentially start generalizing this into potentially start generalizing this into that what will end up being called the that what will end up being called the product rule and let me just do that product rule and let me just do that from this equation let me call from this equation let me call this one and i'm saying from one and i'm saying from one i can write that n0 0 is equal to p one i can write that n0 0 is equal to p of", "image_path": "img_data/video_07_chunk_30.jpg"}
{"video": "video_07", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "one i can write that n0 0 is equal to p of y is equal to 0 given x equal to 0 y is equal to 0 given x equal to 0 time n0 0 + n01 a simple time n0 0 + n01 a simple modification from a simple product modification from a simple product of that expression and then what of that expression and then what i can actually see also is i can actually see also is i can replace this with something that i have this with something that i have seen earlier", "image_path": "img_data/video_07_chunk_31.jpg"}
{"video": "video_07", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "this with something that i have seen earlier i have seen that p of earlier i have seen that p of x is equal to 0 is equal to n0 0 + x is equal to 0 is equal to n0 0 + n01 / n01 / n and therefore i can actually from this therefore i can actually from this two i can get that the n 0 is equal to p of y is equal to0 given x equal to z", "image_path": "img_data/video_07_chunk_32.jpg"}
{"video": "video_07", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "0 is equal to p of y is equal to0 given x equal to z p of y is equal to0 given x equal to z and i will be replacing u this n0 0 + and i will be replacing u this n0 0 + n01 with p of x = 0 * n with p of x = to0 * n and therefore i can write that n0 0 therefore i can write that n0 0 divided by n", "image_path": "img_data/video_07_chunk_33.jpg"}
{"video": "video_07", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "therefore i can write that n0 0 divided by n is p of yal 0 given xal to0 but n0 0 ided n is the probability of y is equal ided n is the probability of y is equal 0 and x equal to 0 and therefore i just 0 and x equal to 0 and therefore i just wrote it as a product of the wrote it as a product of the conditional p of y is equal to 0 given conditional p of y is equal to 0 given xal to0 times p of xal to z this is basically one", "image_path": "img_data/video_07_chunk_34.jpg"}
{"video": "video_07", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "to0 times p of xal to z this is basically one p of xal to z this is basically one expression i got out of the first expression i got out of the first equation over here i can actually do the equation over here i can actually do the same for equation two 3 and same for equation two 3 and four and this will actually allow me to four and this will actually allow me to generalize and if you see the form over generalize and if you see the form over here i'm writing something the here i'm writing something the joint is equal to some product let me joint is equal to some product let me write down the generalization of this formula which generalization of this formula which i'll be calling the product rule and", "image_path": "img_data/video_07_chunk_35.jpg"}
{"video": "video_07", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "generalization of this formula which i'll be calling the product rule and i'll be calling the product rule and the product rule says that p of x the product rule says that p of x comma y is equal to p of x given y * p of y or p of x given y * p of y or equivalently p of y x is equal to p of y given x * p of x which is basically what we had done just which is basically what we had done just now with x in our problem someone", "image_path": "img_data/video_07_chunk_36.jpg"}
{"video": "video_07", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "which is basically what we had done just now with x in our problem someone now with x in our problem someone gave us the x and wanted to find out gave us the x and wanted to find out sorry someone is yes has specified the x sorry someone is yes has specified the x i wanted to find out the conditional p i wanted to find out the conditional p of y given x and that conditional is of y given x and that conditional is definitely the joint divided by the p of definitely the joint divided by the p of x which already have calculated from the x which already have calculated from the using if you the sum rule the using if you the sum rule the this is the product rule and will allow this is the product rule and will allow us to actually do a lot of things this", "image_path": "img_data/video_07_chunk_37.jpg"}
{"video": "video_07", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "this is the product rule and will allow us to actually do a lot of things this us to actually do a lot of things this product rule in some instances in many product rule in some instances in many problems we have been given if problems we have been given if you a joint we know information you a joint we know information about the joint but we would to about the joint but we would to factorize the joint into some components factorize the joint into some components hoping that some of these components hoping that some of these components would simple to estimate this is the estimate this is the main usage of this rule in estimation pro usage of this rule in estimation pro problems that we will see but in", "image_path": "img_data/video_07_chunk_38.jpg"}
{"video": "video_07", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "usage of this rule in estimation pro problems that we will see but in problems that we will see but in addition to that it also defines a very addition to that it also defines a very i will call it a foundational kind of i will call it a foundational kind of rule that we call the base rule that we call the base rule as follows given that p of x comma rule as follows given that p of x comma y is equal to p of y comma x we do y is equal to p of y comma x we do not really change anything by swapping not really change anything by swapping the variables around and effectively you the variables around and effectively you can actually see that also in the can actually see that also in the diagram because effectively they diagram because effectively they point to the same quadrant", "image_path": "img_data/video_07_chunk_39.jpg"}
{"video": "video_07", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "point to the same quadrant we can actually do the we can actually do the following we can actually write here the following we can actually write here that p of x given y * p of y is equal then to p x given y * p of y is equal then to p of y given x * p of x or p of y given x is equal to p of x or p of y given x is equal to p of x given y", "image_path": "img_data/video_07_chunk_40.jpg"}
{"video": "video_07", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "x or p of y given x is equal to p of x given y time p of y / p of x this is called the base rule and some of the components over here have taken the sort of over here have taken the sort of following semantics in this kind of following semantics in this kind of discussion this is called", "image_path": "img_data/video_07_chunk_41.jpg"}
{"video": "video_07", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "following semantics in this kind of discussion this is called discussion this is called evidence it tells a story about the evidence it tells a story about the data this is called prior and it allow us to tell us prior and it allow us to tell us something about the why before we something about the why before we actually start an experiment and actually start an experiment and this is actually what had just happened this is actually what had just happened in our case someone gave us some in our case someone gave us some information about the why and this is information about the why and this is a very important quantity called", "image_path": "img_data/video_07_chunk_42.jpg"}
{"video": "video_07", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "information about the why and this is a very important quantity called likelihood and tell us how likely is the data we are seeing x as given the data we are seeing x as given the fact that we have a specific fact that we have a specific value in the y variable we will value in the y variable we will see this rule over and over again in see this rule over and over again in this course more specifically this", "image_path": "img_data/video_07_chunk_43.jpg"}
{"video": "video_07", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "see this rule over and over again in this course more specifically this in this course more specifically this probability distribution kind of probability distribution kind of encompasses the whole domain of sort encompasses the whole domain of sort of machine learning more specifically of machine learning more specifically supervised kind of learning but also supervised kind of learning but also some others and in our some others and in our case x will actually be the data that case x will actually be the data that someone is giving us let's say x is someone is giving us let's say x is an image with let's say a person in there and why will", "image_path": "img_data/video_07_chunk_44.jpg"}
{"video": "video_07", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "image with let's say a person in there and why will be let's say one when be let's say one when there is a person or zero when there is there person or zero when there is there isn't in many of our discussions inside the many of our discussions inside the perception system we'll actually have perception system we'll actually have this sort of many examples and this sort of many examples and someone will actually ask us to do someone will actually ask us to do predictions as to the presence or predictions as to the presence or not let's say of a person on an image", "image_path": "img_data/video_07_chunk_45.jpg"}
{"video": "video_07", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "predictions as to the presence or not let's say of a person on an image not let's say of a person on an image and this prediction will be fully and this prediction will be fully quantified using this conditional quantified using this conditional random variable that sorry this random variable that sorry this conditional distribution we will be conditional distribution we will be making predictions the -called making predictions the -called yuts from using this conditional yuts from using this conditional probability distribution and that's why probability distribution and that's why this kind of rule is quite important this kind of rule is quite important because it will allow us to do this because it will allow us to do this prediction despite the fact in some prediction despite the fact in some instances will be potentially not", "image_path": "img_data/video_07_chunk_46.jpg"}
{"video": "video_07", "start": "0:23:30", "end": "0:23:54.233333", "timestamp": "0:23:30 - 0:23:54.233333", "text": "prediction despite the fact in some instances will be potentially not instances will be potentially not have any prior information we have a have any prior information we have a uniform prior or we will not uniform prior or we will not necessarily engage this actually rule necessarily engage this actually rule but we will be working only with the but we will be working only with the likelihood i think it's a good idea to likelihood i think it's a good idea to actually know this rule by to actually know this rule by to understand it and know this kind to understand it and know this kind of rule as well also the other two in of rule as well also the other two in fact this rule was derived out of the fact this rule was derived out of the product rule", "image_path": "img_data/video_07_chunk_47.jpg"}
{"video": "video_09", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video i would to go through the programming environment that you the programming environment that you need to have in order to be successful need to have in order to be successful in this course starting let's say in this course starting let's say with the compute i think it's quite with the compute i think it's quite important to have at least access to important to have at least access to a capable laptop any laptop that has a capable laptop any laptop that has been purchased the last three years been purchased the last three years starting from the max with the m1 starting from the max with the m1 processor to latest versions of let's processor to latest versions of let's say", "image_path": "img_data/video_09_chunk_0.jpg"}
{"video": "video_09", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "processor to latest versions of let's say version 11 of an intel processor if version 11 of an intel processor if you are on the windows you are on the windows platform or even better on an ubuntu platform or even better on an ubuntu platform that will actually be platform that will actually be sufficient you need at least 16 gabt of sufficient you need at least 16 gabt of memory and also you need to have memory and also you need to have access to ideally you want to have access to ideally you want to have access to your personal gpu but if you access to your personal gpu but if you even if you don't we will be using even if you don't we will be using collab the google collab for collab the google collab for accelerating workloads that we need", "image_path": "img_data/video_09_chunk_1.jpg"}
{"video": "video_09", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "collab the google collab for accelerating workloads that we need accelerating workloads that we need to do either for assignments or for your to do either for assignments or for your project the starting point if project the starting point if you is to get familiar with how to you is to get familiar with how to set up programming environments and set up programming environments and development environments using docker development environments using docker containers i think the docker containers i think the docker container is if you is an essential container is if you is an essential almost a de facto standard in data almost a de facto standard in data science and ai today you need to be science and ai today you need to be familiar with it specifically for your familiar with it specifically for your project and potentially some assignments project and potentially some assignments but mainly for your project", "image_path": "img_data/video_09_chunk_2.jpg"}
{"video": "video_09", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "project and potentially some assignments but mainly for your project but mainly for your project may become in the form of a docker may become in the form of a docker container and the video over here container and the video over here which you can actually access from which you can actually access from the resources page over here on the resources page over here on the site will walk you through and educate site will walk you through and educate you what is a docker co how you can you what is a docker co how you can actually to understand if you a actually to understand if you a docker file what it is and how to docker file what it is and how to author your own and configure your author your own and configure your environment without necessarily having environment without necessarily having to configure for every project your home to configure for every project your home po machine we will be also assuming", "image_path": "img_data/video_09_chunk_3.jpg"}
{"video": "video_09", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "to configure for every project your home po machine we will be also assuming po machine we will be also assuming at some point that you will be at some point that you will be having an installation of an integrated having an installation of an integrated development environment there are two development environment there are two environments that are very popular one environments that are very popular one is visual studio code which is the most is visual studio code which is the most popular and there is another one called popular and there is another one called pie charm and both of them are actually pie charm and both of them are actually capable i believe that for the needs capable i believe that for the needs of the project specifically to work of the project specifically to work seamlessly with docker containers you seamlessly with docker containers you probably need to have access to the pro", "image_path": "img_data/video_09_chunk_4.jpg"}
{"video": "video_09", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "seamlessly with docker containers you probably need to have access to the pro probably need to have access to the pro version of pycharm which is free for version of pycharm which is free for students and academia i think you students and academia i think you should be fine in either way you will should be fine in either way you will actually see in some elements of actually see in some elements of during the delivery i'll be using my own during the delivery i'll be using my own which is called the visual studio code which is called the visual studio code over here and you can actually access over here and you can actually access from that link the i also have from that link the i also have included here some installation included here some installation instructions about how to instantiate instructions about how to instantiate especially if you're on windows", "image_path": "img_data/video_09_chunk_5.jpg"}
{"video": "video_09", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "instructions about how to instantiate especially if you're on windows especially if you're on windows environment wsl2 stands is for windows sub system stands is for windows sub system number two and it's basically number two and it's basically instantiates an ubuntu virtual instantiates an ubuntu virtual machine in your laptop and if your machine in your laptop and if your laptop as i said is equipped with u laptop as i said is equipped with u nvidia gpus it also by nvidia gpus it also by passes through that gpu over to the passes through that gpu over to the ubuntu machine you are having a ubuntu machine you are having a fairly compelling devel vel an", "image_path": "img_data/video_09_chunk_6.jpg"}
{"video": "video_09", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "ubuntu machine you are having a fairly compelling devel vel an fairly compelling devel vel an environment if you have that but environment if you have that but nevertheless even on a mac with as nevertheless even on a mac with as i said with the latest generation you i said with the latest generation you should be able to be successful in this should be able to be successful in this project and in any case we have also project and in any case we have also the other environment that we also the other environment that we also need to be familiar with called google need to be familiar with called google collaboratory those google collaboratory those google collaboratory is the only game in town collaboratory is the only game in town that provides gpu acceleration free that provides gpu acceleration free gpu acceleration but there are some", "image_path": "img_data/video_09_chunk_7.jpg"}
{"video": "video_09", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "that provides gpu acceleration free gpu acceleration but there are some free gpu acceleration but there are some constraints with respect to the constraints with respect to the ability of you using it for many hours ability of you using it for many hours at a time it may be worthwhile at a time it may be worthwhile considering paying for the pro version considering paying for the pro version which i believe is around $20 when the which i believe is around $20 when the time comes to execute the heavy time comes to execute the heavy processing for your project let's say processing for your project let's say i think the expense going to be i think the expense going to be around $ 20 to $40 if you purchase one around $ 20 to $40 if you purchase one or two month subscription there for the or two month subscription there for the pro version of google collaboratory", "image_path": "img_data/video_09_chunk_8.jpg"}
{"video": "video_09", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "or two month subscription there for the pro version of google collaboratory pro version of google collaboratory those who are familiar with aws those who are familiar with aws you should know that these are going you should know that these are going incure charges but the incure charges but the aws is another possibility and hugging phase is also possibility and hugging phase is also space is also another possibility hiding space is also another possibility hiding face spaces is also providing some face spaces is also providing some free resources and also they are free resources and also they are allowing you to deploy doer allowing you to deploy doer containers for free up to the containers for free up to the resource constraints that i impose", "image_path": "img_data/video_09_chunk_9.jpg"}
{"video": "video_09", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "containers for free up to the resource constraints that i impose resource constraints that i impose this is also another recommended this is also another recommended computing resource we don't recommend computing resource we don't recommend kaggle not because kagle is not kaggle not because kagle is not important kagle is def facto destination important kagle is def facto destination for accessing competitions and data for accessing competitions and data these days but definitely the these days but definitely the computer environment of kagle is subar computer environment of kagle is subar to the google collaborator that you to the google collaborator that you actually have over here at least in actually have over here at least in terms of compute just focus on google terms of compute just focus on google collab if you are to limit your yourself collab if you are to limit your yourself to just running", "image_path": "img_data/video_09_chunk_10.jpg"}
{"video": "video_09", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "collab if you are to limit your yourself to just running notebooks this is basically notebooks this is basically the some other instructions about the some other instructions about setting up local development setting up local development environments and also some instructions environments and also some instructions on how to get familiar with kid you need on how to get familiar with kid you need to have a github account to be to have a github account to be successful in this course and to be successful in this course and to be able to submit your assignments which able to submit your assignments which i'll come in a moment and there is i'll come in a moment and there is for some of the books that we recommend for some of the books that we recommend they mind you that there is code they mind you that there is code available you want to clone or fork", "image_path": "img_data/video_09_chunk_11.jpg"}
{"video": "video_09", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "they mind you that there is code available you want to clone or fork available you want to clone or fork those repositories and be familiar with repositories and be familiar with them because you may want to step them because you may want to step through and you need to step actually be through and you need to step actually be stepping through the code after each stepping through the code after each lecture in order for you to absorb lecture in order for you to absorb the python implementations that we quote the python implementations that we quote or we go through that's or we go through that's basically it with respect to the basically it with respect to the environment we don't have anything else environment we don't have anything else the development environment and we the development environment and we also have another resource here that i also have another resource here that i will walk you through what you need", "image_path": "img_data/video_09_chunk_12.jpg"}
{"video": "video_09", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "also have another resource here that i will walk you through what you need will walk you through what you need to do in order for you to submit an to do in order for you to submit an assignment typically we would require assignment typically we would require you to set up a github repository once you to set up a github repository once for the specific course at the beginning for the specific course at the beginning of the semester and you commit into that of the semester and you commit into that github repository in folders your github repository in folders your assignments and maybe the project assignments and maybe the project requires its own github repositor requires its own github repositor you effectively at the end you have one you effectively at the end you have one for the assignments and another for your for the assignments and another for your project the format you won't be", "image_path": "img_data/video_09_chunk_13.jpg"}
{"video": "video_09", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "for the assignments and another for your project the format you won't be project the format you won't be including there may range from plain including there may range from plain python files to markdown files to images python files to markdown files to images and notebooks and specifically for and notebooks and specifically for notebooks if you are running it in notebooks if you are running it in collab then we also describe here how collab then we also describe here how you would commit those how you would commit those how you would add collaborators with a share button collaborators with a share button over there to be you collaborators over there to be you collaborators your collaborate with your ta that your collaborate with your ta that when the ta get the link on the collab", "image_path": "img_data/video_09_chunk_14.jpg"}
{"video": "video_09", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "your collaborate with your ta that when the ta get the link on the collab when the ta get the link on the collab then they will be able to open it and then they will be able to open it and grade your assignment that's grade your assignment that's basically all it's needed and it is basically all it's needed and it is described here evidently we need you to described here evidently we need you to take and copy and paste that url into take and copy and paste that url into bright space in order for us to be able bright space in order for us to be able to know where your url is and be able to know where your url is and be able to grade it that's not really grade it that's not really much here but there is some kind of much here but there is some kind of process of getting at least set up process of getting at least set up initially and i wanted to cover now also", "image_path": "img_data/video_09_chunk_15.jpg"}
{"video": "video_09", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "process of getting at least set up initially and i wanted to cover now also initially and i wanted to cover now also some kind of requirements we do some kind of requirements we do require you to know python for this require you to know python for this course and definitely there is a course and definitely there is a very nice free book for python if very nice free book for python if you are kind of a more book oriented and you are kind of a more book oriented and you want to read something but there is you want to read something but there is also certain very popular also certain very popular courses out there free of charge one courses out there free of charge one probably the best one to start if you do probably the best one to start if you do not know basic python is harvard cs not know basic python is harvard cs 50p p stands for python it's the you", "image_path": "img_data/video_09_chunk_16.jpg"}
{"video": "video_09", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "not know basic python is harvard cs 50p p stands for python it's the you 50p p stands for python it's the familiar know familiar cs50 but for the python cs50 but for the python version and is accessible also here version and is accessible also here including some slide content and including some slide content and videos as well also on the edx platform videos as well also on the edx platform which you can also install in your which you can also install in your mobile device and or mobile device and or ipad there are some other links over ipad there are some other links over here that we have included there here that we have included there is courses that we have", "image_path": "img_data/video_09_chunk_17.jpg"}
{"video": "video_09", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "here that we have included there is courses that we have is courses that we have found to be useful for students some found to be useful for students some of them are paid but definitely they of them are paid but definitely they have free trials which you could have free trials which you could exercise because you only need to go exercise because you only need to go through certain modules and through certain modules and therefore we have included some of those therefore we have included some of those here all right then some numerical here all right then some numerical version of pythons apart from the basic version of pythons apart from the basic python there is if you some python there is if you some numerical python that you need to be numerical python that you need to be familiar more specifically you need to familiar more specifically you need to be familiar with", "image_path": "img_data/video_09_chunk_18.jpg"}
{"video": "video_09", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "familiar more specifically you need to be familiar with napai which is the numerical python and napai which is the numerical python and you also need to be familiar with you also need to be familiar with pandas apis now pandas themselves are pandas apis now pandas themselves are needed definitely they are not right needed definitely they are not right now at least in the version one the now at least in the version one the fastest data frame kind of fastest data frame kind of implementation there's also polers which implementation there's also polers which offers a compartible to panda api offers a compartible to panda api other pandas or polar is fine but other pandas or polar is fine but definitely pandas is almost the definitely pandas is almost the def facto standard these days and", "image_path": "img_data/video_09_chunk_19.jpg"}
{"video": "video_09", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "definitely pandas is almost the def facto standard these days and def facto standard these days and that's the second library naai and that's the second library naai and pandas are the two li going to be and pandas are the two li going to be familiar with to be able to just you familiar with to be able to just bring in data and manipulate data know bring in data and manipulate data for some of the assignments that may be for some of the assignments that may be needed and as i said you for needed and as i said you for neural networks you probably need to select networks you probably need to select either t of low caras or either t of low caras or pyto and either of the two will be pyto and either of the two will be very sufficient for the needs of", "image_path": "img_data/video_09_chunk_20.jpg"}
{"video": "video_09", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "pyto and either of the two will be very sufficient for the needs of very sufficient for the needs of this project we don't recommend jax at this project we don't recommend jax at this point and in fact jax is going to this point and in fact jax is going to be removed from this website as it is be removed from this website as it is another framework that it is another framework that it is right now it was suggested kind of right now it was suggested kind of earlier for some kind of almost from earlier for some kind of almost from scratch implementations but then we have scratch implementations but then we have kind of migrated our thinking to use kind of migrated our thinking to use just caras and ps all right just caras and ps all right there are also some kind of other there are also some kind of other additional resource obviously you", "image_path": "img_data/video_09_chunk_21.jpg"}
{"video": "video_09", "start": "0:11:00", "end": "0:11:27.166667", "timestamp": "0:11:00 - 0:11:27.166667", "text": "there are also some kind of other additional resource obviously you additional resource obviously you probably know already lead code that can probably know already lead code that can prepare you for interviews and on prepare you for interviews and on that's basically all with respect that's basically all with respect to your programming environment python to your programming environment python and how to submit assignments and how to submit assignments and projects and in a separate kind and projects and in a separate kind of video we will actually go through of video we will actually go through some prerequisites more specifically some prerequisites more specifically some probability kind of concepts to some probability kind of concepts to get you started", "image_path": "img_data/video_09_chunk_22.jpg"}
{"video": "video_10", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now let's start with the syllabus starting the first discussion with starting the first discussion with the books the first book it's almost the books the first book it's almost a de facto standard to start your ai a de facto standard to start your ai journey it's now on the fourth edition journey it's now on the fourth edition which is the one i'm recommending if which is the one i'm recommending if you're buying new if you can find the you're buying new if you can find the third edition used i understand that third edition used i understand that the cost of this book is quite the cost of this book is quite significant it didn't used to be significant it didn't used to be expensive but now i think it's something expensive but now i think it's something $200", "image_path": "img_data/video_10_chunk_0.jpg"}
{"video": "video_10", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "expensive but now i think it's something $200 $200 it's a massive book of more than it's a massive book of more than 1, 100 pages or about that u and it's 1, 100 pages or about that u and it's a very well written and probably one of a very well written and probably one of the best books in your library that you the best books in your library that you will use over and over again and this will use over and over again and this book is a must get either the third book is a must get either the third edition or the fourth edition and edition or the fourth edition and sometimes the third edition is heavily sometimes the third edition is heavily discounted on various kind of discounted on various kind of outlets the second book is i don't", "image_path": "img_data/video_10_chunk_1.jpg"}
{"video": "video_10", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "discounted on various kind of outlets the second book is i don't outlets the second book is i don't have a paper copy of that i have a paper copy of that i have electronic copy it's available to electronic copy it's available to you free of charge as every nyu you free of charge as every nyu student has access to the library and student has access to the library and the library has offering an the library has offering an amazing service which is the amazing service which is the socaled oril safari books this socaled oril safari books this service i will show you in a moment how service i will show you in a moment how to get there but you can actually go to get there but you can actually go to this website over here", "image_path": "img_data/video_10_chunk_2.jpg"}
{"video": "video_10", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "to get there but you can actually go to this website over here to this website over here this one over here and it will ask you to log and it will ask you to log in with your credentials after you log in you will be taken to this resource in you will be taken to this resource over here", "image_path": "img_data/video_10_chunk_3.jpg"}
{"video": "video_10", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "in you will be taken to this resource over here the or learning platform which literally has thousands of books and all has thousands of books and all sorts of books especially with respect sorts of books especially with respect to computer science programming hands-on to computer science programming hands-on books that is most but also some books that is most but also some others and we are looking at the book others and we are looking at the book which is called handson machine learning with called handson machine learning with psychic learn cara and ter of low now psychic learn cara and ter of low now apparently there is a", "image_path": "img_data/video_10_chunk_4.jpg"}
{"video": "video_10", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "psychic learn cara and ter of low now apparently there is a apparently there is a 500 error over here but that's 500 error over here but that's the book that you will be the book that you will be accessing i'm not sure why the error accessing i'm not sure why the error is happening but it will be the is happening but it will be the book which is currently in the third book which is currently in the third edition and it's also a bestselling edition and it's also a bestselling book right now in amazon and other book right now in amazon and other outlets it is hardcoded to tensor outlets it is hardcoded to tensor flow and caras and evidently we have flow and caras and evidently we have the flexibility in the course to the flexibility in the course to accommodate all any framework obviously", "image_path": "img_data/video_10_chunk_5.jpg"}
{"video": "video_10", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "the flexibility in the course to accommodate all any framework obviously accommodate all any framework obviously pytorch and therefore there are other pytorch and therefore there are other books such as the book deep learning books such as the book deep learning with pytorch which is actually this one with pytorch which is actually this one over here and i'm not sure if the over here and i'm not sure if the font is large for you to be able to font is large for you to be able to see but definitely this is a book see but definitely this is a book that is an alternative and going that is an alternative and going back to the site there is however my back to the site there is however my experience is that compared to the", "image_path": "img_data/video_10_chunk_6.jpg"}
{"video": "video_10", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "back to the site there is however my experience is that compared to the experience is that compared to the first one then every other pytorch book first one then every other pytorch book that i saw may not really cut it with that i saw may not really cut it with respect to the depth of the material respect to the depth of the material that is been presented even if you that is been presented even if you are in the pytorch space and you are in the pytorch space and you are familiar with pytorch i do suggest familiar with pytorch i do suggest accessing this book in any case they are accessing this book in any case they are free of charge for you and the third free of charge for you and the third book is free of charge for everyone book is free of charge for everyone outside of the pay wall of safari and it outside of the pay wall of safari and it is sometimes used as an alternative to is sometimes used as an alternative to the second book and it's a book", "image_path": "img_data/video_10_chunk_7.jpg"}
{"video": "video_10", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "is sometimes used as an alternative to the second book and it's a book the second book and it's a book that i'm some somewhat kind of familiar that i'm some somewhat kind of familiar but not i'm not really using it but not i'm not really using it extensively in terms of references as i extensively in terms of references as i teach and it's accessible in this teach and it's accessible in this website the dl. this is a bit more practical dl. this is a bit more practical oriented and but i'm very familiar oriented and but i'm very familiar with the deep learning book.org which is with the deep learning book.org which is written by ian goodfellow and other written by ian goodfellow and other sort of significant personalities and sort of significant personalities and deep learning community and i do deep learning community and i do excessively", "image_path": "img_data/video_10_chunk_8.jpg"}
{"video": "video_10", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "deep learning community and i do excessively extensively kind of make reference to extensively kind of make reference to this book as well as in the notes as this book as well as in the notes as well also during lectures a few words well also during lectures a few words about exactly the stages that i referred about exactly the stages that i referred to in another video that we to in another video that we start with kind of a sequentially to how start with kind of a sequentially to how sensing information from various sensors sensing information from various sensors cameras lighter and on are kind of cameras lighter and on are kind of processed inside an agent the processed inside an agent the processing is actually happening on this processing is actually happening on this first stage which is called perception", "image_path": "img_data/video_10_chunk_9.jpg"}
{"video": "video_10", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "processing is actually happening on this first stage which is called perception first stage which is called perception system and the output of this kind of system and the output of this kind of perception system is to produce a perception system is to produce a sequence of predictions a stream of sequence of predictions a stream of predictions about let's say the location predictions about let's say the location of the objects that are let's say in the of the objects that are let's say in the environment of the agent that the environment of the agent that the agent has a kind of an understanding of agent has a kind of an understanding of what is really is facing and is be able what is really is facing and is be able to subsequently subsequent stages plan to subsequently subsequent stages plan and act we have divided effectively and act we have divided effectively into five lectures it's a fairly sizable", "image_path": "img_data/video_10_chunk_10.jpg"}
{"video": "video_10", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "and act we have divided effectively into five lectures it's a fairly sizable into five lectures it's a fairly sizable set of material that will start you set of material that will start you from the foundations of statistical from the foundations of statistical learning theory and by statistical learning theory and by statistical learning theory i'm mainly referring learning theory i'm mainly referring to the godfather of referring to the godfather of statistical learning theory vapnik and statistical learning theory vapnik and we will start with the first we will start with the first principles of the regression problem and principles of the regression problem and the classification problem and the core the classification problem and the core concept of maximum likelihood estimation concept of maximum likelihood estimation and then we will and generalize this", "image_path": "img_data/video_10_chunk_11.jpg"}
{"video": "video_10", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "concept of maximum likelihood estimation and then we will and generalize this and then we will and generalize this linear and generalized linear kind of linear and generalized linear kind of methods into nonlinear methods and we'll methods into nonlinear methods and we'll be implementing those with neural be implementing those with neural networks aiming to finally get into the networks aiming to finally get into the point where we're able to do at le at point where we're able to do at le at the very least object the very least object detection and we kind of limited detection and we kind of limited discussion to sin understanding to just discussion to sin understanding to just detecting objects in our environment and detecting objects in our environment and we are sort of subsequently getting into the second subsequently getting into the second part which is reason", "image_path": "img_data/video_10_chunk_12.jpg"}
{"video": "video_10", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "subsequently getting into the second part which is reason part which is reason and in that kind of reasoning part we and in that kind of reasoning part we start with where the perception of start with where the perception of system left and the various kind system left and the various kind of object detectors kind of left it of object detectors kind of left it where they just predict u locations of where they just predict u locations of the various objects surrounding us the various objects surrounding us however they are of reflexive nature and however they are of reflexive nature and therefore every prediction could therefore every prediction could actually be noisy and by noisy i mean actually be noisy and by noisy i mean that in one in frame of a video sequence that in one in frame of a video sequence the objects may appear but maybe", "image_path": "img_data/video_10_chunk_13.jpg"}
{"video": "video_10", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "that in one in frame of a video sequence the objects may appear but maybe the objects may appear but maybe later disappear in the subsequent few later disappear in the subsequent few frames because of some kind of random frames because of some kind of random lighting fluctuation has happened in lighting fluctuation has happened in the system we need probabilistic the system we need probabilistic reasoning over time u effectively reasoning over time u effectively tracking capabilities to be able to tracking capabilities to be able to track the movements and the mobility of track the movements and the mobility of the various objects and to correct for the various objects and to correct for instances such as occlusions in fact instances such as occlusions in fact for occlusions we actually may need to for occlusions we actually may need to go into logical reasoning which is a", "image_path": "img_data/video_10_chunk_14.jpg"}
{"video": "video_10", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "for occlusions we actually may need to go into logical reasoning which is a go into logical reasoning which is a higher level reasoning where the higher level reasoning where the representation now become symbolic and representation now become symbolic and because of that symbolic representation because of that symbolic representation we actually can actually accommodate we actually can actually accommodate instances where we have occlusions for instances where we have occlusions for much longer periods of time but in other much longer periods of time but in other domains as well let's say cyber security domains as well let's say cyber security logical reasoning is also extensively logical reasoning is also extensively used because we are able to represent used because we are able to represent and answer queries at a logical and answer queries at a logical level we're able to represent let's say level we're able to represent let's say a cloud deployment a years and other", "image_path": "img_data/video_10_chunk_15.jpg"}
{"video": "video_10", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "level we're able to represent let's say a cloud deployment a years and other a cloud deployment a years and other cloud providers are making heavy h cloud providers are making heavy h usage of logical reasoning in their cber usage of logical reasoning in their cber security systems for example the kind security systems for example the kind of second part sort of finishes with of second part sort of finishes with this kind of presentation of the various this kind of presentation of the various hierarchical kind of view of hierarchical kind of view of reasoning and the third part kind of reasoning and the third part kind of starts to discuss fine we have now starts to discuss fine we have now a good idea about where we are and about a good idea about where we are and about the environment and the question", "image_path": "img_data/video_10_chunk_16.jpg"}
{"video": "video_10", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "a good idea about where we are and about the environment and the question the environment and the question now becomes what do we do about it you now becomes what do we do about it how do we use those and of course know how do we use those and of course we use those in developing a plan we use those in developing a plan planning is itself divided into planning is itself divided into three stages a three lectures the first lecture kind of lectures the first lecture kind of starts with planning with out starts with planning with out interaction and in that kind of interaction and in that kind of discussion we are starting from you discussion we are starting from problems where we do not need much know problems where we do not need much of a subsequent i mean detailed", "image_path": "img_data/video_10_chunk_17.jpg"}
{"video": "video_10", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "know problems where we do not need much of a subsequent i mean detailed of a subsequent i mean detailed information about the state of the information about the state of the environment maybe an atomic state such environment maybe an atomic state such as what is being used in planning let's as what is being used in planning let's say a route from point a to point b is say a route from point a to point b is enough to more complicated approaches enough to more complicated approaches where we do need that kind of additional where we do need that kind of additional kind of sort of information of the kind of sort of information of the object detectors kind of provide we object detectors kind of provide we will be limiting ourselves to in that will be limiting ourselves to in that kind of first lecture to without kind of first lecture to without interacting with the environment and we interacting with the environment and we will be expanding them with the you", "image_path": "img_data/video_10_chunk_18.jpg"}
{"video": "video_10", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "interacting with the environment and we will be expanding them with the you will be expanding them with the cornerstone of reinforcement know cornerstone of reinforcement learning which is mark of decision learning which is mark of decision processes mark of decision process processes mark of decision process are being used extensively in problems are being used extensively in problems that we do know the dynamics of the that we do know the dynamics of the environment or all the probability environment or all the probability distributions that we will need to distributions that we will need to actually solve the problem for actually solve the problem for example take the way that we are example take the way that we are reserving tickets in a ticket reserving tickets in a ticket system an airline ticketing system an airline ticketing system we actually the whole problem can be", "image_path": "img_data/video_10_chunk_19.jpg"}
{"video": "video_10", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "system an airline ticketing system we actually the whole problem can be we actually the whole problem can be posed as an mdp problem where we are posed as an mdp problem where we are trying to manage from the point of trying to manage from the point of view of an airline our resources the view of an airline our resources the seats and fairs develop if seats and fairs develop if you fair classes the same you fair classes the same problem is posed as in problems problem is posed as in problems car rentals and a myriad of other car rentals and a myriad of other problems reinforcement learning problems reinforcement learning discussion starts with when we do know discussion starts with when we do know all the dynamics and that is called mark all the dynamics and that is called mark of decision processes and then it", "image_path": "img_data/video_10_chunk_20.jpg"}
{"video": "video_10", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "all the dynamics and that is called mark of decision processes and then it of decision processes and then it evolves into when we don't know the evolves into when we don't know the dynamics and then in that case we are dynamics and then in that case we are looking at algorithms that are able looking at algorithms that are able to learn those dynamics and act help us to learn those dynamics and act help us act optimally now the final part is reserve for a now the final part is reserve for a discussion on natural language discussion on natural language processing ultimately the course will evolve where ultimately the course will evolve where natural language processing is going to natural language processing is going to be integrated in into the rest of the be integrated in into the rest of the discussion we had up to now and but", "image_path": "img_data/video_10_chunk_21.jpg"}
{"video": "video_10", "start": "0:11:00", "end": "0:11:26.833333", "timestamp": "0:11:00 - 0:11:26.833333", "text": "be integrated in into the rest of the discussion we had up to now and but discussion we had up to now and but still we need this kind of an still we need this kind of an background because in many other background because in many other problems the agent may take the form problems the agent may take the form let's say of a bot they may actually let's say of a bot they may actually take the form of a chat system where you take the form of a chat system where you are trying to get to extract as much are trying to get to extract as much information out of a person who is information out of a person who is interacting with it as possible and interacting with it as possible and still satisfy the utility of the end still satisfy the utility of the end user the human in this case", "image_path": "img_data/video_10_chunk_22.jpg"}
{"video": "video_12", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in an earlier video we have seen the prediction methods associated with prediction methods associated with reinforcement learning and more reinforcement learning and more specifically we saw both mon carlo and specifically we saw both mon carlo and how the temporal difference method how the temporal difference method manages to reduce the mon carlo variance manages to reduce the mon carlo variance at the same time picking up an increased at the same time picking up an increased bias relative to pure mon carlo kind of bias relative to pure mon carlo kind of methods and also providing us the methods and also providing us the advantage of online learning now to advantage of online learning now to learn an optimal policy will continue to", "image_path": "img_data/video_12_chunk_0.jpg"}
{"video": "video_12", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "advantage of online learning now to learn an optimal policy will continue to learn an optimal policy will continue to engage what we have seen in the mdp engage what we have seen in the mdp videos as what an algorithm actually videos as what an algorithm actually called polic iteration and we actually called polic iteration and we actually we will be generalizing it in the we will be generalizing it in the following sense in the mtp u discussion following sense in the mtp u discussion we have been working towards evalua we have been working towards evalua evaluation of a value function either evaluation of a value function either the v or the q and then after we the v or the q and then after we evaluated the function we were evaluated the function we were determining with a gritty step and determining with a gritty step and an optimal policy and gradually we", "image_path": "img_data/video_12_chunk_1.jpg"}
{"video": "video_12", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "determining with a gritty step and an optimal policy and gradually we an optimal policy and gradually we will basically alternating between an will basically alternating between an evaluation and an improvement step and evaluation and an improvement step and then ultimately we have seen that this then ultimately we have seen that this is a contraction and this actually led is a contraction and this actually led us to an optimal to a convergence to an us to an optimal to a convergence to an towards an optimal policy over here what towards an optimal policy over here what we will do is we will generalize we will do is we will generalize that kind of polic iteration in a couple that kind of polic iteration in a couple of things which i want to write of things which i want to write down in gpi that covers a whole range of down in gpi that covers a whole range of reinforcement learning kind of", "image_path": "img_data/video_12_chunk_2.jpg"}
{"video": "video_12", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "down in gpi that covers a whole range of reinforcement learning kind of reinforcement learning kind of algorithms instead of kind of algorithms instead of kind of let's represent this kind of contraction let's represent this kind of contraction towards the sort of other cube star towards the sort of other cube star or the p star the optimal kind of or the p star the optimal kind of policy and the optimal kind of value policy and the optimal kind of value function that we are interested in function that we are interested in instead of actually taking starting from instead of actually taking starting from some kind of initial q value and some kind of initial q value and some kind of a policy and we always kind of a policy and we always kind of strive", "image_path": "img_data/video_12_chunk_3.jpg"}
{"video": "video_12", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "kind of a policy and we always kind of strive to sort of estimate a q function and to sort of estimate a q function and then use this kind of q function to then use this kind of q function to greedily u form if you a policy greedily u form if you a policy this is basically my q1 for the first this is basically my q1 for the first generation that's kind of my pi1 you generation that's kind of my pi1 know q2 and pi 2 something that we have seen q2 and pi 2 something that we have seen in poish iteration and gradually kind of in poish iteration and gradually kind of converging into the optimal value converging into the optimal value function the optimal kind of policy function the optimal kind of policy what we will actually be happening over", "image_path": "img_data/video_12_chunk_4.jpg"}
{"video": "video_12", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "function the optimal kind of policy what we will actually be happening over what we will actually be happening over here is that we will take here is that we will take incomplete steps we will generalize the policy iteration to allow us to", "image_path": "img_data/video_12_chunk_5.jpg"}
{"video": "video_12", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "allow us to take incomplete steps towards its goal which the goal is the estimate of goal which the goal is the estimate of the value function and the kind of the value function and the kind of policy this means that we actually policy this means that we actually will see that we may be starting from will see that we may be starting from let's say some kind of a policy over", "image_path": "img_data/video_12_chunk_6.jpg"}
{"video": "video_12", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "will see that we may be starting from let's say some kind of a policy over let's say some kind of a policy over here and work ourselves towards if here and work ourselves towards if you a value function but this value you a value function but this value function may not necessarily be on that function may not necessarily be on that kind of trajectory and then take kind of trajectory and then take kind of greedy steps to go back to some kind of greedy steps to go back to some kind of policy and on but still be policy and on but still be able to convert as we go towards if you able to convert as we go towards if you the sort of final determination of the value function and determination of the value function and the optimal kind of policy this kind the optimal kind of policy this kind of a general ization will and the", "image_path": "img_data/video_12_chunk_7.jpg"}
{"video": "video_12", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "the optimal kind of policy this kind of a general ization will and the of a general ization will and the proof that this converges is basically proof that this converges is basically what gpi is actually will do for what gpi is actually will do for us and as we'll see the algorith we us and as we'll see the algorith we will be dealing with which will be dealing with which will be called model free control come under called model free control come under that kind of umbrella another key that kind of umbrella another key distinguishing attribute of various kind distinguishing attribute of various kind of model free control algorithms is of model free control algorithms is whether or not they're working with the whether or not they're working with the -called on policy assumption", "image_path": "img_data/video_12_chunk_8.jpg"}
{"video": "video_12", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "whether or not they're working with the -called on policy assumption and in the on policy assumption is that we have the same policy transitioning the environment and learning", "image_path": "img_data/video_12_chunk_9.jpg"}
{"video": "video_12", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "environment and learning how to control optimally the agent this is the typical letter that we have used pi for this kind of policy we have used pi for this kind of policy and to contrast that we have the and to contrast that we have the -called off policy methods where we", "image_path": "img_data/video_12_chunk_10.jpg"}
{"video": "video_12", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "methods where we have different policy to transition the environment and therefore generate trajectories sort of episodes", "image_path": "img_data/video_12_chunk_11.jpg"}
{"video": "video_12", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "generate trajectories sort of episodes ptc this policy will be called behavioral policy and will be symbolized by new", "image_path": "img_data/video_12_chunk_12.jpg"}
{"video": "video_12", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "new compared we have a different policy compared we have a different policy transition the environment and from the policy we learn to act and this will be actually be called act and this will be actually be called the target policy", "image_path": "img_data/video_12_chunk_13.jpg"}
{"video": "video_12", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "act and this will be actually be called the target policy pi the reason we actually have these two type of algorithms control algorithms type of algorithms control algorithms is that in many instances we actually have found that instances we actually have found that sort of off policy methods are can offer sort of off policy methods are can offer the an advantage compared to one the an advantage compared to one policy methods and they have been policy methods and they have been extensively used when we let's say would extensively used when we let's say would to mimic a certain behavior that to mimic a certain behavior that we've see from another agent when we've see from another agent when we actually have the requirement to", "image_path": "img_data/video_12_chunk_14.jpg"}
{"video": "video_12", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "we've see from another agent when we actually have the requirement to actually have the requirement to explore far more in the environment than explore far more in the environment than what potentially the target policy what potentially the target policy allows us by distinguishing by allows us by distinguishing by differentiating between two different differentiating between two different policies the behavioral policy that policies the behavioral policy that generates experiences and the target generates experiences and the target policy we actually able to satisfy both policy we actually able to satisfy both of those requirements the first method we will requirements the first method we will actually see and the m def control is actually see and the m def control is called epsilon gy monal control and called epsilon gy monal control and despite it kind of complicated name it despite it kind of complicated name it end ends up being a direct extension of", "image_path": "img_data/video_12_chunk_15.jpg"}
{"video": "video_12", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "despite it kind of complicated name it end ends up being a direct extension of end ends up being a direct extension of what we have seen first the monte what we have seen first the monte carlo estimation method coupled with a greedy policy method coupled with a greedy policy but it's this greedy policy has a but it's this greedy policy has a kind of twist as a kind of reminder kind of twist as a kind of reminder in that video on when we discussed mon in that video on when we discussed mon carlo estimation of the value carlo estimation of the value function we have focused our attention function we have focused our attention to estimating the state value to estimating the state value function and the state value function and the state value function was updated on a per episode basis", "image_path": "img_data/video_12_chunk_16.jpg"}
{"video": "video_12", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "function and the state value function was updated on a per episode basis was updated on a per episode basis because only at the end of this episode because only at the end of this episode we had the quantity which we actually we had the quantity which we actually needed for the estimation of the state needed for the estimation of the state value function which was basically the value function which was basically the actual return the gt as we actually call actual return the gt as we actually call it over here now there is going to be a it over here now there is going to be a problem with applying control problem with applying control policy because at the end of the day policy because at the end of the day we can actually cannot take a gry step we can actually cannot take a gry step when we have state because at the end when we have state because at the end of the day we need an action state of the day we need an action state value function we need a q function in", "image_path": "img_data/video_12_chunk_17.jpg"}
{"video": "video_12", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "of the day we need an action state value function we need a q function in value function we need a q function in order to actually apply any control order to actually apply any control policy only when we know the an estimate policy only when we know the an estimate of the q function that basically of the q function that basically conveys the information as to how viable conveys the information as to how viable an action we have taken at a specific an action we have taken at a specific state is only then we can apply a state is only then we can apply a control policy what we can control policy what we can actually go back to this actually go back to this section we're discussing this is the section we're discussing this is the reason why we need to", "image_path": "img_data/video_12_chunk_18.jpg"}
{"video": "video_12", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "section we're discussing this is the reason why we need to reason why we need to apply a greedy step however at the apply a greedy step however at the same time if we are applying a grey step same time if we are applying a grey step on actions out of the same policy if on actions out of the same policy if this kind of policy is deterministic you this kind of policy is deterministic you may actually understand that the actions may actually understand that the actions we are selecting at every moment at we are selecting at every moment at every state is going to be one and every state is going to be one and the same for no matter how much the same for no matter how much kind of experience we're experience kind of experience we're generating there is an explo we're generating there is an explo ation issue that we are trying to solve", "image_path": "img_data/video_12_chunk_19.jpg"}
{"video": "video_12", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "we're generating there is an explo ation issue that we are trying to solve ation issue that we are trying to solve with this twist if you on that with this twist if you on that kind of gridy step we can definitely kind of gridy step we can definitely take a gitty step but what we will do is take a gitty step but what we will do is we will modify slightly the gridy step we will modify slightly the gridy step to introduce some form of to introduce some form of randomization the introduce if you randomization the introduce if you a constant you can imagine it a constant you can imagine it as being kind of a coin tossing that as being kind of a coin tossing that we do for every gitty step when the with we do for every gitty step when the with a coin which is bent with a coin which is bent with the bias epsilon and every time that", "image_path": "img_data/video_12_chunk_20.jpg"}
{"video": "video_12", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "a with a coin which is bent with the bias epsilon and every time that the bias epsilon and every time that it is the probability of it is the probability of epsilon is the probability of coming ups epsilon is the probability of coming ups with heads and 1 minus epsilon is always with heads and 1 minus epsilon is always the probability of coming ups with tails the probability of coming ups with tails every time that we take the every time that we take the that comes up with heads then we are that comes up with heads then we are do not follow the greedy policy do not follow the greedy policy and we are selecting an action out of and we are selecting an action out of the remaining actions that we have", "image_path": "img_data/video_12_chunk_21.jpg"}
{"video": "video_12", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "and we are selecting an action out of the remaining actions that we have the remaining actions that we have available at every state we are available at every state we are effectively randomizing that kind of action randomizing that kind of action selection with a probability of epsilon selection with a probability of epsilon and when we have 1 minus epsilon in and when we have 1 minus epsilon in other words when we have tals then we other words when we have tals then we are following the gitty action and we are following the gitty action and we are selecting if you the action are selecting if you the action that corresponds in other words to the that corresponds in other words to the maximum possible two functions out of maximum possible two functions out of the ones that we have available to us the ones that we have available to us that is in a broad terms what the", "image_path": "img_data/video_12_chunk_22.jpg"}
{"video": "video_12", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "the ones that we have available to us that is in a broad terms what the that is in a broad terms what the greedy approach is and the u what greedy approach is and the u what is actually that second step as i said is actually that second step as i said as i mentioned that policy as i mentioned that policy improvement step is actually giving us improvement step is actually giving us more exploration as we go along we more exploration as we go along we have given us opportunities to see a have given us opportunities to see a more diverse set of experiences more diverse set of experiences although it is going to be a compromise although it is going to be a compromise that something that we will solve a bit that something that we will solve a bit later with another approach actually later with another approach actually called sara and in that approach it", "image_path": "img_data/video_12_chunk_23.jpg"}
{"video": "video_12", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "later with another approach actually called sara and in that approach it called sara and in that approach it will actually be we actually going will actually be we actually going to recover both the benefits of the to recover both the benefits of the monte carlo and the temporal difference monte carlo and the temporal difference methods this is the u however methods this is the u however before we go there this is the flow before we go there this is the flow of the epsilon greedy monte carlo of the epsilon greedy monte carlo we are effectively generating the we are effectively generating the episode just we have been doing in episode just we have been doing in any monte carlo method we are u sort of any monte carlo method we are u sort of for each step of the episode we", "image_path": "img_data/video_12_chunk_24.jpg"}
{"video": "video_12", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "any monte carlo method we are u sort of for each step of the episode we for each step of the episode we are computing the return and we are computing the return and we append the returns and to calculate append the returns and to calculate if you the average of the returns if you the average of the returns this is basically the q function and this is basically the q function and this is the corresponding kind of backup this is the corresponding kind of backup three for estimating the q function three for estimating the q function that we have seen before and over that we have seen before and over here we are determining if you the here we are determining if you the optimal action and but before we", "image_path": "img_data/video_12_chunk_25.jpg"}
{"video": "video_12", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "here we are determining if you the optimal action and but before we optimal action and but before we taking this action before we actually taking this action before we actually setting up the policy based on this setting up the policy based on this maximum action we are flipping a point maximum action we are flipping a point and if it is epsilon we are and if it is epsilon we are randomizing the action and when it's randomizing the action and when it's minus one minus epsilon then we are minus one minus epsilon then we are indeed taking the a star the optimal indeed taking the a star the optimal kind of action this is the gist of kind of action this is the gist of the algorith it's consist of estimation the algorith it's consist of estimation step that follows if you the", "image_path": "img_data/video_12_chunk_26.jpg"}
{"video": "video_12", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "the algorith it's consist of estimation step that follows if you the step that follows if you the monte carlo prediction step that we have monte carlo prediction step that we have seen earlier and sort seen earlier and sort of a slightly different greedy step of a slightly different greedy step for policy improvement to effectively for policy improvement to effectively implement what we call generalized implement what we call generalized policy iteration and the interesting policy iteration and the interesting thing is that this algorith also has thing is that this algorith also has been shown to converge to an optimal been shown to converge to an optimal policy closing the discussion on the policy closing the discussion on the kind of model free control we will see kind of model free control we will see sara which is one of the most important sara which is one of the most important kind of reinforcement learning", "image_path": "img_data/video_12_chunk_27.jpg"}
{"video": "video_12", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "sara which is one of the most important kind of reinforcement learning kind of reinforcement learning algorithms that actually naturally flows algorithms that actually naturally flows out of this epsilon greedy monte carlo out of this epsilon greedy monte carlo and if i may kind of start the and if i may kind of start the discussion sara will involve two steps discussion sara will involve two steps will marry the u temporal will marry the u temporal difference method estimation or prediction of the q", "image_path": "img_data/video_12_chunk_28.jpg"}
{"video": "video_12", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "prediction of the q function that's the first thing and then of course the second will maintains the epsilon greedy policy improvement in all we are still in this generalized policy iteration kind this generalized policy iteration kind of umbrella of algorithms and u in the", "image_path": "img_data/video_12_chunk_29.jpg"}
{"video": "video_12", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "this generalized policy iteration kind of umbrella of algorithms and u in the of umbrella of algorithms and u in the mont carlo approach that we have seen in mont carlo approach that we have seen in before we have been kind of before we have been kind of generating kind of episodes we were generating kind of episodes we were estimating if you this value estimating if you this value function the q function over many function the q function over many episodes keeping tab of the states episodes keeping tab of the states and actions that we've been selecting and actions that we've been selecting for the policy that we wanted to for the policy that we wanted to evaluate over here we will evaluate over here we will migrate into the temporal difference", "image_path": "img_data/video_12_chunk_30.jpg"}
{"video": "video_12", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "evaluate over here we will migrate into the temporal difference migrate into the temporal difference for all the advantages we have listed in for all the advantages we have listed in the temporal difference prediction video the temporal difference prediction video discussion and for those who have not discussion and for those who have not seen that kind of video suggest you go seen that kind of video suggest you go and watch it and effectively the and watch it and effectively the tree that we will be forming the tree that we will be forming the update tree for updating the q update tree for updating the q function and therefore estimating it is function and therefore estimating it is going to start from let's say an a state going to start from let's say an a state that we have been and taking the act taking an action a", "image_path": "img_data/video_12_chunk_31.jpg"}
{"video": "video_12", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "that we have been and taking the act taking an action a and taking the act taking an action a this is basically the action before we were in some kind of a state s let me just start with a state then s let me just start with a state then start with a state s taking an action start with a state s taking an action committing to an action committing to an action a obviously we are going to be my", "image_path": "img_data/video_12_chunk_32.jpg"}
{"video": "video_12", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "committing to an action a obviously we are going to be my a obviously we are going to be my migrating into another migrating into another state s prime after and at state s prime after and at the same time receiving a reward r the same time receiving a reward r and at this new state we are and at this new state we are taking an action a prime what we will effectively do that do here is to for this specific s comma", "image_path": "img_data/video_12_chunk_33.jpg"}
{"video": "video_12", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "that do here is to for this specific s comma for this specific s comma a q function we will be using this q a q function we will be using this q function for s prime a prime to update function for s prime a prime to update it and we'll see now how using exactly it and we'll see now how using exactly the same formula we have seen in the td the same formula we have seen in the td prediction and that formula is as prediction and that formula is as follows this is a q of st a", "image_path": "img_data/video_12_chunk_34.jpg"}
{"video": "video_12", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "q of st a is equal to q of st a plus alpha rt + alpha rt + 1+ gamma q of st + 1 comma a t + q of st + 1 comma a t + 1 minus q of std a t and this is basically what we call the t and this is basically what we call the td", "image_path": "img_data/video_12_chunk_35.jpg"}
{"video": "video_12", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "t and this is basically what we call the td target and that's basically the s account of update which kind of account of update which kind of naturally flows as given the td naturally flows as given the td discussion the temporal difference discussion the temporal difference discussion we had effectively the discussion we had effectively the state sorry the state action value state sorry the state action value function or q function is updated function or q function is updated from the earlier one the earlier value from the earlier one the earlier value it has by alpha which is it has by alpha which is a small number in the direction of the", "image_path": "img_data/video_12_chunk_36.jpg"}
{"video": "video_12", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "it has by alpha which is a small number in the direction of the a small number in the direction of the -called td error and that's the td error the difference in other words between the q difference in other words between the q function that we have in memory and function that we have in memory and the -called dd target which is the -called dd target which is effectively the implements the it effectively the implements the it is really the backup given the state is really the backup given the state sd plus one that we call here s prime", "image_path": "img_data/video_12_chunk_37.jpg"}
{"video": "video_12", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "is really the backup given the state sd plus one that we call here s prime sd plus one that we call here s prime is the sort of st + s prime is the sort of st + one in fact we have been writing it in one in fact we have been writing it in kind of a reverse way this is st + 1 kind of a reverse way this is st + 1 is = to s prime a t + 1 is equal to a is = to s prime a t + 1 is equal to a prime and here we have st is equal to s prime and here we have st is equal to s and a is equal to a that's that", "image_path": "img_data/video_12_chunk_38.jpg"}
{"video": "video_12", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "prime and here we have st is equal to s and a is equal to a that's that and a is equal to a that's that is the sara update the second the is the sara update the second the maintaining if you the epsilon maintaining if you the epsilon gitty policy i'm going to write it here gitty policy i'm going to write it here with words this is basically an own with words this is basically an own policy we only have one policy here policy we only have one policy here pi we continually continue to estimate q of s comma", "image_path": "img_data/video_12_chunk_39.jpg"}
{"video": "video_12", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "q of s comma a and at the same time a change via the epsilon greedy policy improvement", "image_path": "img_data/video_12_chunk_40.jpg"}
{"video": "video_12", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "change via the epsilon greedy policy improvement the po the pi this is in the side you actually can see the general form of actually can see the general form of this kind of algorithm and let me take you to that algorithm and let me take you to that side now this is the general form it's now this is the general form it's worth noting that the sort of", "image_path": "img_data/video_12_chunk_41.jpg"}
{"video": "video_12", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "worth noting that the sort of convergence and this is basically one a convergence and this is basically one a good example of why we kind of good example of why we kind of introduced the generalization in this introduced the generalization in this kind of policy improvement notice with kind of policy improvement notice with the original kind of algorith of policy the original kind of algorith of policy improvement we were always been hitting improvement we were always been hitting this upper line that kind of sort of this upper line that kind of sort of represents if you a constraint represents if you a constraint in terms of the sort of likeing sort in terms of the sort of likeing sort of optimization linear programming and of optimization linear programming and on we see these kind of lines as", "image_path": "img_data/video_12_chunk_42.jpg"}
{"video": "video_12", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "of optimization linear programming and on we see these kind of lines as on we see these kind of lines as constraints and we never hit constraints and we never hit that because with the policy improvement that because with the policy improvement sorry with the estimate over here we sorry with the estimate over here we are doing the estimate online as we are doing the estimate online as every experience that goes by we are every experience that goes by we are doing an update and therefore the doing an update and therefore the epsilon gitty improvement over here is epsilon gitty improvement over here is actually happening from that update actually happening from that update that we have we are doing in the that we have we are doing in the previous time step every single time previous time step every single time step there is opportunity for update and", "image_path": "img_data/video_12_chunk_43.jpg"}
{"video": "video_12", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "previous time step every single time step there is opportunity for update and step there is opportunity for update and therefore an opportunity for doing this therefore an opportunity for doing this epsilon gitty improvement that's why we epsilon gitty improvement that's why we actually call sara an online actually call sara an online algorithm and this is basically the algorithm and this is basically the pseudo code for that we initialize the q that we initialize the q table this is a table with states and table this is a table with states and actions and we choose actions and we choose initialize we pick up a starting initialize we pick up a starting state and we are choosing if you an", "image_path": "img_data/video_12_chunk_44.jpg"}
{"video": "video_12", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "state and we are choosing if you an and we are choosing if you an action from s using whatever policy we action from s using whatever policy we initializing we are initializing it initializing we are initializing it with and we are repeating this with and we are repeating this is basically for each step of the is basically for each step of the episode we are doing that and of course episode we are doing that and of course we are updating the q function over many we are updating the q function over many episodes this is basically what we just discussed that's basically what we just discussed that's the q function update and", "image_path": "img_data/video_12_chunk_45.jpg"}
{"video": "video_12", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "basically what we just discussed that's the q function update and the q function update and the a prime here is selected out of the a prime here is selected out of the policy improvement what the policy improvement what the td target accommodates that kind of a td target accommodates that kind of a policy improvement step includes that policy improvement step includes that kind of policy improvement kind of policy improvement step all right this is basically step all right this is basically the sara kind of algorithm and i the sara kind of algorithm and i think it's worthwhile now looking at an think it's worthwhile now looking at an example where we in python where we example where we in python where we will basically see an", "image_path": "img_data/video_12_chunk_46.jpg"}
{"video": "video_12", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "example where we in python where we will basically see an will basically see an implementation for a specific implementation for a specific environment that we have brought here environment that we have brought here if you go to the course the repos of if you go to the course the repos of the course site you actually will see the course site you actually will see here under sara a directory called here under sara a directory called grid word and two and one grid word and two and one subdirectory called environment plus the subdirectory called environment plus the main driver program which is a python main driver program which is a python notebook and this environment that notebook and this environment that you see here is will actually involve you see here is will actually involve a grid world of 5x5", "image_path": "img_data/video_12_chunk_47.jpg"}
{"video": "video_12", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "you see here is will actually involve a grid world of 5x5 cells and in this grid world we cells and in this grid world we have u all possible access up down left have u all possible access up down left and right we will be exercising and right we will be exercising obviously the sarsa algorithm that we obviously the sarsa algorithm that we just discussed and the just discussed and the build canvas effectively builds the build canvas effectively builds this image over here this canvas this image over here this canvas where the agent is located and are", "image_path": "img_data/video_12_chunk_48.jpg"}
{"video": "video_12", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "this image over here this canvas where the agent is located and are where the agent is located and are certain rewards negative rewards and a certain rewards negative rewards and a positive reward on the specific cells of positive reward on the specific cells of course you can programmatically kind of course you can programmatically kind of set the rewards at any cell you want set the rewards at any cell you want then we have the usual kind then we have the usual kind of functions the step function of functions the step function this is the most important this is the most important function the step function that it is function the step function that it is taking an action and transitions the taking an action and transitions the environment into a new environment into a new cell and this is effectively the", "image_path": "img_data/video_12_chunk_49.jpg"}
{"video": "video_12", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "environment into a new cell and this is effectively the cell and this is effectively the code for that u it is not a the code for that u it is not a necessarily a stochastic environment necessarily a stochastic environment you can actually see that it is you can actually see that it is transitioning kind of the environment deterministically there is also the inet step function that also the inet step function that we will be getting the next state we will be getting the next state that's the next state and the reward that's the next state and the reward is going to be defined", "image_path": "img_data/video_12_chunk_50.jpg"}
{"video": "video_12", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "that's the next state and the reward is going to be defined the reward is going to be defined over here and that's basically it you will here and that's basically it you will have if you the these two s prime if you the these two s prime and r coming out of the step function and r coming out of the step function that is really the environment it's a that is really the environment it's a very simple environment and then of very simple environment and then of course we have the agent this agent course we have the agent this agent is the algorithm itself some is the algorithm itself some parameterizations of the sara agent parameterizations of the sara agent this is going to have an alpha of", "image_path": "img_data/video_12_chunk_51.jpg"}
{"video": "video_12", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "parameterizations of the sara agent this is going to have an alpha of this is going to have an alpha of 0.01 this is alpha was met in the q 0.01 this is alpha was met in the q function update equation a discount function update equation a discount factor gamma of 0.9 and epsilon of 0.1 factor gamma of 0.9 and epsilon of 0.1 epsilon was used for the epsilon gritty epsilon was used for the epsilon gritty policy improvement then we have a q policy improvement then we have a q table that evidently consist of four table that evidently consist of four tables one for this action tables one for this action for each action up down left right for each action up down left right there is a method called", "image_path": "img_data/video_12_chunk_52.jpg"}
{"video": "video_12", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "for each action up down left right there is a method called there is a method called learn in this method we are learn in this method we are sampling from the environment we sampling from the environment we have the sort of sarsa coming in as have the sort of sarsa coming in as arguments and what we produce at the arguments and what we produce at the output is we updating the q function output is we updating the q function we're updating the q function and we're updating the q function and we have then after that we have some we have then after that we have some other methods the get action the other methods the get action here is as we discussed in the during the", "image_path": "img_data/video_12_chunk_53.jpg"}
{"video": "video_12", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "get action here is as we discussed in the during the as we discussed in the during the policy improvement we will have some policy improvement we will have some form of randomization based on the form of randomization based on the biased coin that is has a crossover biased coin that is has a crossover probability of with epsilon and probability of with epsilon and therefore we are taking a random action therefore we are taking a random action every time that we are within epsilon every time that we are within epsilon and one minus epsilon is what we're and one minus epsilon is what we're actually taking the best action out of actually taking the best action out of the existing table the out of the existing table the out of the existing q table the existing q table that's basically that's the", "image_path": "img_data/video_12_chunk_54.jpg"}
{"video": "video_12", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "the existing q table that's basically that's the that's basically that's the greedy step that's the randomiz step greedy step that's the randomiz step epsilon and that's the whole thing is epsilon and that's the whole thing is called eps grey all right that's called eps grey all right that's basically it the main routine is basically it the main routine is instantiating the agent the it and instantiating the agent the it and starts with a episode creating episodes starts with a episode creating episodes let's say 1,000 episodes we are starting let's say 1,000 episodes we are starting from an reset function of the from an reset function of the environment and we also get our first environment and we also get our first action theet function in the", "image_path": "img_data/video_12_chunk_55.jpg"}
{"video": "video_12", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "environment and we also get our first action theet function in the action theet function in the environment gives us an initial state environment gives us an initial state that we can start the process and we are that we can start the process and we are taking an action based on the get action taking an action based on the get action method that we just saw and then we method that we just saw and then we have an infinite loop that we will break have an infinite loop that we will break every time that there is a new the every time that there is a new the existing episode kind of ends and the existing episode kind of ends and then we go into the next episode and then we go into the next episode and on during the episode we are on during the episode we are rendering we will see that we're rendering we will see that we're able", "image_path": "img_data/video_12_chunk_56.jpg"}
{"video": "video_12", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "rendering we will see that we're able to trigger the environment to trigger the environment if we just run it you will see if we just run it you will see that the window will actually pop up that the window will actually pop up with the agent moving and acting in that with the agent moving and acting in that kind of environment per the sara kind of environment per the sara function and of course the sara kind function and of course the sara kind of control policy we have the step of control policy we have the step function the get action and the learn function the get action and the learn method as we have seen from method as we have seen from earlier in the agent these are", "image_path": "img_data/video_12_chunk_57.jpg"}
{"video": "video_12", "start": "0:29:00", "end": "0:29:16.666667", "timestamp": "0:29:00 - 0:29:16.666667", "text": "method as we have seen from earlier in the agent these are earlier in the agent these are from the environment and then the from the environment and then the agent determines the next action and agent determines the next action and of course updates the q function based of course updates the q function based on that and then the whole process kind on that and then the whole process kind of repeats according to the temporal of repeats according to the temporal difference kind of method in terms of difference kind of method in terms of state estimation", "image_path": "img_data/video_12_chunk_58.jpg"}
{"video": "video_13", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we now go into the second sort of method of prediction we will sort of method of prediction we will address first of all the prediction address first of all the prediction problem and then the control problem and then the control problem that kind of naturally flows after monte that kind of naturally flows after monte carlo that is called temporal difference carlo that is called temporal difference and this is kind of an important and this is kind of an important category of methods that i category of methods that i think they are going to at some point think they are going to at some point merge together with other approaches merge together with other approaches we have seen in solving if you we have seen in solving if you a more elegantly the prediction", "image_path": "img_data/video_13_chunk_0.jpg"}
{"video": "video_13", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "we have seen in solving if you a more elegantly the prediction a more elegantly the prediction the value function prediction problem we the value function prediction problem we have seen in the mon car prediction that have seen in the mon car prediction that the mod carand of a value function the mod carand of a value function update is as follows let's say v pi first t is equal", "image_path": "img_data/video_13_chunk_1.jpg"}
{"video": "video_13", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "pi first t is equal v pi of s plus alpha the return after this state minus the previous estimate st this is what was estimate st this is what was happening in the monte carlo approach happening in the monte carlo approach and what we actually is happening in and what we actually is happening in the temporal difference is that again of the temporal difference is that again of course this means that we have to", "image_path": "img_data/video_13_chunk_2.jpg"}
{"video": "video_13", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "the temporal difference is that again of course this means that we have to of course this means that we have to wait until the actual realization of the wait until the actual realization of the episode kind of happens and all episode kind of happens and all the experience kind of formulated within the experience kind of formulated within the episode until we are able to update the episode until we are able to update this kind of value function in the this kind of value function in the temporal difference what we actually temporal difference what we actually do is we have another sort of an do is we have another sort of an advant what it turns out to be an advant what it turns out to be an advantage is that we don't have to wait advantage is that we don't have to wait until the episode completes until the episode completes and the equation becomes v i of", "image_path": "img_data/video_13_chunk_3.jpg"}
{"video": "video_13", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "until the episode completes and the equation becomes v i of and the equation becomes v i of st is equal to v pi capital v that is of st is equal to v pi capital v that is of st minus i'm sorry plus minus i'm sorry plus alpha of the following the this is rt + is rt + 1 plus gamma v of st + 1", "image_path": "img_data/video_13_chunk_4.jpg"}
{"video": "video_13", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "is rt + 1 plus gamma v of st + 1 plus gamma v of st + 1 minus v of s now what is really the big now what is really the big deal about it the you can consider deal about it the you can consider the temporal difference learning the temporal difference learning approach as kind of some kind of a approach as kind of some kind of a hybrid between what we have we're doing hybrid between what we have we're doing in mont carlo and what we are doing in u in mont carlo and what we are doing in u a dynamic programming and in the dynamic a dynamic programming and in the dynamic programming kind of video we have seen programming kind of video we have seen that there is boot strapping and here", "image_path": "img_data/video_13_chunk_5.jpg"}
{"video": "video_13", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "programming kind of video we have seen that there is boot strapping and here that there is boot strapping and here you see the bootstrapping in front of you see the bootstrapping in front of you we are using the value function you we are using the value function of the next state and it's estimate if of the next state and it's estimate if you in order to put strap the you in order to put strap the value function of the state that we value function of the state that we are interested in and at the same are interested in and at the same time we are effectively time we are effectively maintaining the sort of the monte carlo maintaining the sort of the monte carlo kind of approach in the sense that we kind of approach in the sense that we are still", "image_path": "img_data/video_13_chunk_6.jpg"}
{"video": "video_13", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "kind of approach in the sense that we are still generating if you experiences generating if you experiences that we would to sort of that we would to sort of consider in that evaluation let me wr consider in that evaluation let me wr write this down the -called one step more specifically we'll refer to this as td0 temporal difference methods combines the bootstrapping", "image_path": "img_data/video_13_chunk_7.jpg"}
{"video": "video_13", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "combines the bootstrapping of dynamic programming and the sampling of monte carlo to give you a kind of a an example carlo to give you a kind of a an example from real life that you can always kind from real life that you can always kind of refer to and kind of map it to of refer to and kind of map it to that kind of equation let's assume that kind of equation let's assume that i'm coming from", "image_path": "img_data/video_13_chunk_8.jpg"}
{"video": "video_13", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "that kind of equation let's assume that i'm coming from that i'm coming from brooklyn let's call this the sb the state that i am in brooklyn sb the state that i am in brooklyn right now and at some point i need to right now and at some point i need to go back to new jersey in and go back to new jersey in and therefore i have two options either and therefore i have two options either to take an action to go through the brooklyn", "image_path": "img_data/video_13_chunk_9.jpg"}
{"video": "video_13", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "to take an action to go through the brooklyn action to go through the brooklyn bridge or to take another action and go bridge or to take another action and go through the manhattan bridge that's through the manhattan bridge that's my a1 and a2 is manhattan and let's also now define two other states the past brooklyn and the other states the past brooklyn and the past man cut and bridge states that i", "image_path": "img_data/video_13_chunk_10.jpg"}
{"video": "video_13", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "other states the past brooklyn and the past man cut and bridge states that i past man cut and bridge states that i end up after i kind of crossed those end up after i kind of crossed those two bridges just what happens in two bridges just what happens in kind of real life the moment you go in kind of real life the moment you go through a kind of a significant kind through a kind of a significant kind of milestone in your commute let's say of milestone in your commute let's say crossing a very too busy kind of crossing a very too busy kind of landmarks over here then you can landmarks over here then you can actually update your estimate now actually update your estimate now that i have passed the brooklyn bridge that i have passed the brooklyn bridge now i have another 40 now i have another 40 minutes to reach my kind of destination", "image_path": "img_data/video_13_chunk_11.jpg"}
{"video": "video_13", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "now i have another 40 minutes to reach my kind of destination minutes to reach my kind of destination then that is this value over here then that is this value over here the fact that you can now update the fact that you can now update the sb using another guess we are sb using another guess we are effectively estimating using yet another effectively estimating using yet another estimate and this is basically what the estimate and this is basically what the bootstrapping is the we kind of bootstrapping is the we kind of receive our reward here which is this receive our reward here which is this the moment we sort of pass we the moment we sort of pass we actually taking this action and we are actually taking this action and we are sort of landing into these s", "image_path": "img_data/video_13_chunk_12.jpg"}
{"video": "video_13", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "actually taking this action and we are sort of landing into these s sort of landing into these s prime states and we are actually using the states and we are actually using the estimates of these states in order estimates of these states in order for us to bootstrap and update in for us to bootstrap and update in the estimate of the state again we're the estimate of the state again we're doing one step unrolling here and doing one step unrolling here and this is the what a diff temporal this is the what a diff temporal difference actually is that's difference actually is that's now i think we should come to the now i think we should come to the discussion as to", "image_path": "img_data/video_13_chunk_13.jpg"}
{"video": "video_13", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "now i think we should come to the discussion as to is that temporal difference method is that temporal difference method has any other advantage compared to has any other advantage compared to monto carlo obviously we don't have to monto carlo obviously we don't have to wait until the episode ends until for wait until the episode ends until for us to update sometimes the episodes are us to update sometimes the episodes are long that the that's long that the that's kind of a significant advantage kind of a significant advantage effectively making the temporal effectively making the temporal difference method i will call it as part difference method i will call it as part of what's called online of what's called online learning the other kind of", "image_path": "img_data/video_13_chunk_14.jpg"}
{"video": "video_13", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "of what's called online learning the other kind of learning the other kind of significant difference is that it significant difference is that it turns out that they do converge turns out that they do converge into the there's not really any into the there's not really any issue of using a guess or an estimate to issue of using a guess or an estimate to bootstrap and update another estimate bootstrap and update another estimate they it has been shown although we're they it has been shown although we're not going to do the any not going to do the any detailed discussion here the temporal detailed discussion here the temporal difference methods are converging and difference methods are converging and in fact depending on the setting", "image_path": "img_data/video_13_chunk_15.jpg"}
{"video": "video_13", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "difference methods are converging and in fact depending on the setting in fact depending on the setting of this alpha over there if it is of this alpha over there if it is kind of a small value it converges and kind of a small value it converges and everything is fine the one thing and everything is fine the one thing that you can actually see the that you can actually see the convergence is by running an example convergence is by running an example in the previous video we have seen an in the previous video we have seen an a mark of reward process and using if a mark of reward process and using if you two terminal states and i think you two terminal states and i think it's worthwhile going through that it's worthwhile going through that example with a temporal difference", "image_path": "img_data/video_13_chunk_16.jpg"}
{"video": "video_13", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "it's worthwhile going through that example with a temporal difference example with a temporal difference now and we've seen how the carlo kind of now and we've seen how the carlo kind of approximates the sort of true value approximates the sort of true value function and i just want to show function and i just want to show you the sort of how the monte you the sort of how the monte carlo and temporal difference methods carlo and temporal difference methods now compare and this is where the monte compare and this is where the monte carlo is depends on the values of carlo is depends on the values of alpha because we are effectively alpha because we are effectively waiting until the episode kind of waiting until the episode kind of finishes then we have a", "image_path": "img_data/video_13_chunk_17.jpg"}
{"video": "video_13", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "waiting until the episode kind of finishes then we have a finishes then we have a significant kind of i will call it significant kind of i will call it convergence slower convergence as compared to temporal difference as compared to temporal difference where actually we have where actually we have definitely faster convergence and you definitely faster convergence and you can actually see some behavior which can actually see some behavior which kind of matches the stochastic approximation matches the stochastic approximation type of algorith the type of algorith the ones that are those", "image_path": "img_data/video_13_chunk_18.jpg"}
{"video": "video_13", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "type of algorith the ones that are those where the lower the alpha is then we where the lower the alpha is then we have slower kind of convergence but then have slower kind of convergence but then the residual kind of error that we the residual kind of error that we actually get in terms of u root min actually get in terms of u root min square error that is between the true square error that is between the true function and the estimated value function and the estimated value function is lower than perhaps function is lower than perhaps converging faster and that's kind of converging faster and that's kind of a classic behavior with a good radios a classic behavior with a good radios but bottom line is a temporal difference but bottom line is a temporal difference seems to be performing better than seems to be performing better than monte carlo methods before proceeding", "image_path": "img_data/video_13_chunk_19.jpg"}
{"video": "video_13", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "seems to be performing better than monte carlo methods before proceeding monte carlo methods before proceeding into closing this soall prediction into closing this soall prediction problem with temporal difference problem with temporal difference learning methods i think it's worthwhile learning methods i think it's worthwhile addressing an enhancement which kind of addressing an enhancement which kind of brides the gap between what we've seen brides the gap between what we've seen in monte carlo and the singl step in monte carlo and the singl step temporal difference learning that we temporal difference learning that we just saw and this actually called td of just saw and this actually called td of lambda it's kind of known for lambda it's kind of known for with this notation and we'll see with this notation and we'll see exactly what lambda is in a moment but exactly what lambda is in a moment but the main advantage of this enhancement", "image_path": "img_data/video_13_chunk_20.jpg"}
{"video": "video_13", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "exactly what lambda is in a moment but the main advantage of this enhancement the main advantage of this enhancement is that it decouples the sort of action decouples the sort of action actions from value updates the you actions from value updates the in the single step process we are know in the single step process we are taking an action at every time step taking an action at every time step and also we are updating the value at and also we are updating the value at every time step and this does not really every time step and this does not really make a necessarily the best it is not make a necessarily the best it is not necessarily the best choice we actually necessarily the best choice we actually you may well prefer to keep the", "image_path": "img_data/video_13_chunk_21.jpg"}
{"video": "video_13", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "necessarily the best choice we actually you may well prefer to keep the you may well prefer to keep the actions coming in as frequently as actions coming in as frequently as possible but at the same time to only possible but at the same time to only update states when there are significant update states when there are significant updates that taking place and not every updates that taking place and not every time step one small correction i wanted to step one small correction i wanted to post on the formula of the return the post on the formula of the return the right formula is actually this one and right formula is actually this one and this correction was needed because this correction was needed because when we went to episodic experiences when we went to episodic experiences later on in reinforcement learning", "image_path": "img_data/video_13_chunk_22.jpg"}
{"video": "video_13", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "when we went to episodic experiences later on in reinforcement learning later on in reinforcement learning we kind of discovered that this we kind of discovered that this was an incorrect formula there was a was an incorrect formula there was a couple of typos here that's the couple of typos here that's the right one i'm going to post that right one i'm going to post that on the mtp section and we have not on the mtp section and we have not really used explicitly this form of that really used explicitly this form of that formula anyway in a subsequent kind of formula anyway in a subsequent kind of discussion i think we should be discussion i think we should be and this is another place where we and this is another place where we have actually had a couple of typos on have actually had a couple of typos on the return now when we have an the return now when we have an episode capital t of duration capital t", "image_path": "img_data/video_13_chunk_23.jpg"}
{"video": "video_13", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "the return now when we have an episode capital t of duration capital t episode capital t of duration capital t and we are effectively calculating and we are effectively calculating the return starting from when we are the return starting from when we are at state at time step t that's at state at time step t that's the second type over there that's the second type over there that we need to be aware of and this that we need to be aware of and this one will actually carry over when one will actually carry over when we're discussing now the returns we're discussing now the returns for multiple steps multiple step for multiple steps multiple step time diff different temporal time diff different temporal difference methods", "image_path": "img_data/video_13_chunk_24.jpg"}
{"video": "video_13", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "time diff different temporal difference methods after end steps our return actually will be written as gt let me write it as written as gt let me write it as after end steps it will be rt + 1+ gma after end steps it will be rt + 1+ gma rt + 2 plus dot plus gamma to rt + 2 plus dot plus gamma to the power of n minus1 r of t + the power of n minus1 r of t + n plus gamma n", "image_path": "img_data/video_13_chunk_25.jpg"}
{"video": "video_13", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "the power of n minus1 r of t + n plus gamma n v s of t plus n i actually wrote also v s of t plus n i actually wrote also and then i kind of recorded some and then i kind of recorded some other equation this equation is other equation this equation is effectively the one that kind of effectively the one that kind of summarizes how we will actually be using summarizes how we will actually be using the endstep return in the updated the endstep return in the updated equation for the n step equation for the n step temporal difference method and as you temporal difference method and as you can see the only thing that actually can see the only thing that actually changes over here is that instead of", "image_path": "img_data/video_13_chunk_26.jpg"}
{"video": "video_13", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "can see the only thing that actually changes over here is that instead of changes over here is that instead of having this term which is actually we having this term which is actually we call td target we have the end call td target we have the end return the endep return in its place return the endep return in its place the endstep return in its place for n is the endstep return in its place for n is equal to 1 effectively it is the rt + 1 equal to 1 effectively it is the rt + 1 + gamma v st + 1 we are back into the + gamma v st + 1 we are back into the td zero as we'll call it we td zero as we'll call it we understand exactly why the zero the understand exactly why the zero the lambda is equal z corresponds to this", "image_path": "img_data/video_13_chunk_27.jpg"}
{"video": "video_13", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "understand exactly why the zero the lambda is equal z corresponds to this lambda is equal z corresponds to this case u and that's basically the case u and that's basically the sort of the end result of the sort of the end result of the endep return now we can actually endep return now we can actually decouple the estimate of the value decouple the estimate of the value function and the bootstrapping actually function and the bootstrapping actually we do over there for and the we do over there for and the sort of action that we and the sort of action that we are taking we can continue take the are taking we can continue take the actions at every step and while we actions at every step and while we will be updating after end steps the", "image_path": "img_data/video_13_chunk_28.jpg"}
{"video": "video_13", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "actions at every step and while we will be updating after end steps the will be updating after end steps the value function that's basically value function that's basically that equation allows us to do now out that equation allows us to do now out of some studies which are not going of some studies which are not going to be presented here but they are to be presented here but they are definitely included in the saturn's definitely included in the saturn's book for those who are interested to book for those who are interested to deep dive a bit further they have deep dive a bit further they have shown that the way that the end step shown that the way that the end step what is really the best end to choose what is really the best end to choose it's basically is it the it's basically is it the hyper parameter how we are actually go hyper parameter how we are actually go about defining this hyperparameter and", "image_path": "img_data/video_13_chunk_29.jpg"}
{"video": "video_13", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "hyper parameter how we are actually go about defining this hyperparameter and about defining this hyperparameter and what actually has been found is that what actually has been found is that there are certain kind of trade-offs there are certain kind of trade-offs between choosing the sort of n is between choosing the sort of n is equal to one the one step and having equal to one the one step and having large n and as you can understand and large n and as you can understand and the algorith that we are effectively the algorith that we are effectively concluded out of this discussion is concluded out of this discussion is just called the td lambda algorith which just called the td lambda algorith which is basically what it does is that is basically what it does is that instead of selecting one out of the n it instead of selecting one out of the n it tries to average", "image_path": "img_data/video_13_chunk_30.jpg"}
{"video": "video_13", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "instead of selecting one out of the n it tries to average many returns endep returns many ends many returns endep returns many ends in other words the let's call it the in other words the let's call it the many end kind of algorith or lambda many end kind of algorith or lambda algorithm and the way that it is algorithm and the way that it is being presented is that we are actually presented is that we are actually defining the average return with a defining the average return with a weight that it is negative weight that it is negative exponential as it actually shown and of exponential as it actually shown and of course applies up to it is such that course applies up to it is such that the summation of all the weights in is the summation of all the weights in is averaging sometimes to one and the", "image_path": "img_data/video_13_chunk_31.jpg"}
{"video": "video_13", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "the summation of all the weights in is averaging sometimes to one and the averaging sometimes to one and the equation of the waiting of the returns equation of the waiting of the returns to average them is this equation over to average them is this equation over here which is the way it is actually here which is the way it is actually plotted over here on the as a plotted over here on the as a time index a time step index time as a time index a time step index up to the of course the episode up to the of course the episode kind of length and now let's write down kind of length and now let's write down the final kind of td lambda update the final kind of td lambda update equation and also show that with lambda equation and also show that with lambda is equal to z this corresponds to the", "image_path": "img_data/video_13_chunk_32.jpg"}
{"video": "video_13", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "equation and also show that with lambda is equal to z this corresponds to the is equal to z this corresponds to the one step temporal difference also known one step temporal difference also known as td of zer and with lambda is equal to as td of zer and with lambda is equal to one is roughly equivalent to the monte one is roughly equivalent to the monte carlo the equation is carlo the equation is v let say t+ v let say t+ n since we were also adding policy pi n since we were also adding policy pi let's add it s is equal to let's add it s is equal to v t + nus1", "image_path": "img_data/video_13_chunk_33.jpg"}
{"video": "video_13", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "let's add it s is equal to v t + nus1 of s plus alpha g t of lambda minus v of t + n - one pi of s that this is the equation and", "image_path": "img_data/video_13_chunk_34.jpg"}
{"video": "video_13", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "pi of s that this is the equation and s that this is the equation and now we said that we're going to now we said that we're going to define also the gt of define also the gt of lambda which we have seen already we lambda which we have seen already we have seen already the equation now we have seen already the equation now we need to show that for lambda is equal to need to show that for lambda is equal to z this corresponds to notation y is dd of zero the zero to notation y is dd of zero the zero here corresponds to lambda or to the on here corresponds to lambda or to the on step td and we'll take the equation", "image_path": "img_data/video_13_chunk_35.jpg"}
{"video": "video_13", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "step td and we'll take the equation td and we'll take the equation that we have already quoted this is that we have already quoted this is gt of lambda was- 1 - lambda summation from n lambda was- 1 - lambda summation from n is = to 1 to infinity this is n is equal to 1 in infinity this is n is equal to 1 in other words lambda n -1 gt of other words lambda n -1 gt of n and we'll write it as this equation n and we'll write it as this equation here which is exactly identical to this", "image_path": "img_data/video_13_chunk_36.jpg"}
{"video": "video_13", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "n and we'll write it as this equation here which is exactly identical to this here which is exactly identical to this but we just changed the notation to be a but we just changed the notation to be a bit more compatible to the saturn kind bit more compatible to the saturn kind of book notation where he is actually of book notation where he is actually using this kind of notation to using this kind of notation to indicate a return from the returns from indicate a return from the returns from t to t plus n this between t to t plus n this between the t and t plus n kind of a time step the t and t plus n kind of a time step that is effectively the that is effectively the returns in td in ep td are trated", "image_path": "img_data/video_13_chunk_37.jpg"}
{"video": "video_13", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "that is effectively the returns in td in ep td are trated returns in td in ep td are trated into two portions one is really the true into two portions one is really the true returns with all the discounted kind returns with all the discounted kind of rewards and the other is this of rewards and the other is this remaining term or the missing term which remaining term or the missing term which is approximated by the discounted kind is approximated by the discounted kind of value function and as you can see of value function and as you can see from the core side the notation kind of from the core side the notation kind of makes sense because the gt from t to t + makes sense because the gt from t to t + one is effectively what we had for the one is effectively what we had for the one step the t+ two for the two step the one step the t+ two for the two step the three step t plus three for the three step t plus three for the three-step td and on all the way to", "image_path": "img_data/video_13_chunk_38.jpg"}
{"video": "video_13", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "three step t plus three for the three-step td and on all the way to three-step td and on all the way to the u td infinity in fact and we'll the u td infinity in fact and we'll see exactly how we will account for that see exactly how we will account for that remaining what for the remaining what for the terms after the terminating kind terms after the terminating kind of state we can actually now break this of state we can actually now break this summation over this infinity terms summation over this infinity terms into two parts and actually this is into two parts and actually this is actually shown over here when we have actually shown over here when we have actually have plotted this weights that actually have plotted this weights that we are line into two parts the first", "image_path": "img_data/video_13_chunk_39.jpg"}
{"video": "video_13", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "actually have plotted this weights that we are line into two parts the first we are line into two parts the first part is what is actually shown here as part is what is actually shown here as the white portion and the second part is the white portion and the second part is what everything is after that the kind what everything is after that the kind of a shaded region over here and of a shaded region over here and of course the key term over there is the course the key term over there is the capital t which is the episode length capital t which is the episode length and what we need to do now is we go and what we need to do now is we go back to the whiteboard and start you back to the whiteboard and start writing a couple of considerations know writing a couple of considerations down for all indices from n is equal", "image_path": "img_data/video_13_chunk_40.jpg"}
{"video": "video_13", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "down for all indices from n is equal 1 for n is equal to 1 2 t minus one the returns are weighted", "image_path": "img_data/video_13_chunk_41.jpg"}
{"video": "video_13", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "are weighted with this kind of decaying lambda factors we i say decaying because as you factors we i say decaying because as you can see it was a decaying kind of can see it was a decaying kind of geometric progression of that because geometric progression of that because lambda is less than or equal to one they lambda is less than or equal to one they get weighted with the they're get weighted with the they're weighted by the factors the following i'm going to take this i'm going to", "image_path": "img_data/video_13_chunk_42.jpg"}
{"video": "video_13", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "i'm going to take this i'm going to take this and break it down into two take this and break it down into two parts the first part is parts the first part is 1 1us lambda summation from u n is equal to 1 to n to capital t minus t", "image_path": "img_data/video_13_chunk_43.jpg"}
{"video": "video_13", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "to n to capital t minus t -1 lambda to n + n minus one and times g of t + n that's the first term and then we'll see that the second term is 1 see that the second term is 1 minus lambda to t - tus1 gt this is basically the", "image_path": "img_data/video_13_chunk_44.jpg"}
{"video": "video_13", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "tus1 gt this is basically the this is basically the expression and let me just expression and let me just effectively highlight the following as effectively highlight the following as we can see anything that it is you we can see anything that it is why we actually came to this kind know why we actually came to this kind of consideration especially of consideration especially with respect to the last term over there with respect to the last term over there the first thing is that anything any instant t where we are anything any instant t where we are going beyond if you the terminating", "image_path": "img_data/video_13_chunk_45.jpg"}
{"video": "video_13", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "anything any instant t where we are going beyond if you the terminating going beyond if you the terminating kind of state any anything that kind of state any anything that it is with ind this is t + anything that it is with ind this is t + n greater than or equal to n greater than or equal to t or effectively n greater than equal to t minus t then n greater than equal to t minus t then all the returns all returns are in fact", "image_path": "img_data/video_13_chunk_46.jpg"}
{"video": "video_13", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "returns are in fact the full return ct course all the rewards after the ct course all the rewards after the terminating state are actually zero and terminating state are actually zero and of course this means that the full of course this means that the full return is basically what is return is basically what is what it corresponds to those cases", "image_path": "img_data/video_13_chunk_47.jpg"}
{"video": "video_13", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "return is basically what is what it corresponds to those cases what it corresponds to those cases that is the reason why i actually that is the reason why i actually wrote from n is equal to 1 to capital t wrote from n is equal to 1 to capital t minus t minus1 because this is just minus t minus1 because this is just before this condition gets to be before this condition gets to be satisfied right it's everything satisfied right it's everything before this state over here as we before this state over here as we actually can see on the site the first actually can see on the site the first factor the first term if you in factor the first term if you in this summation we just broke up this summation we just broke up corresponds to the weights that we have corresponds to the weights that we have applied they're actually shown here in applied they're actually shown here in this white region of this", "image_path": "img_data/video_13_chunk_48.jpg"}
{"video": "video_13", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "this white region of this geometrically decaying if you geometrically decaying if you weight curve and then the last term the weight curve and then the last term corresponds to this weight last term corresponds to this weight over here that anything over here that anything is beyond capital t which is the is beyond capital t which is the episode will actually be weighted by episode will actually be weighted by 1 minus lambda to t capital t 1 minus lambda to t capital t minus one and that's basically minus t minus one and that's basically what we also have kind of", "image_path": "img_data/video_13_chunk_49.jpg"}
{"video": "video_13", "start": "0:25:00", "end": "0:25:24.200000", "timestamp": "0:25:00 - 0:25:24.200000", "text": "minus t minus one and that's basically what we also have kind of what we also have kind of mentioned here on the site in terms of mentioned here on the site in terms of equation all right this is basically equation all right this is basically everything that we actually can everything that we actually can mention regarding td of n this is the mention regarding td of n this is the sort of endep td and what actually we sort of endep td and what actually we need to do next is we need to start need to do next is we need to start considering the control problem and considering the control problem and this will be done in a different this will be done in a different video", "image_path": "img_data/video_13_chunk_50.jpg"}
{"video": "video_14", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we will introduce reinforcement learning and reinforcement learning and reinforcement learning is definitely a reinforcement learning is definitely a natural conclusion to the topic of natural conclusion to the topic of planning with interaction and definitely planning with interaction and definitely follows after the series of videos follows after the series of videos that we have developed on mark of that we have developed on mark of decision processes as in mdp we have decision processes as in mdp we have been assuming full knowledge of the been assuming full knowledge of the problem dynamics and reward functions problem dynamics and reward functions and over here we'll start removing that", "image_path": "img_data/video_14_chunk_0.jpg"}
{"video": "video_14", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "problem dynamics and reward functions and over here we'll start removing that and over here we'll start removing that kind of assumption and we will be called kind of assumption and we will be called to estimate those quantities from to estimate those quantities from experiences some of this experiences some of this experiences initially will take the form of initially will take the form of sampling and we'll look at monte sampling and we'll look at monte carlo methods and the temporal carlo methods and the temporal difference type of methods as two difference type of methods as two instances of sampling and then we will instances of sampling and then we will introduce a flexible approximation to introduce a flexible approximation to those estimates in the form of neural those estimates in the form of neural networks and conclude the discussion", "image_path": "img_data/video_14_chunk_1.jpg"}
{"video": "video_14", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "those estimates in the form of neural networks and conclude the discussion networks and conclude the discussion with deep reinforcement with deep reinforcement learning we have done in the mdp learning we have done in the mdp also in reinforcement learning we will also in reinforcement learning we will divide if you these considerations into two steps one is the considerations into two steps one is the -called prediction step where our job -called prediction step where our job is really now to estimate the is really now to estimate the corresponding value functions either the corresponding value functions either the v functions or the q functions now v functions or the q functions now because these functions are going because these functions are going to assume no knowledge of any model to assume no knowledge of any model dynamics and therefore will be estimates dynamics and therefore will be estimates we need to learn them the learned we need to learn them the learned function functions are typically", "image_path": "img_data/video_14_chunk_2.jpg"}
{"video": "video_14", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "we need to learn them the learned function functions are typically function functions are typically denoted with capital letters instead of denoted with capital letters instead of small letters we will be using small letters we will be using capital v and capital q for those and capital v and capital q for those and we will start with what is actually we will start with what is actually called the first one method which called the first one method which is called the monte carlo or mc method for the prediction mc method for the prediction problem the prediction", "image_path": "img_data/video_14_chunk_3.jpg"}
{"video": "video_14", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "mc method for the prediction problem the prediction problem the prediction problem all right and according to problem all right and according to this kind of method and just a recap this kind of method and just a recap from the other video when we were from the other video when we were looking at the mdp we had this looking at the mdp we had this equation for the v function that a equation for the v function that a small v of the for every state small v of the for every state s and this is an expectation over all s and this is an expectation over all policies of the returns from that", "image_path": "img_data/video_14_chunk_4.jpg"}
{"video": "video_14", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "s and this is an expectation over all policies of the returns from that policies of the returns from that of state which is the return gt given state which is the return gt given that we are in state that we are in state s and as a kind of remainder the s and as a kind of remainder the policies we have plenty of policies we have plenty of policies to that are in any problem policies to that are in any problem let's say for example we have 1 let's say for example we have 1 2 3 four five states and let's say if we 2 3 four five states and let's say if we had two actions per state either left had two actions per state either left let's say or right then we", "image_path": "img_data/video_14_chunk_5.jpg"}
{"video": "video_14", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "had two actions per state either left let's say or right then we let's say or right then we could have here two to the power of 1 2 3 4 5 2 to two to the power of 1 2 3 4 5 2 to the power of five or 32 policies 32 possible policies in general we will have as many as the general we will have as many as the cardinality of the set a of actions to cardinality of the set a of actions to the cardinality of the set of states the cardinality of the set of states as you can understand the number of", "image_path": "img_data/video_14_chunk_6.jpg"}
{"video": "video_14", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "the cardinality of the set of states as you can understand the number of as you can understand the number of policies that exist in any mdp policies that exist in any mdp problem is actually quite could be quite problem is actually quite could be quite large and therefore one of the reason large and therefore one of the reason why we are actually looking at these why we are actually looking at these approximations here later on is to make approximations here later on is to make sure that we have we're converting an sure that we have we're converting an invisible to be solved problem invisible to be solved problem into a problem that can be reasonably into a problem that can be reasonably expected to be practical infusible to expected to be practical infusible to solve with those approximations back to the sort of approximations back to the sort of mon carlo of prediction problem this was", "image_path": "img_data/video_14_chunk_7.jpg"}
{"video": "video_14", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "approximations back to the sort of mon carlo of prediction problem this was mon carlo of prediction problem this was when we knew the dynamics and when we knew the dynamics and therefore what we were doing there is therefore what we were doing there is to you to do actually a to you to do actually a onestep the -called bellman onestep the -called bellman expectation kind of backup where for every action we were backup where for every action we were ended up in some state s ended up in some state s prime the next state and this was u prime the next state and this was u effectively after the action is taken", "image_path": "img_data/video_14_chunk_8.jpg"}
{"video": "video_14", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "prime the next state and this was u effectively after the action is taken effectively after the action is taken we were migrating to s prime and we were migrating to s prime and we were recovering some kind of reward and of recovering some kind of reward and of course the policy was responsible for policy was responsible for transitioning for taking if you transitioning for taking if you that those one of the available kind that those one of the available kind of actions to us from that state of actions to us from that state s and the -called estimate of v pi -called estimate of v pi of s ended up with a bellman equation", "image_path": "img_data/video_14_chunk_9.jpg"}
{"video": "video_14", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "-called estimate of v pi of s ended up with a bellman equation expectation equation if you that was basically the dynamic programming the basically the dynamic programming the -call dynamic programming solution ended up being a kind of a bootstrapping equation and we call bootstrapping equation and we call that bo strapping equation the fact that bo strapping equation the fact that we were using the v pi of s prime to", "image_path": "img_data/video_14_chunk_10.jpg"}
{"video": "video_14", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "that bo strapping equation the fact that we were using the v pi of s prime to we were using the v pi of s prime to calculate the v pi of calculate the v pi of s and if i trace back that specific s and if i trace back that specific equation i will we call this equation i will we call this iterative policy evaluation when we were iterative policy evaluation when we were discussing in that video the mdp discussing in that video the mdp policy evaluation for state value policy evaluation for state value functions of course we can we have an functions of course we can we have an equivalent one for the q functions there equivalent one for the q functions there and actually you can see very clearly and actually you can see very clearly that the vk of s prime over here is", "image_path": "img_data/video_14_chunk_11.jpg"}
{"video": "video_14", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "and actually you can see very clearly that the vk of s prime over here is that the vk of s prime over here is actually being used to calculate the actually being used to calculate the updated value function for the state updated value function for the state s and this is basically what we call by s and this is basically what we call by bootstrapping in essence we are bootstrapping in essence we are suggesting that i don't have to in order suggesting that i don't have to in order to calculate all these kind of averages to calculate all these kind of averages i need to just know the value function i need to just know the value function of this accessors kind of states in of this accessors kind of states in order for me to calculate this one and order for me to calculate this one and i don't need to expand all the way", "image_path": "img_data/video_14_chunk_12.jpg"}
{"video": "video_14", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "order for me to calculate this one and i don't need to expand all the way i don't need to expand all the way into the diagram because in the into the diagram because in the effectively this state at some point effectively this state at some point it was calculated it took the calculated it took the s32 for example took the s32 for example took the place of s33 and it was itself evaluated place of s33 and it was itself evaluated at some previous kind of at some previous kind of iteration we are pretending that iteration we are pretending that s32 is the kind of a sort of root state s32 is the kind of a sort of root state over here but effectively what we did if", "image_path": "img_data/video_14_chunk_13.jpg"}
{"video": "video_14", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "s32 is the kind of a sort of root state over here but effectively what we did if over here but effectively what we did if you recall was to evaluate all states you recall was to evaluate all states these v's are now all states these v's are now vectors and they are evaluating all vectors and they are evaluating all states at the same time states at the same time that's basically what was the that's basically what was the bootstrapping and equivalently bootstrapping and equivalently we can actually see a diagram over here we can actually see a diagram over here just to sort of that kind of just to sort of that kind of demonstrates that kind of what i just demonstrates that kind of what i just said with a dynamic programming we have", "image_path": "img_data/video_14_chunk_14.jpg"}
{"video": "video_14", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "demonstrates that kind of what i just said with a dynamic programming we have said with a dynamic programming we have full expansion very wide expansion of full expansion very wide expansion of the tree where the at the root the of the tree where the at the root of the tree is a state that we are of the tree is a state that we are evaluating the value function of and evaluating the value function of and here we are bootstrapping that value here we are bootstrapping that value from the successor states as we just from the successor states as we just discussed and the intuition behind discussed and the intuition behind this and obviously in the dynamic kind this and obviously in the dynamic kind of programing the whole intuition is of programing the whole intuition is that you can always start from some kind that you can always start from some kind of a simple problem where let's say of a simple problem where let's say you are kind of close to some kind", "image_path": "img_data/video_14_chunk_15.jpg"}
{"video": "video_14", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "of a simple problem where let's say you are kind of close to some kind say you are kind of close to some kind of a terminating kind of state that you of a terminating kind of state that you kind of know the reward and there is kind of know the reward and there is because it's terminating you are not because it's terminating you are not interested on any future rewards interested on any future rewards there's no such future if you for there's no such future if you for you and then you can if you and then you can if the value functions of these kind know the value functions of these kind of states you can calculate the value of states you can calculate the value functions of the states which are above functions of the states which are above you and vi are the kind of a you and vi are the kind of a bootstrapping method and then you're bootstrapping method and then you're going into more and more larger and", "image_path": "img_data/video_14_chunk_16.jpg"}
{"video": "video_14", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "bootstrapping method and then you're going into more and more larger and going into more and more larger and larger kind of u problems for more larger kind of u problems for more and more difficult problems than what and more difficult problems than what you started with but as i said in you started with but as i said in reality we are not breaking down the reality we are not breaking down the problem into sub problems but we are problem into sub problems but we are iteratively solving this bellman iteratively solving this bellman equation iterative polic valuation equation iterative polic valuation based on bman equation now in mont carlo state value equation now in mont carlo state value prediction what we will do since we prediction what we will do since we don't have the dynamics and", "image_path": "img_data/video_14_chunk_17.jpg"}
{"video": "video_14", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "prediction what we will do since we don't have the dynamics and don't have the dynamics and effectively the dynamics are actually effectively the dynamics are actually were used to calculate the u were used to calculate the u the sort of the bootstrapping and the sort of the bootstrapping and the previous in the case of the previous in the case of dynamic programming the value function dynamic programming the value function what we actually going to do over here what we actually going to do over here is we are going to sample to is we are going to sample to create if you experiences and create if you experiences and the assumption here is that we can first the assumption here is that we can first of all that we have if you a of all that we have if you a simulator that will be able to simulator that will be able to sample experiences from the system", "image_path": "img_data/video_14_chunk_18.jpg"}
{"video": "video_14", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "simulator that will be able to sample experiences from the system sample experiences from the system the model is being used only at a the model is being used only at a sampling level to generate this sampling level to generate this experiences and it is a much easier experiences and it is a much easier sort of proposition as compared to sort of proposition as compared to estimating these models or learning estimating these models or learning if you the model dynamics and what we will dynamics and what we will do is for every state and also state do is for every state and also state and action pair that we are vising for and action pair that we are vising for the corres for the v and the corres for the v and the corresponding q functions what we will", "image_path": "img_data/video_14_chunk_19.jpg"}
{"video": "video_14", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "the corres for the v and the corresponding q functions what we will corresponding q functions what we will do is we'll keep scores what are those do is we'll keep scores what are those scores as what are the returns scores as what are the returns that we have experienced after those that we have experienced after those states and keeping a score of these states and keeping a score of these kind of states then it will allow us kind of states then it will allow us to calculate the value function as we to calculate the value function as we'll see another distinguishing factor we'll see another distinguishing factor over here is that in order for us to over here is that in order for us to get to the sort of sample return get to the sort of sample return we will need to reach if you", "image_path": "img_data/video_14_chunk_20.jpg"}
{"video": "video_14", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "get to the sort of sample return we will need to reach if you we will need to reach if you the terminating state and therefore we the terminating state and therefore we are talking about and sort of generating are talking about and sort of generating kind of episodes of u and of some kind of episodes of u and of some kind of u length and then we will be kind of u length and then we will be able to retrieve those returns able to retrieve those returns per episode and then average across many per episode and then average across many thousands of episodes in order for many thousands of episodes in order for us to be able to approximate as much as us to be able to approximate as much as we can the value function", "image_path": "img_data/video_14_chunk_21.jpg"}
{"video": "video_14", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "us to be able to approximate as much as we can the value function we can the value function effectively we are going to effectively we are going to be approximating and let me make this kind of as i reach the end the terminating of as i reach the end the terminating kind of state we are going to be approximating state we are going to be approximating this expectation over here with the this expectation over here with the many trajectory sums averages many trajectory sums averages across many trajectories and let's call across many trajectories and let's call this the return across each trajectory", "image_path": "img_data/video_14_chunk_22.jpg"}
{"video": "video_14", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "across many trajectories and let's call this the return across each trajectory this the return across each trajectory starting let's say from the moment we starting let's say from the moment we were in state t sorry in state s at were in state t sorry in state s at time t let's say this will be the return time t let's say this will be the return on trajectory let's call this trajectory on trajectory let's call this trajectory tow also trajectory is a term which tow also trajectory is a term which is used extensively in motion control is used extensively in motion control but a synonym of that is the but a synonym of that is the episode which is a bit more common episode which is a bit more common across reinforcement learning on that across reinforcement learning on that kind of episode dow we will have some kind of episode dow we will have some kind of a summation", "image_path": "img_data/video_14_chunk_23.jpg"}
{"video": "video_14", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "kind of episode dow we will have some kind of a summation of the u discounted k is equal let's say to z discounted k is equal let's say to z to t minus capital t minus one let's to t minus capital t minus one let's assume that this is the length that this assume that this is the length that this trajectory kind of lasted of all trajectory kind of lasted of all discounted kind of future rewards this is going to be rewards this is going to be the return and then what we'll the return and then what we'll do now is we'll write", "image_path": "img_data/video_14_chunk_24.jpg"}
{"video": "video_14", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "the return and then what we'll do now is we'll write do now is we'll write the update kind of method the update kind of method the update kind of formulation for each of update kind of formulation for each of the elements of the state value function the elements of the state value function the step first step is that the step first step is that for each time step for step t that s is phys", "image_path": "img_data/video_14_chunk_25.jpg"}
{"video": "video_14", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "t that s is phys it in an episode we increment we keep this score via this kind of counter increment the counter n ofs the effective counts the visitations", "image_path": "img_data/video_14_chunk_26.jpg"}
{"video": "video_14", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "counter n ofs the effective counts the visitations ofs the effective counts the visitations the number of visitations that kind of the number of visitations that kind of state and calculate the total return as g capital t of s is equal gt ofs plus gt", "image_path": "img_data/video_14_chunk_27.jpg"}
{"video": "video_14", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "gt ofs plus gt and effectively we are the second step is that at the end of thousands is an example many i will call it episodes the value we calculate the estimate which is now capital v as we estimate which is now capital v as we said capital v of s it is g t of s divided by n of", "image_path": "img_data/video_14_chunk_28.jpg"}
{"video": "video_14", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "said capital v of s it is g t of s divided by n of s it is g t of s divided by n of s and as the number of times that we are s and as the number of times that we are u sort of n ofs is visited is u sort of n ofs is visited is becoming larger and larger then we will becoming larger and larger then we will effectively converge this as an in other words of s tends to infinity this will converts to v pi", "image_path": "img_data/video_14_chunk_29.jpg"}
{"video": "video_14", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "infinity this will converts to v pi of s and i think i forgotten to add the pi over here in the capital v and we the pi over here in the capital v and we also do the following trick to called incremental mean sample mean or mean approximation where i think we this exa it's exactly", "image_path": "img_data/video_14_chunk_30.jpg"}
{"video": "video_14", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "where i think we this exa it's exactly the same trick that we had done in the same trick that we had done in another video when we were dealing another video when we were dealing with recursive state estimation and kind with recursive state estimation and kind of a calman filters where we are of a calman filters where we are effectively writing a statistic in a effectively writing a statistic in a kind of recursive form the same thing kind of recursive form the same thing can be done in either the mean or the can be done in either the mean or the covariance or correlation matrices here covariance or correlation matrices here is the just to remind everyone we can is the just to remind everyone we can actually write the sample mean actually write the sample mean of the", "image_path": "img_data/video_14_chunk_31.jpg"}
{"video": "video_14", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "actually write the sample mean of the kf sample that is actually coming in kf sample that is actually coming in from 1 to k as 1 / from 1 to k as 1 / k of summation from let's say j is equal 1 to k oops xj let's say the xj was a sample all right and then we have can write this right and then we have can write this as one 1/ small letter k of breaking", "image_path": "img_data/video_14_chunk_32.jpg"}
{"video": "video_14", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "right and then we have can write this as one 1/ small letter k of breaking as one 1/ small letter k of breaking down the sum into j is = 1 to k minus down the sum into j is = 1 to k minus 1 xj plus xk and then if we do the another step we will be able to write this as the will be able to write this as the previous mean estimate + 1 / small letter k estimate + 1 / small letter k of x", "image_path": "img_data/video_14_chunk_33.jpg"}
{"video": "video_14", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "estimate + 1 / small letter k of x k minus new k minus one the previous estimate plus 1 / one the previous estimate plus 1 / k the current sample minus the previous k the current sample minus the previous estimate and back then in that kind of estimate and back then in that kind of other video we actually call this some other video we actually call this some kind of indication of the calman gain if kind of indication of the calman gain if you recall all this kind of stuff you recall all this kind of stuff it's exactly the same formula and if you it's exactly the same formula and if you use this actually formula over here use this actually formula over here we'll be able to do the sort of", "image_path": "img_data/video_14_chunk_34.jpg"}
{"video": "video_14", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "use this actually formula over here we'll be able to do the sort of we'll be able to do the sort of the up the value function with the monte carlo method can be written as n of s these two steps is equal to as n of s these two steps is equal to n of s + one incrementing if you n of s + one incrementing if you the counter every time we visit state the counter every time we visit state s and the second is to s and the second is to calculate the v of", "image_path": "img_data/video_14_chunk_35.jpg"}
{"video": "video_14", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "s and the second is to calculate the v of s let's say v pi of s is equal to v pi of s plus let me call this alpha gt which is the current return as it is estimated for that specific for it is estimated for that specific episode", "image_path": "img_data/video_14_chunk_36.jpg"}
{"video": "video_14", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "it is estimated for that specific episode that specific episode minus v pi of s the this is the previous estimate these are the previous estimate these are the previous estimates now obviously we can estimates now obviously we can distinguish there with the an index k distinguish there with the an index k but in implementation you have a left but in implementation you have a left hand side and the right hand side to hand side and the right hand side to be sort of separating in a let's be sort of separating in a let's say implementation in software you", "image_path": "img_data/video_14_chunk_37.jpg"}
{"video": "video_14", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "be sort of separating in a let's say implementation in software you say implementation in software you don't need necessarily to put the don't need necessarily to put the indices alpha in this case is one indices alpha in this case is one over n ofs and this we can call it the ofs and this we can call it the forgetting factor and obviously this forgetting factor is less than one this allows us factor is less than one this allows us to have a bit more flexibility on how to have a bit more flexibility on how many terms we're able to many terms we're able to average and we can actually call", "image_path": "img_data/video_14_chunk_38.jpg"}
{"video": "video_14", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "many terms we're able to average and we can actually call average and we can actually call the this implementation the running sample mean of the state value mean of the state value function also worth noting a couple of function also worth noting a couple of things from this kind of formula the things from this kind of formula the first is that the estimate no there's first is that the estimate no there's no bootstrapping", "image_path": "img_data/video_14_chunk_39.jpg"}
{"video": "video_14", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "first is that the estimate no there's no bootstrapping in other words we do not use an estimate of the value function of a another state there's no bootstrapping here and as effectively the estimates", "image_path": "img_data/video_14_chunk_40.jpg"}
{"video": "video_14", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "state there's no bootstrapping here and as effectively the estimates and as effectively the estimates between states are kind of independent between states are kind of independent of each other and this is also kind of each other and this is also kind of an advantage because computationally an advantage because computationally wise we can actually in any problem that wise we can actually in any problem that we have to estimate value functions for we have to estimate value functions for a subset of the states or even just a subset of the states or even just a single kind of state we're able to do single kind of state we're able to do with m carlo methods and the second is with m carlo methods and the second is that the value function can only be updated", "image_path": "img_data/video_14_chunk_41.jpg"}
{"video": "video_14", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "function can only be updated at the end of each episode because this is empasis on episode because this is empasis on only because this basically where we get only because this basically where we get that kind of return and this is that kind of return and this is basically something that we will work basically something that we will work to kind of provide an enhancement on to kind of provide an enhancement on that with a temporal difference that with a temporal difference methods now as an example of what is methods now as an example of what is actually called a also a simple", "image_path": "img_data/video_14_chunk_42.jpg"}
{"video": "video_14", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "methods now as an example of what is actually called a also a simple actually called a also a simple example where we don't have any explicit example where we don't have any explicit kind of actions over here or kind of actions over here or decisions where this is actually decisions where this is actually called mark of reward called mark of reward process we have a kind of process we have a kind of a straight line kind of sequence of a straight line kind of sequence of states where the agent may start states where the agent may start from a state over here sh here a", "image_path": "img_data/video_14_chunk_43.jpg"}
{"video": "video_14", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "from a state over here sh here a b c d e and there is a actually this is a one way a actually this is a one way arrow goes into a terminating state on arrow goes into a terminating state on the right hand side where the right hand side where the transition the reward over here is one transition the reward over here is one and it's a terminating state all of the and it's a terminating state all of the other rewards are actually zero other rewards are actually zero and actually the agent starts with c", "image_path": "img_data/video_14_chunk_44.jpg"}
{"video": "video_14", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "other rewards are actually zero and actually the agent starts with c and actually the agent starts with c starts with that's a starting state of starts with that's a starting state of the agent and there is the agent and there is a left terminating state as well that a left terminating state as well that is also express reward of zero but over also express reward of zero but over here in that right terminating state we here in that right terminating state we have a reward of one and we would have a reward of one and we would to come up with u an the to come up with u an the state value function estimate using", "image_path": "img_data/video_14_chunk_45.jpg"}
{"video": "video_14", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "to come up with u an the state value function estimate using the state value function estimate using let's say first kind of a dynamic let's say first kind of a dynamic programming and what we will do is we programming and what we will do is we will use the formula let's say v pi will use the formula let's say v pi of s is summation over let's say s is summation over let's say actions of the policy of a given actions of the policy of a given s over here we have two potential s over here we have two potential actions that the agent can take actions that the agent can take either we go left or right at every either we go left or right at every state these", "image_path": "img_data/video_14_chunk_46.jpg"}
{"video": "video_14", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "either we go left or right at every state these are the general formula but it has to be are the general formula but it has to be kind of specialize to the there's a kind of specialize to the there's a transition model as we said and then we have here r+ r+ gamma v pi of s prime this was the kind of a bootstrap prime this was the kind of a bootstrap this is a dynamic programming solution and gamma here is also equal to", "image_path": "img_data/video_14_chunk_47.jpg"}
{"video": "video_14", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "and gamma here is also equal to one all right the effectively ag also need to write also that agent and go left or right one step one state that it step with equal", "image_path": "img_data/video_14_chunk_48.jpg"}
{"video": "video_14", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "right one step one state that it step with equal probability all right that's basically the policy this is basically the policy this is let me call it pi of a given s is 0.5 for left and 0.5 for", "image_path": "img_data/video_14_chunk_49.jpg"}
{"video": "video_14", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "s is 0.5 for left and 0.5 for right all right and i can actually right all right and i can actually take now each of these possible states and actually write down what will actually be that down what will actually be that formula will actually tell us this formula will actually tell us this will be and this is also the before i be and this is also the before i do i can actually also write the kind do i can actually also write the kind of a backup diagram for every state s of a backup diagram for every state s i have two actions either left or", "image_path": "img_data/video_14_chunk_50.jpg"}
{"video": "video_14", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "of a backup diagram for every state s i have two actions either left or i have two actions either left or right and for each action i may end up right and for each action i may end up in potentially in two neighboring states sometimes only one because it's terminating but that's basically terminating but that's basically in general the backup diagram this is in general the backup diagram this is the s prime this is the transition model", "image_path": "img_data/video_14_chunk_51.jpg"}
{"video": "video_14", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "the s prime this is the transition model here p of s prime comma r given s comma prime comma r given s comma a and this is the policy pi a and this is the policy pi this is according to this formula this is according to this formula v pi of a s will be a is going to a s will be a is going to be for a i can actually go be for a i can actually go right or left if i go left i", "image_path": "img_data/video_14_chunk_52.jpg"}
{"video": "video_14", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "be for a i can actually go right or left if i go left i right or left if i go left i will have here i can go left with will have here i can go left with probability one2 and over here i can actually have one2 and over here i can actually have the immediate reward go left will the immediate reward go left will actually be zero will be zero plus gamma which is zero will be zero plus gamma which is one vp of t let's call it l let's call vp of t let's call it l let's call this tl and this call this tr for", "image_path": "img_data/video_14_chunk_53.jpg"}
{"video": "video_14", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "vp of t let's call it l let's call this tl and this call this tr for this tl and this call this tr for right plus and the other one is plus and the other one is uh2 of 0 + uh2 of 0 + vp of b that's the other action and vp of b that's the other action and this will actually be this will actually be since the vp of tl is zero and this since the vp of tl is zero and this will be v pi of b /", "image_path": "img_data/video_14_chunk_54.jpg"}
{"video": "video_14", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "will be v pi of b / pi of b / 2 that's kind of equation 2 that's kind of equation one the v pi of b will be i'm going to write now b will be i'm going to write now quicker v pi of a + 12 0 + v pi of quicker v pi of a + 12 0 + v pi of c which is v pi of a plus v pi of c which is v pi of a plus v pi of c/ 2 that's equation", "image_path": "img_data/video_14_chunk_55.jpg"}
{"video": "video_14", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "c/ 2 that's equation two and then we have v pi of c is equal v pi of b plus v pi of d now i think v pi of b plus v pi of d now i think everything is clear over two equation everything is clear over two equation 3 v pi of d is equal v pi of c plus v pi 3 v pi of d is equal v pi of c plus v pi of e over two 4 and finally b pi e over two 4 and finally b pi of", "image_path": "img_data/video_14_chunk_56.jpg"}
{"video": "video_14", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "e over two 4 and finally b pi of e is equal v pi of d + 1 which is coming e is equal v pi of d + 1 which is coming from the fact that we are going to into from the fact that we are going to into tr ided by two and that's equation tr ided by two and that's equation five and it's easy to solve this five and it's easy to solve this problem from one i can have that v pi problem from one i can have that v pi of b is equal to 2 v pi of b is equal to 2 v pi of a from two i have that 4 v pi of a is a from two i have that 4 v pi of a is equal v pi of a plus v pi of", "image_path": "img_data/video_14_chunk_57.jpg"}
{"video": "video_14", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "a from two i have that 4 v pi of a is equal v pi of a plus v pi of equal v pi of a plus v pi of c in other words v pi of c is equal 3 v c in other words v pi of c is equal 3 v pi of a from three i have v pi of d is a from three i have v pi of d is equal 4 v pi of a and from four i have v pi of v is a and from four i have v pi of v is equal 4 v pi of a and from five i have v pi of a is = to", "image_path": "img_data/video_14_chunk_58.jpg"}
{"video": "video_14", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "equal 4 v pi of a and from five i have v pi of a is = to a and from five i have v pi of a is = to 1 / 1 / 6 and therefore i'm able to solve all of 6 and therefore i'm able to solve all of these other expressions this is going to be 4 6 expressions this is going to be 4 6 or 2/3 v pi of a 2/3 every pi of a and this will be a 12 i'm sorry this is just 2/3 not v by of", "image_path": "img_data/video_14_chunk_59.jpg"}
{"video": "video_14", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "a 12 i'm sorry this is just 2/3 not v by of i'm sorry this is just 2/3 not v by of f just 2/3 this is 1/2 and this is 1/3 that's basically six 1/3 1/2/3 and that's 2/3 also and that's", "image_path": "img_data/video_14_chunk_60.jpg"}
{"video": "video_14", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "1/2/3 and that's 2/3 also and that's 2/3 also and that's basically the solution of the value basically the solution of the value function as a vector of across all states i make a mistake over here it this 2/3 + 1 half which is 56 that's the vector of", "image_path": "img_data/video_14_chunk_61.jpg"}
{"video": "video_14", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "half which is 56 that's the vector of that's the vector of solutions and what we will see with solutions and what we will see with monte carlo i'm just going to show you monte carlo i'm just going to show you the graph of which is basically the left the graph of which is basically the left side kind of graph here what we will side kind of graph here what we will see with mon carlo is that we are going see with mon carlo is that we are going to this is basically what we to this is basically what we estimated the black line over estimated the black line over here for all the states this is here for all the states this is basically the true values from the dynamic programming values from the dynamic programming solution and what the m car actually solution and what the m car actually will start is will start with some kind", "image_path": "img_data/video_14_chunk_62.jpg"}
{"video": "video_14", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "solution and what the m car actually will start is will start with some kind will start is will start with some kind of initial kind of vector of initial kind of vector of values and it will start values and it will start trying to estimate the value function trying to estimate the value function based on a number of trajectories with based on a number of trajectories with this averaging the running average that we averaging the running average that we have just discussed this is with one have just discussed this is with one trajectory or one episode 10 episodes trajectory or one episode 10 episodes the green line and the 100 episode the green line and the 100 episode estimates are the ones which are estimates are the ones which are obviously going to be coming closer", "image_path": "img_data/video_14_chunk_63.jpg"}
{"video": "video_14", "start": "0:32:00", "end": "0:32:05.500000", "timestamp": "0:32:00 - 0:32:05.500000", "text": "estimates are the ones which are obviously going to be coming closer obviously going to be coming closer and closer into the true value and closer into the true value function", "image_path": "img_data/video_14_chunk_64.jpg"}
{"video": "video_15", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we actually will see how we will be able to provide how we will be able to provide approximate solutions to the mdp approximate solutions to the mdp problems that and that we have problems that and that we have specified via the definition of the specified via the definition of the optimality bman optimality recursions as we've seen from the recursions as we've seen from the recycling kind of robot we have recycling kind of robot we have nonlinear equations to solve exhaustive nonlinear equations to solve exhaustive search can be used for these solving search can be used for these solving nonlinear equations however they're nonlinear equations however they're extremely expensive and completely", "image_path": "img_data/video_15_chunk_0.jpg"}
{"video": "video_15", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "nonlinear equations however they're extremely expensive and completely extremely expensive and completely invasible to solve such invasible to solve such equations for many practical problems equations for many practical problems where the number of states and actions where the number of states and actions are usually fairly large and therefore are usually fairly large and therefore the u even for small problems the number the u even for small problems the number of states are exploding and therefore of states are exploding and therefore it is kind of we need to come up to some it is kind of we need to come up to some approximations we actually look here approximations we actually look here at two algorithms which are very i at two algorithms which are very i will call it popular to show solve", "image_path": "img_data/video_15_chunk_1.jpg"}
{"video": "video_15", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "at two algorithms which are very i will call it popular to show solve will call it popular to show solve these type of problems the first these type of problems the first is called policy iteration and the other is called policy iteration and the other is actually called value iteration is actually called value iteration both of them are used extensively for both of them are used extensively for solving this problems we'll go through solving this problems we'll go through each one of them as we will see those each one of them as we will see those algorithms are kind of contain very algorithms are kind of contain very familiar to us steps the familiar to us steps the prediction step and this actually shown prediction step and this actually shown here where it shows the block diagram here where it shows the block diagram focused on the policy itation which is", "image_path": "img_data/video_15_chunk_2.jpg"}
{"video": "video_15", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "here where it shows the block diagram focused on the policy itation which is focused on the policy itation which is actually a kind of fairly key algorithm actually a kind of fairly key algorithm to kind of understand it involves to kind of understand it involves the policy evaluation given a the policy evaluation given a policy we are evaluating it in terms of policy we are evaluating it in terms of producing let's say the u value producing let's say the u value function for let's say the all the function for let's say the all the states of the problem and we are states of the problem and we are obviously using the iterative policy obviously using the iterative policy evaluation scheme that we have", "image_path": "img_data/video_15_chunk_3.jpg"}
{"video": "video_15", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "obviously using the iterative policy evaluation scheme that we have evaluation scheme that we have already seen and then what we do is already seen and then what we do is we and this is where the we and this is where the approximation is we are taking a approximation is we are taking a greedy step over here to produce a new greedy step over here to produce a new improved hopefully policy and the first improved hopefully policy and the first step was prediction step as we discussed step was prediction step as we discussed now this step is the second step is now this step is the second step is going to be called the control step going to be called the control step because effectively we are because effectively we are controlling the policy on that step controlling the policy on that step by changing it and that's basically by changing it and that's basically it's a heuristic that we are using", "image_path": "img_data/video_15_chunk_4.jpg"}
{"video": "video_15", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "by changing it and that's basically it's a heuristic that we are using it's a heuristic that we are using the greedy approach is the kind of a the greedy approach is the kind of a heuristic approach we are using and in heuristic approach we are using and in terms of what is really happening i terms of what is really happening i think it's always a good idea to have think it's always a good idea to have this kind of contraction in our this kind of contraction in our mind where we start with some kind of a mind where we start with some kind of a initial values of value function and an initial values of value function and an initial kind of policy we are evaluating initial kind of policy we are evaluating this policy and therefore produce", "image_path": "img_data/video_15_chunk_5.jpg"}
{"video": "video_15", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "initial kind of policy we are evaluating this policy and therefore produce this policy and therefore produce another vipi for that specific another vipi for that specific kind of policy after we take a greedy kind of policy after we take a greedy kind of step we move into a new policy kind of step we move into a new policy and that we have to evaluate again and that we have to evaluate again and the whole thing is kind of and the whole thing is kind of repeated and actually can be shown that repeated and actually can be shown that ultimately we end up in both the desired ultimately we end up in both the desired v star and the associated by star and v star and the associated by star and i think it's an example will actually i think it's an example will actually help us understand the second part the", "image_path": "img_data/video_15_chunk_6.jpg"}
{"video": "video_15", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "i think it's an example will actually help us understand the second part the help us understand the second part the control part where what exactly this control part where what exactly this control part is doing and that example control part is doing and that example that i want to use is the example from that i want to use is the example from the kind of a grid world all right we the kind of a grid world all right we have the usual grid world that we have the usual grid world that we have seen also during policy evaluation have seen also during policy evaluation with the associated if you python with the associated if you python code as we have seen is exactly the same code as we have seen is exactly the same kind of problem we have a random policy kind of problem we have a random policy equ probable random policy over here equ probable random policy over here with four possible", "image_path": "img_data/video_15_chunk_7.jpg"}
{"video": "video_15", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "equ probable random policy over here with four possible actions and we already have seen actions and we already have seen the starting state we already have the starting state we already have seen the complete sort of seen the complete sort of convergence via this kind of iterative convergence via this kind of iterative approach to iterative function approach to iterative function value function evaluation over here value function evaluation over here we are starting from the sort of we are starting from the sort of well all zero value function and", "image_path": "img_data/video_15_chunk_8.jpg"}
{"video": "video_15", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "we are starting from the sort of well all zero value function and well all zero value function and the starting policy which is this equ the starting policy which is this equ probable random policy and that the probable random policy and that the second part is the new thing for us second part is the new thing for us we have we already can go through we have we already can go through and start understanding at each step of and start understanding at each step of the algorith what actually is happening the algorith what actually is happening the we compute for that random the we compute for that random policy a value function and that", "image_path": "img_data/video_15_chunk_9.jpg"}
{"video": "video_15", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "the we compute for that random policy a value function and that policy a value function and that is this specific kind of table and is this specific kind of table and then the second step which is the gitty then the second step which is the gitty step is kind of easily understood here step is kind of easily understood here the value functions we greedily the value functions we greedily moving selecting actions towards moving selecting actions towards states that will improve our value if the improve our value if the value is zero and the value over here value is zero and the value over here is minus one out of the four possible is minus one out of the four possible actions in this kind of state we have", "image_path": "img_data/video_15_chunk_10.jpg"}
{"video": "video_15", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "is minus one out of the four possible actions in this kind of state we have actions in this kind of state we have selected the one that will actually move selected the one that will actually move the reward will improve the reward will improve the reward and therefore it will reach if you and therefore it will reach if you this kind of terminating kind of this kind of terminating kind of state and in that where the value is state and in that where the value is actually also higher the greedy part actually also higher the greedy part is to move towards states based on the is to move towards states based on the values of the each state and that's basically what we do state and that's basically what we do then now we have a new policy that new", "image_path": "img_data/video_15_chunk_11.jpg"}
{"video": "video_15", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "state and that's basically what we do then now we have a new policy that new then now we have a new policy that new policy will also need to be evaluated policy will also need to be evaluated there may be several steps that are there may be several steps that are needed to evaluate if you that kind needed to evaluate if you that kind of policy but if we are to evaluate this of policy but if we are to evaluate this kind of policy we'll and one more time kind of policy we'll and one more time we'll actually this new policy again we'll actually this new policy again it's a new policy now we have a new it's a new policy now we have a new policy this will actually result into policy this will actually result into this table of the value function this table of the value function and this table of this value function and this table of this value function again by selecting the greedy", "image_path": "img_data/video_15_chunk_12.jpg"}
{"video": "video_15", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "and this table of this value function again by selecting the greedy again by selecting the greedy approach compare what where we were approach compare what where we were over here you notice that we were in over here you notice that we were in this new policy still at a equ probable this new policy still at a equ probable state over here we have minus two this minus two will have two this minus two will have two possible actions that are optimal to possible actions that are optimal to move to greedily to this state or to move to greedily to this state or to this state where the value is minus", "image_path": "img_data/video_15_chunk_13.jpg"}
{"video": "video_15", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "move to greedily to this state or to this state where the value is minus this state where the value is minus 1.7 we are selecting if you 1.7 we are selecting if you the actions that it will be the actions that it will be the will actually be the ones will actually be the ones will give us the largest possible will give us the largest possible value as far as this state is concerned value as far as this state is concerned and that is basically the two and that is basically the two actions that we have selected out of the actions that we have selected out of the four here we have yet another four here we have yet another new policy for that specific new policy for that specific new policy we are going to evaluate", "image_path": "img_data/video_15_chunk_14.jpg"}
{"video": "video_15", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "new policy for that specific new policy we are going to evaluate policy we are going to evaluate again this policy mind you in some cases again this policy mind you in some cases there are two decimal digits which are there are two decimal digits which are going to determine the best possible going to determine the best possible actions that we are selecting in the actions that we are selecting in the gritty step and because here we are gritty step and because here we are presenting a single decimal dig digit presenting a single decimal dig digit then the selections may not be entirely then the selections may not be entirely evident but they are resolved if you are evident but they are resolved if you are to run the actual code that we to run the actual code that we also provide the python code of all also provide the python code of all right that's a new policy and we", "image_path": "img_data/video_15_chunk_15.jpg"}
{"video": "video_15", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "also provide the python code of all right that's a new policy and we right that's a new policy and we continue and after a while we observe continue and after a while we observe also that the policy is not changing also that the policy is not changing and this policy has converged to and this policy has converged to what we call the optimal policy p star and this is basically the conclusion of the algorithm which is conclusion of the algorithm which is actually called the policy iteration we actually called the policy iteration we have provided python code for another have provided python code for another grid world", "image_path": "img_data/video_15_chunk_16.jpg"}
{"video": "video_15", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "have provided python code for another grid world slightly different than this one that slightly different than this one that you can actually see on the links of you can actually see on the links of the on the links of this page the on the links of this page over here in this kind of repository and over here in this kind of repository and the last algorith will actually we see the last algorith will actually we see for approximate dynamic programming for approximate dynamic programming solutions to mdp problems finite mdp solutions to mdp problems finite mdp problems is the -called value iteration which is a -called value iteration which is a special case of policy iteration in special case of policy iteration in policy iteration we already have seen in policy iteration we already have seen in that other example over here", "image_path": "img_data/video_15_chunk_17.jpg"}
{"video": "video_15", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "policy iteration we already have seen in that other example over here that other example over here that we had mentioned the blocks the that we had mentioned the blocks the grid world kind of problem that the grid world kind of problem that already we have seen that the policy already we have seen that the policy was obtained much earlier than an was obtained much earlier than an exact evaluation of the value function exact evaluation of the value function was available to us that kind of was available to us that kind of motivated the value iteration which motivated the value iteration which is i will call it a shortcut that is i will call it a shortcut that actually we can take is instead of doing", "image_path": "img_data/video_15_chunk_18.jpg"}
{"video": "video_15", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "is i will call it a shortcut that actually we can take is instead of doing actually we can take is instead of doing the value the policy evaluation the value the policy evaluation with all the iterations we just do with all the iterations we just do just one iteration and despite the fact just one iteration and despite the fact that the policy evaluation the that the policy evaluation the evaluation of the policy is not really evaluation of the policy is not really yeah it is trending but it is yeah it is trending but it is definitely not exact we are able to definitely not exact we are able to get convergence still by exercising get convergence still by exercising exactly the second step as we have", "image_path": "img_data/video_15_chunk_19.jpg"}
{"video": "video_15", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "get convergence still by exercising exactly the second step as we have exactly the second step as we have seen it u earlier in the polic seen it u earlier in the polic iteration the second step is iteration the second step is maintained as is but the first step is maintained as is but the first step is we apply this kind of we apply this kind of shortcut and this is basically the shortcut and this is basically the what people are calling the kind the what people are calling the kind of the value iteration the of the value iteration example which in value iteration example which in terms of python code is prov right over terms of python code is prov right over here is the grid word that is also here is the grid word that is also available in your textbook with", "image_path": "img_data/video_15_chunk_20.jpg"}
{"video": "video_15", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "here is the grid word that is also available in your textbook with available in your textbook with slightly different kind of final rewards slightly different kind of final rewards but still with the same model but still with the same model transition model that we see here and transition model that we see here and we start to evaluate if you the we start to evaluate if you the iterate iteratively the policy iterate iteratively the policy evaluation kind of step as the evaluation kind of step as the formula is exactly the same as we formula is exactly the same as we have seen it earlier we have to solve have seen it earlier we have to solve this nonlinear equation now this l", "image_path": "img_data/video_15_chunk_21.jpg"}
{"video": "video_15", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "have seen it earlier we have to solve this nonlinear equation now this l this nonlinear equation now this l linear equation is going to be l linear equation is going to be solved as follows first we are going solved as follows first we are going ahead and defining the capital p ahead and defining the capital p matrix in fact is a it's not really a matrix in fact is a it's not really a matrix it's a dictionary where the matrix it's a dictionary where the values of the dictionary are basically values of the dictionary are basically the 11 states of the problem we have a the 11 states of the problem we have a 3x4 world with one state being not 3x4 world with one state being not possible to make to go to it is 11 possible to make to go to it is 11 states total and for each of the states total and for each of the states we actually can specify to what state we", "image_path": "img_data/video_15_chunk_22.jpg"}
{"video": "video_15", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "states total and for each of the states we actually can specify to what state we actually can specify to what state we actually can go to which what are the actually can go to which what are the three children states or we can reach three children states or we can reach from each of the states from state from each of the states from state zero we can go to zero with sort of this kind of zero with sort of this kind of probability from state zero we can go to probability from state zero we can go to one with this probability and from state one with this probability and from state zero we can go to state four with zero we can go to state four with this probability and if we are to", "image_path": "img_data/video_15_chunk_23.jpg"}
{"video": "video_15", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "zero we can go to state four with this probability and if we are to this probability and if we are to take action zero where action zero is take action zero where action zero is defined here to be let's say north defined here to be let's say north similarly we have three other tablets similarly we have three other tablets for that indicate the what the for that indicate the what the transitions for east south and west and transitions for east south and west and in general for each row we have four in general for each row we have four tablets and or equivalent we have four tablets and or equivalent we have four matrices of 11 by 11 as we have seen in matrices of 11 by 11 as we have seen in an earlier video when we defined the an earlier video when we defined the transition model all right then we", "image_path": "img_data/video_15_chunk_24.jpg"}
{"video": "video_15", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "an earlier video when we defined the transition model all right then we transition model all right then we actually exercise the value actually exercise the value iteration as we discussed the value iteration as we discussed the value iteration is going to involve just iteration is going to involve just one update of the value function here one update of the value function here we see the formula that the of the v we see the formula that the of the v star as it is being exercised only star as it is being exercised only for one iteration i is the number of for one iteration i is the number of iterations it goes from 1 to 100 iterations it goes from 1 to 100 this is just one update of the v star", "image_path": "img_data/video_15_chunk_25.jpg"}
{"video": "video_15", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "iterations it goes from 1 to 100 this is just one update of the v star this is just one update of the v star based on the q stars which are resulting based on the q stars which are resulting from taking and committing to a specific from taking and committing to a specific action we have four action therefore action we have four action therefore we have two star values and then we take the maximum values and then we take the maximum out of those and to determine our v out of those and to determine our v star that is the backup bman optimality star that is the backup bman optimality backup diagram we have seen also in an backup diagram we have seen also in an earlier video finally we are printing earlier video finally we are printing that optimal policy as we will see", "image_path": "img_data/video_15_chunk_26.jpg"}
{"video": "video_15", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "earlier video finally we are printing that optimal policy as we will see that optimal policy as we will see despite the fact that initially our despite the fact that initially our value function is not really at all value function is not really at all close to what it will end up being close to what it will end up being the policy has reached a point of the policy has reached a point of convergence which will be extremely convergence which will be extremely quick and therefore that kind of quick and therefore that kind of proves the point that we don't have to proves the point that we don't have to have convergence of the value function have convergence of the value function the as we said the final value the as we said the final value function is going to be this one", "image_path": "img_data/video_15_chunk_27.jpg"}
{"video": "video_15", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "the as we said the final value function is going to be this one function is going to be this one but the same policy kind of continues but the same policy kind of continues much early u it converges much very much early u it converges much very quickly and maintains the same quickly and maintains the same sort of policy from let's say the sort of policy from let's say the fifth sixth iteration this was the second iteration this was the second algorith that i wanted to cover and i algorith that i wanted to cover and i want to close this kind of algorithmic want to close this kind of algorithmic kind of discussion with some kind of discussion with some discussion about how you want to study a discussion about how you want to study a little bit about this type of algorithms", "image_path": "img_data/video_15_chunk_28.jpg"}
{"video": "video_15", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "discussion about how you want to study a little bit about this type of algorithms little bit about this type of algorithms evidently there is a lot of math evidently there is a lot of math involved in mark of decision involved in mark of decision processes we started with a lot of processes we started with a lot of definitions the sort of u domain is definitions the sort of u domain is if you reach and but it's if you reach and but it's worthwhile kind of paying attention worthwhile kind of paying attention initially at the definitions because without really a definitions because without really a deep understanding of what is really deep understanding of what is really going on at a foundational kind of level going on at a foundational kind of level is difficult to understand how these", "image_path": "img_data/video_15_chunk_29.jpg"}
{"video": "video_15", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "going on at a foundational kind of level is difficult to understand how these is difficult to understand how these algorith actually are working the algorith actually are working the more most important is to exercise with more most important is to exercise with various examples you're understanding various examples you're understanding we started with a couple of we started with a couple of examples only some of them were kind examples only some of them were kind of written down with a pencil and paper of written down with a pencil and paper but really the experimentation with but really the experimentation with actual python code will just such as actual python code will just such as this example start with simple this example start with simple examples in your for site will be", "image_path": "img_data/video_15_chunk_30.jpg"}
{"video": "video_15", "start": "0:15:30", "end": "0:15:59", "timestamp": "0:15:30 - 0:15:59", "text": "this example start with simple examples in your for site will be examples in your for site will be other examples as well that we can other examples as well that we can experiment with and when we go into experiment with and when we go into the next topic which is reinforcement the next topic which is reinforcement learning where we do not have an learning where we do not have an assumption about the full observability assumption about the full observability of the mdp then things will of the mdp then things will actually become a little bit even actually become a little bit even more complicated than that where we have more complicated than that where we have now to learn some of the things that now to learn some of the things that we wanted to we assumed as known here we wanted to we assumed as known here in mdp", "image_path": "img_data/video_15_chunk_31.jpg"}
{"video": "video_16", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "and now that we have seen how we will be able to calculate the q and the able to calculate the q and the v functions for an let's say and the v functions for an let's say an environment either deterministic or an environment either deterministic or stochastic what we want to do is we stochastic what we want to do is we want to start discussing about two other want to start discussing about two other functions which are kind of essentials functions which are kind of essentials in order for us to solve the mdp problem in order for us to solve the mdp problem and by solving the mdp problem evidently and by solving the mdp problem evidently we mean to find the optimal policy we mean to find the optimal policy that's basically what we are sort of that's basically what we are sort of trying to do the", "image_path": "img_data/video_16_chunk_0.jpg"}
{"video": "video_16", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "that's basically what we are sort of trying to do the there are two equations and two other there are two equations and two other functions that allow us to actually functions that allow us to actually provide that kind of solution of course provide that kind of solution of course they are based on the wellknown by noun they are based on the wellknown by noun vaq functions and they are called the vaq functions and they are called the -cal bellman -cal bellman optimality conditions or functions optimality conditions or functions and the first one is simply says and the first one is simply says the following something kind of the following something kind of initially it looks a bit confusing but initially it looks a bit confusing but it is kind of fairly intuitive", "image_path": "img_data/video_16_chunk_1.jpg"}
{"video": "video_16", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "initially it looks a bit confusing but it is kind of fairly intuitive it is kind of fairly intuitive if you have a range of policies that if you have a range of policies that you have evaluated and these policies you have evaluated and these policies each policy will give us if you a v each policy will give us if you a v pi of s then you can actually take pi of s then you can actually take the maximum for across all of those the maximum for across all of those value functions each one of them value functions each one of them as we said corresponds to a policy to as we said corresponds to a policy to determine what we call the optimal determine what we call the optimal value function and this applies to", "image_path": "img_data/video_16_chunk_2.jpg"}
{"video": "video_16", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "determine what we call the optimal value function and this applies to value function and this applies to both the v functions and the q functions both the v functions and the q functions we'll be calling this optimal v we'll be calling this optimal v functions v stars and the op the functions v stars and the op the corresponding q stars q functions q corresponding q stars q functions q stars and we will be using them in stars and we will be using them in order for us to be able to reach if order for us to be able to reach if you the that kind of designed you the that kind of designed outcome which is really the optimal kind outcome which is really the optimal kind of policy the way to actually of policy the way to actually understand this is probably u using understand this is probably u using an example and then we will see how the", "image_path": "img_data/video_16_chunk_3.jpg"}
{"video": "video_16", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "understand this is probably u using an example and then we will see how the an example and then we will see how the recursive formulas kind of looks look recursive formulas kind of looks look we have actually over here an we have actually over here an a very simple example which i think is a very simple example which i think is quite instructive and this is actually quite instructive and this is actually called this kind of a cleaning robot and called this kind of a cleaning robot and the cleaning robot environment is as the cleaning robot environment is as follows there are effectively six follows there are effectively six states the state zero is a battery states the state zero is a battery tocharge state and the state five is tocharge state and the state five is the because of acquisition of a", "image_path": "img_data/video_16_chunk_4.jpg"}
{"video": "video_16", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "tocharge state and the state five is the because of acquisition of a the because of acquisition of a of garbage and the robot is picking garbage and the robot is picking up that kind of garbage and then that is basically of garbage and then that is basically the state that the transition from the state that the transition from state four to state five results in the state four to state five results in the reward of five while the transition from reward of five while the transition from one to zero results with a reward of one to zero results with a reward of results with a reward of one and all of results with a reward of one and all of other rewards are zero the transition position the", "image_path": "img_data/video_16_chunk_5.jpg"}
{"video": "video_16", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "other rewards are zero the transition position the zero the transition position the environment is could be a deterministic environment is could be a deterministic environment or could be a stochastic environment or could be a stochastic environment let's take a look at a environment let's take a look at a stochastic environment because in stochastic environment because in that environment we have if you that environment we have if you the determination of the v star and the determination of the v star and the qar more specifically in this the qar more specifically in this specific example we are working with specific example we are working with the qar and in the deterministic the qar and in the deterministic environment we have a probability of environment we have a probability of 0.8 to reach the desired state let's", "image_path": "img_data/video_16_chunk_6.jpg"}
{"video": "video_16", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "environment we have a probability of 0.8 to reach the desired state let's 0.8 to reach the desired state let's say go left or right that these are the say go left or right that these are the two actions we have either left or right two actions we have either left or right and the u but there is also a and the u but there is also a probability of other probabilities probability of other probabilities non-zero probabilities to not achieve non-zero probabilities to not achieve the goal and in this case no the goal and in this case no transition to the desired state stay transition to the desired state stay put with the probability 0.15 and put with the probability 0.15 and move in the opposite in fact direction move in the opposite in fact direction with probability 5% this is what with probability 5% this is what makes the environment stochastic as we", "image_path": "img_data/video_16_chunk_7.jpg"}
{"video": "video_16", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "with probability 5% this is what makes the environment stochastic as we makes the environment stochastic as we discussed and u what we actually can discussed and u what we actually can do for each of the for each of do for each of the actions for all states we will be the actions for all states we will be let's say there are two possible let's say there are two possible intended actions to go left or right intended actions to go left or right for each of the possible actions we for each of the possible actions we will be able to get the q functions a q will be able to get the q functions a q of s comma a as we have just said and", "image_path": "img_data/video_16_chunk_8.jpg"}
{"video": "video_16", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "will be able to get the q functions a q of s comma a as we have just said and of s comma a as we have just said and we are going ahead and using the we are going ahead and using the welln by now bman expectation backups welln by now bman expectation backups and the associated kind of iterative and the associated kind of iterative way of solving for the q function we way of solving for the q function we have plotted over here the blue and have plotted over here the blue and the dashed red lines the blue and the dashed red lines the blue and the dashed red lines what qar is the dashed red lines what qar is actually going to give us the qar is actually going to give us the qar is going to be", "image_path": "img_data/video_16_chunk_9.jpg"}
{"video": "video_16", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "actually going to give us the qar is going to be the maximum across this just two polic the maximum across this just two polic we have here for all states it we have here for all states it will actually be the qar will be the will actually be the qar will be the blue level over here the red level over blue level over here the red level over there and on up all the way until there and on up all the way until state four and of course we have going state four and of course we have going to have the same value for 0 and five to have the same value for 0 and five but 0 and five states are kind of but 0 and five states are kind of terminating states this is actually terminating states this is actually coming out of textbook of a textbook coming out of textbook of a textbook that i have included in the references that i have included in the references where that introduced us es if you", "image_path": "img_data/video_16_chunk_10.jpg"}
{"video": "video_16", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "that i have included in the references where that introduced us es if you where that introduced us es if you this example understand now another example understand now another example where we have just working with the where we have just working with the vistar and calculating if you the vistar and calculating if you the vistar we have here the -cal recycling vistar we have here the -cal recycling robot example from sat's textbook robot example from sat's textbook that this robot is just basically that this robot is just basically collects soda cans and it has collects soda cans and it has effectively two states in that effectively two states in that represent if you battery level represent if you battery level the higher and low and the at each", "image_path": "img_data/video_16_chunk_11.jpg"}
{"video": "video_16", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "represent if you battery level the higher and low and the at each the higher and low and the at each state the agent has to make certain state the agent has to make certain decisions e either to continue to search decisions e either to continue to search for the cans to remain stationary for the cans to remain stationary and wait for someone to bring at a and wait for someone to bring at a can to head back to the charger and can to head back to the charger and recharge the battery and recharge the battery and effectively we have action sets which are the determined", "image_path": "img_data/video_16_chunk_12.jpg"}
{"video": "video_16", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "effectively we have action sets which are the determined action sets which are the determined based on what makes sense for the robot based on what makes sense for the robot to do for example not necessarily to do for example not necessarily to recharge the battery when the state of recharge the battery when the state of the battery is high and definitely we the battery is high and definitely we need to do that when it's low we need to do that when it's low we define if you two different sets define if you two different sets of actions that each one of actions that each one corresponds to the state of corresponds to the state of the robot the rewards are zero most the robot the rewards are zero most of the time but they are becoming of the time but they are becoming positive when the robot secures an empty", "image_path": "img_data/video_16_chunk_13.jpg"}
{"video": "video_16", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "of the time but they are becoming positive when the robot secures an empty positive when the robot secures an empty can and or large and negative if we just can and or large and negative if we just basically exhaust the battery without basically exhaust the battery without really being able to reach the really being able to reach the charger the evidently charger the evidently this is a kind of a more complicated this is a kind of a more complicated kind of a example where we have kind of a example where we have here some other kind of here some other kind of parameters the robot is parameters the robot is search around and this", "image_path": "img_data/video_16_chunk_14.jpg"}
{"video": "video_16", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "parameters the robot is search around and this search around and this obviously this results into kind of obviously this results into kind of battery being gradually depleted we battery being gradually depleted we are providing some kind of probability are providing some kind of probability for that for the battery to go for that for the battery to go into depletion kind of state and for into depletion kind of state and for change state if the energy is high change state if the energy is high then a period that's basically if it period that's basically if it is at the high state then it will be a is at the high state then it will be a depletion probability the depletion probability the depletion probability is 1us alpha", "image_path": "img_data/video_16_chunk_15.jpg"}
{"video": "video_16", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "depletion probability the depletion probability is 1us alpha depletion probability is 1us alpha over here and maintains if you the here and maintains if you the high state with the probability of alpha high state with the probability of alpha when the period of searching when the period of searching undertaken with an energy level is low undertaken with an energy level is low then there is remains low with beta and then there is remains low with beta and depletes the battery probability of one depletes the battery probability of one minus better in that latter minus better in that latter stage the robot must be rescued and the stage the robot must be rescued and the battery is then recharged back to high", "image_path": "img_data/video_16_chunk_16.jpg"}
{"video": "video_16", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "stage the robot must be rescued and the battery is then recharged back to high battery is then recharged back to high all of these are represented all of these are represented by a finite state machine finite by a finite state machine finite state machine is a very well used very state machine is a very well used very popular way of determining if you popular way of determining if you rules and actions based on states and rules and actions based on states and things that in practical systems things that in practical systems this is the final state machine this is the final state machine for the that recycling robot with a for the that recycling robot with a corresponding actions that are", "image_path": "img_data/video_16_chunk_17.jpg"}
{"video": "video_16", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "for the that recycling robot with a corresponding actions that are corresponding actions that are the solid circles over here and the solid circles over here and the corresponding probabilities and corresponding probabilities and rewards the probabilities are ob rewards the probabilities are ob evidently associated with the evidently associated with the probability of transitioning from a state given transitioning from a state given an action to another state an action to another state effectively this s a s prime the effectively this s a s prime the triplets that we have seen and for each triplets that we have seen and for each s a prime triplet we have a correspond s a prime triplet we have a correspond reward evidently the environment is", "image_path": "img_data/video_16_chunk_18.jpg"}
{"video": "video_16", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "s a prime triplet we have a correspond reward evidently the environment is reward evidently the environment is stochastic therefore the reward stochastic therefore the reward function must be a triplet as function must be a triplet as compared to the terministic environments compared to the terministic environments where the reward function is just where the reward function is just two terms sna since in the deterministic two terms sna since in the deterministic environment the s primes are environment the s primes are the expected ones and we don't have any the expected ones and we don't have any stochasticity in the environment we stochasticity in the environment we don't have to specify them don't have to specify them that's basically it that those are the", "image_path": "img_data/video_16_chunk_19.jpg"}
{"video": "video_16", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "don't have to specify them that's basically it that those are the that's basically it that those are the table over here kind of captures the table over here kind of captures the complete environment and what we the complete environment and what we will do now is we'll calculate the will do now is we'll calculate the -call v as an expression from -call v as an expression from effectively repeating exactly what we effectively repeating exactly what we have seen earlier with the have seen earlier with the expectation backup but now this backup expectation backup but now this backup is actually going to be called is actually going to be called optimality backup to solve this kind of example we backup to solve this kind of example we just want to write the v star just want to write the v star of", "image_path": "img_data/video_16_chunk_20.jpg"}
{"video": "video_16", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "just want to write the v star of s as an expression and this expression s as an expression and this expression will actually be the we definitely will actually be the we definitely going to have a max operator for going to have a max operator for all actions that belong if you to all actions that belong if you to the set of ation for that kind of state the set of ation for that kind of state and over here we have said that is and over here we have said that is going to be q pi star of s comma", "image_path": "img_data/video_16_chunk_21.jpg"}
{"video": "video_16", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "q pi star of s comma a i mean this makes sense and this of a i mean this makes sense and this of course is identical to course is identical to maximum of v pi of s across policy pi maximum of v pi of s across policy pi but actually we're writing this but actually we're writing this way because it is we are able to sort way because it is we are able to sort of i mean get the sort of the of i mean get the sort of the expression kind of an easier way to expression kind of an easier way to understand if we replace the", "image_path": "img_data/video_16_chunk_22.jpg"}
{"video": "video_16", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "expression kind of an easier way to understand if we replace the understand if we replace the expression for the q function the expression for the q function the expression of the q function was expression of the q function was again this is we have max a i'm going to again this is we have max a i'm going to skip this a of s for simplicity here we skip this a of s for simplicity here we had an expectation over pi star and again the pi over pi star this star and again the pi over pi star this expression i hope it is understood that expression i hope it is understood that behind this kind of the p star behind this kind of the p star over here is in fact a conditional over here is in fact a conditional it is an expectation over the sort", "image_path": "img_data/video_16_chunk_23.jpg"}
{"video": "video_16", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "over here is in fact a conditional it is an expectation over the sort it is an expectation over the sort of s prime states that we the future of s prime states that we the future states that we are able to states that we are able to hit by taking a optimal actions to hit by taking a optimal actions we have here the return we have here the return the expected returns in other words the expected returns in other words given that we are at t at state s and given that we are at t at state s and we are taking an action", "image_path": "img_data/video_16_chunk_24.jpg"}
{"video": "video_16", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "given that we are at t at state s and we are taking an action a and we can actually continue here is an expectation what is really the expectation what is really the that expectation can actually now be that expectation can actually now be written the derivation is not going written the derivation is not going to necessarily be given to necessarily be given here v star", "image_path": "img_data/video_16_chunk_25.jpg"}
{"video": "video_16", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "to necessarily be given here v star s + one given s is = s a t isal to given s is = s a t isal to a and finally it is max over a and finally it is max over a of replacing the expectation with a of replacing the expectation with summation over s prime and rewards r of summation over s prime and rewards r of the mdp dynamics", "image_path": "img_data/video_16_chunk_26.jpg"}
{"video": "video_16", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "summation over s prime and rewards r of the mdp dynamics r + gamma v star of s prime which of course says the star of s prime which of course says the following the value of a state under an optimal", "image_path": "img_data/video_16_chunk_27.jpg"}
{"video": "video_16", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "under an optimal policy must be equal to the expected return must be equal to the expected return must be equal to the expected return for the best action", "image_path": "img_data/video_16_chunk_28.jpg"}
{"video": "video_16", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "return must be equal to the expected return for the best action from that state and this is basically what the u v kind of represents and in addition kind of represents and in addition to that i think it's worthwhile drawing also the backup diagram", "image_path": "img_data/video_16_chunk_29.jpg"}
{"video": "video_16", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "drawing also the backup diagram from that kind of state s we have a set of actions if we a set of actions if we take the maximum operator over each of these actions operator over each of these actions is associated with a state action is associated with a state action pair if we take the maximum out of pair if we take the maximum out of these actions state action sorry state action", "image_path": "img_data/video_16_chunk_30.jpg"}
{"video": "video_16", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "these actions state action sorry state action value functions which is actually q value functions which is actually q for if we take the maximum out of these for if we take the maximum out of these kind of q functions then effectively we kind of q functions then effectively we are defining over here we have defining over here we have q pi star first comma a and then we pi star first comma a and then we have over here we have v star have over here we have v star of", "image_path": "img_data/video_16_chunk_31.jpg"}
{"video": "video_16", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "have over here we have v star of s if we knew the optimal q function for each of these kind of function for each of these kind of states we are able to determine with the states we are able to determine with the max operator the optimal v function for max operator the optimal v function for this datas the other thing i think is the other thing i think is worthwhile kind of mentioning is what worthwhile kind of mentioning is what we have mentioned earlier i want to put", "image_path": "img_data/video_16_chunk_32.jpg"}
{"video": "video_16", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "worthwhile kind of mentioning is what we have mentioned earlier i want to put we have mentioned earlier i want to put this in writing the thing is that writing the thing is that the for each state s we go through all policies this was the cleaning robot example we go through all policies a buy", "image_path": "img_data/video_16_chunk_33.jpg"}
{"video": "video_16", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "policies this was the cleaning robot example we go through all policies a buy example we go through all policies a buy and pick up the policy that maximizes vy office the maximization it's done", "image_path": "img_data/video_16_chunk_34.jpg"}
{"video": "video_16", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "maximization it's done independently for each state and we may end up with a different", "image_path": "img_data/video_16_chunk_35.jpg"}
{"video": "video_16", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "up with a different policies that maximize policies by that maximize policies by that maximize v pi of s for different states and this is basically what we have seen in this kind of a cleaning have seen in this kind of a cleaning robot example where the policy left robot example where the policy left was optimal on let's say state", "image_path": "img_data/video_16_chunk_36.jpg"}
{"video": "video_16", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "robot example where the policy left was optimal on let's say state was optimal on let's say state one and the policy right was actually one and the policy right was actually optimal in state two if we kind of do the exercise of showing exactly how the vistar of showing exactly how the vistar calculation will happen for the calculation will happen for the recycling kind of robot we have a just recycling kind of robot we have a just to repeat the v star of s is max over to repeat the v star of s is max over a of a summation from s prime across", "image_path": "img_data/video_16_chunk_37.jpg"}
{"video": "video_16", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "to repeat the v star of s is max over a of a summation from s prime across a of a summation from s prime across s prime and rewards of the mtp dynamics s prime and rewards of the mtp dynamics s prime r given s comma a of s prime r given s comma a of r+ gamma v star of s prime and for the recycling prime and for the recycling kind of robot what we need to do is kind of robot what we need to do is to calculate in other words a v to calculate in other words a v star for the h where h is the -called", "image_path": "img_data/video_16_chunk_38.jpg"}
{"video": "video_16", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "to calculate in other words a v star for the h where h is the -called star for the h where h is the -called high state high battery state and this is state high battery state and this is effectively max we have a number of max we have a number of actions that we are going to actions that we are going to have in the sort of high state and if have in the sort of high state and if you recall the problem statement the set you recall the problem statement the set of actions in the high state is of actions in the high state is just search and wait just search and wait while in the kind of a low state it", "image_path": "img_data/video_16_chunk_39.jpg"}
{"video": "video_16", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "just search and wait while in the kind of a low state it while in the kind of a low state it was we had actually three possible was we had actually three possible actions we will be symbolizing search with s weight with w we need to also produce the vistar w we need to also produce the vistar of the low state it will be of the low state it will be sort of recharge with r", "image_path": "img_data/video_16_chunk_40.jpg"}
{"video": "video_16", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "recharge with r because we don't want to use r because add is also the symbol for our because add is also the symbol for our instantaneous reward that now instantaneous reward that now that we have here we need to have we that we have here we need to have we expecting to see a summation of all expecting to see a summation of all future states s prime and associated future states s prime and associated rewards and the lookup table that we rewards and the lookup table that we had defined earlier in the in had defined earlier in the in that problem statement and if you go", "image_path": "img_data/video_16_chunk_41.jpg"}
{"video": "video_16", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "had defined earlier in the in that problem statement and if you go that problem statement and if you go back to that kind of a site then you back to that kind of a site then you can actually see the lookup table the can actually see the lookup table was giving us from s via in lookup table was giving us from s via in action to s prime it is really this action to s prime it is really this lookup table that we have to start lookup table that we have to start plugging in into this kind of equations plugging in into this kind of equations it is max of what now the it is max of what now the first is across two action i'm first is across two action i'm expecting to see at least over expecting to see at least over here first it is the probability of s", "image_path": "img_data/video_16_chunk_42.jpg"}
{"video": "video_16", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "expecting to see at least over here first it is the probability of s here first it is the probability of s prime being high given that we are in high and we are searching of the reward is going to be", "image_path": "img_data/video_16_chunk_43.jpg"}
{"video": "video_16", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "the reward is going to be r it's a triplet the reward function is a triplet high searching function is a triplet high searching high plus gamma v star of high plus probability of low given high comma plus probability of low given high comma [music] [music] searching are low searching sorry high", "image_path": "img_data/video_16_chunk_44.jpg"}
{"video": "video_16", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "searching are low searching sorry high low searching in fact searching low the action is in the middle plus gamma v star of l we have", "image_path": "img_data/video_16_chunk_45.jpg"}
{"video": "video_16", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "we have future states of s prime to be future states of s prime to be either remaining in the high or either remaining in the high or transitioning to low and that's transitioning to low and that's basically what we have here basically what we have here comma and we have the second action comma and we have the second action is in the high the second action is in the high state is to either search or weight state is to either search or weight the second action is weight we have the second action is weight we have exactly the same thing", "image_path": "img_data/video_16_chunk_46.jpg"}
{"video": "video_16", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "the second action is weight we have exactly the same thing kwh plus gamma v star of h plus probability of low now the second h plus probability of low now the second state s prime given high weight r of high weight low plus gamma v", "image_path": "img_data/video_16_chunk_47.jpg"}
{"video": "video_16", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "weight r of high weight low plus gamma v high weight low plus gamma v star of low that will actually be the expression for the varar be the expression for the varar high and then we can actually plug in high and then we can actually plug in the corresponding transition the corresponding transition probabilities here with the that were probabilities here with the that were given that involves alpha and betas and given that involves alpha and betas and come up with an associated reward fun come up with an associated reward fun functions values that are given in the", "image_path": "img_data/video_16_chunk_48.jpg"}
{"video": "video_16", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "come up with an associated reward fun functions values that are given in the functions values that are given in the table and come up with an table and come up with an expression that it is will involve expression that it is will involve only the v star i'm going to give only the v star i'm going to give you that expression by using if you that expression by using if you the site in the site you can the site in the site you can actually see the steps that we have actually see the steps that we have taken as well this is the final well this is the final expression that involves the rewards for searching the rewards", "image_path": "img_data/video_16_chunk_49.jpg"}
{"video": "video_16", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "involves the rewards for searching the rewards for searching the rewards for weight which are all numbers for weight which are all numbers the only unknown here is the v star the only unknown here is the v star of high we have v star of high over of high we have v star of high over there and v star of high on the right there and v star of high on the right hand side and also we have v star of low hand side and also we have v star of low on the right hand side that's on the right hand side that's the first equation evidently it's a the first equation evidently it's a nonlinear equation that's a first nonlinear equation that's a first nonlinear equation and we can actually nonlinear equation and we can actually go ahead and repeat the same exercise go ahead and repeat the same exercise for the second linear equation v star of for the second linear equation v star of low we have two nonlinear low we have two nonlinear equations with two unknowns and the u", "image_path": "img_data/video_16_chunk_50.jpg"}
{"video": "video_16", "start": "0:25:30", "end": "0:25:49.533333", "timestamp": "0:25:30 - 0:25:49.533333", "text": "low we have two nonlinear equations with two unknowns and the u equations with two unknowns and the u process that we will specify right process that we will specify right now called policy iteration is able to now called policy iteration is able to help us come up with a solution help us come up with a solution for this nonlinear problem", "image_path": "img_data/video_16_chunk_51.jpg"}
{"video": "video_17", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in fact we have another value function that i think it's worthwhile function that i think it's worthwhile mentioning in fact this second one which mentioning in fact this second one which we be calling the state value u sorry we be calling the state value u sorry the state action value function is the state action value function is actually bit more useful than just actually bit more useful than just the state value function the v the state value function the v function from now on i'll be calling function from now on i'll be calling this two functions the previous one we this two functions the previous one we have seen the state value function as a have seen the state value function as a v function and the second one which will", "image_path": "img_data/video_17_chunk_0.jpg"}
{"video": "video_17", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "have seen the state value function as a v function and the second one which will v function and the second one which will introduce now we will call it the state introduce now we will call it the state action value function and actually you action value function and actually you can see here its form is actually we can see here its form is actually we call it the q function and similar to call it the q function and similar to the previous one the v function it is an the previous one the v function it is an expectation it is an expectation because expectation it is an expectation because of the stochasticity of the environment of the stochasticity of the environment and the next states that we are sort and the next states that we are sort of going to end up visiting of", "image_path": "img_data/video_17_chunk_1.jpg"}
{"video": "video_17", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "and the next states that we are sort of going to end up visiting of going to end up visiting of what of the instantaneous of what of the instantaneous reward and plus gamma the state value reward and plus gamma the state value function of the next states and this one function of the next states and this one is of course requires us to commit on is of course requires us to commit on an action it's committing on an action it's committing on an action a and following the policy pi action a and following the policy pi after that this is effectively the reason why we effectively the reason why we actually have the commitment over actually have the commitment over here as conditional we commit we", "image_path": "img_data/video_17_chunk_2.jpg"}
{"video": "video_17", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "actually have the commitment over here as conditional we commit we here as conditional we commit we have condition on a and also that action have condition on a and also that action was taken while we were on state s and was taken while we were on state s and the backup equivalent backup d diagram the backup equivalent backup d diagram that we have seen is actually shown in that we have seen is actually shown in this kind of figure let me write this kind of figure let me write this down because it will actually be down because it will actually be important to actually run through an important to actually run through an example to evaluate a policy using the q example to evaluate a policy using the q functions now we will call this as we functions now we will call this as we said the state", "image_path": "img_data/video_17_chunk_3.jpg"}
{"video": "video_17", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "functions now we will call this as we said the state action value function we'll call it q and the expression will be q pi of s comma a it is another comma a it is another expectation over pi of", "image_path": "img_data/video_17_chunk_4.jpg"}
{"video": "video_17", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "expectation over pi of the rt + 1+ gamma v pi of st + 1 is equal to s pi of st + 1 is equal to s prime given st is equal to s and a is prime given st is equal to s and a is equal to a all right and also in terms of the backup diagram we are in state s", "image_path": "img_data/video_17_chunk_5.jpg"}
{"video": "video_17", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "backup diagram we are in state s diagram we are in state s and we are able to take and we are able to take go take many actions potentially from go take many actions potentially from state s sample and action out of state s will s sample and action out of state s will actually lead us to other states and over here we to other states and over here we have this is where the q pi of s comma have this is where the q pi of s comma a is located corresponds to a is located corresponds to corresponds give us the value behind", "image_path": "img_data/video_17_chunk_6.jpg"}
{"video": "video_17", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "a is located corresponds to corresponds give us the value behind corresponds give us the value behind a state action pair for each of these actions pair for each of these actions obviously will have a obviously will have a different action soorry different action soorry different value and then we actually what we do value and then we actually what we do here is we will take this expect what here is we will take this expect what value is that we can acquire out value is that we can acquire out of this kind of function it will of this kind of function it will actually depend on what states future actually depend on what states future states will be able to go through that states will be able to go through that kind of action nevertheless there is", "image_path": "img_data/video_17_chunk_7.jpg"}
{"video": "video_17", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "states will be able to go through that kind of action nevertheless there is the vp let's say that we take this action this is where the s prime is a set of the s prime is associated set of the s prime is associated with whatever state we ended with whatever state we ended up if you given that kind of up if you given that kind of function this is v pi of s prime and function this is v pi of s prime and obviously over here we have the", "image_path": "img_data/video_17_chunk_8.jpg"}
{"video": "video_17", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "function this is v pi of s prime and obviously over here we have the rewards which are going to be able to be picked up from the to be picked up from the transition to s prime in transition to s prime in this setting this v if we had in this setting this v if we had known explicitly the state action known explicitly the state action the value function then we able to pick the value function then we able to pick up the right action", "image_path": "img_data/video_17_chunk_9.jpg"}
{"video": "video_17", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "the value function then we able to pick up the right action in other words to be greedy and pick up in other words to be greedy and pick up the right action the one that offers the right action the one that offers the highest possible q value and this is the highest possible q value and this is the reason why i said earlier that these q reason why i said earlier that these q values are a bit more direct and a bit values are a bit more direct and a bit more i will call it desired to be more i will call it desired to be obtained as compared to the just the obtained as compared to the just the v values obviously to evaluate this q v values obviously to evaluate this q function we need to ob replace the function we need to ob replace the expectation we have done earlier", "image_path": "img_data/video_17_chunk_10.jpg"}
{"video": "video_17", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "function we need to ob replace the expectation we have done earlier expectation we have done earlier with the for the v function and that with the for the v function and that expectation is replaced with the expectation is replaced with the summation over s prime and r the rewards summation over s prime and r the rewards of the mdp dynamics probability distribution s dynamics probability distribution s prime next state reward given s comma a prime next state reward given s comma a times the reward r plus gamma v pi of", "image_path": "img_data/video_17_chunk_11.jpg"}
{"video": "video_17", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "prime next state reward given s comma a times the reward r plus gamma v pi of times the reward r plus gamma v pi of s prime and with that kind of expression prime and with that kind of expression in place we're actually able to also in place we're actually able to also iteratively estimate that q function iteratively estimate that q function in a similar way as we have done in the in a similar way as we have done in the state value function another state value function another expression which is actually pretty expression which is actually pretty useful as well is to just useful as well is to just observe how we actually can go from q observe how we actually can go from q functions to state to v functions to state to v functions we actually can write the v pi of s as a", "image_path": "img_data/video_17_chunk_12.jpg"}
{"video": "video_17", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "functions to state to v functions we actually can write the v pi of s as a we actually can write the v pi of s as a summation as it's evident from here over summation as it's evident from here over all possible actions from state s actions from state s of the probability that we are going of the probability that we are going to take this action from s times q pi of to take this action from s times q pi of s comma a that is definitely another formula a that is definitely another formula that is evident from this kind of that is evident from this kind of back c diagram that we have also used in", "image_path": "img_data/video_17_chunk_13.jpg"}
{"video": "video_17", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "that is evident from this kind of back c diagram that we have also used in back c diagram that we have also used in the v function definition and of the v function definition and of course combining the two formulas the course combining the two formulas the this one with the summation and the v this one with the summation and the v pi of s expression we can plug it in pi of s expression we can plug it in over here and we actually can see now over here and we actually can see now that we have this kind of recursive way that we have this kind of recursive way of calculating the q pi of s comma a and of calculating the q pi of s comma a and this also is shown on your site", "image_path": "img_data/video_17_chunk_14.jpg"}
{"video": "video_17", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "of calculating the q pi of s comma a and this also is shown on your site this also is shown on your site with this formula over there and with this formula over there and therefore we are fine we will be able to therefore we are fine we will be able to calculate this q function and calculate this q function and in some instances the q in some instances the q function it will be easier to calculate function it will be easier to calculate especially for kind of a smaller especially for kind of a smaller problems where the action space problems where the action space capital a over here is fairly small capital a over here is fairly small and it's a more direct way of coming up and it's a more direct way of coming up with a better policies we'll see that in with a better policies we'll see that in a moment on what is actually called", "image_path": "img_data/video_17_chunk_15.jpg"}
{"video": "video_17", "start": "0:08:00", "end": "0:08:23.866667", "timestamp": "0:08:00 - 0:08:23.866667", "text": "with a better policies we'll see that in a moment on what is actually called a moment on what is actually called policy iteration and in some instances policy iteration and in some instances we'll prefer to go back into the v we'll prefer to go back into the v function and that v function is function and that v function is going to be a bit more easier to going to be a bit more easier to compute especially with because it is compute especially with because it is a bit more compact representation it a bit more compact representation it just not does not depend on action just not does not depend on action spaces it only it depends on the states", "image_path": "img_data/video_17_chunk_16.jpg"}
{"video": "video_18", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now in terms of a numerical example obviously we need to have a python obviously we need to have a python program to do an evaluation of the state program to do an evaluation of the state value function given a policy this is value function given a policy this is actually going to be called the actually going to be called the prediction problem evaluate the vp of s given", "image_path": "img_data/video_18_chunk_0.jpg"}
{"video": "video_18", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "evaluate the vp of s given a policy pi a given s that's that is pi a given s that's that is really the problem we actually going to really the problem we actually going to be facing and we have introduced if be facing and we have introduced if you the diagram the three you the diagram the three representation of the problem the three representation of the problem the three representation of the problem involve representation of the problem involve starting from kind of a su state and starting from kind of a su state and following a set of actions and expanding following a set of actions and expanding for each action", "image_path": "img_data/video_18_chunk_1.jpg"}
{"video": "video_18", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "following a set of actions and expanding for each action all the other subsequent all the other subsequent states that are opening up for us we can states that are opening up for us we can actually go to given the gd word is a actually go to given the gd word is a stochastic problem it's a stochastic mtp stochastic problem it's a stochastic mtp any action let's take this action was any action let's take this action was to go right starting from state to go right starting from state s33 for the state s33 if we are to go right then s33 if we are to go right then following if you the transition pro following if you the transition pro probabilities here definitely we could", "image_path": "img_data/video_18_chunk_2.jpg"}
{"video": "video_18", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "following if you the transition pro probabilities here definitely we could probabilities here definitely we could go to s43 that's definitely a possibility with s43 that's definitely a possibility with probability 80% definitely probability of staying 80% definitely probability of staying put on s33 with probability 10% and s33 with probability 10% and definitely another possibility is to go over here with possibility is to go over here with which is the state s32 with probability another 10% s32 with probability another 10%", "image_path": "img_data/video_18_chunk_3.jpg"}
{"video": "video_18", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "s32 with probability another 10% i think this is u just i think this is u just running if you that kind of program running if you that kind of program seeing what kind of states we have and seeing what kind of states we have and potentially going in some kind of potentially going in some kind of recursive fashion as we have just recursive fashion as we have just seen in that kind of bellman seen in that kind of bellman expectation equation where if we had if we knew equation where if we had if we knew the value functions of these the value functions of these states we could actually go and estimate states we could actually go and estimate the value function of s33 if i write", "image_path": "img_data/video_18_chunk_4.jpg"}
{"video": "video_18", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "states we could actually go and estimate the value function of s33 if i write the value function of s33 if i write the v pi of s33 for the specific action v pi of s33 for the specific action which is to go right then i am going to take some kind right then i am going to take some kind of expect some kind of instantaneous reward let's call it instantaneous reward let's call it r plus gamma this will actually be now a some", "image_path": "img_data/video_18_chunk_5.jpg"}
{"video": "video_18", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "gamma this will actually be now a some this will actually be i'm this will actually be i'm transitioning i'm taking any transitioning i'm taking any action are and i am actually this action are and i am actually this is where i am given if you this is where i am given if you this kind of funtion i will be transition kind of funtion i will be transition using a probability into one of those using a probability into one of those stages probabilities will actually end stages probabilities will actually end up being associated with my transition up being associated with my transition model this probability here is", "image_path": "img_data/video_18_chunk_6.jpg"}
{"video": "video_18", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "up being associated with my transition model this probability here is model this probability here is p of s43 given s33 comma a is equal s33 comma a is equal to right times pp of s43 plus i have another term here", "image_path": "img_data/video_18_chunk_7.jpg"}
{"video": "video_18", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "plus i have another term here p of s 33 given s33 comma a is equal s33 comma a is equal right * vp of s33 plus p", "image_path": "img_data/video_18_chunk_8.jpg"}
{"video": "video_18", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "s33 plus p of s32 given s33 the same thing that will be the calculations that will actually would have happened if will actually would have happened if we knew if we have fixed if you the we knew if we have fixed if you the gma to a value let's say 0.1 or whatever gma to a value let's say 0.1 or whatever and we knew that this kind of value and we knew that this kind of value functions given the fact that the", "image_path": "img_data/video_18_chunk_9.jpg"}
{"video": "video_18", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "and we knew that this kind of value functions given the fact that the functions given the fact that the transitions are known this will this transitions are known this will this probability will be 0.8 this who have probability will be 0.8 this who have been 0.1 and this will actually be 0.1 been 0.1 and this will actually be 0.1 then we would be able to estimate then we would be able to estimate that kind of a value for that kind of a value for that kind of state if we have to follow this kind of state if we have to follow this kind of policy let's say going always right policy let's say going always right and now that we have seen in a little and now that we have seen in a little bit how that previous kind equation that", "image_path": "img_data/video_18_chunk_10.jpg"}
{"video": "video_18", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "and now that we have seen in a little bit how that previous kind equation that bit how that previous kind equation that we have produced kind of is being we have produced kind of is being used it's worthwhile just going used it's worthwhile just going through now the python code to see how through now the python code to see how this is going to be done and this is going to be done and to understand the python code i think to understand the python code i think it's worthwhile mentioning the simplest it's worthwhile mentioning the simplest possible way of actually solving this possible way of actually solving this kind of a problem of the valuation of kind of a problem of the valuation of the state value function is to observe the state value function is to observe that if we are to write all these kind that if we are to write all these kind of equations that we wrote for s33 for of equations that we wrote for s33 for all states all other states then we're", "image_path": "img_data/video_18_chunk_11.jpg"}
{"video": "video_18", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "of equations that we wrote for s33 for all states all other states then we're all states all other states then we're going to have the as many going to have the as many equations as the number of states and equations as the number of states and these equations are linear we can these equations are linear we can actually solve these equations for actually solve these equations for the unknowns which are the state values the unknowns which are the state values and that basically that's the that's and that basically that's the end of the estimation process now the end of the estimation process now preferably we do not want to use i preferably we do not want to use i will call it u any matrix inversions or will call it u any matrix inversions or anything el assciated with these kind", "image_path": "img_data/video_18_chunk_12.jpg"}
{"video": "video_18", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "will call it u any matrix inversions or anything el assciated with these kind anything el assciated with these kind of solutions associated with system of solutions associated with system of linear equations obviously there are of linear equations obviously there are ways that we can solve this system ways that we can solve this system without really this kind of numerical without really this kind of numerical kind of complications and one way that kind of complications and one way that we will be need to understand here to we will be need to understand here to for us to understand the python code is for us to understand the python code is the iterative way of solving the systems the iterative way of solving the systems of equations which is actually pretty of equations which is actually pretty common the equation we will actually be the equation we will actually be migrating to is the following i'm", "image_path": "img_data/video_18_chunk_13.jpg"}
{"video": "video_18", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "the equation we will actually be migrating to is the following i'm migrating to is the following i'm going to write down the equation it going to write down the equation it is v i will define as small letter k as the v i will define as small letter k as the iteration index v k minus + one of iteration index v k minus + one of s it will be summation over a pi of a given summation over a pi of a given s and summation of over s prime and r", "image_path": "img_data/video_18_chunk_14.jpg"}
{"video": "video_18", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "summation over a pi of a given s and summation of over s prime and r s and summation of over s prime and r of the mdp dynamics which is of course known r + v k of s prime and this is known as the -called iterative policy evaluation and this is what is being used in", "image_path": "img_data/video_18_chunk_15.jpg"}
{"video": "video_18", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "policy evaluation and this is what is being used in and this is what is being used in practice we will be using this practice we will be using this equation is actually very equation is actually very straightforward in the python code you straightforward in the python code you will see that the state values will see that the state values will be presented by a vector we have be presented by a vector we have let's say 4 by 3 12 - 1 11 states and let's say 4 by 3 12 - 1 11 states and we have we having a vector we have we having a vector of representing the this v over here and of representing the this v over here and we start with some initial value on", "image_path": "img_data/video_18_chunk_16.jpg"}
{"video": "video_18", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "of representing the this v over here and we start with some initial value on we start with some initial value on that vector let's say all that vector let's say all zeros and we then depending on which zeros and we then depending on which s prime we ended up to we pick up that s prime we ended up to we pick up that value of that vector to calculate value of that vector to calculate this thing and then we update that this thing and then we update that vector as we go and we can vector as we go and we can actually do that in place or using two actually do that in place or using two separate vectors in the python code you separate vectors in the python code you will see two separate vectors you can will see two separate vectors you can also do that in place", "image_path": "img_data/video_18_chunk_17.jpg"}
{"video": "video_18", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "will see two separate vectors you can also do that in place also do that in place replacements and therefore just replacements and therefore just using in other words one vector for using in other words one vector for both of these states you will see in the both of these states you will see in the python code this will be called v new python code this will be called v new and the other will be called v old and the other will be called v old let's go to the python code and see let's go to the python code and see what's happens over here we will the what's happens over here we will the python code is in the process of being python code is in the process of being migrated to a standalone code migrated to a standalone code right now the code is sitting in the right now the code is sitting in the in line with a text under the section", "image_path": "img_data/video_18_chunk_18.jpg"}
{"video": "video_18", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "right now the code is sitting in the in line with a text under the section in line with a text under the section policy evaluation or policy evaluation or prediction and here we see the grid word prediction and here we see the grid word that we have we are using a library that we have we are using a library called jim is an library called jim is an library that itself is involving it's that itself is involving it's actually today is called actually today is called gymnasium and given that we have already gymnasium and given that we have already specified the g world environment over specified the g world environment over here which primarily is a class of all here which primarily is a class of all the associated entities in that the associated entities in that environment plus a number of methods", "image_path": "img_data/video_18_chunk_19.jpg"}
{"video": "video_18", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "the associated entities in that environment plus a number of methods environment plus a number of methods the most important of which is a step the most important of which is a step method where the environment is method where the environment is transition is giving us the reward transition is giving us the reward and the next state the policy and the next state the policy evaluation function is really the main evaluation function is really the main function of interest here it takes us function of interest here it takes us argument the policy that we have to argument the policy that we have to evaluate the environment specifics evaluate the environment specifics that we have instantiated over here a that we have instantiated over here a discount factor gamma and an epsilon discount factor gamma and an epsilon that it will actually be used as part of that it will actually be used as part of a stopping criteria because it's an", "image_path": "img_data/video_18_chunk_20.jpg"}
{"video": "video_18", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "that it will actually be used as part of a stopping criteria because it's an a stopping criteria because it's an iterative kind of algorith we have to iterative kind of algorith we have to escape from that kind of infinite loop escape from that kind of infinite loop we have applied over here given we have applied over here given when we see that a state value function when we see that a state value function is as a vector that state value vector is as a vector that state value vector is not really changing that much we is not really changing that much we can actually stop the iterations over can actually stop the iterations over there that's the what i explained there that's the what i explained as v old and v new we start with as v old and v new we start with some value state value function some value state value function which is an 11 dimensional kind of which is an 11 dimensional kind of vector v that we call v old over here", "image_path": "img_data/video_18_chunk_21.jpg"}
{"video": "video_18", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "which is an 11 dimensional kind of vector v that we call v old over here vector v that we call v old over here and another 11 dimensional vector v and another 11 dimensional vector v that we call the vue that is vk that we call the vue that is vk plus one and the vk and that in the plus one and the vk and that in the iterative policy evaluation in the iterative policy evaluation equation and we are actually looping equation and we are actually looping over states to calculate over states to calculate first based on the policy the bman first based on the policy the bman expectation this is basically the expectation this is basically the equation we just wrote here to calculate", "image_path": "img_data/video_18_chunk_22.jpg"}
{"video": "video_18", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "expectation this is basically the equation we just wrote here to calculate equation we just wrote here to calculate the elements of the venue and if we are to progress to venue and if we are to progress to proceed then we are setting the v old proceed then we are setting the v old is equal to v in order for us to start is equal to v in order for us to start another iteration and move into a another iteration and move into a more accurate v state value more accurate v state value function it can be shown that the function it can be shown that the iterative policy evaluation will", "image_path": "img_data/video_18_chunk_23.jpg"}
{"video": "video_18", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "function it can be shown that the iterative policy evaluation will iterative policy evaluation will reach if you a vector of state reach if you a vector of state values that it is identical to a values that it is identical to a non-iterative approach if we had follow non-iterative approach if we had follow it and that is basically what we're it and that is basically what we're reporting here we are we have reporting here we are we have starting we are trying to evaluate starting we are trying to evaluate now a specific policies called a random now a specific policies called a random policy in the earlier we call it policy in the earlier we call it stochastic policy random and stochastic policy random and stochastic is one and the same thing and that", "image_path": "img_data/video_18_chunk_24.jpg"}
{"video": "video_18", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "stochastic policy random and stochastic is one and the same thing and that is one and the same thing and that policy is a policy that it is policy is a policy that it is defined here by it's a uniform defined here by it's a uniform uniformly kind of random policy we uniformly kind of random policy we evaluating it and we're producing the evaluating it and we're producing the output of which the expected the output of which the expected the non-iterative version will actually non-iterative version will actually result into to this vector of values result into to this vector of values and the iterative one will result and the iterative one will result into a vector which is very close into a vector which is very close to those values if you run the code", "image_path": "img_data/video_18_chunk_25.jpg"}
{"video": "video_18", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "into a vector which is very close to those values if you run the code to those values if you run the code i think i just quoted the u number 11 to i think i just quoted the u number 11 to be the number of states that this python be the number of states that this python code is u responding with but code is u responding with but unfortunately it is not associated with unfortunately it is not associated with this grid word environment the grid this grid word environment the grid world environment that it is this world environment that it is this code is actually assumes and that's why code is actually assumes and that's why it returns if you 14 it returns if you 14 states is this one that it is states is this one that it is presented in the saturn's book and", "image_path": "img_data/video_18_chunk_26.jpg"}
{"video": "video_18", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "states is this one that it is presented in the saturn's book and presented in the saturn's book and that's actually the classic 4x4 gd world that's actually the classic 4x4 gd world where we actually have two where we actually have two terminating kind of states and 14 other terminating kind of states and 14 other states where we are able to u act states where we are able to u act according to this equip probable according to this equip probable policy we can go up down left and policy we can go up down left and right with equal probability and the right with equal probability and the immediate kind of reward will actually immediate kind of reward will actually be minus one for all", "image_path": "img_data/video_18_chunk_27.jpg"}
{"video": "video_18", "start": "0:14:00", "end": "0:14:29.966667", "timestamp": "0:14:00 - 0:14:29.966667", "text": "be minus one for all transitions and therefore transitions and therefore effectively what the python code is effectively what the python code is actually doing is it is really giving us actually doing is it is really giving us as the k number of iterations as the k number of iterations progress effectively it is converging progress effectively it is converging into this table over here if you see into this table over here if you see the numbers in the python code and the numbers in the python code and the numbers over here in this actually table numbers over here in this actually table which are also in your notes they which are also in your notes they are kind of identical", "image_path": "img_data/video_18_chunk_28.jpg"}
{"video": "video_19", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "back now to the trajectory towards if you an americal example towards if you an americal example some kind of path to an americal example some kind of path to an americal example we cannot do any numerics over here we cannot do any numerics over here unless we start replacing this unless we start replacing this expectation into something that involves expectation into something that involves some form of summations and therefore we some form of summations and therefore we need to sort of come go to need to sort of come go to and write this bellman expectation and write this bellman expectation equation for the state value function equation for the state value function for v pi of s", "image_path": "img_data/video_19_chunk_0.jpg"}
{"video": "video_19", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "equation for the state value function for v pi of s as follows let's try to as follows let's try to see what is what will actually end up see what is what will actually end up being as a formula we can actually do being as a formula we can actually do the derivation it will you can the derivation it will you can actually skip this derivation i am going actually skip this derivation i am going to just do it for complete completion to just do it for complete completion for being complete here but you can for being complete here but you can skip forward this kind of video if you skip forward this kind of video if you are bor with this kind of derivation of are bor with this kind of derivation of the bman expectation but i think it's the bman expectation but i think it's worthwhile also kind of following worthwhile also kind of following along", "image_path": "img_data/video_19_chunk_1.jpg"}
{"video": "video_19", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "worthwhile also kind of following along u all right the first thing i want u all right the first thing i want to do is to write the u let me title to do is to write the u let me title this as derivation of the bellman expectation equation for v pi of s", "image_path": "img_data/video_19_chunk_2.jpg"}
{"video": "video_19", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "expectation equation for v pi of s i want to start with the formula we just saw v pi of s the state value we just saw v pi of s the state value of s under a policy pi is this kind of s under a policy pi is this kind of expectation of the return gt the average returns in other words gt the average returns in other words that we will be getting from state", "image_path": "img_data/video_19_chunk_3.jpg"}
{"video": "video_19", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "gt the average returns in other words that we will be getting from state that we will be getting from state s given in other words st is equal to s given in other words st is equal to s and this can be written s and this can be written as expectation we will replace now the as expectation we will replace now the gt let me put a pi over gt let me put a pi over here we will replace a gt with the rt here we will replace a gt with the rt + + one plus gamma gt + 1 and this one plus gamma gt + 1 and this return we have actually seen that why", "image_path": "img_data/video_19_chunk_4.jpg"}
{"video": "video_19", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "one plus gamma gt + 1 and this return we have actually seen that why return we have actually seen that why this can be written this as a this can be written this as a function of the immediate reward and the function of the immediate reward and the next and the future returns next and the future returns for the future state that comes up for the future state that comes up after s which is the s prime given after s which is the s prime given st is equal to s and therefore we can actually break it s and therefore we can actually break it down into two parts the expectation down into two parts the expectation again of rt + one that's the first part again of rt + one that's the first part i'm sorry i forgot the conditional", "image_path": "img_data/video_19_chunk_5.jpg"}
{"video": "video_19", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "again of rt + one that's the first part i'm sorry i forgot the conditional given st is equal to s plus gamma expectation over a pi of gt + 1 given s expectation over a pi of gt + 1 given s t is equal to s these are the two terms t is equal to s these are the two terms we will now take each of these we will now take each of these terms and we are going to write them terms and we are going to write them as follows the", "image_path": "img_data/video_19_chunk_6.jpg"}
{"video": "video_19", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "as follows the expectation the first term let's call this first term a and let's call this second term b term a and let's call this second term b a can be written as expectation of over pi well we can replace the expectation in other words you can", "image_path": "img_data/video_19_chunk_7.jpg"}
{"video": "video_19", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "pi well we can replace the expectation in other words you can expectation in other words you can replace now the expectation with a summation we can write that for a which is the expectation over pi of rt + which is the expectation over pi of rt + 1 given st is equal to s is equal to a summation over all reward wordss that belong to", "image_path": "img_data/video_19_chunk_8.jpg"}
{"video": "video_19", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "summation over all reward wordss that belong to over all reward wordss that belong to the set of rewards are r times that's a the set of rewards are r times that's a definition of an expectation for that definition of an expectation for that random variable that random variable is random variable that random variable is the future reward it's the future reward it's expectation over the set of rewards expectation over the set of rewards all possible rewards it's r times the all possible rewards it's r times the probability distribution of rewards probability distribution of rewards the probability distribution of rewards the probability distribution of rewards is p of rt + 1 is equal to r given s is p of rt + 1 is equal to r given s is equal to s i hope you agree that's the definition", "image_path": "img_data/video_19_chunk_9.jpg"}
{"video": "video_19", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "equal to s i hope you agree that's the definition s i hope you agree that's the definition of an expectation operation and we know that we operation and we know that we can take actually this and write it as can take actually this and write it as follows is summation over r of r * we can actually write this probability as another probability as another probability distribution that will involve as a", "image_path": "img_data/video_19_chunk_10.jpg"}
{"video": "video_19", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "probability as another probability distribution that will involve as a distribution that will involve as a marginalization of another more marginalization of another more complicated probability distribution complicated probability distribution that will actually be the mdp dynamics that will actually be the mdp dynamics that will involve mdp dynamic all the that will involve mdp dynamic all the ndp dynamics we can actually write ndp dynamics we can actually write this as the probability as a summation this as the probability as a summation over all s primes belong to the set of over all s primes belong to the set of states s of p of rt + 1 is equal to r comma st s of p of rt + 1 is equal to r comma st + 1 is = s + 1 is = s prime given s is equal to", "image_path": "img_data/video_19_chunk_11.jpg"}
{"video": "video_19", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "+ 1 is = s prime given s is equal to s in fact we can actually even roll in here not only just the s primes the here not only just the s primes the future states but also the action i future states but also the action i can actually add here another can actually add here another summation and do the marginalization of summation and do the marginalization of the complete thing that i the complete thing that i need comma a t is equal to a given st is equal to", "image_path": "img_data/video_19_chunk_12.jpg"}
{"video": "video_19", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "comma a t is equal to a given st is equal to all right from here to here i hope it's understood given if here i hope it's understood given if you the sum rule of probability you the sum rule of probability that we have seen in another video by that we have seen in another video by marginalizing this probabbly marginalizing this probabbly distribution we are getting this by distribution we are getting this by eliminating effectively the random eliminating effectively the random variables of actions and future states", "image_path": "img_data/video_19_chunk_13.jpg"}
{"video": "video_19", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "eliminating effectively the random variables of actions and future states variables of actions and future states and now what we can actually do and now what we can actually do here we can u write this term and we can actually term and we can actually write this as another summation again over as another summation again over actions we can actually condition now actions we can actually condition now to bring the empty b dynamics into", "image_path": "img_data/video_19_chunk_14.jpg"}
{"video": "video_19", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "actions we can actually condition now to bring the empty b dynamics into to bring the empty b dynamics into the picture mdb dynamics needs s the picture mdb dynamics needs s prime and r on one hand and after the prime and r on one hand and after the given symbol we need to take the a after given symbol we need to take the a after the g after the given symbol right we the g after the given symbol right we can actually do this and we can write it as p of st + 1 is = to s prime comma rt + of st + 1 is = to s prime comma rt + 1 is = r given", "image_path": "img_data/video_19_chunk_15.jpg"}
{"video": "video_19", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "of st + 1 is = to s prime comma rt + 1 is = r given the things that we need st is equal to s comma a is equal to s comma a is equal to a times let me write the times over here the p of a is equal to here the p of a is equal to a given s is equal to a given s is equal to s that space basically just a direct application", "image_path": "img_data/video_19_chunk_16.jpg"}
{"video": "video_19", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "s that space basically just a direct application basically just a direct application of the product rule before we apply of the product rule before we apply the sum rule to go from here to here the sum rule to go from here to here via the marginalization or equivalent via the marginalization or equivalent from here to here and over here we from here to here and over here we took the joint probability distribution took the joint probability distribution wrot it as a product of a conditional wrot it as a product of a conditional plus the marginal that is the plus the marginal that is the product rule now we have all the product rule now we have all the elements that we need by the way this elements that we need by the way this thing over here by convention notational", "image_path": "img_data/video_19_chunk_17.jpg"}
{"video": "video_19", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "elements that we need by the way this thing over here by convention notational thing over here by convention notational convention if you we call this the convention if you we call this the pi of a givenness all right now we have the first term done the first term done the first term this a is done and it is involves the this a is done and it is involves the summations that we need and for any summations that we need and for any numerical problem we're able to just numerical problem we're able to just find a capital a term over there now the", "image_path": "img_data/video_19_chunk_18.jpg"}
{"video": "video_19", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "numerical problem we're able to just find a capital a term over there now the find a capital a term over there now the next thing we need actually to do is to next thing we need actually to do is to come up with the second term right come up with the second term right expression that involves summation with expression that involves summation with respect to the second term in other respect to the second term in other words replace this expectation again for words replace this expectation again for the second term all right now we have b we have here the by definition of the expectation operation", "image_path": "img_data/video_19_chunk_19.jpg"}
{"video": "video_19", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "the by definition of the expectation operation in other words over gt + 1 all over in other words over gt + 1 all over future return the future return in future return the future return in the next time is gt + 1 a random the next time is gt + 1 a random variable times the probability of gt + 1 variable times the probability of gt + 1 is equal to gt + 1 given in fact since i was not given in fact since i was not using t indices for any of the specific", "image_path": "img_data/video_19_chunk_20.jpg"}
{"video": "video_19", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "given in fact since i was not using t indices for any of the specific using t indices for any of the specific values it will take i'll just going to values it will take i'll just going to use g i'm just going to use g here g i'm just going to use g here i'm going to eliminate the gt+ one here i'm going to eliminate the gt+ one here and the gt+ one there i'm just going and the gt+ one there i'm just going to have this is the sort to have this is the sort of replacement and now i need to of replacement and now i need to apply a similar kind of thinking as apply a similar kind of thinking as earlier and i'm going to write this earlier and i'm going to write this properly distribution as", "image_path": "img_data/video_19_chunk_21.jpg"}
{"video": "video_19", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "distribution as resulting from a marginalization resulting from a marginalization operation that involves four all four other involves four all four other summations which i had in my summations which i had in my mdp dynamic probability mdp dynamic probability distribution i need definitely i distribution i need definitely i need my r i need my actions i need my s primes and i have here my g's over primes and i have here my g's over here and this is a g and this will", "image_path": "img_data/video_19_chunk_22.jpg"}
{"video": "video_19", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "primes and i have here my g's over here and this is a g and this will here and this is a g and this will be replaced now by gt + 1 is equal to g comma st + 1 is = to s prime comma a t comma st + 1 is = to s prime comma a t is = to a comma and rt + 1 is equal to r comma and rt + 1 is equal to r given i forgot the given here i forgot given i forgot the given here i forgot this conditional let me write it", "image_path": "img_data/video_19_chunk_23.jpg"}
{"video": "video_19", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "given i forgot the given here i forgot this conditional let me write it this conditional let me write it properly it is p of gt + 1 is equal to g properly it is p of gt + 1 is equal to g given st is equal to s i forgot i now can do the given st is equal to s and close the st is equal to s and close the parenthesis and now i hope everyone kind parenthesis and now i hope everyone kind of understands why we have we of understands why we have we actually can go from here", "image_path": "img_data/video_19_chunk_24.jpg"}
{"video": "video_19", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "of understands why we have we actually can go from here actually can go from here after we marginalize all the other after we marginalize all the other variables to here in this expression variables to here in this expression over there this is effectively the over there this is effectively the same rule all right let's process same rule all right let's process that and as you probably have guessed i that and as you probably have guessed i move the some terms that i need on the move the some terms that i need on the other side we take the summations side we take the summations again the one is for r s prime a again the one is for r s prime a g and we had the g over here and now g and we had the g over here and now we will write this as gt + 1 is equal to", "image_path": "img_data/video_19_chunk_25.jpg"}
{"video": "video_19", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "we will write this as gt + 1 is equal to g given st+ one is equal to s prime a t is equal to prime a t is equal to a rt t + 1 is = to", "image_path": "img_data/video_19_chunk_26.jpg"}
{"video": "video_19", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "prime a t is equal to a rt t + 1 is = to r and st is equal to s since we have taken this on the other side then obviously it's we need to side then obviously it's we need to apply the product rule and the product apply the product rule and the product rule is the second is finished with the is the second is finished with the second term st + 1 is equal to s second term st + 1 is equal to s prime comma a is equal to a + 1 is equal to", "image_path": "img_data/video_19_chunk_27.jpg"}
{"video": "video_19", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "a is equal to a + 1 is equal to r given st is equal to r given st is equal to s and now we will write this again as 1 s and now we will write this again as 1 2 3 four summations and we can actually 2 3 four summations and we can actually take the second term first write it as take the second term first write it as we have done in the past with two terms we have done in the past with two terms that apply in the product rule that it that apply in the product rule that it will reveal the mtp dynamics at the will reveal the mtp dynamics at the first term and of course the policy at first term and of course the policy at the that's a second term and the", "image_path": "img_data/video_19_chunk_28.jpg"}
{"video": "video_19", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "first term and of course the policy at the that's a second term and the that's a second term and the first term over here i think it's first term over here i think it's worthwhile commenting on the worthwhile commenting on the fact that the future rewards that we fact that the future rewards that we have here as gt plus one this have here as gt plus one this probability distribution does not probability distribution does not depend on many of the terms that are depend on many of the terms that are quoted here after the condition and than quoted here after the condition and than the state s prime and is because of the marvian prime and is because of the marvian assumption of the mdp we definitely", "image_path": "img_data/video_19_chunk_29.jpg"}
{"video": "video_19", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "prime and is because of the marvian assumption of the mdp we definitely assumption of the mdp we definitely do not have any dependency on future do not have any dependency on future returns and then the state we are returns and then the state we are right now the state that we are right now the state that we are not we are right now the state that we not we are right now the state that we have transition to the s prime have transition to the s prime definitely we don't have any dependency definitely we don't have any dependency on previous actions or even the current on previous actions or even the current instantaneous reward that we have instantaneous reward that we have received the only thing that we can received the only thing that we can give the future returns in terms of this", "image_path": "img_data/video_19_chunk_30.jpg"}
{"video": "video_19", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "received the only thing that we can give the future returns in terms of this give the future returns in terms of this probability distribution is the st+ one probability distribution is the st+ one state now we are almost there because we state now we are almost there because we actually can notice this term over here and this term is the by definition an expectation but this time is an expectation but this time is an expectation over pi of can actually write it as gt + 1 given", "image_path": "img_data/video_19_chunk_31.jpg"}
{"video": "video_19", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "expectation over pi of can actually write it as gt + 1 given of can actually write it as gt + 1 given st + 1 is equal to s st + 1 is equal to s prime which by definition prime which by definition is the v pi of s prime actually can write this prime actually can write this final expression as r s prime a", "image_path": "img_data/video_19_chunk_32.jpg"}
{"video": "video_19", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "r s prime a this is gam this is a v pi of s gam this is a v pi of s prime time pi of a given s * prime time pi of a given s * p of st + 1 is = to s prime rt + 1 is = p of st + 1 is = to s prime rt + 1 is = to r st is = to s and a is equal to r st is = to s and a is equal to a and this is basically the conclusion a and this is basically the conclusion as far as the sort of second b", "image_path": "img_data/video_19_chunk_33.jpg"}
{"video": "video_19", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "a and this is basically the conclusion as far as the sort of second b as far as the sort of second b term of the v pi of s expression we term of the v pi of s expression we started with and we actually can started with and we actually can see what we have done we have the see what we have done we have the expression v pi of s is equal to the first term well but s is equal to the first term well but the first term was a summation over r that belong to the set of ws times some", "image_path": "img_data/video_19_chunk_34.jpg"}
{"video": "video_19", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "summation over r that belong to the set of ws times some summation over a summation of s prime this was pi of a given s times p which is s + 1 is equal to s prime", "image_path": "img_data/video_19_chunk_35.jpg"}
{"video": "video_19", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "p which is s + 1 is equal to s prime rt + 1 is = r given s is = to s comma a is equal to a given s is = to s comma a is equal to a that was our first term plus we had that was our first term plus we had gamma our second term which the second gamma our second term which the second term is this one that we actually see term is this one that we actually see over here which i'm not going to write over here which i'm not going to write again this second term is again this second term is here gamma times the second term and here gamma times the second term and actually we can see here the common", "image_path": "img_data/video_19_chunk_36.jpg"}
{"video": "video_19", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "here gamma times the second term and actually we can see here the common actually we can see here the common terms we have pi of s and the transition terms we have pi of s and the transition including all of the summations that are including all of the summations that are dependent on these s dependent on these s primes that we have here and the primes that we have here and the also the actions pi of s we also the actions pi of s we actually can write it we can take actually can write it we can take this out and what will remain finally it this out and what will remain finally it is the following expression v pi of is going to be expression v pi of is going to be equal some summation over a", "image_path": "img_data/video_19_chunk_37.jpg"}
{"video": "video_19", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "expression v pi of is going to be equal some summation over a of pi a given s times let me see the final expression over here times summation of s prime belongs to s and summation of", "image_path": "img_data/video_19_chunk_38.jpg"}
{"video": "video_19", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "s and summation of rewards of p st + 1 is = to s prime rt + 1 is equal p st + 1 is = to s prime rt + 1 is equal to r given st is equal to s n a is equal to r given st is equal to s n a is equal to a of r + a of r + gamma v pi of x plan", "image_path": "img_data/video_19_chunk_39.jpg"}
{"video": "video_19", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "gamma v pi of x plan this is the pice this is now a formula that we actually can reveal to us can give us actually can reveal to us can give us the path to a numerical kind ofam the path to a numerical kind ofam numerical evaluation of the state of numerical evaluation of the state of value the value of a state s from value the value of a state s from the value of the state that follows given the mdp", "image_path": "img_data/video_19_chunk_40.jpg"}
{"video": "video_19", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "the value of the state that follows given the mdp the state that follows given the mdp dynamics that we know and given a policy that we are going to be assuming policy that we are going to be assuming that probably distribution of a that probably distribution of a policy given that kind of state policy given that kind of state what kind of action we are going to be what kind of action we are going to be sampling out of it and or what kind of sampling out of it and or what kind of action we are deterministically going to action we are deterministically going to sort of follow when every time we hit sort of follow when every time we hit this kind of state this is", "image_path": "img_data/video_19_chunk_41.jpg"}
{"video": "video_19", "start": "0:21:00", "end": "0:21:05.700000", "timestamp": "0:21:00 - 0:21:05.700000", "text": "sort of follow when every time we hit this kind of state this is this kind of state this is the general kind of formula and now we the general kind of formula and now we will see the numerical example", "image_path": "img_data/video_19_chunk_42.jpg"}
{"video": "video_20", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now that we have seen the what policy is and the in general the stochastic and the in general the stochastic version of this policy it's worthwhile version of this policy it's worthwhile now starting to discuss a very now starting to discuss a very important kind of definition which is actually called definition which is actually called value functions the no matter what value functions the no matter what algorithm will actually involve value algorithm will actually involve value functions will try to estimate value functions will try to estimate value functions for other states or states and functions for other states or states and actions in order for them to be able to", "image_path": "img_data/video_20_chunk_0.jpg"}
{"video": "video_20", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "functions for other states or states and actions in order for them to be able to actions in order for them to be able to move into a better objective and that move into a better objective and that objective is a policy a better policy objective is a policy a better policy than one they were before as we will than one they were before as we will see these algori are going to be see these algori are going to be iterative recursive in nature and iterative recursive in nature and therefore we need to have these therefore we need to have these functions to define these functions to define these functions because these are what they will try because these are what they will try to sort of base their decisions to sort of base their decisions to go into a better policy we will start go into a better policy we will start with the -called", "image_path": "img_data/video_20_chunk_1.jpg"}
{"video": "video_20", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "go into a better policy we will start with the -called state value function and we will symbolize it this value function with a small letter value function with a small letter v subscript pi and with an argument v subscript pi and with an argument of s is evidently any state that we of s is evidently any state that we are we have in the system and pi is the are we have in the system and pi is the policy that we will have we will", "image_path": "img_data/video_20_chunk_2.jpg"}
{"video": "video_20", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "are we have in the system and pi is the policy that we will have we will policy that we will have we will follow being in this kind of state s and follow being in this kind of state s and v is the name of the value v is the name of the value function all right we will now function all right we will now write the definition is write the definition is that the v pi of s is an expectation that the v pi of s is an expectation another expectation over policies another expectation over policies pi of the return g a time instant t pi of the return g a time instant t given that we are at this moment in s", "image_path": "img_data/video_20_chunk_3.jpg"}
{"video": "video_20", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "pi of the return g a time instant t given that we are at this moment in s given that we are at this moment in s it is effectively the average of all returns effectively the average of all returns that we can get by being in that kind of that we can get by being in that kind of state as the name implies state value state as the name implies state value is a function that gives us the value of is a function that gives us the value of being in state s we can being in state s we can actually write this expectation as the actually write this expectation as the expectation we can replace the gt expectation we can replace the gt with what we have seen earlier as a with what we have seen earlier as a summation", "image_path": "img_data/video_20_chunk_4.jpg"}
{"video": "video_20", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "with what we have seen earlier as a summation from k is equal to 0 to from k is equal to 0 to infinity gamma k r t + r t + 1+ 1+ k given st is equal to s and we can actually write this as the expectation over policy expectation over policy pi of rt + 1 + gamma rt + 2 + gamma 2 rt", "image_path": "img_data/video_20_chunk_5.jpg"}
{"video": "video_20", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "expectation over policy pi of rt + 1 + gamma rt + 2 + gamma 2 rt pi of rt + 1 + gamma rt + 2 + gamma 2 rt + 3 plus and on given st is equal to + 3 plus and on given st is equal to s and we can actually write this as the s and we can actually write this as the expectation over pi of rt + 1+ expectation over pi of rt + 1+ gamma rt + 2+ gamma rt + 3 plus and on", "image_path": "img_data/video_20_chunk_6.jpg"}
{"video": "video_20", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "2+ gamma rt + 3 plus and on given s is equal to s and actually what we can see here we s and actually what we can see here we can actually see this is a effectively the see this is a effectively the return gt + return gt + 1 and actually we can show although this 1 and actually we can show although this derivation of this will not be shown derivation of this will not be shown we can show", "image_path": "img_data/video_20_chunk_7.jpg"}
{"video": "video_20", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "we can show nevertheless that the vp of s is the expectation over pi of the s is the expectation over pi of the instantaneous reward rt + instantaneous reward rt + 1 plus vp of s prime and this vp of s 1 plus vp of s prime and this vp of s prime came from writing this return at t prime came from writing this return at t and connected to the return that we will and connected to the return that we will be getting at the next time in the", "image_path": "img_data/video_20_chunk_8.jpg"}
{"video": "video_20", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "and connected to the return that we will be getting at the next time in the be getting at the next time in the next if you state given st is equal to given st is equal to s and this is a very important s and this is a very important equation that was derived from belman and it's actually because we it expectation is bman is called bman expectation equation", "image_path": "img_data/video_20_chunk_9.jpg"}
{"video": "video_20", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "expectation equation a bman expectation equation right effectively what it does equation right effectively what it does allow us to do is allow us to compute the state to compute the state value function from u state value function from u state value functions of next states and next states functions of next states and on we it actually defines if and on we it actually defines if you some way of calculation by you some way of calculation by looking at this equation in a threee", "image_path": "img_data/video_20_chunk_10.jpg"}
{"video": "video_20", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "you some way of calculation by looking at this equation in a threee looking at this equation in a threee form it's actually easier to see if form it's actually easier to see if we are in let's say in state s we will we are in let's say in state s we will be denoting as actions nodes with solid denoting as actions nodes with solid color black in this case these are color black in this case these are all the possible actions and each action all the possible actions and each action will lead us to a number of possible future number of possible future states let me call this future states in states let me call this future states in general s prime", "image_path": "img_data/video_20_chunk_11.jpg"}
{"video": "video_20", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "states let me call this future states in general s prime and this is effectively the over here we effectively the over here we have taking let's say an action in have taking let's say an action in general a the policy is the one that general a the policy is the one that mapped the state to a number of actions potentially the to a number of actions potentially the policy is stochastic we policy is stochastic we effectively ended up taking an action effectively ended up taking an action and that action the environment is and that action the environment is stochastic and that stochasticity of the", "image_path": "img_data/video_20_chunk_12.jpg"}
{"video": "video_20", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "and that action the environment is stochastic and that stochasticity of the stochastic and that stochasticity of the environment lead us to not just one environment lead us to not just one but more than one out of not just but more than one out of not just the intended but we could end up in the intended but we could end up in the grid world in any other cell the grid world in any other cell given the transition model of the mdp given the transition model of the mdp problem we are dealing with this is problem we are dealing with this is actually called it's actually a very actually called it's actually a very useful representation of the useful representation of the calculation of what this equation kind calculation of what this equation kind of represents", "image_path": "img_data/video_20_chunk_13.jpg"}
{"video": "video_20", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "calculation of what this equation kind of represents and this is actually called backup diagrams for the state value function vp and it's called actually backup vp and it's called actually backup diagram because it allows us to diagram because it allows us to calculate from the v the state value function of subsequent states", "image_path": "img_data/video_20_chunk_14.jpg"}
{"video": "video_20", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "from the v the state value function of subsequent states the value function of the current state we'll use this of the current state we'll use this kind of equation over here to do kind of equation over here to do that and see if you an americal that and see if you an americal example now on how this calculation is example now on how this calculation is actually happening before going into a specific happening before going into a specific example i think it's worthwhile kind of example i think it's worthwhile kind of clarifying something that in clarifying something that in passing as we sort of discussed the passing as we sort of discussed the state value function we have mentioned state value function we have mentioned this expectation over policy by and", "image_path": "img_data/video_20_chunk_15.jpg"}
{"video": "video_20", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "state value function we have mentioned this expectation over policy by and this expectation over policy by and the there's a kind of some form the there's a kind of some form of confusion that may raise from the of confusion that may raise from the fact that earlier we have talked about fact that earlier we have talked about stochastic and deterministic policies stochastic and deterministic policies i want to clarify this point before i want to clarify this point before going into this kind of numerical going into this kind of numerical example we may actually have the example we may actually have the term to clarify first the term to clarify first the term stochastic policy in the case stochastic policy in the case specifically in this kind of gd world specifically in this kind of gd world problem we have here we may have this problem we have here we may have this kind of policy that we called pi of a kind of policy that we called pi of a given an s and this policy could be", "image_path": "img_data/video_20_chunk_16.jpg"}
{"video": "video_20", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "kind of policy that we called pi of a given an s and this policy could be given an s and this policy could be 0.25 and on 1/4 in other 0.25 and on 1/4 in other words and equal probable selection of words and equal probable selection of actions for up down left and right for actions for up down left and right for example this effectively for any example this effectively for any state i'm able to select from any of state i'm able to select from any of the four possible", "image_path": "img_data/video_20_chunk_17.jpg"}
{"video": "video_20", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "state i'm able to select from any of the four possible actions and this is what actually we actions and this is what actually we called a stochastic policy and as called a stochastic policy and as earlier and as compared to a earlier and as compared to a deterministic which in this case deterministic which in this case action which in this case there's going action which in this case there's going to be only one action that can be to be only one action that can be taken for a specific state all right taken for a specific state all right that's basically the that's basically the starting kind of point of this kind the starting kind of point of this kind of discussion when we take the of discussion when we take the expectation over a policy", "image_path": "img_data/video_20_chunk_18.jpg"}
{"video": "video_20", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "of discussion when we take the expectation over a policy expectation over a policy pi i think it's worthwhile clarifying pi i think it's worthwhile clarifying that we definitely do not mean an that we definitely do not mean an expectation over pi but we mean an expectation over pi but we mean an expectation that pi is a dependency expectation that pi is a dependency almost conditioned on the we almost conditioned on the we should actually see the should actually see the pi after the given symbol over here pi after the given symbol over here let's try to make it a bit let's try to make it a bit sort of a bit more explicit sort of a bit more explicit by writing that a stochastic", "image_path": "img_data/video_20_chunk_19.jpg"}
{"video": "video_20", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "sort of a bit more explicit by writing that a stochastic policy and i think it's best not to use stastics much to attach it to use stastics much to attach it to policy but to attach it to the mdp does policy but to attach it to the mdp does not make the mdp stochastic a stochastic policy stochastic a stochastic policy for example every time that we have for example every time that we have let's say with 1/4 probability we", "image_path": "img_data/video_20_chunk_20.jpg"}
{"video": "video_20", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "for example every time that we have let's say with 1/4 probability we have let's say with 1/4 probability we sample an action out of this kind of sample an action out of this kind of uniform probability in this case uniform probability in this case distribution let's say we sample the distribution let's say we sample the action up let's say and we are in the action up let's say and we are in state s11 we may actually have a s11 we may actually have a deterministic mtp problem which it means deterministic mtp problem which it means that an up action in state 1 s11 will that an up action in state 1 s11 will actually deterministically take us to state s12", "image_path": "img_data/video_20_chunk_21.jpg"}
{"video": "video_20", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "actually deterministically take us to state s12 deterministically take us to state s12 this example i think is this example i think is sufficient to support that kind of sufficient to support that kind of argument that a stochastic policy does argument that a stochastic policy does not make the ndp stochastic the m not make the ndp stochastic the m the mdp is stochastic that's why we call stochastic mdps is made stochastic by the transition", "image_path": "img_data/video_20_chunk_22.jpg"}
{"video": "video_20", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "stochastic by the transition model and in that grid world problem that we have the transition problem that we have the transition model was this we whatever action we model was this we whatever action we select we will go to the desired action select we will go to the desired action with probability 0.8 and on other actions left and 0.8 and on other actions left and right on the other states with right on the other states with probability 0.1 one it is really the", "image_path": "img_data/video_20_chunk_23.jpg"}
{"video": "video_20", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "right on the other states with probability 0.1 one it is really the probability 0.1 one it is really the unreliable the unreliability of the transition of unreliability of the transition of the action that is actually make that the action that is actually make that mdp problem stochastic when we mdp problem stochastic when we take this expectation i think it's take this expectation i think it's in other textbooks you will see this in other textbooks you will see this expectation something which i a bit expectation something which i a bit better than the saturn's notation better than the saturn's notation that i have adopted over here is over s that i have adopted over here is over s prime", "image_path": "img_data/video_20_chunk_24.jpg"}
{"video": "video_20", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "that i have adopted over here is over s prime it's expectation over s prime that is it's expectation over s prime that is sampled out of the probability sampled out of the probability distribution of s prime comm distribution of s prime comm r given s comma a and this is effectively can a and this is effectively can also be written as an expectation also be written as an expectation over s prime with a probability of s prime with a probability of s prime comma r given s comma pi of a given", "image_path": "img_data/video_20_chunk_25.jpg"}
{"video": "video_20", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "comma r given s comma pi of a given s comma pi of a given s this is where you see s this is where you see the dependency on this kind of policy pi the dependency on this kind of policy pi but as we discussed even if this policy but as we discussed even if this policy is stochastic if this properly is stochastic if this properly distribution is not in other words distribution is not in other words the in other words what i mean is the in other words what i mean is that", "image_path": "img_data/video_20_chunk_26.jpg"}
{"video": "video_20", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "the in other words what i mean is that u let's make it a bit more specific if u let's make it a bit more specific if the p the transition model the p the transition model the transition model of s transition model of s prime comma r given s comma prime comma r given s comma a is one for the specific action for is one for the specific action for the specific action we have taken the specific action we have taken the mdp is", "image_path": "img_data/video_20_chunk_27.jpg"}
{"video": "video_20", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "mdp is deterministic and if it is less than one then the mdp is stochastic which means that this s prime here we have multiplicity of s primes here we have multiplicity of s primes that we may end up on as compared that we may end up on as compared to deterministic which is we have the to deterministic which is we have the specific s prime always following the s", "image_path": "img_data/video_20_chunk_28.jpg"}
{"video": "video_20", "start": "0:14:30", "end": "0:14:41.800000", "timestamp": "0:14:30 - 0:14:41.800000", "text": "to deterministic which is we have the specific s prime always following the s specific s prime always following the s using one of the actions that we using one of the actions that we have select even if this action is have select even if this action is taking out itself out of a is sampled taking out itself out of a is sampled itself out of a probability distribution", "image_path": "img_data/video_20_chunk_29.jpg"}
{"video": "video_21", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "to understand the first probabilistic kind of model transition kind of model transition kind of model we be using a book your example from we be using a book your example from your textbook which is really a grid your textbook which is really a grid word of a small word in other words word of a small word in other words consist of this cells it's a 4x3 word consist of this cells it's a 4x3 word and this cell here is blocked you and this cell here is blocked you cannot really transition into that kind cannot really transition into that kind of cell and this grid word is a of cell and this grid word is a stochastic environment it starts if you from environment it starts if you from the goal of the agent that's always", "image_path": "img_data/video_21_chunk_0.jpg"}
{"video": "video_21", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "environment it starts if you from the goal of the agent that's always the goal of the agent that's always starts in this kind of location is to starts in this kind of location is to maximize to move if you into a goal maximize to move if you into a goal state and there are two terminal state and there are two terminal states here one is the that will offer a states here one is the that will offer a reward of plus one and another with a reward of plus one and another with a reward of minus one in a kind of an reward of minus one in a kind of an optimal way in the sense that it will optimal way in the sense that it will try to maximize the reward try to maximize the reward at the end of the of that game which in at the end of the of that game which in this case is u it's not really a game this case is u it's not really a game it's just basically a small war this", "image_path": "img_data/video_21_chunk_1.jpg"}
{"video": "video_21", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "this case is u it's not really a game it's just basically a small war this it's just basically a small war this reaching this terminating state with a reaching this terminating state with a maximum possible reward all right the maximum possible reward all right the environment is stochastic in a sense that despite the stochastic in a sense that despite the fact that the agent is intending let's fact that the agent is intending let's say to go north then the agent has say to go north then the agent has can end up in any of the other states can end up in any of the other states it cannot move outside of the world it cannot move outside of the world it will bounce back onto this wall it will bounce back onto this wall and it will go north with and it will go north with probability of 0 .8 but it will actually", "image_path": "img_data/video_21_chunk_2.jpg"}
{"video": "video_21", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "and it will go north with probability of 0 .8 but it will actually probability of 0 .8 but it will actually go into this state s21 with probability go into this state s21 with probability 10% and it will bump against the wall 10% and it will bump against the wall and remain on that kind of state that and remain on that kind of state that started s11 with another 10% probability started s11 with another 10% probability these are this is the sort of these are this is the sort of war that we are going to use in war that we are going to use in order for us to build if you an order for us to build if you an understanding about what is really the understanding about what is really the transition the probabilistic kind of transition the probabilistic kind of transition model we have kind of", "image_path": "img_data/video_21_chunk_3.jpg"}
{"video": "video_21", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "transition the probabilistic kind of transition model we have kind of transition model we have kind of talked about over here i forgot talked about over here i forgot also to mention that there is if you also to mention that there is if you a penalty of u minus a penalty of u minus 0.04 if we just endlessly move into 0.04 if we just endlessly move into non-terminating state all of the non-terminating state all of the cells that you see here without numbers cells that you see here without numbers they do contain the minus they do contain the minus 0.04 reward this reward is 0.04 reward this reward is negative and slightly negative", "image_path": "img_data/video_21_chunk_4.jpg"}
{"video": "video_21", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "0.04 reward this reward is negative and slightly negative and slightly negative a very small number because it will u a very small number because it will u lead to selecting at the end of the day lead to selecting at the end of the day policies that are not going to if policies that are not going to if it was positive it would actually mean it was positive it would actually mean that the agent could over time gain that the agent could over time gain far more than one u reward by just far more than one u reward by just endlessly moving in that kind of endlessly moving in that kind of envir inside that kind of environment envir inside that kind of environment with a minus 0.04 we avoid kind of with a minus 0.04 we avoid kind of reaching this type of policies than reaching this type of policies than intuitively we can actually understand", "image_path": "img_data/video_21_chunk_5.jpg"}
{"video": "video_21", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "reaching this type of policies than intuitively we can actually understand intuitively we can actually understand what will be the best what will be the best trajectory to reach if you a good trajectory to reach if you a good terminating kind of state let's say this terminating kind of state let's say this state over here the state s43 obviously it is really this s43 obviously it is really this trajectory over here to go north and trajectory over here to go north and then to go east to reach this trajectory then to go east to reach this trajectory to this terminating kind of to this terminating kind of state and the reason why it is this state and the reason why it is this and not for example this one is that and not for example this one is that while it is actually moving around here while it is actually moving around here given the stochastic nature of the", "image_path": "img_data/video_21_chunk_6.jpg"}
{"video": "video_21", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "while it is actually moving around here given the stochastic nature of the given the stochastic nature of the environment despite the fact as we environment despite the fact as we discussed to intention to go north it discussed to intention to go north it may end up into the minus one termina may end up into the minus one termina kind of state and therefore kind of los kind of state and therefore kind of los in quotes if you that game we in quotes if you that game we have 11 states and therefore we're able have 11 states and therefore we're able to define sort of these type of to define sort of these type of matrices and i'm going matrices and i'm going to have a matrix of", "image_path": "img_data/video_21_chunk_7.jpg"}
{"video": "video_21", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "matrices and i'm going to have a matrix of an 11 by 11 kind of matrix it's not really to scale here but this is not really to scale here but this is basically an s11 state all the way to s basically an s11 state all the way to s 43 and this is let's say the current 43 and this is let's say the current state axis and the other state the state axis and the other state the other axis is the -called s prime and other axis is the -called s prime and the s prime axis is also the s prime axis is also s11 to s43", "image_path": "img_data/video_21_chunk_8.jpg"}
{"video": "video_21", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "the s prime axis is also s11 to s43 the matrix effectively is giving us the matrix effectively is giving us here it has some kind of a t tabular here it has some kind of a t tabular form of the underlying transition form of the underlying transition probabilistic model we know that probabilistic model we know that conditional probabilities can be fully conditional probabilities can be fully specified by matrices the p of s prime given s comma a the we need to have a the we need to have one axis assigned to the initial", "image_path": "img_data/video_21_chunk_9.jpg"}
{"video": "video_21", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "a the we need to have one axis assigned to the initial one axis assigned to the initial state another axis associated with the s state another axis associated with the s prime we are missing the a and for prime we are missing the a and for we can define one of those matrices for define one of those matrices for the action north it is also i the action north it is also i think in the textbook it is actually think in the textbook it is actually not really north but up then the not really north but up then the second matrix will be exactly of the second matrix will be exactly of the same notation will be for", "image_path": "img_data/video_21_chunk_10.jpg"}
{"video": "video_21", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "second matrix will be exactly of the same notation will be for same notation will be for down and the third one for right and the fourth one for i'm not going to even draw it for going to even draw it for left four matrices with four left four matrices with four matrices we have fully specified the matrices we have fully specified the probabilistic transition model and probabilistic transition model and let's put some kind of numbers down let's put some kind of numbers down from s11 to remain to s11 this can", "image_path": "img_data/video_21_chunk_11.jpg"}
{"video": "video_21", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "let's put some kind of numbers down from s11 to remain to s11 this can from s11 to remain to s11 this can happen with probability happen with probability 0.1 from s11 to move into the another 0.1 from s11 to move into the another one to s12 for example this will be with one to s12 for example this will be with probability 0.8 according to the probability 0.8 according to the transition model that transition to the transition model that we have introduced the we have introduced the stochastic environment and then we stochastic environment and then we then the finally we will see at some then the finally we will see at some point", "image_path": "img_data/video_21_chunk_12.jpg"}
{"video": "video_21", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "then the finally we will see at some point s13 s u and then s21 will start to be s13 s u and then s21 will start to be specified in the s21 this will be 0.1 here the s21 this will be 0.1 here the summation of the probabilities of all summation of the probabilities of all the possible s prime states will be the possible s prime states will be equal to 1.0 all rows will actually equal to 1.0 all rows will actually sum to 1.0 as we might expect and the sum to 1.0 as we might expect and the same thing can be specified for all same thing can be specified for all other three matrices and that's how we other three matrices and that's how we end up with something that it is", "image_path": "img_data/video_21_chunk_13.jpg"}
{"video": "video_21", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "other three matrices and that's how we end up with something that it is end up with something that it is effectively a computable transition effectively a computable transition model for this specific small duel of model for this specific small duel of course is trivial and we are able to course is trivial and we are able to just use these transition probabilities just use these transition probabilities now we'll see how we are going to be now we'll see how we are going to be using to come up with optimal strategy using to come up with optimal strategy on how to move in this kind of block on how to move in this kind of block word optimally we had defined earlier as word optimally we had defined earlier as part of the mdb dynamics a part of the mdb dynamics a second component of mdb dynamics was second component of mdb dynamics was to do with the probability distribution to do with the probability distribution of rewards the again conditional on", "image_path": "img_data/video_21_chunk_14.jpg"}
{"video": "video_21", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "to do with the probability distribution of rewards the again conditional on of rewards the again conditional on the state and the action we actually the state and the action we actually taken and what we will do now next is we taken and what we will do now next is we will use this probability distribution will use this probability distribution and more specifically take a statistic and more specifically take a statistic out of it which is the out of it which is the expectation and we will to define expectation and we will to define what we call an expected reward also what we call an expected reward also known as a reward function this is known as a reward function this is symbolized in some instances as symbolized in some instances as r of sa a also in some textbooks as the", "image_path": "img_data/video_21_chunk_15.jpg"}
{"video": "video_21", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "symbolized in some instances as r of sa a also in some textbooks as the r of sa a also in some textbooks as the function r that has arguments the state that we r that has arguments the state that we were and the action that we have were and the action that we have taken and this actually is an taken and this actually is an expectation operation expectation operation expectation over the rt at time instant t the reward at time", "image_path": "img_data/video_21_chunk_16.jpg"}
{"video": "video_21", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "the rt at time instant t the reward at time instant t the reward at time instant t given that we were in st minus instant t given that we were in st minus one is equal to s and i took an action a minus one is s and i took an action a minus one is equal to a and let me write it down is the a and let me write it down is the expected reward received after we execute", "image_path": "img_data/video_21_chunk_17.jpg"}
{"video": "video_21", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "received after we execute action a from status obviously our the reward that we will be obviously our the reward that we will be accumulating is going to be a function accumulating is going to be a function of the next state which is we are going of the next state which is we are going to end up evidently the next state is a to end up evidently the next state is a stochastic state and then of course a stochastic state and then of course that state will actually have a specific", "image_path": "img_data/video_21_chunk_18.jpg"}
{"video": "video_21", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "a stochastic state and then of course that state will actually have a specific that state will actually have a specific reward associated with another action reward associated with another action will be taken to end up in an sble prime will be taken to end up in an sble prime state and on in all subsequent states state and on in all subsequent states and this expectation is over the and this expectation is over the reward this actually can be written as reward this actually can be written as summation over r that belongs to the set of rewards r that belongs to the set of rewards calligraphic r", "image_path": "img_data/video_21_chunk_19.jpg"}
{"video": "video_21", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "r that belongs to the set of rewards calligraphic r summation of s prime being in s p s prime comma r given s comma p s prime comma r given s comma a and of course this thing over here a and of course this thing over here comes from the definition of the comes from the definition of the expectation operation which is defined here as operation which is defined here as the expected value let's say of a random the expected value let's say of a random variable let's say x variable let's say x is some", "image_path": "img_data/video_21_chunk_20.jpg"}
{"video": "video_21", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "variable let's say x is some summation of x p of x and also it's worthwhile and you see x and also it's worthwhile and you see it in u will use it as well in many it in u will use it as well in many instances we have a reward function instances we have a reward function which is now has three arguments this which is now has three arguments this is the s a s prime and this is an expectation", "image_path": "img_data/video_21_chunk_21.jpg"}
{"video": "video_21", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "prime and this is an expectation of rt given st minus one is equal to s as you expect st minus one is equal to a you expect st minus one is equal to a and st is equal to s and st is equal to s prime and this will actually be given prime and this will actually be given as an expectation this as an expectation this expectation is can be written as expectation is can be written as summation of", "image_path": "img_data/video_21_chunk_22.jpg"}
{"video": "video_21", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "expectation is can be written as summation of the joint of r this for r belongs to the joint of r this for r belongs to the set of rewards and then we over here the set of rewards and then we over here we have a ratio of the associated kind we have a ratio of the associated kind of conditional we write the conditional we write the conditional as in usual way as the joint conditional as in usual way as the joint s prime comma r given s comma s prime comma r given s comma a divided by p of s prime given s comma a divided by p of s prime given s comma a for the first time here we understand a for the first time here we understand it from the definition of an expectation", "image_path": "img_data/video_21_chunk_23.jpg"}
{"video": "video_21", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "a for the first time here we understand it from the definition of an expectation it from the definition of an expectation operation the second term is a operation the second term is a conditional over here that involves the conditional over here that involves the s prime effectively the s prime effectively the conditional will be the joint divided by conditional will be the joint divided by the marginal of s prime and that's the marginal of s prime and that's basically how we ended up with this kind basically how we ended up with this kind of expression for the reward function of expression for the reward function we need we have two reward functions we need we have two reward functions one is or the other is going to be used one is or the other is going to be used for sort of finding if you for sort of finding if you what is happening in terms of u the", "image_path": "img_data/video_21_chunk_24.jpg"}
{"video": "video_21", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "for sort of finding if you what is happening in terms of u the what is happening in terms of u the solution that we are going to come after solution that we are going to come after all right another we talk about all right another we talk about rewards the goal we can actually write rewards the goal we can actually write that the goal of the agent is to maximize this will actually give us also the pretext to write down some the pretext to write down some expression about this very specific", "image_path": "img_data/video_21_chunk_25.jpg"}
{"video": "video_21", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "the pretext to write down some expression about this very specific expression about this very specific goal the goal of the agent is to goal the goal of the agent is to maximize the expected the to the total let me count expected the to the total let me count the total amount of reward it receives period it receives that's basically the goal and now that's basically the goal and now evidence ly this does not mean maximize", "image_path": "img_data/video_21_chunk_26.jpg"}
{"video": "video_21", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "that's basically the goal and now evidence ly this does not mean maximize evidence ly this does not mean maximize the immediate reward but maximize a the immediate reward but maximize a cumulative reward over the long term cumulative reward over the long term over the long run that's over the long run that's basically the goal and we'll basically the goal and we'll actually need to define now a quantity actually need to define now a quantity that we will be calling a return that it that we will be calling a return that it will have elements of what we just wrote will have elements of what we just wrote down here the return is given by an", "image_path": "img_data/video_21_chunk_27.jpg"}
{"video": "video_21", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "return the return is given by an expression we will call it gt is the cumulative discounted cumulative discounted reward that we can actually get reward that we can actually get starting from state st the discount we will explain now st the discount we will explain now what the gamma factor is doing here", "image_path": "img_data/video_21_chunk_28.jpg"}
{"video": "video_21", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "st the discount we will explain now what the gamma factor is doing here this is it could go forever or it will terminate at the specific terminate at the specific termination terminating kind of state termination terminating kind of state from t is equal to 0 to infinity let from t is equal to 0 to infinity let me write down the expression for the me write down the expression for the former part gamma k rt+ k", "image_path": "img_data/video_21_chunk_29.jpg"}
{"video": "video_21", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "me write down the expression for the former part gamma k rt+ k this is the sort of infinite what we call infinite horizon a problem return and we say when we say about return and we say when we say about infinite horizon we don't mean that the infinite horizon we don't mean that the interaction will last for an infinite interaction will last for an infinite amount of time but what we mean that we amount of time but what we mean that we do not impose a specific deadline to the do not impose a specific deadline to the mtp", "image_path": "img_data/video_21_chunk_30.jpg"}
{"video": "video_21", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "do not impose a specific deadline to the mtp problem all right evidently that fact if we do impose deadlines the fact if we do impose deadlines the strategy and the associated in other strategy and the associated in other words policy will actually be quite words policy will actually be quite different remember what happens in a", "image_path": "img_data/video_21_chunk_31.jpg"}
{"video": "video_21", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "words policy will actually be quite different remember what happens in a different remember what happens in a basketball game and the complete basketball game and the complete change of strategy for a team that has change of strategy for a team that has only let's say 10 seconds to finish the only let's say 10 seconds to finish the game and let's say the difference is game and let's say the difference is let's say two points and the losing kind let's say two points and the losing kind of team completely changes the strategy of team completely changes the strategy on those last 10 seconds of the game on those last 10 seconds of the game trying to score as much as they can trying to score as much as they can and win the game otherwise the previous and win the game otherwise the previous strategy is basically no longer strategy is basically no longer applicable that is an example of from applicable that is an example of from real life of something that has now a", "image_path": "img_data/video_21_chunk_32.jpg"}
{"video": "video_21", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "applicable that is an example of from real life of something that has now a real life of something that has now a deadline and the how this affects the deadline and the how this affects the strategy did with respect to the strategy did with respect to the discount factor over here you can discount factor over here you can understand that when gamma is equal to understand that when gamma is equal to zero then effectively we have gt zero then effectively we have gt is equal to rt + 1 effectively the we have absolutely no incentive to look beyond", "image_path": "img_data/video_21_chunk_33.jpg"}
{"video": "video_21", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "1 effectively the we have absolutely no incentive to look beyond absolutely no incentive to look beyond the immediate reward as far as the agent the immediate reward as far as the agent is concerned when the gamma is something concerned when the gamma is something which is very close to one then what which is very close to one then what we have here is we have a gt that we have here is we have a gt that it is going to offer long-term incentive to the agent and when we have a gamma which", "image_path": "img_data/video_21_chunk_34.jpg"}
{"video": "video_21", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "and when we have a gamma which is greater than zero but a small is greater than zero but a small number then we can adjust on number then we can adjust on by gam on how long or shortterm we by gam on how long or shortterm we can be varing long and short", "image_path": "img_data/video_21_chunk_35.jpg"}
{"video": "video_21", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "be varing long and short long-term incentive the choice of gamma is a kind of the choice of gamma is a kind of considered to be a hyper parameter in considered to be a hyper parameter in this kind of mtp problem definition is this kind of mtp problem definition is one of those parameters if you of one of those parameters if you of the problem and therefore it of the problem and therefore it plays kind of an important role on the plays kind of an important role on the selected strate strategy at the end as u selected strate strategy at the end as u that example sort of i gave you that example sort of i gave you indicated all right now that we have indicated all right now that we have seen the return and kind of make it kind of a connect", "image_path": "img_data/video_21_chunk_36.jpg"}
{"video": "video_21", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "seen the return and kind of make it kind of a connect and kind of make it kind of a connect that to the long-term goal of the agent that to the long-term goal of the agent i think it's going to the time i think it's going to the time has come to define a very important has come to define a very important formally a very important term which is called policy what is a policy is a mapping from", "image_path": "img_data/video_21_chunk_37.jpg"}
{"video": "video_21", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "mapping from state to action and in fact when we typically talk about policies we are defining talk about policies we are defining if you an action for all if you an action for all possible states that we have in the possible states that we have in the problem or at least the ones that we are problem or at least the ones that we are visiting and as we'll see later this visiting and as we'll see later this distinguishes also exactly how many distinguishes also exactly how many states we are visiting would distinguish states we are visiting would distinguish also the definition of the also the definition of the various algorithms that we are going to", "image_path": "img_data/video_21_chunk_38.jpg"}
{"video": "video_21", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "also the definition of the various algorithms that we are going to various algorithms that we are going to be involving to solve the mdp problem be involving to solve the mdp problem right now we are at the definition kind right now we are at the definition kind of stage we have effectively of stage we have effectively two types of policies we have two types of policies we have -cal deterministic policy and we have a random or stochastic", "image_path": "img_data/video_21_chunk_39.jpg"}
{"video": "video_21", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "policy and we have a random or stochastic in both instances can be accommodated by treating effectively this pi as by treating effectively this pi as probability distribution we can actually write pi distribution we can actually write pi the pi is going to be the symbol for the pi is going to be the symbol for our policy and we can define it as our policy and we can define it as something that has the action that we something that has the action that we are taken given the state that we are in are taken given the state that we are in and this should be interpreted as a and this should be interpreted as a probability over all possible", "image_path": "img_data/video_21_chunk_40.jpg"}
{"video": "video_21", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "and this should be interpreted as a probability over all possible probability over all possible actions we can have let's say actions we can have let's say a0 given s p of a1 given s let's say p of a cardinality set of a s let's say p of a cardinality set of a given s and in the case of the terministic s and in the case of the terministic policy if you are in s you will actually policy if you are in s you will actually have one of the terms being one and all have one of the terms being one and all the others are zero in the case of", "image_path": "img_data/video_21_chunk_41.jpg"}
{"video": "video_21", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "have one of the terms being one and all the others are zero in the case of the others are zero in the case of stochastic policy you have all the stochastic policy you have all the elements effectively being non elements effectively being non zero the obviously the summation of zero the obviously the summation of these elements will actually equal to these elements will actually equal to 1.0 that's basically the u sort 1.0 that's basically the u sort of po the definition of a policy it of po the definition of a policy it will define for us i mean this is will define for us i mean this is basically what we're after we are basically what we're after we are going to find the optimal policy to going to find the optimal policy to that it will effectively that it will effectively allow us to do the following for every", "image_path": "img_data/video_21_chunk_42.jpg"}
{"video": "video_21", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "that it will effectively allow us to do the following for every allow us to do the following for every state that the agent will be able to state that the agent will be able to be in we will be able to be in we will be able to draw an action let's say up right draw an action let's say up right and we are in terminating state right and we are in terminating state over here obviously in every cell in over here obviously in every cell in every of these states we may be able every of these states we may be able to have more than one action as the to have more than one action as the optimal and therefore and", "image_path": "img_data/video_21_chunk_43.jpg"}
{"video": "video_21", "start": "0:22:00", "end": "0:22:27.033333", "timestamp": "0:22:00 - 0:22:27.033333", "text": "to have more than one action as the optimal and therefore and optimal and therefore we need to continue to but therefore we need to continue to but another important thing is that we need another important thing is that we need to define all of the for all stage all of the for all stage all of the actions we this policy the actions we this policy effectively will we need to be effectively will we need to be specified globally for all the states we specified globally for all the states we are in", "image_path": "img_data/video_21_chunk_44.jpg"}
{"video": "video_22", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we'll start seeing another broad family of methods that another broad family of methods that are coming under the headline of are coming under the headline of planning in this time as compared to planning in this time as compared to an earlier video we have seen planning an earlier video we have seen planning actually will happen with interactions actually will happen with interactions with the environment with the environment will actually be able to send us a will actually be able to send us a signal a reward that it will in broad signal a reward that it will in broad ter is being used to guide the u ter is being used to guide the u strategy of actions that we will take in strategy of actions that we will take in that to act optimally in that", "image_path": "img_data/video_22_chunk_0.jpg"}
{"video": "video_22", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "strategy of actions that we will take in that to act optimally in that to act optimally in that environment and that environment will environment and that environment will also be stochastic which means that despite the stochastic which means that despite the fact that we may be intending to reach a fact that we may be intending to reach a desired state when we perform a desired state when we perform a specific action the stochasticity of the specific action the stochasticity of the environment will actually lead us to environment will actually lead us to other states ear than the intended state other states ear than the intended state planning with interactions also take a planning with interactions also take a form of two methods as well one is form of two methods as well one is called mark of decision processes and on called mark of decision processes and on those we'll be assuming", "image_path": "img_data/video_22_chunk_1.jpg"}
{"video": "video_22", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "called mark of decision processes and on those we'll be assuming those we'll be assuming that we have an understanding of the that we have an understanding of the model complete model of that environment model complete model of that environment the model dynamics as we say as we the model dynamics as we say as we define them and the second category will define them and the second category will be reinforcement learning where in be reinforcement learning where in this case we will need to learn those this case we will need to learn those dynamics in order to perform optimally dynamics in order to perform optimally in that environment let's now introduce in that environment let's now introduce some important notation in our attempt some important notation in our attempt to start the discussion about to start the discussion about introducing if you the mdp the", "image_path": "img_data/video_22_chunk_2.jpg"}
{"video": "video_22", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "to start the discussion about introducing if you the mdp the introducing if you the mdp the markco decision process problem markco decision process problem statement we will be having if you statement we will be having if you a diagram the one we had seen a diagram the one we had seen in earlier kind of videos where the in earlier kind of videos where the agent is sitting on one side and the agent is sitting on one side and the environment on the other and what environment on the other and what actually as a reminder what we mean by actually as a reminder what we mean by environment we mean everything which is environment we mean everything which is outside the scope the control outside the scope the control scope of an agent and this means for scope of an agent and this means for example in a self-driving car let's say example in a self-driving car let's say a robot i've seen it in another a robot i've seen it in another video everything let's say outside of", "image_path": "img_data/video_22_chunk_3.jpg"}
{"video": "video_22", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "a robot i've seen it in another video everything let's say outside of video everything let's say outside of that control responsibility of that control responsibility of that kind of vehicle including other vehicles kind of vehicle including other vehicles the pedestrians and on will be the pedestrians and on will be considered to be the environment and considered to be the environment and the agent is going to be taking the agent is going to be taking some kind of an actions and the result some kind of an actions and the result of those actions are going to change of those actions are going to change the state of the environment the state of the environment the state is always something which is going to be is always something which is going to be assumed to be fully observed and contain", "image_path": "img_data/video_22_chunk_4.jpg"}
{"video": "video_22", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "is always something which is going to be assumed to be fully observed and contain assumed to be fully observed and contain within that kind of environment box and within that kind of environment box and the agent being let's say the agent being let's say in state st as it is shown in the in state st as it is shown in the diagram is taking some action a and diagram is taking some action a and then the results of this action are then the results of this action are a change state st +1 and a reward a change state st +1 and a reward signal rt + one which are sent to the signal rt + one which are sent to the agent to be able to", "image_path": "img_data/video_22_chunk_5.jpg"}
{"video": "video_22", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "signal rt + one which are sent to the agent to be able to as we have discussed in another video as we have discussed in another video let's try to put some kind of notation let's try to put some kind of notation about this kind of agent environment about this kind of agent environment interface we have seen agent environment interface we have seen agent environment interfaces before in another video but now the before in another video but now the emphasis here will be on introducing emphasis here will be on introducing specific notations to introduce the mark specific notations to introduce the mark of decision process model in of decision process model in this setting we have an agent and we this setting we have an agent and we have an environment and the agent being", "image_path": "img_data/video_22_chunk_6.jpg"}
{"video": "video_22", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "this setting we have an agent and we have an environment and the agent being have an environment and the agent being in state st is decides to take an in state st is decides to take an action at and that action changes the action at and that action changes the state of the environment and it leads state of the environment and it leads the agent to a next state which is st+ the agent to a next state which is st+ one here and the environment is one here and the environment is sending that reward rt+ one for sending that reward rt+ one for the agent to be able to consume it the agent to be able to consume it and we'll see now exactly how this and we'll see now exactly how this consumption will what it will result to", "image_path": "img_data/video_22_chunk_7.jpg"}
{"video": "video_22", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "and we'll see now exactly how this consumption will what it will result to consumption will what it will result to the important points here in this the important points here in this kind of setting and let me just go kind of setting and let me just go back to that sort of writing a little back to that sort of writing a little bit about this kind of bit about this kind of notation we have if you notation we have if you the following it's actually happening the following it's actually happening we are in some kind of a are in some kind of a state st which capital st will be the state st which capital st will be the random variable that indicates the state random variable that indicates the state at time t and let me call this to be", "image_path": "img_data/video_22_chunk_8.jpg"}
{"video": "video_22", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "random variable that indicates the state at time t and let me call this to be at time t and let me call this to be s the we are taking some kind of s the we are taking some kind of an action at which is going to be a in action at which is going to be a in this case and we are this case and we are led into a reward s st+ one we are into a reward s st+ one we are getting a reward for that and a new state", "image_path": "img_data/video_22_chunk_9.jpg"}
{"video": "video_22", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "reward for that and a new state s prime this is definitely an agent decision and this is the environment responsibility that's responsibility of an agent that's responsibility of an agent that's environment responsibility now that we environment responsibility now that we have reached this kind of state s have reached this kind of state s prime we will be taking an action prime we will be taking an action we'll be taking an action this action is we'll be taking an action this action is again the arrow 8 + 1 is equal to let's", "image_path": "img_data/video_22_chunk_10.jpg"}
{"video": "video_22", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "we'll be taking an action this action is again the arrow 8 + 1 is equal to let's again the arrow 8 + 1 is equal to let's say a prime in the in our kind of prime in the in our kind of notation over here means next notation over here means next state here next action moving over and state here next action moving over and then this is agent moving over to rt + 2 is equal to r and then st + one", "image_path": "img_data/video_22_chunk_11.jpg"}
{"video": "video_22", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "r and then st + one that's basically r the not r is a scalar and is actually called reward in terms of notations we have a for action and s small letter s will be the specific", "image_path": "img_data/video_22_chunk_12.jpg"}
{"video": "video_22", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "action and s small letter s will be the specific state and the prime operator will be stands for the prime operator will be stands for next instead of having if you next instead of having if you this kind of complicated notation over here we complicated notation over here we will probably be omitting in some will probably be omitting in some instances the capital a which is instances the capital a which is basically the name of the variable and basically the name of the variable and we can just quote only the specific we can just quote only the specific instance of that variable which is the instance of that variable which is the specific action this of course", "image_path": "img_data/video_22_chunk_13.jpg"}
{"video": "video_22", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "instance of that variable which is the specific action this of course specific action this of course continues sometimes continuously forever continues sometimes continuously forever or sometimes it's also going to or sometimes it's also going to terminate in what we will be calling terminate in what we will be calling an episode in terms of an episode in terms of notation again we have divided this notation again we have divided this whole experience into multiple whole experience into multiple interactions that we'll call sa interactions that we'll call sa a r", "image_path": "img_data/video_22_chunk_14.jpg"}
{"video": "video_22", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "interactions that we'll call sa a r comma s prime a prime r prime comma dot comma s prime a prime r prime comma dot and on these are basically dot and on these are basically our interactions and this is basically interactions and this is basically our experience the mdp problem and let me also go back to the site to let me also go back to the site to introduce now the to introduce now the", "image_path": "img_data/video_22_chunk_15.jpg"}
{"video": "video_22", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "introduce now the to introduce now the to introduce now the notation that we have just quoted all notation that we have just quoted all right we see here the right we see here the state calligraphic s is the set of state calligraphic s is the set of states that we have the same with states that we have the same with calligraphic a as the set of finite calligraphic a as the set of finite actions we have a finite mdp actions we have a finite mdp problem we have a finite set of problem we have a finite set of actions all of these notation i just actions all of these notation i just mentioned t is a time index but in is mentioned t is a time index but in is basically here is not necessarily has to basically here is not necessarily has to take the semantics of time take the semantics of time it is basically some kind of", "image_path": "img_data/video_22_chunk_16.jpg"}
{"video": "video_22", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "take the semantics of time it is basically some kind of it is basically some kind of sequential index and capital t is a sequential index and capital t is a time where we our episode kind of time where we our episode kind of terminates and let's say for example terminates and let's say for example in the game let's say of tik tac toe in the game let's say of tik tac toe the game ends with one of the players the game ends with one of the players kind of winning and t is going to kind of winning and t is going to be the maximum possible duration of a be the maximum possible duration of a tic tac to game ever and then the", "image_path": "img_data/video_22_chunk_17.jpg"}
{"video": "video_22", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "be the maximum possible duration of a tic tac to game ever and then the tic tac to game ever and then the termination may actually happen before termination may actually happen before that capital t is reached the u that capital t is reached the u episode is the horizon from t is equal episode is the horizon from t is equal to z where the interaction starts 2 t to z where the interaction starts 2 t minus one and to is going to be called a minus one and to is going to be called a trajectory it is this kind of sequence trajectory it is this kind of sequence of experiences over that kind of over an experiences over that kind of over an apisod that we have just defined apisod that we have just defined these triplets of stage actions and", "image_path": "img_data/video_22_chunk_18.jpg"}
{"video": "video_22", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "apisod that we have just defined these triplets of stage actions and these triplets of stage actions and rewards all right these are basically rewards all right these are basically the sort of starting notation for the sort of starting notation for defining the mdp problem and then we defining the mdp problem and then we will see the last two how they get to will see the last two how they get to be defined and then now let's go back be defined and then now let's go back to that sort of u whiteboard over to that sort of u whiteboard over here and let's define now formally here and let's define now formally the mdp", "image_path": "img_data/video_22_chunk_19.jpg"}
{"video": "video_22", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "here and let's define now formally the mdp i'll call it as caligraphic m the mdp i'll call it as caligraphic m is a five tle it starts with a set of is a five tle it starts with a set of states it is capital p states it is capital p calligraphic p will designate here the calligraphic p will designate here the -called transition model this is the set of states", "image_path": "img_data/video_22_chunk_20.jpg"}
{"video": "video_22", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "model this is the set of states the transition model the reward set of reward functions that we have defined and a set of actions set of reward set of actions and another factor that is called gamma another factor that is called gamma which actually stands for the discount", "image_path": "img_data/video_22_chunk_21.jpg"}
{"video": "video_22", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "another factor that is called gamma which actually stands for the discount factor this is the discount factor we'll see how the discount factor gets to be used i should also point out that in many textbooks you've seen a slightly many textbooks you've seen a slightly different notation however the number of different notation however the number of textbooks is kind of equally divided", "image_path": "img_data/video_22_chunk_22.jpg"}
{"video": "video_22", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "different notation however the number of textbooks is kind of equally divided textbooks is kind of equally divided here we have adopted the sat here we have adopted the sat notation which is really the standard notation which is really the standard reinforcement learning textbook out reinforcement learning textbook out there especially with respect to there especially with respect to the re reward you may actually see the re reward you may actually see the reward not with t plus one but with the reward not with t plus one but with t in some other textbooks i just want to t in some other textbooks i just want to mention that just in case you are mention that just in case you are comparing notes and you are looking comparing notes and you are looking at other sort of sources to at other sort of sources to understand mark of decision process understand mark of decision process rather than the notes i think", "image_path": "img_data/video_22_chunk_23.jpg"}
{"video": "video_22", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "understand mark of decision process rather than the notes i think rather than the notes i think it's worthwhile kind of drawing a it's worthwhile kind of drawing a diagram and in this kind of diagram and in this kind of diagram what we will draw here is what we will draw here is basically some kind of a dependency basically some kind of a dependency diagram dependency kind of network diagram dependency kind of network we have here the u state sorry the first one we have this state s the first one we have this state s that transitions via the transition model transitions via the transition model to s prime", "image_path": "img_data/video_22_chunk_24.jpg"}
{"video": "video_22", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "transitions via the transition model to s prime and this transition is effectively and this transition is effectively sort of due to two things first of sort of due to two things first of all the action a induces the transition all the action a induces the transition s prime has dependency on this action s prime has dependency on this action a and also the reward that we are a and also the reward that we are getting is also has dependency on both getting is also has dependency on both action and the state that you are in", "image_path": "img_data/video_22_chunk_25.jpg"}
{"video": "video_22", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "getting is also has dependency on both action and the state that you are in action and the state that you are in this diagram similar to the other this diagram similar to the other diagrams we've seen in some other videos diagrams we've seen in some other videos when we for example we were doing hidden when we for example we were doing hidden mark of models we've seen in the mark of models we've seen in the probabilistic reasoning over time probabilistic reasoning over time they do allow us to kind of come up with they do allow us to kind of come up with interesting probabilistic models that interesting probabilistic models that govern if you the operation of a govern if you the operation of a mark of decision process and here it's mark of decision process and here it's actually evident that r and s prime are actually evident that r and s prime are determined from sna and let's keep", "image_path": "img_data/video_22_chunk_26.jpg"}
{"video": "video_22", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "actually evident that r and s prime are determined from sna and let's keep determined from sna and let's keep that let me write this down r reward that let me write this down r reward and s prime take place well not take place but are determined from at the same time", "image_path": "img_data/video_22_chunk_27.jpg"}
{"video": "video_22", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "determined from at the same time when we execute the step function with arguments s and a this is this kind of step function this is this kind of step function is the one that kind of is the one that kind of transitions the environment and also transitions the environment and also does that kind of reward and this is", "image_path": "img_data/video_22_chunk_28.jpg"}
{"video": "video_22", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "transitions the environment and also does that kind of reward and this is does that kind of reward and this is basically the two arguments it returns basically the two arguments it returns two things that returns this step two things that returns this step function two things that are coming as function two things that are coming as input two things that are being returned input two things that are being returned and based on that we can definitely and based on that we can definitely start writing the first if you start writing the first if you probabilistic model that will definitely probabilistic model that will definitely would will be part of what will be would will be part of what will be called mdp dynamics first we are expecting to", "image_path": "img_data/video_22_chunk_29.jpg"}
{"video": "video_22", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "dynamics first we are expecting to see probability distributions and the shape or form of distributions and the shape or form of this probability distributions will be this probability distributions will be definitely governed by this diagram definitely governed by this diagram which is a dependency diagram that which is a dependency diagram that sort of showcases dependencies between the showcases dependencies between the instances of underlying random variables instances of underlying random variables here and let's see one obvious one here and let's see one obvious one the first one i want to cover is the probability of", "image_path": "img_data/video_22_chunk_30.jpg"}
{"video": "video_22", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "probability of s prime given s comma a s prime given s comma a then what a s prime given s comma a then what is the probability of the next state is the probability of the next state given that i'm in state s right now and given that i'm in state s right now and i'm deciding to take committing to take i'm deciding to take committing to take the action a this is effectively the action a this is effectively what we will be governing the what we will be governing the -cal transition model we'll be calling", "image_path": "img_data/video_22_chunk_31.jpg"}
{"video": "video_22", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "what we will be governing the -cal transition model we'll be calling -cal transition model we'll be calling a transition model and i think in terms of notation we can sort notation we can sort of not in terms of notation but we can of not in terms of notation but we can actually write this transition model as actually write this transition model as some kind of marginalization over a very marginalization over a very interesting that this is what we'll be interesting that this is what we'll be calling mdb dynamics distribution for calling mdb dynamics distribution for all the rewards that belongs to the set", "image_path": "img_data/video_22_chunk_32.jpg"}
{"video": "video_22", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "calling mdb dynamics distribution for all the rewards that belongs to the set all the rewards that belongs to the set of rewards r we have the marginalization over all r of s prime marginalization over all r of s prime comma r given s comma a this is what we will be at the end of the day calling will be at the end of the day calling mdp dynamics this will be the complete mdp dynamics this will be the complete set of all of the whole behavior of set of all of the whole behavior of this kind of diagram and we are", "image_path": "img_data/video_22_chunk_33.jpg"}
{"video": "video_22", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "set of all of the whole behavior of this kind of diagram and we are this kind of diagram and we are obtaining the transition model by obtaining the transition model by marginalizing over all rewards now this marginalizing over all rewards now this is what we call the sum rule in one is what we call the sum rule in one of the videos when we reviewing of the videos when we reviewing probability the loss of probability the loss of probability the two laws the sum and the product the two laws the sum and the product rule of probability and that's basically rule of probability and that's basically it's a kind of a straightforward summing over r to straightforward summing over r to obtain the transition model this obtain the transition model this transition model will actually have an transition model will actually have an example in a moment that's the first", "image_path": "img_data/video_22_chunk_34.jpg"}
{"video": "video_22", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "transition model will actually have an example in a moment that's the first example in a moment that's the first type of mtb dynamics i want to type of mtb dynamics i want to explicitly call out and the second is explicitly call out and the second is this probability distribution which this probability distribution which is the probability distribution over is the probability distribution over rewards are given that we have we are in state s given that we have we are in state s and a it is this guy over here this a it is this guy over here this can also be written as a can also be written as a marginalization now over the s", "image_path": "img_data/video_22_chunk_35.jpg"}
{"video": "video_22", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "marginalization now over the s prime set of from the set of all prime set of from the set of all states that we have in the problem over states that we have in the problem over the all of the next states that belong the all of the next states that belong to the calligraphic s all the set of all to the calligraphic s all the set of all states we have in the problem and this states we have in the problem and this is basically p of s prime comma r given is basically p of s prime comma r given s comma a and this is the probability distribution of", "image_path": "img_data/video_22_chunk_36.jpg"}
{"video": "video_22", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "distribution of rewards this transition model is a prob distribution of next states these two distribution of next states these two as you actually can see they are as you actually can see they are directly related to this quantity this directly related to this quantity this identical quantity that will be calling identical quantity that will be calling from now on the -called mdp dynamics which is probably distribution a conditional prob", "image_path": "img_data/video_22_chunk_37.jpg"}
{"video": "video_22", "start": "0:19:00", "end": "0:19:18.500000", "timestamp": "0:19:00 - 0:19:18.500000", "text": "dynamics which is probably distribution a conditional prob distribution a conditional prob distribution of the next state and distribution of the next state and reward which as we said are determined reward which as we said are determined jointly in one step and given the jointly in one step and given the fact that we are in state s and to fact that we are in state s and to connection a", "image_path": "img_data/video_22_chunk_38.jpg"}
{"video": "video_23", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in an earlier video we have introduced logical reasoning and we said that you logical reasoning and we said that consist of two main topics one is know consist of two main topics one is the syntax and the other is the syntax and the other is the semantics and we started writing semantics and we started writing sentences to understand the s the sentences to understand the s the language we expressed propositional kind language we expressed propositional kind of logic and looked at ways that we can of logic and looked at ways that we can actually make inferences in an early actually make inferences in an early video also we've seen an algor called video also we've seen an algor called model checking and backtracking search model checking and backtracking search on how we can make inferences using this on how we can make inferences using this type of approach and here in this", "image_path": "img_data/video_23_chunk_0.jpg"}
{"video": "video_23", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "on how we can make inferences using this type of approach and here in this type of approach and here in this video we will see the second in this video we will see the second approach that we call propositional approach that we call propositional logic theorem proving where we use logic theorem proving where we use syntax alone to do inferences this is syntax alone to do inferences this is the second and final approach that we the second and final approach that we will discuss in this series and we will discuss in this series and we will just need to introduce a couple of u just need to introduce a couple of u sort of inference rules we will sort of inference rules we will not be evaluating models or constructing not be evaluating models or constructing models but we will be using a couple", "image_path": "img_data/video_23_chunk_1.jpg"}
{"video": "video_23", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "not be evaluating models or constructing models but we will be using a couple models but we will be using a couple of things a couple of tools available to of things a couple of tools available to us the first one is effectively us the first one is effectively the a table of logical equivalences and the a table of logical equivalences and this is the table of logical this is the table of logical equivalences we will stop the equivalences we will stop the video now and come back after we video now and come back after we fix the table over here on this page fix the table over here on this page also we need a set of inference also we need a set of inference rules and as such", "image_path": "img_data/video_23_chunk_2.jpg"}
{"video": "video_23", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "also we need a set of inference rules and as such we will list two inference rules that we'll be using the first one is called we'll be using the first one is called modus ponents and the rule is actually written in a as a kind of a fraction where the in a as a kind of a fraction where the numerator is the premise and the nominator is the premise and the nominator is the conclusion", "image_path": "img_data/video_23_chunk_3.jpg"}
{"video": "video_23", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "premise and the nominator is the conclusion and for the premise we have if we have an implication and also the validity of the sentence a then we can conclude of the sentence a then we can conclude that beta is also true and the second that beta is also true and the second ruer we will be using is called end elimination and similarly it express as a par", "image_path": "img_data/video_23_chunk_4.jpg"}
{"video": "video_23", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "elimination and similarly it express as a par is that every time that we have a and beta we can conclude that beta is sorry beta we can conclude that beta is sorry that a is true of course we can also conclude that beta is true but i wrote it here as a with a true but i wrote it here as a with a and all right the", "image_path": "img_data/video_23_chunk_5.jpg"}
{"video": "video_23", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "true but i wrote it here as a with a and all right the application of this u propositional application of this u propositional kind of theorem proving approach that kind of theorem proving approach that use syntax alone will go as follows use syntax alone will go as follows first of all we have an example here we first of all we have an example here we are going to show and this example will show and this example will demonstrate this is the example i demonstrate this is the example i show this is this were basically the show this is this were basically the infin solu now the query comes what infin solu now the query comes what we need to show the nega negative", "image_path": "img_data/video_23_chunk_6.jpg"}
{"video": "video_23", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "infin solu now the query comes what we need to show the nega negative we need to show the nega negative p12 this is again from the hus world and p12 this is again from the hus world and negative p21 is true this is basically what we need to sol all right we will be using the table that we have developed during the", "image_path": "img_data/video_23_chunk_7.jpg"}
{"video": "video_23", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "sol all right we will be using the table that we have developed during the table that we have developed during the an earlier video when we looked at the an earlier video when we looked at the wbus world in detail and during that a set of detail and during that a set of we had a knowledge base this knowledge we had a knowledge base this knowledge base consisted of a set of rules and the base consisted of a set of rules and the rule that we are going to be using rule that we are going to be using here is rule number two let me go here is rule number two let me go back to that and actually see how this back to that and actually see how this table came to be developed we had table came to be developed we had this wus world where we started", "image_path": "img_data/video_23_chunk_8.jpg"}
{"video": "video_23", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "table came to be developed we had this wus world where we started this wus world where we started writing this kind of rules and we writing this kind of rules and we assume some kind of initial rules one of assume some kind of initial rules one of them was rule number two over here them was rule number two over here we will start from this rule that we will start from this rule that definitely is a rule that involves the definitely is a rule that involves the propositional symbols that are going propositional symbols that are going to be involved over there and we can actually see that there and we can actually see that and that's our starting that and that's our starting point let's look at this", "image_path": "img_data/video_23_chunk_9.jpg"}
{"video": "video_23", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "that and that's our starting point let's look at this starting at r2 which is b11 if and only if p12 or p21 we look now at the how we p21 we look now at the how we can starting from that kind of lo how we can starting from that kind of lo how we can modify it gradually in order to", "image_path": "img_data/video_23_chunk_10.jpg"}
{"video": "video_23", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "can starting from that kind of lo how we can modify it gradually in order to can modify it gradually in order to reach a point where we reach a point where we have this sentence to be evaluated have this sentence to be evaluated to two to true we will start from u to two to true we will start from u using the -called by conditional using the -called by conditional elimination we'll be replacing this elimination we'll be replacing this the by conditional with the equivalent over on the right with the equivalent over on the right hand side we will just do that we hand side we will just do that we write that", "image_path": "img_data/video_23_chunk_11.jpg"}
{"video": "video_23", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "hand side we will just do that we write that b11 implies p12 or p21 and p12 or p21 implies b11 and this is my r8 p21 implies b11 and this is my r8 rule which i can store in the database rule which i can store in the database in the knowledge base i already in the knowledge base i already started with a rule in the knowledge started with a rule in the knowledge base i'm adding this r8 in the knowledge", "image_path": "img_data/video_23_chunk_12.jpg"}
{"video": "video_23", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "started with a rule in the knowledge base i'm adding this r8 in the knowledge base i'm adding this r8 in the knowledge base and let me write down what thing i base and let me write down what thing i have used i use the by conditional b conditional elimination now that i have reached this sentence over here i reached this sentence over here i will apply then now the first inference", "image_path": "img_data/video_23_chunk_13.jpg"}
{"video": "video_23", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "reached this sentence over here i will apply then now the first inference will apply then now the first inference rule which is the end rule which is the end elimination from end elimination from end elimination i can do that elimination i can do that p12 or p21 implies b11 and this is from b11 and this is from end elimination this is now my rule r9 and it's evident how they and", "image_path": "img_data/video_23_chunk_14.jpg"}
{"video": "video_23", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "elimination this is now my rule r9 and it's evident how they and r9 and it's evident how they and elimination could applied here i have elimination could applied here i have alpha plus alpha and beta implies alpha plus alpha and beta implies alpha i have or beta i have selected beta here and that is basically selected beta here and that is basically what i did and that's basically it what i did and that's basically it and then we have the we took and then we have the we took now this sentence and we apply the now this sentence and we apply the -called contraposition rule the -called contraposition rule the contraposition rule basically the way contraposition rule basically the way to sort of work with proposition logic", "image_path": "img_data/video_23_chunk_15.jpg"}
{"video": "video_23", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "contraposition rule basically the way to sort of work with proposition logic to sort of work with proposition logic theorem proving is that you have to theorem proving is that you have to have some kind of experience by looking have some kind of experience by looking at some examples in your textbook and at some examples in your textbook and trying to understand where you trying to understand where you are and get some experience on are and get some experience on which one is the most reasonable rule to which one is the most reasonable rule to logical equivalence to replace your logical equivalence to replace your sentence at any moment in time to go sentence at any moment in time to go gradually into that kind of solution gradually into that kind of solution the as i said we will apply now the as i said we will apply now the contraposition rule", "image_path": "img_data/video_23_chunk_16.jpg"}
{"video": "video_23", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "the as i said we will apply now the contraposition rule on this r9 and we'll arrive at the replace r9 and we'll arrive at the replace this with this ne negation of b11 implies negation of p12 or b11 implies negation of p12 or p21 and this is the rule p21 and this is the rule r10 and we did contraposition all right and now we will apply modus ponents where we", "image_path": "img_data/video_23_chunk_17.jpg"}
{"video": "video_23", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "contraposition all right and now we will apply modus ponents where we will apply modus ponents where we have from another earlier rule have from another earlier rule that we had in the knowledge base the in that we had in the knowledge base the in the knowledge base back when we're the knowledge base back when we're discussing the knowledge base we had discussing the knowledge base we had the rule r4 we had we did the rule r4 we had we did the model evaluation by using the model evaluation by using the first four rules one of the rules", "image_path": "img_data/video_23_chunk_18.jpg"}
{"video": "video_23", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "the model evaluation by using the first four rules one of the rules first four rules one of the rules r4 is that there is no breeze in cell 11 r4 is that there is no breeze in cell 11 one this rule r4 still is our one this rule r4 still is our knowledge base and we will use it now knowledge base and we will use it now when we apply the -cal modus ponents when we apply the -cal modus ponents inference rule from r10 and r4 we will arrive and which is going to give us the rule 11 which i'm going to", "image_path": "img_data/video_23_chunk_19.jpg"}
{"video": "video_23", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "going to give us the rule 11 which i'm going to us the rule 11 which i'm going to write now we have the negation of b11 write now we have the negation of b11 implies negation of p12 or p21 comma negation b11 the conclusion is the negation of p 1 2 or p 2", "image_path": "img_data/video_23_chunk_20.jpg"}
{"video": "video_23", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "negation of p 1 2 or p 2 1 and this was my conclusion and 1 and this was my conclusion and therefore this by again using the de therefore this by again using the de morgan rule we can get the desired morgan rule we can get the desired result which is this was basically result which is this was basically due to modus ponents and the finally the conclusion let's let me call the", "image_path": "img_data/video_23_chunk_21.jpg"}
{"video": "video_23", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "ponents and the finally the conclusion let's let me call the conclusion let's let me call the conclusion r12 the negation of p12 or r12 the negation of p12 or p21 is of course replaced with p21 is of course replaced with negation of p12 or negation of [music] [music] p21 and which is basically the desired p21 and which is basically the desired result using the dorgan logical equivalence and that's basically kind of reach this", "image_path": "img_data/video_23_chunk_22.jpg"}
{"video": "video_23", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "dorgan logical equivalence and that's basically kind of reach this that's basically kind of reach this kind of point where we have proven kind of point where we have proven that the query is true using a number of that the query is true using a number of steps and as i said practice is steps and as i said practice is everything in that how to use this everything in that how to use this logical equivalence and the -cal logical equivalence and the -cal inference rules in order to reach that inference rules in order to reach that same conclusion evidently this is the same conclusion evidently this is the by hand sort of demonstration of the by hand sort of demonstration of the topic of theorem proving and in topic of theorem proving and in propositional logic we have propositional logic we have automatic theorem proving kind of", "image_path": "img_data/video_23_chunk_23.jpg"}
{"video": "video_23", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "propositional logic we have automatic theorem proving kind of automatic theorem proving kind of algorithms that are slightly outside of algorithms that are slightly outside of the scope of this course as we're trying the scope of this course as we're trying to get into just the basics here and to get into just the basics here and there are certain tools that will there are certain tools that will actually do this for you and the actually do this for you and the construction of this tool took a lot of construction of this tool took a lot of time and effort to make them kind of time and effort to make them kind of efficient for all kind of practical efficient for all kind of practical problems provid that we understand if problems provid that we understand if you the basics hopefully you'll be you the basics hopefully you'll be able to apply those tools in any able to apply those tools in any time you have logical inference", "image_path": "img_data/video_23_chunk_24.jpg"}
{"video": "video_23", "start": "0:12:30", "end": "0:12:42.433333", "timestamp": "0:12:30 - 0:12:42.433333", "text": "able to apply those tools in any time you have logical inference any time you have logical inference just doing the syntax alone out of just doing the syntax alone out of the two methods we have looked at the two methods we have looked at the second method the theorem proving is second method the theorem proving is also the most practical", "image_path": "img_data/video_23_chunk_25.jpg"}
{"video": "video_24", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in an earlier video we have seen the definition of responses that we can definition of responses that we can actually get from issuing queries in actually get from issuing queries in the knowledge base in the -cal ask in the knowledge base in the -cal ask operations we also saw the definition of operations we also saw the definition of those responses when we tell something those responses when we tell something about the knowledge base we try to add about the knowledge base we try to add into the knowledge base about the into the knowledge base about the sentence in this video what we will sentence in this video what we will do is we'll actually now start looking do is we'll actually now start looking at the algorithms that allow if you at the algorithms that allow if you this a machine to be able to efficiently this a machine to be able to efficiently do these kind of logical conclusions", "image_path": "img_data/video_24_chunk_0.jpg"}
{"video": "video_24", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "this a machine to be able to efficiently do these kind of logical conclusions do these kind of logical conclusions and provide if you those kind of and provide if you those kind of responses and kind of an algorithmic responses and kind of an algorithmic kind of setting and we'll see two kind of setting and we'll see two algorithms out of the many possible algorithms out of the many possible one is called model checking we'll one is called model checking we'll see that mod checking now and also we see that mod checking now and also we will also see theorem proving as another will also see theorem proving as another category of algorithms that we can category of algorithms that we can actually engage if we move now to this engage if we move now to this first one which is logical reason", "image_path": "img_data/video_24_chunk_1.jpg"}
{"video": "video_24", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "engage if we move now to this first one which is logical reason first one which is logical reason through model checking we have a through model checking we have a situation where we have the knowledge base and we tell sorry we ask something about let's say the p12 we are actually go back to the p12 we are actually go back to the this sentence p12 refers to the this sentence p12 refers to the earlier video we've seen the hubus world", "image_path": "img_data/video_24_chunk_2.jpg"}
{"video": "video_24", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "this sentence p12 refers to the earlier video we've seen the hubus world and we are need to understand how the machine will be able to respond how the machine will be able to respond back to us with whatever u it back to us with whatever u it knows from the knowledge base it's knows from the knowledge base it's either going to be true or false or i do either going to be true or false or i do not know these are the three responses not know these are the three responses we can actually have and we actually we can actually have and we actually going to be issuing this query when the going to be issuing this query when the state of the world is at t is equal to 4 state of the world is at t is equal to 4 and if you go back to this kind of", "image_path": "img_data/video_24_chunk_3.jpg"}
{"video": "video_24", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "state of the world is at t is equal to 4 and if you go back to this kind of and if you go back to this kind of video then we actually had over video then we actually had over here the sequence of actions we here the sequence of actions we actually in this at this moment in time actually in this at this moment in time and we have we are trying and we have we are trying to understand at this moment in time how to understand at this moment in time how we are making this sentence we are making this sentence this inference in this case this inference in this case this inference will actually be true inference will actually be true trying to see how we actually get this", "image_path": "img_data/video_24_chunk_4.jpg"}
{"video": "video_24", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "inference will actually be true trying to see how we actually get this trying to see how we actually get this conclusion at this moment in conclusion at this moment in time we have we will be time we have we will be assuming that we have four rule assuming that we have four rule five rules we have rules one 2 five rules we have rules one 2 three four and five we will have this three four and five we will have this information in the knowledge base information in the knowledge base it's r1 to r5 only and let's let me write", "image_path": "img_data/video_24_chunk_5.jpg"}
{"video": "video_24", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "it's r1 to r5 only and let's let me write only and let's let me write all this kind of information down we all this kind of information down we have the bmus world with the have the bmus world with the following rules the first point is that the ask operation about then neg negative about then neg negative p12 is", "image_path": "img_data/video_24_chunk_6.jpg"}
{"video": "video_24", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "about then neg negative p12 is issued when the knowledge base is at t is equal to 4 and this base is at t is equal to 4 and this moment in time at t is equal to 4 we assume we have five rules let say five sentences in", "image_path": "img_data/video_24_chunk_7.jpg"}
{"video": "video_24", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "rules let say five sentences in the let's say r1 to r5 in the knowledge r1 to r5 in the knowledge base if you go back and base if you go back and actually see what will actually be actually see what will actually be those symbols that are involved in those symbols that are involved in these rules i have p11 these rules i have p11 b11 b 21 p12 p21 p22 and p 31 and the same yeah", "image_path": "img_data/video_24_chunk_8.jpg"}
{"video": "video_24", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "b11 b 21 p12 p21 p22 and p 31 and the same yeah p21 p22 and p 31 and the same yeah i can actually confirm but these are i can actually confirm but these are basically the symbols we're going to basically the symbols we're going to have let's write them down these are have let's write them down these are all the symbols that are involved in all the symbols that are involved in my rules at this moment in my rules at this moment in time i we have five rules in the time i we have five rules in the knowledge base and the following symbols are involved", "image_path": "img_data/video_24_chunk_9.jpg"}
{"video": "video_24", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "symbols are involved this is b11 b21 p12 p21 p22 and p31 we have 1 2 3 4 5 six seven p31 we have 1 2 3 4 5 six seven symbols we have seven symbols in symbols we have seven symbols in total", "image_path": "img_data/video_24_chunk_10.jpg"}
{"video": "video_24", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "symbols we have seven symbols in total and according to this kind of model checking u approach we will do the model checking u approach we will do the following we will enumerate all models and this is", "image_path": "img_data/video_24_chunk_11.jpg"}
{"video": "video_24", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "models and this is i would call these models with the index i would call these models with the index mi and i is since all the symbols are i is since all the symbols are propositional then the set of models propositional then the set of models that we actually have here the number of that we actually have here the number of models is 128 in total from one to 2 models is 128 in total from one to 2 the^ of 7 which is 128 evidently we will go from model 128 evidently we will go from model where the state of the world is u where the state of the world is u the world model is going to be the world model is going to be 00000000 0 to all ones we have", "image_path": "img_data/video_24_chunk_12.jpg"}
{"video": "video_24", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "the world model is going to be 00000000 0 to all ones we have 00000000 0 to all ones we have here this kind of table i'm here this kind of table i'm going to pause the video and come back going to pause the video and come back to it in this table which is also in to it in this table which is also in your textbook you can actually see here your textbook you can actually see here the seven propositional symbols the seven propositional symbols involved and in addition to that what we involved and in addition to that what we will have is for each of the five rules will have is for each of the five rules that we have stored in the knowledge that we have stored in the knowledge base we go ahead and using our truth", "image_path": "img_data/video_24_chunk_13.jpg"}
{"video": "video_24", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "that we have stored in the knowledge base we go ahead and using our truth base we go ahead and using our truth tables that we have seen earlier we are tables that we have seen earlier we are able to for each of the rules that we able to for each of the rules that we have quoted in our wmus world example have quoted in our wmus world example we can actually go ahead and determine we can actually go ahead and determine the true or false value of each of the true or false value of each of the rules we actually write them rules we actually write them here and as we have seen the here and as we have seen the knowledge base is going to be satisfied if base is going to be satisfied if all the rules that we have inside it are all the rules that we have inside it are satisfied and therefore we can actually", "image_path": "img_data/video_24_chunk_14.jpg"}
{"video": "video_24", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "all the rules that we have inside it are satisfied and therefore we can actually satisfied and therefore we can actually determine the very last determine the very last column and in this case it is false column and in this case it is false because r5 is not satisfied u the because r5 is not satisfied u the knowledge base are satisfied if all of knowledge base are satisfied if all of the rules there is an end the rules there is an end relationship between those truth relationship between those truth values over here that defines whether or values over here that defines whether or not the knowledge base is satisfied not the knowledge base is satisfied we actually go ahead and for each of the we actually go ahead and for each of the rows of the of our propositional kind rows of the of our propositional kind of symbols we determine of symbols we determine the corresponding truth values for our", "image_path": "img_data/video_24_chunk_15.jpg"}
{"video": "video_24", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "of symbols we determine the corresponding truth values for our the corresponding truth values for our rules and therefore the truth value of rules and therefore the truth value of the knowledge base and we proceed all the knowledge base and we proceed all the way until we reach the 128th row the way until we reach the 128th row and actually what we can actually see and actually what we can actually see here is the following and here is the following and actually let me write this actually let me write this some of this down at least you have some of this down at least you have it we have let's say r2 was as an it we have let's say r2 was as an example we have the r2 the rule r2 example we have the r2 the rule r2 being", "image_path": "img_data/video_24_chunk_16.jpg"}
{"video": "video_24", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "example we have the r2 the rule r2 being b11 if an only if p12 or p21 this is this was the rule p21 this is this was the rule and we have here let's say let's take and we have here let's say let's take the very first rle we have the very first rle we have a p12 is false p12 is", "image_path": "img_data/video_24_chunk_17.jpg"}
{"video": "video_24", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "p12 is false and p21 is also false and p21 is also false definitely we have from false definitely we have from that p12 or p 21 is false and therefore we have now false and therefore we have now b11 is", "image_path": "img_data/video_24_chunk_18.jpg"}
{"video": "video_24", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "is false there is no we also have b11 fals from another rule of the fals from another rule of the knowledge base the b11 fals of the knowledge base the b11 fals i believe it was the rule one if i'm i believe it was the rule one if i'm not mistaken actually was the rule for", "image_path": "img_data/video_24_chunk_19.jpg"}
{"video": "video_24", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "mistaken actually was the rule for we have there's no bree in one we have there's no bree in one this was the you the this was the you the percept received at that moment percept received at that moment one and four we have moment one and four we have this is of course false we this is of course false we have the following sentence have the following sentence false if and only if false if and only if false of course this is false of course this is equivalent to", "image_path": "img_data/video_24_chunk_20.jpg"}
{"video": "video_24", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "false of course this is equivalent to false implies false and false implies false and false implies false the false implies false is true we the false implies false is true we have true and therefore the sentence this rule r2 is true we actually wrote it over here and this", "image_path": "img_data/video_24_chunk_21.jpg"}
{"video": "video_24", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "r2 is true we actually wrote it over here and this actually wrote it over here and this was how we determine was how we determine the true value of all these kind of the true value of all these kind of rules according to u to this example rules according to u to this example at least the another at least the another thing we actually said is that thing we actually said is that the set of models the knowledge models the knowledge base is satisfied and i'm actually referring", "image_path": "img_data/video_24_chunk_22.jpg"}
{"video": "video_24", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "satisfied and i'm actually referring now to the population of this column now to the population of this column over here when let's say r1 and r2 and r3 and r4 and r5 is true and that's how we get to true and that's how we get to determine if you this column determine if you this column all right out of the", "image_path": "img_data/video_24_chunk_23.jpg"}
{"video": "video_24", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "determine if you this column all right out of the all right out of the 128 models we only have three models that models we only have three models that satisfy the knowledge base now that satisfy the knowledge base now that we have completed everything we have completed everything we have three models that safy the knowledge three models that safy the knowledge base and in all of these models that base and in all of these models that satisfy the knowledge base the satisfy the knowledge base the p one two the p12 is false the negative p12 will be true", "image_path": "img_data/video_24_chunk_24.jpg"}
{"video": "video_24", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "p12 is false the negative p12 will be true false the negative p12 will be true this is the final conclusion out of 128 models only three satisfy the knowledge base and in those three models three in those three models p12 is false", "image_path": "img_data/video_24_chunk_25.jpg"}
{"video": "video_24", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "three in those three models p12 is false therefore what the knowledge base will therefore what the knowledge base will respond is with a true that there is no respond is with a true that there is no pit in other words in b12 and this is how we came to define that value that's how the define that value that's how the machine will actually do it initially in machine will actually do it initially in the hubus video we used our own kind of the hubus video we used our own kind of brain to do that kind of brain to do that kind of determination but now at least we have determination but now at least we have one method which is exhaustive model", "image_path": "img_data/video_24_chunk_26.jpg"}
{"video": "video_24", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "determination but now at least we have one method which is exhaustive model one method which is exhaustive model checking over here that will allow us to checking over here that will allow us to do and sometimes obviously this to do and sometimes obviously this model checking for more complicated model checking for more complicated words is going to take too long and words is going to take too long and it's going to be very fairly involved it's going to be very fairly involved we will see an kind of an accelerated we will see an kind of an accelerated version of this algorith that avoids us version of this algorith that avoids us to evaluate all possible models which is to evaluate all possible models which is evidently exponential we'll start a discussion exponential we'll start a discussion about the backtracking search an about the backtracking search an algorith that will actually help us", "image_path": "img_data/video_24_chunk_27.jpg"}
{"video": "video_24", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "about the backtracking search an algorith that will actually help us algorith that will actually help us elevate the problem we've seen with elevate the problem we've seen with exhaustive model checking in the first exhaustive model checking in the first algorithm we have looked in an earlier algorithm we have looked in an earlier video and in this problem i'll video and in this problem i'll just present the problem on a kind of just present the problem on a kind of a real life problem that it's going to a real life problem that it's going to be drawn over here i have a printer be drawn over here i have a printer let's say that has three positions let's say that has three positions one two three that's a color printer and", "image_path": "img_data/video_24_chunk_28.jpg"}
{"video": "video_24", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "let's say that has three positions one two three that's a color printer and one two three that's a color printer and receives red green and blue receives red green and blue individual kind of cages and the u sort individual kind of cages and the u sort of a green cartrid as a constraint it of a green cartrid as a constraint it has to be located in position two for has to be located in position two for the color printer to work let me the color printer to work let me write this down and what we actually we going to do here is we are going to look at all", "image_path": "img_data/video_24_chunk_29.jpg"}
{"video": "video_24", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "down and what we actually we going to do here is we are going to look at all here is we are going to look at all possibilities that starting from u possibilities that starting from u and create if you a tree a graph and create if you a tree a graph that looks a tree and we will that looks a tree and we will start with the nodes of this tree start with the nodes of this tree will be the locations it's going be the locations it's going to be either one or two or three and the to be either one or two or three and the edges will represent assignments to edges will represent assignments to those locations by constructing a those locations by constructing a tree this we will see now how tree this we will see now how backtracking algorithm is", "image_path": "img_data/video_24_chunk_30.jpg"}
{"video": "video_24", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "tree this we will see now how backtracking algorithm is backtracking algorithm is working and how we can eliminate all working and how we can eliminate all the possible possibilities of the possible possibilities of assignment and therefore reduce the assignment and therefore reduce the assignment computational the assignment computational the computational kind of load associated computational kind of load associated with this type of assignments all right with this type of assignments all right we have we start with the node we have we start with the node one which is the location one and we one which is the location one and we definitely can go u and let's say definitely can go u and let's say assign red for that location we", "image_path": "img_data/video_24_chunk_31.jpg"}
{"video": "video_24", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "definitely can go u and let's say assign red for that location we assign red for that location we arrive at location two and we can arrive at location two and we can assign green and we can sort of go to green and we can sort of go to location three and we can assign location three and we can assign let's say blue and then this is let's say blue and then this is arrive if you at a termination arrive if you at a termination point where we cannot really expand the point where we cannot really expand the tree anymore the second one is the tree anymore the second one is the second option that we have here and we second option that we have here and we in temporarily we will ignore this kind", "image_path": "img_data/video_24_chunk_32.jpg"}
{"video": "video_24", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "second option that we have here and we in temporarily we will ignore this kind in temporarily we will ignore this kind of constraint and then we'll revisit of constraint and then we'll revisit this constraint to see exactly what kind this constraint to see exactly what kind of branches of these trees can be of branches of these trees can be eliminated this is green we arrive eliminated this is green we arrive at the node two and on we at the node two and on we expand but i actually forgot here to expand but i actually forgot here to expand further here with red green expand further here with red green this could actually be either green or this could actually be either green or blue at location two to arrive at three blue at location two to arrive at three and then evidently this is green to and then evidently this is green to arrive at a kind of termination node", "image_path": "img_data/video_24_chunk_33.jpg"}
{"video": "video_24", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "and then evidently this is green to arrive at a kind of termination node arrive at a kind of termination node similarly over here we have a red and similarly over here we have a red and blue to arrive at three and over here blue to arrive at three and over here we have blue to arrive to the we have blue to arrive to the termination node and over here we have termination node and over here we have red anyway it's a kind of a very red anyway it's a kind of a very simple graph where now we simple graph where now we have here red and green actually green", "image_path": "img_data/video_24_chunk_34.jpg"}
{"video": "video_24", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "have here red and green actually green here red and green actually green over here and red over there to arrive on this third node and then we have a red and green and as it is evident now that we green and as it is evident now that we have that kind of constraint we look at have that kind of constraint we look at this kind of a graph and we can this kind of a graph and we can actually detect here all the possib actually detect here all the possib possibilities that are compatible with", "image_path": "img_data/video_24_chunk_35.jpg"}
{"video": "video_24", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "actually detect here all the possib possibilities that are compatible with possibilities that are compatible with this graph in terms of terminate all the this graph in terms of terminate all the possible branches here you can possible branches here you can actually see that this branch is actually see that this branch is definitely compatible to our constraint definitely compatible to our constraint this branch and these this branch and these branches are not this assignments are branches are not this assignments are not and this is also compatible with our not and this is also compatible with our constraint and of course this is constraint and of course this is not this is green over here this is not this is green over here this is not all right as you can see the u the all right as you can see the u the assignment the this evaluation of the", "image_path": "img_data/video_24_chunk_36.jpg"}
{"video": "video_24", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "all right as you can see the u the assignment the this evaluation of the assignment the this evaluation of the complete graph could actually happen complete graph could actually happen first and then we kind of identified first and then we kind of identified what is visible or not however we what is visible or not however we could have eliminated this expansion already because expansion already because definitely we have at location two we definitely we have at location two we cannot really have blue we could cannot really have blue we could have eliminated the whole", "image_path": "img_data/video_24_chunk_37.jpg"}
{"video": "video_24", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "cannot really have blue we could have eliminated the whole subgraph if you if i may call it this over here and we could this over here and we could definitely have eliminated this but definitely have eliminated this but this is of course a very simple example this is of course a very simple example and the number of nodes and possible and the number of nodes and possible assignments are sort of not necessarily are sort of not necessarily that large evidently given that the that large evidently given that the assignments i mean the color here we assignments i mean the color here we will now create a and the nodes will now create a and the nodes will correspond to locations what we", "image_path": "img_data/video_24_chunk_38.jpg"}
{"video": "video_24", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "will now create a and the nodes will correspond to locations what we will correspond to locations what we will do now next is to see how this will do now next is to see how this approach is applied in this logical approach is applied in this logical reasoning domain where similarly we reasoning domain where similarly we will be are able to eliminate u a lot will be are able to eliminate u a lot of the already existing from earlier of the already existing from earlier inf feasible solutions and expand only inf feasible solutions and expand only those who are going to be feasible those who are going to be feasible and that's really the gist of and that's really the gist of backtracking to enable this kind", "image_path": "img_data/video_24_chunk_39.jpg"}
{"video": "video_24", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "and that's really the gist of backtracking to enable this kind backtracking to enable this kind of sort of computational of sort of computational advantage i will to make this kind advantage i will to make this kind of analogy a bit more evident i have of analogy a bit more evident i have this kind of knowledge this kind of knowledge base and in my knowledge base i have the base and in my knowledge base i have the following sentences i have more following sentences i have more specifically five sentences i have specifically five sentences i have the symbols i use the symbols q p and r the symbols i use the symbols q p and r i have this p or q that's the first i have this p or q that's the first sentence the second is p or negative", "image_path": "img_data/video_24_chunk_40.jpg"}
{"video": "video_24", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "i have this p or q that's the first sentence the second is p or negative sentence the second is p or negative q then we have the netive p or q then we have the netive p or q and then we have p or q and then we have p or q or r and the fifth one is going to be r and the fifth one is going to be netive p or r that's this is the netive p or r that's this is the equivalent", "image_path": "img_data/video_24_chunk_41.jpg"}
{"video": "video_24", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "netive p or r that's this is the equivalent situation we have here in this kind situation we have here in this kind of knowledge base and now instead of knowledge base and now instead of nodes that we have here representing nodes that we have here representing locations of the cages the nodes here locations of the cages the nodes here will correspond to the symbols and will correspond to the symbols and the u in setad of edges that the u in setad of edges that corresponds to assignments to a color corresponds to assignments to a color then the assignments will correspond to then the assignments will correspond to a true or false value for the symbols a true or false value for the symbols that are on the corresponding", "image_path": "img_data/video_24_chunk_42.jpg"}
{"video": "video_24", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "a true or false value for the symbols that are on the corresponding that are on the corresponding symbol which is at the node symbol which is at the node i'm starting the evaluation what is really the evaluation what is really the question here the question is to come question here the question is to come up with the set of models that satisfy up with the set of models that satisfy the knowledge base that's the knowledge base that's the question the set of models that's a", "image_path": "img_data/video_24_chunk_43.jpg"}
{"video": "video_24", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "the set of models that's a f the knowledge base and this using our earlier kind of notation is capital our earlier kind of notation is capital m of the knowledge base this we are trying to understand base this we are trying to understand what is this set from the get- what is this set from the get- go i'm telling you that there going to go i'm telling you that there going to be a negative result there will be no be a negative result there will be no sets the set will be null but you sets the set will be null but the this is just a specific know the this is just a specific property of the specific example of here property of the specific example of here but nevertheless we will see", "image_path": "img_data/video_24_chunk_44.jpg"}
{"video": "video_24", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "property of the specific example of here but nevertheless we will see but nevertheless we will see exactly how similar eliminations of exactly how similar eliminations of expanses could actually be applied by expanses could actually be applied by back tring search we started with the back tring search we started with the symbol let's say p and evidently the assignments for the p and evidently the assignments for the peak could be either false or peak could be either false or true then of course we can have true then of course we can have the sy q here and then", "image_path": "img_data/video_24_chunk_45.jpg"}
{"video": "video_24", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "true then of course we can have the sy q here and then the sy q here and then the and then of course we have also the and then of course we have also expansions in terms of r now that we expansions in terms of r now that we have drawn this tree then we can have drawn this tree then we can actually see here this for q is equal actually see here this for q is equal to false and this is q is equal to false and this is q is equal to true and similarly over here we have q true and similarly over here we have q is equal to false and q is equal to true is equal to false and q is equal to true and immediately actually we can see that", "image_path": "img_data/video_24_chunk_46.jpg"}
{"video": "video_24", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "is equal to false and q is equal to true and immediately actually we can see that and immediately actually we can see that one of the five sentences is not one of the five sentences is not satisfied in this branch over here the satisfied in this branch over here the moment we have false and false for p and moment we have false and false for p and q then we can actually look at sentence q then we can actually look at sentence one and we can see that this whole one and we can see that this whole expansion over here is not expansion over here is not necessary and in fact we are going to be necessary and in fact we are going to be terminating right here and this will be an invisible here and this will be an invisible i mean the knowledge base is not", "image_path": "img_data/video_24_chunk_47.jpg"}
{"video": "video_24", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "here and this will be an invisible i mean the knowledge base is not i mean the knowledge base is not satisfied because of con of equation one satisfied because of con of equation one of the sentence one all right we have now what one all right we have now what about the true over there also this about the true over there also this whole thing is not satisfied this whole expansion is not going to be needed because the moment going to be needed because the moment we reach this point false and true take we reach this point false and true take a look at the sentence two in sentence", "image_path": "img_data/video_24_chunk_48.jpg"}
{"video": "video_24", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "we reach this point false and true take a look at the sentence two in sentence a look at the sentence two in sentence two we also have false because we have a false or false because we have a false or false therefore this is also false and false therefore this is also false and therefore if you recall the knowledge therefore if you recall the knowledge base is not satisfied base is not satisfied in order for the knowledge base to be in order for the knowledge base to be satisfied then all of the sentences need satisfied then all of the sentences need to be true and therefore we to be true and therefore we are going to not expand that one because it's false and expand that one because it's false and then look at over here then this", "image_path": "img_data/video_24_chunk_49.jpg"}
{"video": "video_24", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "expand that one because it's false and then look at over here then this then look at over here then this whole branch also this whole branch over there is also this whole branch over there is also not going to be expanded and this is not going to be expanded and this is because of equation three this branch which involves p of three this branch which involves p of equal to true the p to be true and equal to true the p to be true and the r actually i forgot to write now the r actually i forgot to write now this is becoming important r is equal to this is becoming important r is equal to false here and r is equal to true on the false here and r is equal to true on the other branch for this one we will the", "image_path": "img_data/video_24_chunk_50.jpg"}
{"video": "video_24", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "false here and r is equal to true on the other branch for this one we will the other branch for this one we will the assignment true and r for p and false assignment true and r for p and false for r this is because of five is not for r this is because of five is not going to be satisfied definitely we see satisfied definitely we see not a negative result here as well not a negative result here as well and the assignment true and t and the assignment true and t makes the equation four false and therefore we can conclude", "image_path": "img_data/video_24_chunk_51.jpg"}
{"video": "video_24", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "false and therefore we can conclude that the set of models for the knowledge base is models for the knowledge base is the null set that satisfy the knowledge base is set that satisfy the knowledge base is the null set yes it was a negative is the null set yes it was a negative result but i hope you appreciated the result but i hope you appreciated the function of the backtracking function of the backtracking search and your textbook has if you search and your textbook has if you a pseudo code associated i prefer to do a pseudo code associated i prefer to do here the kind of a visual", "image_path": "img_data/video_24_chunk_52.jpg"}
{"video": "video_24", "start": "0:26:30", "end": "0:26:46.566667", "timestamp": "0:26:30 - 0:26:46.566667", "text": "a pseudo code associated i prefer to do here the kind of a visual here the kind of a visual presentation of the of this kind of presentation of the of this kind of algorith that effectively avoids the algorith that effectively avoids the expansion and the exhaustive kind of expansion and the exhaustive kind of model sort of evaluation in the sort model sort of evaluation in the sort of model checking approach we have seen", "image_path": "img_data/video_24_chunk_53.jpg"}
{"video": "video_25", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in order for us to start discussing about how we can actually have the about how we can actually have the machine do the logical reasoning that we machine do the logical reasoning that we just did using our own brain just did using our own brain in the humus world we need to start in the humus world we need to start discussing the interaction discussing the interaction with the knowledge base the interaction of the knowledge base is kind of important as we have base is kind of important as we have coupled our ability to reason with", "image_path": "img_data/video_25_chunk_0.jpg"}
{"video": "video_25", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "base is kind of important as we have coupled our ability to reason with coupled our ability to reason with what we store in that kind of knowledge what we store in that kind of knowledge base let me just write down again base let me just write down again what is the knowledge base a bit more what is the knowledge base a bit more concretely for us it's going to be a concretely for us it's going to be a set of sentences that represent rules of the world", "image_path": "img_data/video_25_chunk_1.jpg"}
{"video": "video_25", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "represent rules of the world model of the word model percepts and inferences and we will be dealing with this we will be treating those as this we will be treating those as kind of constraints and this will actually become a bit more apparent the moment we become a bit more apparent the moment we kind of deal with go back to the wbus kind of deal with go back to the wbus world in a moment and actually look how", "image_path": "img_data/video_25_chunk_2.jpg"}
{"video": "video_25", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "kind of deal with go back to the wbus world in a moment and actually look how world in a moment and actually look how as we are going as the time as we are going as the time progresses we start from some initial progresses we start from some initial kind of set of rules we kind of set of rules we are sort of adding more into this kind are sort of adding more into this kind of knowledge base and the more we add of knowledge base and the more we add the more constrained the world the more constrained the world the number of world models that we can number of world models that we can satisfy the which we satisfy the which we are going to just basically see that are going to just basically see that in a moment and we will also", "image_path": "img_data/video_25_chunk_3.jpg"}
{"video": "video_25", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "are going to just basically see that in a moment and we will also in a moment and we will also the second thing we need to sort of the second thing we need to sort of introduce here is this concept of introduce here is this concept of this vent diagram which i'm this vent diagram which i'm sure you must have heard before sure you must have heard before these are diagrams that will help us to these are diagrams that will help us to visualize and help understand the knowledge base operator", "image_path": "img_data/video_25_chunk_4.jpg"}
{"video": "video_25", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "understand the knowledge base operator operations and we will introduce and what we can sort of get out of and what we can sort of get out of those operations these are two those operations these are two operations we'll be discussing the -called ask discussing the -called ask operation and tell operation we ask operation and tell operation we ask something what the knowledge base about something what the knowledge base about and we tell the knowledge based and we tell the knowledge based something that we know let something that we know let me just introduce use now the operate", "image_path": "img_data/video_25_chunk_5.jpg"}
{"video": "video_25", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "something that we know let me just introduce use now the operate me just introduce use now the operate operations we have now the operations we have now the tell we have a sentence that we want to tell the a sentence that we want to tell the knowledge base about this is the knowledge base over about this is the knowledge base over here and the knowledge base is can here and the knowledge base is can return three things three return three things three possibilities i knew that this is called", "image_path": "img_data/video_25_chunk_6.jpg"}
{"video": "video_25", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "that this is called entailment a bit more formally and this actually a very important concept actually a very important concept i knew that i don't believe i knew that i don't believe that's a second response we can second response we can get that is also called formally contradiction and the third is yeah i", "image_path": "img_data/video_25_chunk_7.jpg"}
{"video": "video_25", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "contradiction and the third is yeah i can update the knowledge base with this update the knowledge base with this information and this is called formally contingency that's is this is the first operation the second is the first operation the second is the soal ask operation we want to ask the soal ask operation we want to ask about something about this kind of about something about this kind of sentence is this sentence satisfied or sentence is this sentence satisfied or not and the knowledge base will not and the knowledge base will definitely respond to us", "image_path": "img_data/video_25_chunk_8.jpg"}
{"video": "video_25", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "not and the knowledge base will definitely respond to us in three ways this is true the sentence is true the sentence is false and the third one is i do not know these are the this is the summary of this operations we will start summary of this operations we will start introducing now to understand exactly introducing now to understand exactly all these possibilities of responses all these possibilities of responses we get out of the knowledge space let's", "image_path": "img_data/video_25_chunk_9.jpg"}
{"video": "video_25", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "all these possibilities of responses we get out of the knowledge space let's we get out of the knowledge space let's start with the t operation and responses the first one i want to cover is i don't know i do not know that", "image_path": "img_data/video_25_chunk_10.jpg"}
{"video": "video_25", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "know i do not know that and this is what we call formally the entailment and it will use the symbol this symbol is used for entailment and it's looks entailment and it's looks a kind of hammer all right and the hammer all right and the corresponding kind of v diagram which corresponding kind of v diagram which i will explain now", "image_path": "img_data/video_25_chunk_11.jpg"}
{"video": "video_25", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "corresponding kind of v diagram which i will explain now is looks this as we said is looks this as we said earlier in another video we had this earlier in another video we had this models of that satisfy the this models of that satisfy the sentence a1 let's say and we have sentence a1 let's say and we have another set of models that satisfy the another set of models that satisfy the sentence a2 and when we have already in a2 and when we have already in the knowledge base let's say the cent", "image_path": "img_data/video_25_chunk_12.jpg"}
{"video": "video_25", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "base let's say the cent a1 and we are the knowledge base contains a1 and the two sentences a2 and the this intersection over here is what we let's call it this here is what we let's call it this intersection of the models of the intersection of the models of the sentences generally", "image_path": "img_data/video_25_chunk_13.jpg"}
{"video": "video_25", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "intersection of the models of the sentences generally i sentences that we have in the i sentences that we have in the knowledge base is the set of models that satisfy base is the set of models that satisfy the knowledge base the intersection comes from the base the intersection comes from the fact that the set of models satisfy the fact that the set of models satisfy the knowledge base is the set of knowledge base is the set of model satisfy a1 and the set of m model satisfy a1 and the set of m satisfy a2 at the same time therefore satisfy a2 at the same time therefore that's where the intersection can comes that's where the intersection can comes in and this goes back what i said in and this goes back what i said earlier about the more sentences we have", "image_path": "img_data/video_25_chunk_14.jpg"}
{"video": "video_25", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "in and this goes back what i said earlier about the more sentences we have earlier about the more sentences we have the potentially for more constraints the potentially for more constraints that we will face this area the more that we will face this area the more sentences we had in the nler base was sentences we had in the nler base was actually start shrinking when a1 was alone shrinking when a1 was alone then in the knowledge base then the then in the knowledge base then the set of models of the for the sentence a1 set of models of the for the sentence a1 was this but when another sentence came was this but when another sentence came in then the set of models that satisfy in then the set of models that satisfy the knowledge base are shrunk and", "image_path": "img_data/video_25_chunk_15.jpg"}
{"video": "video_25", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "in then the set of models that satisfy the knowledge base are shrunk and the knowledge base are shrunk and this is it this area over here is the this is it this area over here is the set of models for that knowledge base set of models for that knowledge base for knowledge base let's assume now for knowledge base let's assume now that this is the knowledge base and that this is the knowledge base and another sentence a3 is coming in another sentence a3 is coming in and we will tell in other words and we will tell in other words the knowledge base about a3 we are have this situation right now and", "image_path": "img_data/video_25_chunk_16.jpg"}
{"video": "video_25", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "a3 we are have this situation right now and have this situation right now and if when we have entailment then the if when we have entailment then the set of models for a3 will actually look set of models for a3 will actually look this will be a superet of the set of models already superet of the set of models already that satisfy the knowledge base that satisfy the knowledge base we are writing that if and i'll we are writing that if and i'll explain that why if then set of models explain that why if then set of models of the knowledge space and the of the knowledge space and the intersection of the set of models of a intersection of the set of models of a a3 does not change the set of models", "image_path": "img_data/video_25_chunk_17.jpg"}
{"video": "video_25", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "intersection of the set of models of a a3 does not change the set of models a3 does not change the set of models that are satisfied by the knowledge base that are satisfied by the knowledge base then the response will be i know that and therefore we will write the knowledge base entails a3 can be logically concluded from", "image_path": "img_data/video_25_chunk_18.jpg"}
{"video": "video_25", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "entails a3 can be logically concluded from a3 can be logically concluded from what we already know in the knowledge what we already know in the knowledge base a couple of examples over here base a couple of examples over here let's assume simplistically let's assume simplistically that the knowledge base contains the sentence a that let's base contains the sentence a that let's say x is equal to zero and the another sentence zero and the another sentence comes in that x y is equal to z comes in that x y is equal to z then we can", "image_path": "img_data/video_25_chunk_19.jpg"}
{"video": "video_25", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "comes in that x y is equal to z then we can entail beta from a obviously no matter what the value of a obviously no matter what the value of y is then we will actually have y is then we will actually have the response will be this the response will be this sentence is actually coming in already i sentence is actually coming in already i know that this sentence is know that this sentence is actually true and from just knowing a actually true and from just knowing a all right that's basically one", "image_path": "img_data/video_25_chunk_20.jpg"}
{"video": "video_25", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "actually true and from just knowing a all right that's basically one all right that's basically one example the other example we actually example the other example we actually have here is that let's say that we have here is that let's say that we have a sentence a one which is rain r a sentence a one which is rain r for rain let's say and we have another sentence in the say and we have another sentence in the knowledge base let me write it knowledge base let me write it down here as this is the knowledge base at as this is the knowledge base at this moment and this is the snow", "image_path": "img_data/video_25_chunk_21.jpg"}
{"video": "video_25", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "as this is the knowledge base at this moment and this is the snow this moment and this is the snow that is the knowledge that is the knowledge base and what we actually have here base and what we actually have here we have a sentence let me write it down also sentence let me write it down also over here that the tell operation is over here that the tell operation is coming all right this is the sentence coming all right this is the sentence a3 and what we can actually conclude a3 and what we can actually conclude here and this a3 sentence the a3 here and this a3 sentence the a3 sentence is rain or snow definitely we can write that the", "image_path": "img_data/video_25_chunk_22.jpg"}
{"video": "video_25", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "definitely we can write that the knowledge base implies a3 our logical the a3 is logically concluded from that knowledge base concluded from that knowledge base that we have here because evidently that we have here because evidently if we already know that rain is true if we already know that rain is true and snow is true then evidently the or and snow is true then evidently the or relation ship over here will also be", "image_path": "img_data/video_25_chunk_23.jpg"}
{"video": "video_25", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "and snow is true then evidently the or relation ship over here will also be relation ship over here will also be true and that's an explanation of true and that's an explanation of entailment whenever we have entailment whenever we have entailment let's look at i don't entailment let's look at i don't agree with that the -call contradiction in the contradiction the event diagrams are actually quite the event diagrams are actually quite simple we have a set of models let's simple we have a set of models let's say for the sentence", "image_path": "img_data/video_25_chunk_24.jpg"}
{"video": "video_25", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "simple we have a set of models let's say for the sentence a1 and there is a in fact the set of models for the a in fact the set of models for the knowledge base let sayy the knowledge base let sayy the knowledge base we have a knowledge base with some base we have a knowledge base with some sentences inside and we have a sentences inside and we have a sentence coming in let's say sentence coming in let's say a3 and as you can actually see here we a3 and as you can actually see here we have no common", "image_path": "img_data/video_25_chunk_25.jpg"}
{"video": "video_25", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "no common ground on how the world works what i have sensed or inferred and this is basically", "image_path": "img_data/video_25_chunk_26.jpg"}
{"video": "video_25", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "sensed or inferred and this is basically inferred and this is basically when we have a contradiction absolutely when we have a contradiction absolutely no intersection between what we no intersection between what we already know as true and perceived u other true and perceived u other either perception information or axioms either perception information or axioms that the world model how the that the world model how the ations of the world model or other ations of the world model or other inference we have done and other s is inference we have done and other s is actually coming in as a3 and actually coming in as a3 and the u then the knowledge based response", "image_path": "img_data/video_25_chunk_27.jpg"}
{"video": "video_25", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "actually coming in as a3 and the u then the knowledge based response the u then the knowledge based response i'm not agreeing with that because that i'm not agreeing with that because that contradicts everything that i know up contradicts everything that i know up to this point and the third to this point and the third one is i will update the one is i will update the data the database or the knowledge base and this is actually called the contingency this is in this case we", "image_path": "img_data/video_25_chunk_28.jpg"}
{"video": "video_25", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "contingency this is in this case we have another v diagram we have here the set of models that satisfy the knowledge base and when the sentence comes in there is an intersection let's try to understand that part which is the", "image_path": "img_data/video_25_chunk_29.jpg"}
{"video": "video_25", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "intersection let's try to understand that part which is the that part which is the part where we actually can grow the part where we actually can grow the database we can actually write here as database we can actually write here as from one side we have the set of from one side we have the set of models that satisfy the knowledge base models that satisfy the knowledge base intersecting with the set of models intersecting with the set of models that satisfy the a3 that intersection is that satisfy the a3 that intersection is a strict subset of the set of models a strict subset of the set of models that satisfy the knowledge base on one that satisfy the knowledge base on one hand and also that strict subset is", "image_path": "img_data/video_25_chunk_30.jpg"}
{"video": "video_25", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "that satisfy the knowledge base on one hand and also that strict subset is hand and also that strict subset is not null the null set is a strict subset of the null set is a strict subset of the set of models that satisfy the set of models that satisfy the knowledge space and actually we can knowledge space and actually we can interpret this as follows that the interpret this as follows that the sentence a3 reduced the set of models", "image_path": "img_data/video_25_chunk_31.jpg"}
{"video": "video_25", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "a3 reduced the set of models that satisfy the knowledge base and this reduction is effectively the synonym of adding more information the synonym of adding more information to the knowledge space and we to the knowledge space and we actually can write that this actually can write that this the following and also the this is one this is the and also the this is one this is the same the explanation for same the explanation for this part and the explanation", "image_path": "img_data/video_25_chunk_32.jpg"}
{"video": "video_25", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "same the explanation for this part and the explanation this part and the explanation for that part over here for that part over here is that what i know does not contradict alpha 3 then we saw the contradiction here that the intersection between the that the intersection between the knowledge base and the alpha 3 was null", "image_path": "img_data/video_25_chunk_33.jpg"}
{"video": "video_25", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "that the intersection between the knowledge base and the alpha 3 was null knowledge base and the alpha 3 was null in this case and in that case in this case and in that case evidently this is this evidently this is this intersection over here is not going to intersection over here is not going to be zero and u and that is be zero and u and that is basically the two sort of basically the two sort of explanations of this formula let's explanations of this formula let's try to provide a couple of examples try to provide a couple of examples here just one example in fact here just one example in fact in the hus world we saw that the in the hus world we saw that the knowledge base is at some point", "image_path": "img_data/video_25_chunk_34.jpg"}
{"video": "video_25", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "in the hus world we saw that the knowledge base is at some point knowledge base is at some point has this kind of negation that there is has this kind of negation that there is no breeze in location one no breeze in location one and suddenly the sentence a3 one and suddenly the sentence a3 comes in and the sentence a3 says that comes in and the sentence a3 says that there is a breeze in 21 and if you go there is a breeze in 21 and if you go back to the hubus world video then you back to the hubus world video then you actually can see that this is the actually can see that this is the case for or the war that we have", "image_path": "img_data/video_25_chunk_35.jpg"}
{"video": "video_25", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "actually can see that this is the case for or the war that we have case for or the war that we have already specified there was a rule already specified there was a rule that there is no pit and there is that there is no pit and there is absolut also no sentence no sensing absolut also no sentence no sensing that we can detect for in this specific that we can detect for in this specific cell and in fact there is a breeze in 21 cell and in fact there is a breeze in 21 and for these two sort of rules and for these two sort of rules or sentences we have or sentences we have the arrival of a3 definitely adds information to the", "image_path": "img_data/video_25_chunk_36.jpg"}
{"video": "video_25", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "the arrival of a3 definitely adds information to the definitely adds information to the knowledge base and we will list it as knowledge base and we will list it as such as an additional kind of such as an additional kind of rule now if we look at the -called rule now if we look at the -called ask operation responses now we have introduced entailments actually pretty clear we entailments actually pretty clear we have a knowledge base that we are asking have a knowledge base that we are asking something about the sentence", "image_path": "img_data/video_25_chunk_37.jpg"}
{"video": "video_25", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "have a knowledge base that we are asking something about the sentence something about the sentence a and we said that we have three a and we said that we have three responses the one is responses the one is true and this response actually will true and this response actually will come every time that we are able to come every time that we are able to entail a or alpha from the knowledge entail a or alpha from the knowledge base we will definitely respond with false every time we are not able to do that therefore we are able to entail", "image_path": "img_data/video_25_chunk_38.jpg"}
{"video": "video_25", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "false every time we are not able to do that therefore we are able to entail do that therefore we are able to entail the negation of a and then finally we can respond that i do not know we are which means that we cannot entail either the positive", "image_path": "img_data/video_25_chunk_39.jpg"}
{"video": "video_25", "start": "0:20:00", "end": "0:20:14.866667", "timestamp": "0:20:00 - 0:20:14.866667", "text": "cannot entail either the positive or the negative of the sentence", "image_path": "img_data/video_25_chunk_40.jpg"}
{"video": "video_26", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now it's time to see our first world model we will call this wus world and model we will call this wus world and it's actually a construction of a it's actually a construction of a artifici world which is consist of 4x4 artifici world which is consist of 4x4 cells that contain a it was actually cells that contain a it was actually created in back in 1973 many decades created in back in 1973 many decades ago and it was kind of created to study ago and it was kind of created to study logical reasoning we'll see now how logical reasoning we'll see now how this world and logical reasoning come this world and logical reasoning come together definitely we have an agent together definitely we have an agent this agent is actually sh over here in", "image_path": "img_data/video_26_chunk_0.jpg"}
{"video": "video_26", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "together definitely we have an agent this agent is actually sh over here in this agent is actually sh over here in this cell and we'll see now some this cell and we'll see now some of the rules governing if you this of the rules governing if you this agent there's also a beast that is agent there's also a beast that is actually called hus and gave the name to actually called hus and gave the name to that game and evidently the monster is that game and evidently the monster is this is eating anyone who enters that this is eating anyone who enters that kind of cell and the monster kind of cell and the monster can be shot by the agent has an arrow can be shot by the agent has an arrow over here", "image_path": "img_data/video_26_chunk_1.jpg"}
{"video": "video_26", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "can be shot by the agent has an arrow over here and the ig has just a single arrow and the ig has just a single arrow and there are of course other and there are of course other complications the presence of pet complications the presence of pet rooms over here pit rooms the rooms over here pit rooms the moment you fall into enter this pit moment you fall into enter this pit room from any of the possible other room from any of the possible other cells you are going to fall and cells you are going to fall and die and they will either die or and die and they will either die or just stuck there forever which is of just stuck there forever which is of course equivalent to that the", "image_path": "img_data/video_26_chunk_2.jpg"}
{"video": "video_26", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "just stuck there forever which is of course equivalent to that the course equivalent to that the agent goal is to reach a cell the agent goal is to reach a cell which is over here obviously the which is over here obviously the agent has not really know the cell and agent has not really know the cell and goal is located and in order to goal is located and in order to reach the goal the agent is assisted reach the goal the agent is assisted by perception percepts include by perception percepts include stench evidently the monster is humus stench evidently the monster is humus is actually very smelly it kind of is actually very smelly it kind of creates stench sensing in the sort of", "image_path": "img_data/video_26_chunk_3.jpg"}
{"video": "video_26", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "is actually very smelly it kind of creates stench sensing in the sort of creates stench sensing in the sort of surrounding cells there is a breeze surrounding cells there is a breeze which indicates that nearby there is a which indicates that nearby there is a pit and there's also pit and there's also glitter which is only present as a glitter which is only present as a percept the moment you enter that kind percept the moment you enter that kind of cell and therefore the moment you of cell and therefore the moment you receive the glitter from the gold receive the glitter from the gold then that you got a gold and then that you got a gold and you can exit g and that's you can exit g and that's basically how the game ends there are basically how the game ends there are certain i will call it", "image_path": "img_data/video_26_chunk_4.jpg"}
{"video": "video_26", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "basically how the game ends there are certain i will call it certain i will call it other things that i want to mention other things that i want to mention about this environment we have some environment we have some performance sort of awards if i performance sort of awards if i may call this of a th000 1,000 may call this of a th000 1,000 points for picking up the gold minus points for picking up the gold minus 1,000 points for dying and minus one point for each dying and minus one point for each location or actually action that is location or actually action that is taken that the agent would avoid taken that the agent would avoid performing unnecessary actions", "image_path": "img_data/video_26_chunk_5.jpg"}
{"video": "video_26", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "taken that the agent would avoid performing unnecessary actions performing unnecessary actions that is u avoiding to endlessly that is u avoiding to endlessly roam between let's say this cell and roam between let's say this cell and that cell all the time evidently you that cell all the time evidently you get some penalty out of it and then you get some penalty out of it and then you will ultimately the game will end will ultimately the game will end and of course minus 10 points there's and of course minus 10 points there's minus 10 points for using the arrow minus 10 points for using the arrow trying to kill the wus and that is trying to kill the wus and that is another negative reward another negative reward you can", "image_path": "img_data/video_26_chunk_6.jpg"}
{"video": "video_26", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "another negative reward you can turn and you can walk forward you can turn and you can walk forward you can grab an object and you can shoot these grab an object and you can shoot these are the actions that this agent is able are the actions that this agent is able to provide and of course you can climb to provide and of course you can climb out of this cave of this world if you that cave of this world if you that is possible only on the starting is possible only on the starting location evidently the game ends there location evidently the game ends there and of course you can climb out the and of course you can climb out the moment you get into the goal and of moment you get into the goal and of course the game ends as well there", "image_path": "img_data/video_26_chunk_7.jpg"}
{"video": "video_26", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "moment you get into the goal and of course the game ends as well there course the game ends as well there that's basically it there is no that's basically it there is no not really much to else we have already not really much to else we have already mentioned about the sensors and i'm mentioned about the sensors and i'm looking now at your core site where all looking now at your core site where all this kind of information has been sort of information has been sort of mentioned here this is the this is mentioned here this is the this is all the information i just went all the information i just went through we have an environment which is definitely environment which is definitely partially obs observable we do not know partially obs observable we do not know anything about the global state", "image_path": "img_data/video_26_chunk_8.jpg"}
{"video": "video_26", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "partially obs observable we do not know anything about the global state anything about the global state and than the local what is surr me and than the local what is surr me what we're able to perceive through what we're able to perceive through the percepts and of course it's a the percepts and of course it's a sequential decision making kind of sequential decision making kind of process and we'll see a bit later in process and we'll see a bit later in another video how the sequential another video how the sequential processes are related to other things processes are related to other things outside of logical reasoning such as outside of logical reasoning such as mark of decision process and mark of decision process and reinforcement learning we have another reinforcement learning we have another video for that all right i think", "image_path": "img_data/video_26_chunk_9.jpg"}
{"video": "video_26", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "reinforcement learning we have another video for that all right i think video for that all right i think it's time now to start with writing a it's time now to start with writing a couple of things about the couple of things about the rules of the world just before rules of the world just before we start taking actions i will just we start taking actions i will just go back to writing over here and i am go back to writing over here and i am going to start by quoting what we going to start by quoting what we actually call a knowledge base and we actually call a knowledge base and we mentioned already about the knowledge mentioned already about the knowledge base as the database where we store base as the database where we store senten es logical reasoning", "image_path": "img_data/video_26_chunk_10.jpg"}
{"video": "video_26", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "base as the database where we store senten es logical reasoning senten es logical reasoning implications all this kind of stuff that implications all this kind of stuff that we will need to make inferences we will need to make inferences let's quote over here the let's quote over here the initial state of the knowledge base initial state of the knowledge base let me write it here as knowledge base let me write it here as knowledge base and this will contain let's say some and this will contain let's say some rules i will title the rules as rules i will title the rules as capital r with a subscript of an inder capital r with a subscript of an inder number i'm starting with a rule r1 number i'm starting with a rule r1 and the first rule of the game is", "image_path": "img_data/video_26_chunk_11.jpg"}
{"video": "video_26", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "number i'm starting with a rule r1 and the first rule of the game is and the first rule of the game is that there is no pit and then i'm writing two indices pit and then i'm writing two indices one is the index that one is the index that corresponds to the first one is corresponds to the first one is corresponds to this direction this is corresponds to this direction this is let's say the index i and this will let's say the index i and this will be the index j and evidently one is over here and j and evidently one is over here and this rule says there's no pit this rule says there's no pit in location one there's no pit in", "image_path": "img_data/video_26_chunk_12.jpg"}
{"video": "video_26", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "this rule says there's no pit in location one there's no pit in location one there's no pit in location one means that it's a location one means that it's a rule that allows us the game to it's a rule that allows us the game to proceed because if there was a pit in proceed because if there was a pit in u in that location then the game will in u in that location then the game will actually end all right that's the actually end all right that's the first rule the second rule that the first rule the second rule that the knowledge base could knowledge base could contain is the following which is contain is the following which is b11 evidently p11 and b11 everything is b11 evidently p11 and b11 everything is a kind of a prepositional symbol b11 a kind of a prepositional symbol b11 is", "image_path": "img_data/video_26_chunk_13.jpg"}
{"video": "video_26", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "a kind of a prepositional symbol b11 is equivalent the b stands for breeze p stands for pit let me just write stands for pit let me just write this down as well p stands for pit b is a percept for pit b is a percept for breeze b11 is equivalent to having a b11 is equivalent to having a pit in one two or two 2", "image_path": "img_data/video_26_chunk_14.jpg"}
{"video": "video_26", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "b11 is equivalent to having a pit in one two or two 2 one that's when you will receive a p a breeze as a will receive a p a breeze as a percept in let's say this location percept in let's say this location and what i want to emphasize here is and what i want to emphasize here is that with propositional logic and we're that with propositional logic and we're having if you a 16 cell kind of having if you a 16 cell kind of world we have to go and write down all world we have to go and write down all of the cover a complete set of integer of the cover a complete set of integer numbers from 1 to for each of", "image_path": "img_data/video_26_chunk_15.jpg"}
{"video": "video_26", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "of the cover a complete set of integer numbers from 1 to for each of numbers from 1 to for each of these indices i'm just quoting these indices i'm just quoting here just one out of the many that we here just one out of the many that we can actually store in the in that kind can actually store in the in that kind of database let's give another one of database let's give another one r3 let's say this is b 21 is equivalent r3 let's say this is b 21 is equivalent to we have a let's say a breeze in 21 to we have a let's say a breeze in 21 is equivalent to pit in one is equivalent to pit in one or", "image_path": "img_data/video_26_chunk_16.jpg"}
{"video": "video_26", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "is equivalent to pit in one or this is the pit in one or this is the breeze in 21 this is breeze pit breeze in 21 this is breeze pit one or pit in 22 let me write down the 22 let me write down the one 2 three 4 or beit", "image_path": "img_data/video_26_chunk_17.jpg"}
{"video": "video_26", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "4 or beit 31 all right this is the first index this is the first index is i the second index 31 this is i the second index 31 this is exactly what we have here this is exactly what we have here and may i also write i think i and may i also write i think i forgot to mention this rule let's let me forgot to mention this rule let's let me call it r zero because i should have call it r zero because i should have mentioned it in the beginning is that mentioned it in the beginning is that there is no wus", "image_path": "img_data/video_26_chunk_18.jpg"}
{"video": "video_26", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "mentioned it in the beginning is that there is no wus in one the game is not in one the game is not really ending by dying because you're really ending by dying because you're eern al life by this moner all eern al life by this moner all right that's basically a portion of right that's basically a portion of the knowledge base purely from a point the knowledge base purely from a point of view of the game rules and now what we need to do rules and now what we need to do is we need to actually create a is we need to actually create a table on the table i'm just going table on the table i'm just going to post the video that we have", "image_path": "img_data/video_26_chunk_19.jpg"}
{"video": "video_26", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "table on the table i'm just going to post the video that we have to post the video that we have the i'm actually going to draw the i'm actually going to draw this table i created this table i created this table the first column of this table will be the first column of this table will be let's say this the column let's say this the column state the second column is going to be state the second column is going to be percept what i'm receiving in perception percept what i'm receiving in perception the third column is inference and the third is going to be inference and the third is going to be action this agent as it turns out is action this agent as it turns out is a very conservative agent it will a very conservative agent it will actually not take any action unless it", "image_path": "img_data/video_26_chunk_20.jpg"}
{"video": "video_26", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "a very conservative agent it will actually not take any action unless it actually not take any action unless it is confident it's certain that the it is confident it's certain that the cell is a safe cell to move into cell is a safe cell to move into that's one thing to kind of note here that's one thing to kind of note here and i'm going to just start here with and i'm going to just start here with time instance number time instance number one and in that kind of time instance one and in that kind of time instance we have the another symbol that we'll we have the another symbol that we'll call for location l11 is equal to true", "image_path": "img_data/video_26_chunk_21.jpg"}
{"video": "video_26", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "we have the another symbol that we'll call for location l11 is equal to true call for location l11 is equal to true the agent is actually over here and it the agent is actually over here and it is facing east and i will symbolize with is facing east and i will symbolize with u this proposition symbol theta east the u this proposition symbol theta east the fact that is actually facing east that fact that is actually facing east that way and u definitely in terms of percept the definitely in terms of percept the agent is not receiving anything and when agent is not receiving anything and when it comes to propositional logic we have it comes to propositional logic we have to specify every single lack of that percept that truth single lack of that percept that truth let me call this rule", "image_path": "img_data/video_26_chunk_22.jpg"}
{"video": "video_26", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "single lack of that percept that truth let me call this rule let me call this rule r4 that we have no breeze in b11 and we have a whole bunch of b11 and we have a whole bunch of other rules which i'm not going to quote other rules which i'm not going to quote but you can understand that we have u but you can understand that we have u no stance nothing all no stance nothing all these location is free of any percept these location is free of any percept we don't receive anything but we have we don't receive anything but we have to write them down explicitly", "image_path": "img_data/video_26_chunk_23.jpg"}
{"video": "video_26", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "we don't receive anything but we have to write them down explicitly to write them down explicitly and what we can infer out of this and what we can infer out of this i am going to write down i am going to write down the rule for let me call this rule the rule for let me call this rule r six and i can infer that since i have six and i can infer that since i have received no breeze here there is no way received no breeze here there is no way that there is a pit either here or that there is a pit either here or here let me write down that here let me write down that there is no pit this inference which i", "image_path": "img_data/video_26_chunk_24.jpg"}
{"video": "video_26", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "here let me write down that there is no pit this inference which i there is no pit this inference which i can make there is no pit on can make there is no pit on 21 and of course right now what i'm 21 and of course right now what i'm doing is i'm actually using my own doing is i'm actually using my own logical reasoning sort of logical reasoning sort of capability that i have inside my brain capability that i have inside my brain to infer that but later we'll see how to infer that but later we'll see how the machine will actually be able to do the machine will actually be able to do that using a couple of that using a couple of algorithms and the action that algorithms and the action that actually we can take is that we can", "image_path": "img_data/video_26_chunk_25.jpg"}
{"video": "video_26", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "algorithms and the action that actually we can take is that we can actually we can take is that we can move forward in fact east is actually move forward in fact east is actually facing that way we'll take this facing that way we'll take this forward this is that way east is that forward this is that way east is that way and north is that way all right north is that way all right we move forward and then evidently we move forward and then evidently when we move forward we arrive at the when we move forward we arrive at the second instance second row over here where the second row over here where the l21 is through we are here and guess", "image_path": "img_data/video_26_chunk_26.jpg"}
{"video": "video_26", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "second row over here where the l21 is through we are here and guess l21 is through we are here and guess what happens over here we are storing what happens over here we are storing into the knowledge base the into the knowledge base the b21 indicating that there is going to b21 indicating that there is going to be a breeze and this bit bit1 be a breeze and this bit bit1 indicates that this is true indicates that this is true there is a breeze in 2 one there is a breeze in 2 one as you can see and we have as", "image_path": "img_data/video_26_chunk_27.jpg"}
{"video": "video_26", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "there is a breeze in 2 one as you can see and we have as you can see and we have as inference let's say r7 either there's going to be a pit in 31 or it's going to be a pit in 22 that's basically it there is that's basically it there is definitely also we can make inferences definitely also we can make inferences as well we can make an inference that", "image_path": "img_data/video_26_chunk_28.jpg"}
{"video": "video_26", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "definitely also we can make inferences as well we can make an inference that as well we can make an inference that there is no possibility of having a wus there is no possibility of having a wus in neither 31 or 22 we can actually write this down i we can actually write this down i am yeah i can mention it as without am yeah i can mention it as without really quoting explicitly these kind of really quoting explicitly these kind of rules numbers for space rules numbers for space limitations negative limitations negative w31 comma negative", "image_path": "img_data/video_26_chunk_29.jpg"}
{"video": "video_26", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "limitations negative w31 comma negative w22 all right what we do is as i said the agent is very as i said the agent is very conservative and u definitely cannot conservative and u definitely cannot check this cell that it is safe and check this cell that it is safe and cannot check this cell that it is safe cannot check this cell that it is safe the only safe cell it can still go to the only safe cell it can still go to is back to one and it will actually is back to one and it will actually do that it will go back to", "image_path": "img_data/video_26_chunk_30.jpg"}
{"video": "video_26", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "is back to one and it will actually do that it will go back to do that it will go back to 11 one l11 is true and it will face evidently west it will still receive another percept information about negative b11 and it will decide to go to another cell and in that kind of cell it is really", "image_path": "img_data/video_26_chunk_31.jpg"}
{"video": "video_26", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "cell and in that kind of cell it is really and in that kind of cell it is really the cell two one two and actually what we can see here two and actually what we can see here is that there is no we can infer is that there is no we can infer that let me write down the sort of that let me write down the sort of turn to show that explicitly we have an turn to show that explicitly we have an action for turn and we have for action for turn and we have for eastn four we have eastn four", "image_path": "img_data/video_26_chunk_32.jpg"}
{"video": "video_26", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "eastn four we have eastn four the l11 is equal to true theta north the l11 is equal to true theta north is equal to true we continue to not is equal to true we continue to not receive any breeze and receive any breeze and also we can infer that there is no pit in one two infer that there is no pit in one two and the question is now how do we make and the question is now how do we make that kind of inference and the answer is obviously that there is no possibility of that there is no possibility of having a pit over here because otherwise", "image_path": "img_data/video_26_chunk_33.jpg"}
{"video": "video_26", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "that there is no possibility of having a pit over here because otherwise having a pit over here because otherwise we would have received a breeze we would have received a breeze we actually can make this inference and we actually can make this inference and then we can make the then we can make the action for the action is now we are action for the action is now we are facing north and we have actually facing north and we have actually arrived on eant five we have arrived at instance five to five we have arrived at instance five to l one 2 is equal to true and theta is l one 2 is equal to true and theta is equal to", "image_path": "img_data/video_26_chunk_34.jpg"}
{"video": "video_26", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "l one 2 is equal to true and theta is equal to true and then at this moment in true and then at this moment in time we are receiving a st one time we are receiving a st one two and also what we can two and also what we can actually infer here is a couple of more important here is a couple of more important inference here we have the stance we inference here we have the stance we can infer that w 13 is true and we can also infer that there's no", "image_path": "img_data/video_26_chunk_35.jpg"}
{"video": "video_26", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "13 is true and we can also infer that there's no and we can also infer that there's no pit in 22 and now the question is pit in 22 and now the question is why we actually can do this kind of why we actually can do this kind of inferences and the answer is that when inferences and the answer is that when we were occupying l21 when we are we were occupying l21 when we are location at 21 we did not receive a location at 21 we did not receive a stench therefore we have cleared that stench therefore we have cleared that there is no hus present in either this guy no hus present in either this guy or this guy now that we are here", "image_path": "img_data/video_26_chunk_36.jpg"}
{"video": "video_26", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "no hus present in either this guy or this guy now that we are here or this guy now that we are here and we are receiving a stench given the and we are receiving a stench given the fact we have cleared this cell the only fact we have cleared this cell the only cell that it is remaining over here is cell that it is remaining over here is the other cell which is next to us which the other cell which is next to us which is one3 it is really now we can is one3 it is really now we can reason that the wbus is on 13 and reason that the wbus is on 13 and evidently we also know that we cannot evidently we also know that we cannot have a pit over here as well on 2 two have a pit over here as well on 2 two because otherwise we have the breeze and", "image_path": "img_data/video_26_chunk_37.jpg"}
{"video": "video_26", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "have a pit over here as well on 2 two because otherwise we have the breeze and because otherwise we have the breeze and we didn't mind you although i we didn't mind you although i kind of tend to write down the kind of tend to write down the positive percepts over here there's a positive percepts over here there's a whole bunch of other negative percepts whole bunch of other negative percepts as well we what we can then as well we what we can then determine is that evidently we can move determine is that evidently we can move into this cell safely and what we will do is we will safely and what we will do is we will turn and forward i'm just going to turn and forward i'm just going to combine now this turn move", "image_path": "img_data/video_26_chunk_38.jpg"}
{"video": "video_26", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "turn and forward i'm just going to combine now this turn move combine now this turn move forward i turn eastwards and we forward i turn eastwards and we moving into cell 22 moving it to cell 22 l22 is equal now true and we are facing east and then of course we can start evaluating various kind of", "image_path": "img_data/video_26_chunk_39.jpg"}
{"video": "video_26", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "east and then of course we can start evaluating various kind of start evaluating various kind of paths cells as well we can evaluate this paths cells as well we can evaluate this cell and we can actually clear cell and we can actually clear it because over here we have not it because over here we have not received any percept information received any percept information both of these cells are both of these cells are cleared and we can actually move here cleared and we can actually move here we can move here and we can move there we can move here and we can move there and then we realize that we can't really and then we realize that we can't really book any anymore and then we can book any anymore and then we can actually can start going backwards to actually can start going backwards to cut the long story", "image_path": "img_data/video_26_chunk_40.jpg"}
{"video": "video_26", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "actually can start going backwards to cut the long story short we could just turn to face north and we can move turn to face north and we can move then safely to the other cell which we then safely to the other cell which we believe that it is safe this is l 23 then of course we are receiving", "image_path": "img_data/video_26_chunk_41.jpg"}
{"video": "video_26", "start": "0:21:00", "end": "0:21:19.866667", "timestamp": "0:21:00 - 0:21:19.866667", "text": "23 then of course we are receiving s23 b23 and of course we see the glitter b23 and of course we see the glitter 23 then we infer that there is a 23 then we infer that there is a gold present over there and we take the action of grab there and we take the action of grab and the game ends", "image_path": "img_data/video_26_chunk_42.jpg"}
{"video": "video_27", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "we'll start our discussion on logical reasoning from basically two reasoning from basically two perspectives if i may just kind of perspectives if i may just kind of draw a kind of a table over draw a kind of a table over here where the logical reasoning is going to be studied i said from two to be studied i said from two perspectives the first is the syntax perspectives the first is the syntax we'll spend some need to spend some time we'll spend some need to spend some time kind of understanding what the syn text", "image_path": "img_data/video_27_chunk_0.jpg"}
{"video": "video_27", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "we'll spend some need to spend some time kind of understanding what the syn text kind of understanding what the syn text of propositional logic is and the other of propositional logic is and the other is the semantics we need to understand how we will represent word models and on we will represent word models and on let me just write down let me just write down what i just said the two perspectives what i just said the two perspectives and what are the components of this kind and what are the components of this kind of discussion how to represent as", "image_path": "img_data/video_27_chunk_1.jpg"}
{"video": "video_27", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "represent as sentence we will use greek letters for sentences let's say a sentence sentences let's say a sentence a and more importantly is a and more importantly is how we make inferences conclusions in other words using syntax", "image_path": "img_data/video_27_chunk_2.jpg"}
{"video": "video_27", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "inferences conclusions in other words using syntax alone and this will actually be what we will be calling the d know what we will be calling the d discussion theor proving theore improving and in terms of semantics we will discuss about world models and sort of evaluation of these models", "image_path": "img_data/video_27_chunk_3.jpg"}
{"video": "video_27", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "models and sort of evaluation of these models and how to make inferences using a models alone this is basically the components of our discussion and the components of our discussion and this in the discussion actually this in the discussion actually logical reasoning we will be also", "image_path": "img_data/video_27_chunk_4.jpg"}
{"video": "video_27", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "this in the discussion actually logical reasoning we will be also building or assuming the presence of what we call the knowledge base this knowledge base is going to be the almost a database that be the almost a database that however contary to sql databases that however contary to sql databases that you may be familiar with or relational you may be familiar with or relational databases we will be stor ing", "image_path": "img_data/video_27_chunk_5.jpg"}
{"video": "video_27", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "you may be familiar with or relational databases we will be stor ing databases we will be stor ing sentences and logical sentences and logical sentences the u we will start sentences the u we will start the discussion with what is syntax the discussion with what is syntax is a language that consists of propositional symbol", "image_path": "img_data/video_27_chunk_6.jpg"}
{"video": "video_27", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "consists of propositional symbol the symbols is everything that is has this kind of representation you're this kind of representation you're familiar already with many symbols familiar already with many symbols numbers are symbols letters are symbols numbers are symbols letters are symbols from everyday kind of life here the from everyday kind of life here the symbols will actually be such they are symbols will actually be such they are propositional this means that they propositional this means that they will either receive a value of will either receive a value of true or false", "image_path": "img_data/video_27_chunk_7.jpg"}
{"video": "video_27", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "will either receive a value of true or false as well as operator it consist of propositional symbols and operators and these operators will be the not operator the negation operator the negation operator the conjunction operator the conjunction operator disjunction operator the if then disjunction operator the if then operator or implication operator and the operator or implication operator and the if and only if operator these", "image_path": "img_data/video_27_chunk_8.jpg"}
{"video": "video_27", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "operator or implication operator and the if and only if operator these if and only if operator these are the operators and now we will start are the operators and now we will start now seeing how examples of those now seeing how examples of those sentences wr this down this is negation this is conjunction this is this junction this is the implication", "image_path": "img_data/video_27_chunk_9.jpg"}
{"video": "video_27", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "junction this is the implication also known as a rule and this is the double implication mathematically is the if then else statement this is the if then else statement this is the if then that's why i actually called the rule that's why i actually called the rule then this is the or this is the end and then this is the or this is the end and this is the negation the let's now this is the negation the let's now write down first sentence let's", "image_path": "img_data/video_27_chunk_10.jpg"}
{"video": "video_27", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "this is the negation the let's now write down first sentence let's write down first sentence let's call this sentence a1 and this will call this sentence a1 and this will also open up the discussion about also open up the discussion about semantics i have a sentence here semantics i have a sentence here which is rain and wet and if i am to sort of wet and if i am to sort of represent the word model given i have represent the word model given i have a word with just two symbols here and a word with just two symbols here and each of these symbols apparently is each of these symbols apparently is binary it's either true or false", "image_path": "img_data/video_27_chunk_11.jpg"}
{"video": "video_27", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "each of these symbols apparently is binary it's either true or false binary it's either true or false then i have let's say over here the then i have let's say over here the symbol rain r for rain and w symbol rain r for rain and w for wet and i have values zero and one for wet and i have values zero and one this guy also has valu zero and one this guy also has valu zero and one and this is basically all everything and this is basically all everything that can actually happen in this that can actually happen in this world and if i have these two where world and if i have these two where both of them are zero then will actually both of them are zero then will actually be a false which i'm not going to shade be a false which i'm not going to shade if it is also here is false and", "image_path": "img_data/video_27_chunk_12.jpg"}
{"video": "video_27", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "be a false which i'm not going to shade if it is also here is false and if it is also here is false and this is also false and this is true and this is also false and this is true and this is what i'm going to shade over this is what i'm going to shade over here as the cell that makes the world here as the cell that makes the world true and what i used in order for me true and what i used in order for me to determine the true or false to determine the true or false value of this world is the value of this world is the semantic rules we use semantic rules we use semantic rules and i you not do not want to have to and i you not do not want to have to write down the truth table for", "image_path": "img_data/video_27_chunk_13.jpg"}
{"video": "video_27", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "and i you not do not want to have to write down the truth table for to write down the truth table for semantic rules but i actually brought a semantic rules but i actually brought a copy of the book corresponding table that is in your corresponding table that is in your textbook and in that in this table textbook and in that in this table you actually see the two symbols p and q you actually see the two symbols p and q and the possible values that i and the possible values that i actually can take and here we actually can take and here we actually see all of the rules that we see all of the rules that we have simple sentence", "image_path": "img_data/video_27_chunk_14.jpg"}
{"video": "video_27", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "see all of the rules that we have simple sentence have simple sentence that we have actually formed the that we have actually formed the connectives as we call connecting these connectives as we call connecting these two symbols these are extremely two symbols these are extremely simple we have the negation the end simple we have the negation the end the or the implication double the or the implication double implication the exor which i forgot to implication the exor which i forgot to mention and then we have two mention and then we have two others which are a bit more comp others which are a bit more comp composite sort of sentences and composite sort of sentences and we'll come back to that to those two we'll come back to that to those two last columns a bit later but", "image_path": "img_data/video_27_chunk_15.jpg"}
{"video": "video_27", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "we'll come back to that to those two last columns a bit later but last columns a bit later but right now here we have actually used right now here we have actually used this truth table to this column over this truth table to this column over here to create if you to here to create if you to put the meaning behind a word every put the meaning behind a word every possible state of the word i possible state of the word i will write here something which i'll will write here something which i'll be calling now introducing a new term be calling now introducing a new term which actually called a model this is the model of the word", "image_path": "img_data/video_27_chunk_16.jpg"}
{"video": "video_27", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "model this is the model of the word that satisfies the sentence satisfies the sentence and this is the model is coris to w is equal to true and rain is equal to true", "image_path": "img_data/video_27_chunk_17.jpg"}
{"video": "video_27", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "coris to w is equal to true and rain is equal to true we will put this in brackets and this will actually be the brackets and this will actually be the model that satisfies my sentence and model that satisfies my sentence and there is a notation for that and i'll be there is a notation for that and i'll be using the notation the for this model this notation the for this model this is the notation i'll put it in square is the notation i'll put it in square brackets this is the model of a1 in a quite similar", "image_path": "img_data/video_27_chunk_18.jpg"}
{"video": "video_27", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "a1 in a quite similar kind of example just quickly i have a kind of example just quickly i have a sentence a2 and this sentence is sentence a2 and this sentence is rain or snow evidently if we actually see the snow evidently if we actually see the previous table then the model of the previous table then the model of the word that satisfies that kind of word that satisfies that kind of sentence is not only one sentence is not only one let's assume that over here we have let's assume that over here we have 0 one and another let's say force sow is", "image_path": "img_data/video_27_chunk_19.jpg"}
{"video": "video_27", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "let's assume that over here we have 0 one and another let's say force sow is 0 one and another let's say force sow is also zero and one over here i also zero and one over here i actually can shade more than one cell as the as shade more than one cell as the as all of these all of the of these three all of these all of the of these three are satisfying that kind of sentence are satisfying that kind of sentence i can actually write now a new i will i can actually write now a new i will call it notation the capital m all of call it notation the capital m all of these three models these all of these three models satisfy the c capital m this is a set of", "image_path": "img_data/video_27_chunk_20.jpg"}
{"video": "video_27", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "these all of these three models satisfy the c capital m this is a set of satisfy the c capital m this is a set of models capital m for set of the sentence models capital m for set of the sentence a2 and let me write down the set of models that satisfy a2 or alpha 2 and this consist of m1 m2 and m3", "image_path": "img_data/video_27_chunk_21.jpg"}
{"video": "video_27", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "a2 or alpha 2 and this consist of m1 m2 and m3 and let's say this is m1 this is m2 and this is m3 we can see that we this is m3 we can see that we have these two new notations m and have these two new notations m and capital m and this will actually be capital m and this will actually be very useful for us because we'll be very useful for us because we'll be using them to sort of create a using them to sort of create a inferences to make inferences about a bit more complicated models about a bit more complicated models that we'll introduce right now that we'll introduce right now before we actually see a bit more", "image_path": "img_data/video_27_chunk_22.jpg"}
{"video": "video_27", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "that we'll introduce right now before we actually see a bit more before we actually see a bit more complicated kind of model i think it's complicated kind of model i think it's worthwhile staying on this specific worthwhile staying on this specific column and a lot of people column and a lot of people have some kind of problem understanding have some kind of problem understanding a bit the intuition behind these a bit the intuition behind these truth values everyone almost everyone truth values everyone almost everyone is able to understand this column is able to understand this column obviously it's a negation and the end obviously it's a negation and the end and the or well familiar with this and the or well familiar with this if then column or the implication column if then column or the implication column y is zero implies a z is true and", "image_path": "img_data/video_27_chunk_23.jpg"}
{"video": "video_27", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "if then column or the implication column y is zero implies a z is true and y is zero implies a z is true and on there's some kind of difficulty to on there's some kind of difficulty to kind of absorb in terms of intuition kind of absorb in terms of intuition i just want to stay here and give you an i just want to stay here and give you an example let me sort of title this example let me sort of title this implication rule in the application rule we have effectively two symbols and we have effectively two symbols and we have double application as well we have double application as well we have p we have q we have p implies q", "image_path": "img_data/video_27_chunk_24.jpg"}
{"video": "video_27", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "double application as well we have p we have q we have p implies q p we have q we have p implies q and we have p double implication or if and we have p double implication or if then else if q all right then else if q all right if and only if q this is a if and only if q this is a this are the four values we have from false the four values we have from false we have true and this is false we have true and this is false we have false true false", "image_path": "img_data/video_27_chunk_25.jpg"}
{"video": "video_27", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "false we have true and this is false we have false true false have false true false and true we are trying to true and true we are trying to kind of put our heads a bit about you kind of put our heads a bit about what led to these values over here know what led to these values over here in these two columns all right from pure columns all right from pure logical perspective we actually can logical perspective we actually can write the equivalents for the p implies q the first way to think about it", "image_path": "img_data/video_27_chunk_26.jpg"}
{"video": "video_27", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "p implies q the first way to think about it you can think about it from a logical perspective and the way to think about it is that not both", "image_path": "img_data/video_27_chunk_27.jpg"}
{"video": "video_27", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "not both p and not q and this is not p and not q this is equivalent to negative and not q this is equivalent to negative p or q and this will actually be p or q and this will actually be the give us if you the values over here in this kind of column values over here in this kind of column from a pr logical kind of perspective", "image_path": "img_data/video_27_chunk_28.jpg"}
{"video": "video_27", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "values over here in this kind of column from a pr logical kind of perspective from a pr logical kind of perspective we cannot really have we cannot have a p we cannot really have we cannot have a p and the negation of q happening if p and the negation of q happening if p implies q we cannot really have this implies q we cannot really have this condition happen cannot really have p condition happen cannot really have p and no q happening at the same time and no q happening at the same time that is the kind of intuition to that is the kind of intuition to speak it's regarding speak it's regarding this kind of logical equivalence this kind of logical equivalence this i think the an example though i think it i think the an example though i think it provides a bit more intuition into provides a bit more intuition into that let me call this row one", "image_path": "img_data/video_27_chunk_29.jpg"}
{"video": "video_27", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "provides a bit more intuition into that let me call this row one into that let me call this row one row two three and four i want to row two three and four i want to think a little bit about row one just think a little bit about row one just finding a kind of a real world example finding a kind of a real world example to kind of always kind of remember it to kind of always kind of remember it i will translate this row one i will translate this row one if it is not raining", "image_path": "img_data/video_27_chunk_30.jpg"}
{"video": "video_27", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "if it is not raining it cannot be cloudy and not raining cannot be cloudy and not raining cannot be cloudy and this is definitely true and i'm associating with thep the rain and with q whether it is the rain and with q whether it is cloudy or not and i'm just trying to cloudy or not and i'm just trying to make a sort of statement as to from real make a sort of statement as to from real life whether it is i agree with it or", "image_path": "img_data/video_27_chunk_31.jpg"}
{"video": "video_27", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "make a sort of statement as to from real life whether it is i agree with it or life whether it is i agree with it or not and definitely i agree and this not and definitely i agree and this is why i signed here the truth value is why i signed here the truth value true the second line let's say true the second line let's say if it is not raining it can be cloudy and this is also kind of makes sense to me and that's why i kind", "image_path": "img_data/video_27_chunk_32.jpg"}
{"video": "video_27", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "cloudy and this is also kind of makes sense to me and that's why i kind makes sense to me and that's why i kind of put this kind of a truth value of put this kind of a truth value through the third one if it is raining it cannot be cloudy and this is for me it is false and that kind of agrees with false and that kind of agrees with this third line over there and", "image_path": "img_data/video_27_chunk_33.jpg"}
{"video": "video_27", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "false and that kind of agrees with this third line over there and this third line over there and then finally if it is raining it can be cloudy and this is true these are the kind of sentences which i always kind of try to recall which i always kind of try to recall and to understand a little bit more this column and now in little bit more this column and now in terms of double implication i think we", "image_path": "img_data/video_27_chunk_34.jpg"}
{"video": "video_27", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "little bit more this column and now in terms of double implication i think we terms of double implication i think we need to think about double implication need to think about double implication obviously as the equivalent this is obviously as the equivalent this is p if and only if q then this is equivalent to p implies q and q implies p and from that kind of q implies p and from that kind of perspective if we have a column perspective if we have a column this and we have another column this and we have another column that it is", "image_path": "img_data/video_27_chunk_35.jpg"}
{"video": "video_27", "start": "0:18:00", "end": "0:18:18.600000", "timestamp": "0:18:00 - 0:18:18.600000", "text": "this and we have another column that it is can take if you this tr or can take if you this tr or false kind of values then it's just false kind of values then it's just basically a symbol end and we already basically a symbol end and we already know the truth value of that end and know the truth value of that end and then we can actually obtain this then we can actually obtain this actually po very easily", "image_path": "img_data/video_27_chunk_36.jpg"}
{"video": "video_29", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "we conclude the discussion on forward search algorithm with aar which is one search algorithm with aar which is one of the most popular algorithms for both of the most popular algorithms for both robotics and other applications such robotics and other applications such as route planning and this algorith is as route planning and this algorith is also in the kind of a different category also in the kind of a different category than all the rest because it is what than all the rest because it is what is called the informed for a is called the informed for a search algorith in other words we search algorith in other words we somehow use domain expertise in somehow use domain expertise in designing an algorithm that hoping that designing an algorithm that hoping that domain expertise which is going to", "image_path": "img_data/video_29_chunk_0.jpg"}
{"video": "video_29", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "designing an algorithm that hoping that domain expertise which is going to that domain expertise which is going to be expressed via what we call a be expressed via what we call a heuristic is going to help us achieve heuristic is going to help us achieve much better performance as compared to much better performance as compared to the others which are in the category of the others which are in the category of -cal uninformed search algorithms -cal uninformed search algorithms the a star algor and if you recall the a star algor and if you recall the discussion on the dickess in dickess we discussion on the dickess in dickess we had a case we had effectively implemented some form of a effectively implemented some form of a dynamic programming problem and in", "image_path": "img_data/video_29_chunk_1.jpg"}
{"video": "video_29", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "effectively implemented some form of a dynamic programming problem and in dynamic programming problem and in that involved two things the that involved two things the -called past cost and the -called -called past cost and the -called future cost and the or cost to go and future cost and the or cost to go and in this case what we do is and in this case what we do is and digra was actually expanding the nodes digra was actually expanding the nodes by considering if you the past cost by considering if you the past cost and all of these nodes are sort of and all of these nodes are sort of uniformly i will call it expanded uniformly i will call it expanded in this case what we do is we are", "image_path": "img_data/video_29_chunk_2.jpg"}
{"video": "video_29", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "uniformly i will call it expanded in this case what we do is we are in this case what we do is we are actually going to be using both the actually going to be using both the pass cost as well also a pass cost as well also a euristic but this juristic is the one euristic but this juristic is the one that incorporates the domain expertise that incorporates the domain expertise if you go into a specific example if you go into a specific example just to understand the nature of this just to understand the nature of this kind of heris and how this actually help kind of heris and how this actually help us let's take an example actually from us let's take an example actually from your textbook and this example your textbook and this example goes as follows we are given if", "image_path": "img_data/video_29_chunk_3.jpg"}
{"video": "video_29", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "your textbook and this example goes as follows we are given if goes as follows we are given if you a network this network is you a network this network is consist of romanian cities it's consist of romanian cities it's not is a city and each edge is not is a city and each edge is associated cost that cost here is associated cost that cost here is distance and our task over here is to distance and our task over here is to come up with a optimal plan or route come up with a optimal plan or route from the city of arab to the city of from the city of arab to the city of bucharest now to solve this problem with bucharest now to solve this problem with a star we are actually going to be a star we are actually going to be engaging exactly the same forward search", "image_path": "img_data/video_29_chunk_4.jpg"}
{"video": "video_29", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "a star we are actually going to be engaging exactly the same forward search engaging exactly the same forward search algorithm the generic forward search algorithm the generic forward search algorithm that we have specified with a algorithm that we have specified with a priority queue and this is exactly what priority queue and this is exactly what we have done also in the dix kind of we have done also in the dix kind of case but instead of using a priority case but instead of using a priority that is only going to be the pass cost that is only going to be the pass cost we are going to use a priority that it we are going to use a priority that it is associated with the future cost as is associated with the future cost as well it is really this is the cost well it is really this is the cost that will be sort of running the that will be sort of running the algorithm with in terms of priority and algorithm with in terms of priority and the h of s which is theistic that we", "image_path": "img_data/video_29_chunk_5.jpg"}
{"video": "video_29", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "algorithm with in terms of priority and the h of s which is theistic that we the h of s which is theistic that we need to come up using our domain need to come up using our domain expertise is going to be the approximate expertise is going to be the approximate distance from every node to the city distance from every node to the city of bucharest and the say approximate of bucharest and the say approximate because we evidently if we had the because we evidently if we had the actual true distance then the problem actual true distance then the problem would have been solved would have been solved immediately then in this case what immediately then in this case what we do forx we just take the straight", "image_path": "img_data/video_29_chunk_6.jpg"}
{"video": "video_29", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "immediately then in this case what we do forx we just take the straight we do forx we just take the straight line distance evidently the line distance evidently the straight line distance is only an straight line distance is only an approximation of the true distance given approximation of the true distance given the road network we have in front of us the road network we have in front of us all right we will be running the all right we will be running the aar algorithm in fact in your aar algorithm in fact in your textbook you will actually see how textbook you will actually see how the expansions are actually happening the expansions are actually happening in order for us to go through the in order for us to go through the reason why this expans the reason why this expans the specific expansion is actually happening specific expansion is actually happening we have to go through the pip py code", "image_path": "img_data/video_29_chunk_7.jpg"}
{"video": "video_29", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "specific expansion is actually happening we have to go through the pip py code we have to go through the pip py code and i have actually brought a python and i have actually brought a python code a bit later on for another code a bit later on for another application which is associated with application which is associated with robotics we'll see the python robotics we'll see the python implementation of aar in a moment but implementation of aar in a moment but here is your u expansions as they are here is your u expansions as they are considered this is basically your considered this is basically your edge cost which is the pass cost edge cost which is the pass cost and the specific node that we're and the specific node that we're starting with plus theistic the 366", "image_path": "img_data/video_29_chunk_8.jpg"}
{"video": "video_29", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "and the specific node that we're starting with plus theistic the 366 starting with plus theistic the 366 is the associated value of that turistic is the associated value of that turistic for arad then from arad we can for arad then from arad we can actually go into other cities we actually go into other cities we can go into siu timisoara or can go into siu timisoara or zerin and for each one of them zerin and for each one of them we have an associated optimal we have an associated optimal past cost plus our heuristic and we past cost plus our heuristic and we are evidently selecting the node are evidently selecting the node which will off for the least cost for", "image_path": "img_data/video_29_chunk_9.jpg"}
{"video": "video_29", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "are evidently selecting the node which will off for the least cost for which will off for the least cost for us to expand we expand that kind of node us to expand we expand that kind of node and given the fact that we and given the fact that we if our heuristic no there is no if our heuristic no there is no fact here but if theistic is an fact here but if theistic is an underestimate of the true future cost underestimate of the true future cost then the algorith is actually proven then the algorith is actually proven that can converge to the optimal path that can converge to the optimal path solution and that kind of optimal path solution and that kind of optimal path solution you can actually see it over solution you can actually see it over here with the grade out nodes the", "image_path": "img_data/video_29_chunk_10.jpg"}
{"video": "video_29", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "solution you can actually see it over here with the grade out nodes the here with the grade out nodes meaning that there is grade out nodes meaning that there is there are two grade out nodes over here there are two grade out nodes over here and in fact this is the one that is and in fact this is the one that is going to finally be selected to lead us going to finally be selected to lead us to bucharest the optimal ps from ara to bucharest the optimal ps from ara to siu to ru i'm not sure if i you to siu to ru i'm not sure if i say it right pesti and bucharest know say it right pesti and bucharest finally closing your code your", "image_path": "img_data/video_29_chunk_11.jpg"}
{"video": "video_29", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "know say it right pesti and bucharest finally closing your code your finally closing your code your page over here on aar has an your page over here on aar has an demonstration and a python demonstration and a python demonstration of the aar algorith has applied to of the aar algorith has applied to very simple robotics environment very simple robotics environment and actually you can see here the and actually you can see here the animation how effectively the nodes get animation how effectively the nodes get expanded in a form which is somehow beam expanded in a form which is somehow beam forms towards that go state as forms towards that go state as compared to uniformly kind of expanding compared to uniformly kind of expanding the nodes we hear we can actually", "image_path": "img_data/video_29_chunk_12.jpg"}
{"video": "video_29", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "compared to uniformly kind of expanding the nodes we hear we can actually the nodes we hear we can actually clearly see here some kind of being clearly see here some kind of being forming benefit that the aar is forming benefit that the aar is providing and of course this has to do providing and of course this has to do with theistic that we have chosen in with theistic that we have chosen in this case as well now there is in this case as well now there is also an interactive demo that it also an interactive demo that it will allow you to kind of understand the will allow you to kind of understand the sort of mechanics of this kind of mechanics of this kind of algorithms if we actually can go ahead algorithms if we actually can go ahead and define if some kind of form of", "image_path": "img_data/video_29_chunk_13.jpg"}
{"video": "video_29", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "algorithms if we actually can go ahead and define if some kind of form of and define if some kind of form of a wall between the starting state and a wall between the starting state and the goal state you have the opportunity the goal state you have the opportunity to compare for example the dix kind of to compare for example the dix kind of algorith that as we discussed is a algorith that as we discussed is a uniform kind of cost algorithm actually uniform kind of cost algorithm actually can see here the uniform almost can see here the uniform almost circular kind of expansion of the nodes circular kind of expansion of the nodes to the aar algorith that we have just to the aar algorith that we have just done with some kind of a heuristic that done with some kind of a heuristic that we can actually also choose", "image_path": "img_data/video_29_chunk_14.jpg"}
{"video": "video_29", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "done with some kind of a heuristic that we can actually also choose we can actually also choose and here you can actually see again and here you can actually see again the being form in benef the moment we the being form in benef the moment we have some kind of a line of sight have some kind of a line of sight to the gold state how the nodes are to the gold state how the nodes are getting expanded by the using the getting expanded by the using the manhattan distance in this case or we manhattan distance in this case or we can actually also select the ukian can actually also select the ukian distance was also distance the ukian distance was also the distance that we have seen in the distance that we have seen in the robotics demo a few moments ago robotics demo a few moments ago i think it's worthwhile understanding", "image_path": "img_data/video_29_chunk_15.jpg"}
{"video": "video_29", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "robotics demo a few moments ago i think it's worthwhile understanding i think it's worthwhile understanding also where a star can be met in also where a star can be met in reality out there in a kind of a reality out there in a kind of a robotics application the one robotics application the one actually we see here we see the planning actually we see here we see the planning if you subsystem being after the if you subsystem being after the -called perception and probabilistic -called perception and probabilistic reasoning over time subsystems that we reasoning over time subsystems that we have dealt in other videos and have dealt in other videos and definitely we see the outout planning as definitely we see the outout planning as one component inside the planning system one component inside the planning system that's where your aar will actually be that's where your aar will actually be seen and can be applied but there", "image_path": "img_data/video_29_chunk_16.jpg"}
{"video": "video_29", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "that's where your aar will actually be seen and can be applied but there seen and can be applied but there are many other planning modules are many other planning modules that we have and that kind of form some that we have and that kind of form some form of hierarchy and form of hierarchy and hierarchical kind of planning is a kind hierarchical kind of planning is a kind of a very big topic your of a very big topic your traditional kind of textbooks are traditional kind of textbooks are discussing hierarchical planning from a discussing hierarchical planning from a point of from the conceptual kind of point of from the conceptual kind of point of view but i think it's worth point of view but i think it's worth while kind of presenting a use case with while kind of presenting a use case with here planning is very evident and this", "image_path": "img_data/video_29_chunk_17.jpg"}
{"video": "video_29", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "while kind of presenting a use case with here planning is very evident and this here planning is very evident and this is basically the self-driving kind of is basically the self-driving kind of car category you have at the very top car category you have at the very top route planning where you are determining route planning where you are determining some kind of a plan to go from a you some kind of a plan to go from a origin to know origin to destination let's say inside the city destination let's say inside the city then you have the u prediction then you have the u prediction module because evidently you're not module because evidently you're not really alone in the city you are really alone in the city you are there are other agents that there are other agents that are other cars or pedestrians whatever are other cars or pedestrians whatever have you this prediction module is going to", "image_path": "img_data/video_29_chunk_18.jpg"}
{"video": "video_29", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "have you this prediction module is going to this prediction module is going to predict their intentions given predict their intentions given information that is coming to you from information that is coming to you from the perception sub system the prediction the perception sub system the prediction module is predicting where these actors module is predicting where these actors are going next and based on that are going next and based on that predictions the behavioral planner is predictions the behavioral planner is effectively is implemented via what effectively is implemented via what is called a final state machine and it is called a final state machine and it is u planning for", "image_path": "img_data/video_29_chunk_19.jpg"}
{"video": "video_29", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "is called a final state machine and it is u planning for at a very fairly high level let's say at a very fairly high level let's say negotiate an intersection the i negotiate an intersection the i have let's say a traffic light in have let's say a traffic light in front of me a stop signal all of front of me a stop signal all of these are u expected patterns of plans these are u expected patterns of plans that are need to be determined and based that are need to be determined and based on those and the associated kind of on those and the associated kind of transitions that are occurring there transitions that are occurring there between let's say you have now between let's say you have now a green lights and then you have to a green lights and then you have to start you're coming to intersection you start you're coming to intersection you planning to turn left you use the", "image_path": "img_data/video_29_chunk_20.jpg"}
{"video": "video_29", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "start you're coming to intersection you planning to turn left you use the planning to turn left you use the prediction of the other actors to come prediction of the other actors to come up with the trajectory the optimal up with the trajectory the optimal trajectory where is the free space which trajectory where is the free space which i'm actually can go given the behavior i'm actually can go given the behavior which i need to comply to let's say left which i need to comply to let's say left turn on an intersection and that is turn on an intersection and that is basically what trajectory planning is basically what trajectory planning is actually doing is determining if you actually doing is determining if you the sequence of way points that the sequence of way points that are or points that are defined if you are or points that are defined if you this trajectory that the car can", "image_path": "img_data/video_29_chunk_21.jpg"}
{"video": "video_29", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "are or points that are defined if you this trajectory that the car can this trajectory that the car can actually move into that trajectory via actually move into that trajectory via the usage of a controller which follows the usage of a controller which follows the controller is effectively the controller is effectively implementing the trajectory plan implementing the trajectory plan that the planner kind of determined and that the planner kind of determined and that's basically how the vehicle is that's basically how the vehicle is actually going to go through and this actually going to go through and this discussion is sort of outlined in discussion is sort of outlined in this kind of page where you are actually be given page where you are actually be given some more information about each of some more information about each of these kind of subsystems arguably a", "image_path": "img_data/video_29_chunk_22.jpg"}
{"video": "video_29", "start": "0:11:30", "end": "0:11:52.933333", "timestamp": "0:11:30 - 0:11:52.933333", "text": "some more information about each of these kind of subsystems arguably a these kind of subsystems arguably a whole discussion about requires kind whole discussion about requires kind of a lecture of for each one of them of a lecture of for each one of them it's kind of outside of the scope it's kind of outside of the scope but i think it's worthwhile having if but i think it's worthwhile having if you a use case in your you a use case in your mind about hierarchy and how extensively mind about hierarchy and how extensively are used hierarchical planning in are used hierarchical planning in practice", "image_path": "img_data/video_29_chunk_23.jpg"}
{"video": "video_30", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in the pddl section in the corresponding video over there we have corresponding video over there we have actually seen how we can actually seen how we can actually express with this specific express with this specific representation language domain and a representation language domain and a problem and we have mentioned over there problem and we have mentioned over there that there are some kind of that there are some kind of underlying algorithms that actually take underlying algorithms that actually take these specifications and in a kind of a these specifications and in a kind of a domain independent way solve the domain independent way solve the problem for us define if you the problem for us define if you the optimal or some suboptimal plan optimal or some suboptimal plan that we're actually seeking to get", "image_path": "img_data/video_30_chunk_0.jpg"}
{"video": "video_30", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "optimal or some suboptimal plan that we're actually seeking to get that we're actually seeking to get in this video what we will do is we in this video what we will do is we will get to understand some first will get to understand some first principles behind this kind of principles behind this kind of algorithms which are based on what is algorithms which are based on what is actually called here forward search and actually called here forward search and forward search is really u the forward search is really u the most popular if you method most popular if you method definitely does not really meet definitely does not really meet every single problem in deps of every single problem in deps of finding the optimal solution for every finding the optimal solution for every single problem we can actually pose to single problem we can actually pose to it but it is able to help us out in a", "image_path": "img_data/video_30_chunk_1.jpg"}
{"video": "video_30", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "single problem we can actually pose to it but it is able to help us out in a it but it is able to help us out in a vast array of problems this is vast array of problems this is basically what we'll be focusing here basically what we'll be focusing here the first thing i wanted to mention the first thing i wanted to mention is that in trying to get is that in trying to get into to understand this kind of for into to understand this kind of for search methods many of you must have search methods many of you must have heard before about depth first breadth heard before about depth first breadth first or best first kind of search first or best first kind of search the dig stress kind of algorith and the dig stress kind of algorith and on but we will be dealing with on but we will be dealing with this we will be treating those", "image_path": "img_data/video_30_chunk_2.jpg"}
{"video": "video_30", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "on but we will be dealing with this we will be treating those this we will be treating those here just to make sure that everyone is here just to make sure that everyone is on the same page and the before we on the same page and the before we actually get into those i think it's actually get into those i think it's worthwhile mentioning that worthwhile mentioning that we need we the pddl had a we need we the pddl had a factor representation in terms of state factor representation in terms of state and most of the treatment here and most of the treatment here will assume some form of an atomic will assume some form of an atomic state and you can actually go from state and you can actually go from factor to atomic via a process that factor to atomic via a process that actually is called u grounding and", "image_path": "img_data/video_30_chunk_3.jpg"}
{"video": "video_30", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "factor to atomic via a process that actually is called u grounding and actually is called u grounding and you can actually create the an a you can actually create the an a graph based representation that it will graph based representation that it will allow us to solve the problem and this allow us to solve the problem and this is what we will do now the in is what we will do now the in terms of practice practical terms of practice practical applications we have u we applications we have u we in the back of our mind we actually can in the back of our mind we actually can bring up the application of let's say bring up the application of let's say route planning or let's say of a route planning or let's say of a vehicle in fact many self-driving car vehicle in fact many self-driving car vendors and manufacturers are you", "image_path": "img_data/video_30_chunk_4.jpg"}
{"video": "video_30", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "vehicle in fact many self-driving car vendors and manufacturers are you vendors and manufacturers are they do implement this type of know they do implement this type of representation from let's say representation from let's say you have a car which has a camera and you have a car which has a camera and lighter kind of sensors and that lighter kind of sensors and that transformation from that domain transformation from that domain into a problem where you can actually into a problem where you can actually pos it on a two-dimensional map is pos it on a two-dimensional map is very common here in the figure you very common here in the figure you can actually see how the", "image_path": "img_data/video_30_chunk_5.jpg"}
{"video": "video_30", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "very common here in the figure you can actually see how the can actually see how the lighter sensing which is evidently in lighter sensing which is evidently in the three-dimensional kind of space and the three-dimensional kind of space and the camera sensing also in the camera sensing also in the three-dimensional space ended up being a three-dimensional space ended up being a two-dimensional representation of a two-dimensional representation of a map and you can actually see here map and you can actually see here this map also has all the this map also has all the constraints let's say this map came from constraints let's say this map came from a parking lot and the areas over here a parking lot and the areas over here the yellow areas there are areas where the yellow areas there are areas where you can not really a goal but there", "image_path": "img_data/video_30_chunk_6.jpg"}
{"video": "video_30", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "the yellow areas there are areas where you can not really a goal but there you can not really a goal but there black areas are areas where the vehicle black areas are areas where the vehicle actually can move and therefore with actually can move and therefore with if you can imagine it separating this if you can imagine it separating this space into a grid and you can space into a grid and you can actually come up with something that actually come up with something that will actually look this and in will actually look this and in that specific map you have a that specific map you have a starting state and a goal state and your starting state and a goal state and your task is really to go from starting to task is really to go from starting to the gold state in some satisfy the gold state in some satisfy some kind of utility that utility in", "image_path": "img_data/video_30_chunk_7.jpg"}
{"video": "video_30", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "the gold state in some satisfy some kind of utility that utility in some kind of utility that utility in some instances it is associated with some instances it is associated with cost you're all familiar with cost you're all familiar with minimizing the trip time from going from minimizing the trip time from going from a point to some destination and using a point to some destination and using let's say your google maps or it let's say your google maps or it could be also be posed as a gain and could be also be posed as a gain and later on in some other videos we will later on in some other videos we will see that this gain could be associated see that this gain could be associated with some quantity that we call reward with some quantity that we call reward it's always a good to have it both", "image_path": "img_data/video_30_chunk_8.jpg"}
{"video": "video_30", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "with some quantity that we call reward it's always a good to have it both it's always a good to have it both ways in your mind in terms of the cost ways in your mind in terms of the cost from that kind of abstraction we will from that kind of abstraction we will be u going and creating if you a be u going and creating if you a next we create if you a graph next we create if you a graph and let's see how that is going to be and let's see how that is going to be done in general we're going to have the done in general we're going to have the following situation in all of these following situation in all of these problems we're dealing with is we're problems we're dealing with is we're going to have some kind of a node that going to have some kind of a node that represents the state that we are in represents the state that we are in right now let me call this s then there", "image_path": "img_data/video_30_chunk_9.jpg"}
{"video": "video_30", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "represents the state that we are in right now let me call this s then there right now let me call this s then there will be from that kind of state if will be from that kind of state if you remember the discussion about you remember the discussion about preconditions there will be preconditions there will be definitely some actions that we will be definitely some actions that we will be able to take and these actions are able to take and these actions are going to be denoted here with going to be denoted here with just solid circles that's a common thing to do in circles that's a common thing to do in mdp reinforcement learning kind of mdp reinforcement learning kind of community not in the planning community not in the planning community but we'll just adopt this", "image_path": "img_data/video_30_chunk_10.jpg"}
{"video": "video_30", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "community not in the planning community but we'll just adopt this community but we'll just adopt this notation these are basically all the notation these are basically all the possible actions we can take from the possible actions we can take from the state s and each actually action will state s and each actually action will actually lead us to a number of other actually lead us to a number of other states and this thing is also states and this thing is also called the sort of the branching degree called the sort of the branching degree how many branches are we going to how many branches are we going to be generating out of a an action be generating out of a an action that we're taking from a let's say a that we're taking from a let's say a state as you can understand there", "image_path": "img_data/video_30_chunk_11.jpg"}
{"video": "video_30", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "that we're taking from a let's say a state as you can understand there state as you can understand there is going to be a potentially is going to be a potentially exponential kind of explosion of then exponential kind of explosion of then a possible number of states that we a possible number of states that we have to visit in order to find if you have to visit in order to find if you that optimal kind of plan and this that optimal kind of plan and this will actually be a problem that we will actually be a problem that we will need to actually solve and the need to actually solve and the various algorith in their the one various algorith in their the one that are actually implementing in that are actually implementing in practice they do sol for us using practice they do sol for us using heuristics the problems", "image_path": "img_data/video_30_chunk_12.jpg"}
{"video": "video_30", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "practice they do sol for us using heuristics the problems heuristics the problems are such complex that without heuristics are such complex that without heuristics is almost impossible to find in a is almost impossible to find in a reasonable time solutions we will reasonable time solutions we will be calling this stat that we are be calling this stat that we are reaching from let's say a certain action reaching from let's say a certain action s prime and this is called the s prime and this is called the next state the prime stands for next state the prime stands for next state all right just to put state all right just to put some kind of u numbers if we had you some kind of u numbers if we had the branching let's say in general", "image_path": "img_data/video_30_chunk_13.jpg"}
{"video": "video_30", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "some kind of u numbers if we had you know the branching let's say in general know the branching let's say in general of n and our as you can imagine n and our as you can imagine there will be more and there will be more and more sort of actions and on that we more sort of actions and on that we will be able to re to have from a given be able to re to have from a given let's say state h double prime is the let's say state h double prime is the state that follows and the solution state that follows and the solution is let's say down d steps", "image_path": "img_data/video_30_chunk_14.jpg"}
{"video": "video_30", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "state that follows and the solution is let's say down d steps is let's say down d steps then in general we have n to then in general we have n to the power of d nodes in our search tree as you can understand the even with a simple blocks word kind of even with a simple blocks word kind of problem then we have an unbelievable problem then we have an unbelievable amounts of nodes that we have to amounts of nodes that we have to actually visit in order for us to actually visit in order for us to determine in the", "image_path": "img_data/video_30_chunk_15.jpg"}
{"video": "video_30", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "actually visit in order for us to determine in the solution to the problem that's solution to the problem that's basically the search tree that we will basically the search tree that we will be with us in general we will be dealing be with us in general we will be dealing with graphs and in most instances the graphs and in most instances the graphs we will be dealing with are going graphs we will be dealing with are going to be do not necessarily have loops to be do not necessarily have loops we'll actually also make that kind of we'll actually also make that kind of assumption and on the right hand side assumption and on the right hand side over here i'm going to quote a very over here i'm going to quote a very general forward search kind of algorith general forward search kind of algorith that is going to be the solution", "image_path": "img_data/video_30_chunk_16.jpg"}
{"video": "video_30", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "general forward search kind of algorith that is going to be the solution that is going to be the solution for searching and it the algorith is for searching and it the algorith is going to be in general but then it will going to be in general but then it will accommodate many of the well-known kind accommodate many of the well-known kind of search algorith such as the depth the of search algorith such as the depth the breadth and the dig stress kind of breadth and the dig stress kind of algorith and introduce what ends up algorith and introduce what ends up being called dynamic programming being called dynamic programming solution to these problems solution to these problems that dynamic programming is quite that dynamic programming is quite important to get it at the in this kind important to get it at the in this kind of video because it will be used u of video because it will be used u in a subsequent video when we're dealing", "image_path": "img_data/video_30_chunk_17.jpg"}
{"video": "video_30", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "of video because it will be used u in a subsequent video when we're dealing in a subsequent video when we're dealing with markco decision processes the with markco decision processes the algorithm goes as follows we have algorithm goes as follows we have some inputs this inputs will as we some inputs this inputs will as we discussed are is going to be discussed are is going to be called the instead of an s and s prime called the instead of an s and s prime let me call it s the initial i for let me call it s the initial i for initial and sg for the goal initial and sg for the goal state then we define the state then we define the first step is we need to first step the first step is we need to have a data structure that will be", "image_path": "img_data/video_30_chunk_18.jpg"}
{"video": "video_30", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "first step the first step is we need to have a data structure that will be have a data structure that will be calling priority q the priority q and calling priority q the priority q and we understand i mean everyone is we understand i mean everyone is familiar with the concept of a q but familiar with the concept of a q but what is called priority q is going to be what is called priority q is going to be discussed now priority q we will call it q and this priority q will have two methods that we will have two methods that we are of interest to one the first one is are of interest to one the first one is the -cal insert method and we will insert add this point method and we will insert add this point the initial kind of", "image_path": "img_data/video_30_chunk_19.jpg"}
{"video": "video_30", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "method and we will insert add this point the initial kind of state and we also going to have in state and we also going to have in this kind of q this insert method this kind of q this insert method will effectively mean that this state will effectively mean that this state that we are inserting is visited that we are inserting is visited let me write this over here let me write this over here mark si as visited and the second step is a y", "image_path": "img_data/video_30_chunk_20.jpg"}
{"video": "video_30", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "visited and the second step is a y loop and this y loop is while the loop and this y loop is while the q is not empty we have now the sort of let me call it sort of let me call it a the step over here is that we a the step over here is that we are producing an next state are producing an next state let's call it s in", "image_path": "img_data/video_30_chunk_21.jpg"}
{"video": "video_30", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "are producing an next state let's call it s in general from the picking up from the q the highest priority element and the highest priority element and the que is not first in first out the que is not first in first out as it is normally the case from as it is normally the case from as you can of line up in a store to let's you can of line up in a store to let's say to buy coffee but it is can have say to buy coffee but it is can have a policy which is quite different", "image_path": "img_data/video_30_chunk_22.jpg"}
{"video": "video_30", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "say to buy coffee but it is can have a policy which is quite different a policy which is quite different than fiveo and in general we call than fiveo and in general we call this in this get first whatever that this in this get first whatever that policy is we are effectively retrieving policy is we are effectively retrieving the highest priority element out of or the highest priority element out of or state out of that q we are getting or state out of that q we are getting into the next state out of that kind of into the next state out of that kind of method and if it happens that s is belongs to the", "image_path": "img_data/video_30_chunk_23.jpg"}
{"video": "video_30", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "if it happens that s is belongs to the happens that s is belongs to the set let me call it capital s of goal set let me call it capital s of goal states the set of goal state is s states the set of goal state is s capital s g then we can capital s g then we can return that we have a return that we have a success and we have a plan in our hands success and we have a plan in our hands and but if not if that top highest but if not if that top highest priority element is not one of the gold priority element is not one of the gold states", "image_path": "img_data/video_30_chunk_24.jpg"}
{"video": "video_30", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "priority element is not one of the gold states we have here the third we are returning and the and third we are returning and the for search kind of the and the for search kind of terminates but if we are this terminates but if we are this condition is not satisfied then for that condition is not satisfied then for that s what we will do is we have a for s what we will do is we have a for loop for a for the action to belongs to loop for a for the action to belongs to the set of actions that can be taken out of that actions that can be taken out of that kind of state s the set of actions", "image_path": "img_data/video_30_chunk_25.jpg"}
{"video": "video_30", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "actions that can be taken out of that kind of state s the set of actions kind of state s the set of actions that can be taken out of that state s is that can be taken out of that state s is these guys over here 1 2 3 in this case these guys over here 1 2 3 in this case for each one of them we are getting that next state s prime out of some form of a transition model out of some form of a transition model we have seen this transition model u we have seen this transition model u being quoted when we did ban being quoted when we did ban filtering in that kind of basian filter", "image_path": "img_data/video_30_chunk_26.jpg"}
{"video": "video_30", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "being quoted when we did ban filtering in that kind of basian filter filtering in that kind of basian filter or a cursive state estimation u or a cursive state estimation u algorithm when we discuss in that kind algorithm when we discuss in that kind of video we have seen this kind of form of video we have seen this kind of form of a transition model we call this of a transition model we call this transition model f someone is transition model f someone is determining for us where we actually are determining for us where we actually are going next what state are we going to going next what state are we going to hit next for that specific action if we hit next for that specific action if we are in state s and if s prime is not", "image_path": "img_data/video_30_chunk_27.jpg"}
{"video": "video_30", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "s if s prime is not visited then we mark as prime as visited in fact we call it as visited or explored in your notes it is or explored in your notes it is explored let me write this down as explored and we are inserting this into the cu with a usual", "image_path": "img_data/video_30_chunk_28.jpg"}
{"video": "video_30", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "this into the cu with a usual insert method that we have also quoted over here and also quoted over here and else we have a method that's called resolve duplicate aspr we have in general three types", "image_path": "img_data/video_30_chunk_29.jpg"}
{"video": "video_30", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "duplicate aspr we have in general three types aspr we have in general three types of states in the -called forward search the first one is the -called unvisited states in the unvisited states are such as the starting state that we just as the starting state that we just started the evidence these are states started the evidence these are states that we have not really visited", "image_path": "img_data/video_30_chunk_30.jpg"}
{"video": "video_30", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "started the evidence these are states that we have not really visited that we have not really visited yet in at the when the initial the yet in at the when the initial the algor kind of starts initially all of algor kind of starts initially all of the states except from that initial the states except from that initial state is in the unvisited if you state is in the unvisited if you set the second kind of category is set the second kind of category is states that we have a -called u states that we have a -called u dead states these are dead end states", "image_path": "img_data/video_30_chunk_31.jpg"}
{"video": "video_30", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "states these are dead end states the these are the states that have the these are the states that have been visited and for which let me write visited and for which let me write it down visited and all states that can be reached via actions obviously from those", "image_path": "img_data/video_30_chunk_32.jpg"}
{"video": "video_30", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "states that can be reached via actions obviously from those are also visited there is no you in other words they call them dead because words they call them dead because they do not they cannot really they do not they cannot really contribute anything to the search kind contribute anything to the search kind of procedure and this is the third of procedure and this is the third category as you can understand we call category as you can understand we call them the -call alive states alive and this are the states that they have", "image_path": "img_data/video_30_chunk_33.jpg"}
{"video": "video_30", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "states alive and this are the states that they have potentially encountered we have potentially encountered we have visited them but we there are states visited them but we there are states that we have not visited for that we have not visited for those nodes for those states those nodes for those states the therefore it we keep the therefore it we keep them in this kind of list because we them in this kind of list because we would to we call them alive would to we call them alive because we would to do because we would to do something with those nodes", "image_path": "img_data/video_30_chunk_34.jpg"}
{"video": "video_30", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "because we would to do something with those nodes something with those nodes the only alive state initially the only alive state initially is the starting state and these are the states that we store in priority qq i hope everything is clear now as to qq i hope everything is clear now as to what are the things that we need what are the things that we need in order for us to implement the for in order for us to implement the for search but i think i forgot to mention search but i think i forgot to mention something about this line here this", "image_path": "img_data/video_30_chunk_35.jpg"}
{"video": "video_30", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "search but i think i forgot to mention something about this line here this something about this line here this resolve w s prime means that resolve w s prime means that when s prime has been visited before we may need to resolve and some algorithms we have by", "image_path": "img_data/video_30_chunk_36.jpg"}
{"video": "video_30", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "to resolve and some algorithms we have by and some algorithms we have by resolve we mean some kind of an update resolve we mean some kind of an update resolve update some metric let's say the cost associated with the state s prime associated with s prime that is really the job of that kind of last really the job of that kind of last line of that kind of for ass search kind", "image_path": "img_data/video_30_chunk_37.jpg"}
{"video": "video_30", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "really the job of that kind of last line of that kind of for ass search kind line of that kind of for ass search kind of algorithm all right now of algorithm all right now that we have if you the structure that we have if you the structure then we will actually can go and see then we will actually can go and see what kind of policies we can implement what kind of policies we can implement priority policies we can implement priority policies we can implement inside that kind of q and how the inside that kind of q and how the algorith that we have been sort of algorith that we have been sort of known in the past depth and breadth known in the past depth and breadth search first search are kind of search first search are kind of derived out of that kind of general derived out of that kind of general algorithm let's look at depth first search", "image_path": "img_data/video_30_chunk_38.jpg"}
{"video": "video_30", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "let's look at depth first search and the specific instance of this kind and the specific instance of this kind of priority queue there let's of priority queue there let's assume that we have if you a assume that we have if you a problem let me give you a problem let me give you a problem this is a graph let's say the states this is a graph let's say the states are just capital letters are just capital letters here and you can understand how here and you can understand how this also could be associated later this also could be associated later we'll see some kind of a minimum we'll see some kind of a minimum cost type of algorithms as well right cost type of algorithms as well right now the edges are not", "image_path": "img_data/video_30_chunk_39.jpg"}
{"video": "video_30", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "cost type of algorithms as well right now the edges are not weighted this is the input kind of graph and we would to actually graph and we would to actually get into the following sort of get into the following sort of go what is really the starting state go what is really the starting state is a state s that the starting state is a state s that is our starting state si and the goal is our starting state si and the goal state is let's say the gold state is let's say the gold state is c", "image_path": "img_data/video_30_chunk_40.jpg"}
{"video": "video_30", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "state is let's say the gold state is c let's look now some kind of specific policies and what kind of names we can policies and what kind of names we can associate with them for example the associate with them for example the policy lifo or last in first out policy lifo or last in first out corresponds to what we call the depth corresponds to what we call the depth first search which i'm sure you have first search which i'm sure you have heard before dfs here we have a small heard before dfs here we have a small problem we have a graph we have some problem we have a graph we have some kind of initial state s and some goal kind of initial state s and some goal state c and what we actually have done state c and what we actually have done over here is we actually drew a table", "image_path": "img_data/video_30_chunk_41.jpg"}
{"video": "video_30", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "state c and what we actually have done over here is we actually drew a table over here is we actually drew a table that in one column we have the order that in one column we have the order of visitation what kind of another world visitation what kind of another world state we're going next and in what order state we're going next and in what order and the other is the contents of the and the other is the contents of the priority queue that we have seen and priority queue that we have seen and starting from the starting and starting from the starting state s we are visiting s and we in state s we are visiting s and we in the priority que we are storing all the priority que we are storing all the notes of the graph which are adjacent", "image_path": "img_data/video_30_chunk_42.jpg"}
{"video": "video_30", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "the priority que we are storing all the notes of the graph which are adjacent notes of the graph which are adjacent to s we have effectively nodes e to s we have effectively nodes e d c and a that we have actually storing d c and a that we have actually storing into that kind of que and given if you into that kind of que and given if you some kind of policy lifeo let's some kind of policy lifeo let's assume that a was the last state that we assume that a was the last state that we have stored then we can we will go and have stored then we can we will go and visit that first because that is visit that first because that is basically the state that the priority q basically the state that the priority q returns to us the soal get first method returns to us the soal get first method and for that specific state a we are and for that specific state a we are effectively determining over here the", "image_path": "img_data/video_30_chunk_43.jpg"}
{"video": "video_30", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "and for that specific state a we are effectively determining over here the effectively determining over here the u we are adding the into the u we are adding the into the priority q the nodes which are adjacent priority q the nodes which are adjacent to a and we have the only node to a and we have the only node that is adjacent to a that is not really that is adjacent to a that is not really the s that we have already visited is the s that we have already visited is b we add this into the priority q and b we add this into the priority q and given again the life of kind of given again the life of kind of policy b is returned and therefore we policy b is returned and therefore we are storing effectively nothing into", "image_path": "img_data/video_30_chunk_44.jpg"}
{"video": "video_30", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "policy b is returned and therefore we are storing effectively nothing into are storing effectively nothing into the priority que we're not really adding the priority que we're not really adding into the priority que because c is into the priority que because c is already included and the next lifo already included and the next lifo method is going to return c and method is going to return c and the last element over here that we added the last element over here that we added is going to be visited is going to be visited next and for c is are actually also next and for c is are actually also our goal state and the algorith kind our goal state and the algorith kind of stops there this is of stops there this is basically", "image_path": "img_data/video_30_chunk_45.jpg"}
{"video": "video_30", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "of stops there this is basically sabc is really the plan that sabc is really the plan that results out of this kind of exercise results out of this kind of exercise however obviously you can understand however obviously you can understand this plan is suboptimal since already we this plan is suboptimal since already we could go from s to c directly could go from s to c directly that's basically the exercise in that's basically the exercise in terms of a search stre i have drawn also terms of a search stre i have drawn also here the search tre as we discussed here the search tre as we discussed we are starting from this node s we are starting from this node s and therefore we can actually go with and therefore we can actually go with one with cost one to a c d and e", "image_path": "img_data/video_30_chunk_46.jpg"}
{"video": "video_30", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "and therefore we can actually go with one with cost one to a c d and e one with cost one to a c d and e the sort of depth first search will the sort of depth first search will stop when either the plan is found if stop when either the plan is found if we are just have a query such as find we are just have a query such as find all of the nodes which are sort of all of the nodes which are sort of adjacent or reachable from the starting adjacent or reachable from the starting state from a state test if that is the state from a state test if that is the query and it's not necessarily we query and it's not necessarily we don't have a specific goal state then don't have a specific goal state then the algorith will actually continue and the algorith will actually continue and until the priority q is empty mind", "image_path": "img_data/video_30_chunk_47.jpg"}
{"video": "video_30", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "the algorith will actually continue and until the priority q is empty mind until the priority q is empty mind you in the literature you will see also you in the literature you will see also this search three being u produced we do not expand c because that's our goal state or e because we that's our goal state or e because we already have visited that e when already have visited that e when we exploring d that is basically we exploring d that is basically our search three and as you can our search three and as you can understand the depth first sech understand the depth first sech that's where the name came from favors that's where the name came from favors deep explorations from the get-go and as compared to", "image_path": "img_data/video_30_chunk_48.jpg"}
{"video": "video_30", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "deep explorations from the get-go and as compared to from the get-go and as compared to another method we'll see now called another method we'll see now called bread first search where effectively bread first search where effectively the policy there is f4 looking at the policy there is f4 looking at the equivalent if you sort of the equivalent if you sort of table we can actually see that yes table we can actually see that yes we have the -called a c d and e but here the pol in the kind of a", "image_path": "img_data/video_30_chunk_49.jpg"}
{"video": "video_30", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "c d and e but here the pol in the kind of a e but here the pol in the kind of a fifo policy what we do is slightly fifo policy what we do is slightly different for us we're going to write different for us we're going to write now the a c d e as we have seen it now the a c d e as we have seen it earlier also and then if this was earlier also and then if this was a lifeo and a was resulting out of it a lifeo and a was resulting out of it then the first element here is let's say then the first element here is let's say e and we can actually visit e and", "image_path": "img_data/video_30_chunk_50.jpg"}
{"video": "video_30", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "then the first element here is let's say e and we can actually visit e and we can actually visit e and as far as e is concerned then we as far as e is concerned then we have nothing to add it is a c have nothing to add it is a c d all right then we will go d all right then we will go into d and in this d we have nothing to add also we have a c add also we have a c and then we will go into c and c will", "image_path": "img_data/video_30_chunk_51.jpg"}
{"video": "video_30", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "add also we have a c and then we will go into c and c will and then we will go into c and c will be the goal state and the and this be the goal state and the and this algorith will actually terminate mind algorith will actually terminate mind you i want to mention that the order of you i want to mention that the order of visitation does not mean necessarily visitation does not mean necessarily that there is also a plan we it just that there is also a plan we it just happened that the plan was also happened that the plan was also according to the order of visitation according to the order of visitation over here but in order to get into the over here but in order to get into the plan what you need to do is you also plan what you need to do is you also need to store the parent of every need to store the parent of every know that you are actually visiting and know that you are actually visiting and evaluating and therefore out of the that evaluating and therefore out of the that kind of parent relationship then we will", "image_path": "img_data/video_30_chunk_52.jpg"}
{"video": "video_30", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "evaluating and therefore out of the that kind of parent relationship then we will kind of parent relationship then we will get the c has a parent of s has a parent get the c has a parent of s has a parent s and therefore that is really the plan s and therefore that is really the plan in the bread first search the plan in the bread first search is very straightforward to get just when is very straightforward to get just when it because the c is within one it because the c is within one expansion from s now again if we are expansion from s now again if we are to draw the whole tree kind of how it to draw the whole tree kind of how it is going to be transversed then is going to be transversed then apparently if the alon kind of continues apparently if the alon kind of continues over here then we will be visiting also over here then we will be visiting also a and then a has a node that has not", "image_path": "img_data/video_30_chunk_53.jpg"}
{"video": "video_30", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "over here then we will be visiting also a and then a has a node that has not a and then a has a node that has not been inserted in the q yet therefore a been inserted in the q yet therefore a we will also insert here the b and we will also insert here the b and that's basically the call tre that's basically the call tre transversal of the bfs algorithm transversal of the bfs algorithm up to this point we have seen a couple up to this point we have seen a couple of algorithms and now the third one of algorithms and now the third one which is actually quite important as which is actually quite important as it's a special case of an algorith it's a special case of an algorith called dynamic programming this is actually called programming this is actually called the well-known dick stras", "image_path": "img_data/video_30_chunk_54.jpg"}
{"video": "video_30", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "programming this is actually called the well-known dick stras algorith and let me call it as special version of dynamic programming that we version of dynamic programming that we will be also be a discussing in will be also be a discussing in mark of decision processes to motivate this kind of processes to motivate this kind of dynamic programming or shorter path dynamic programming or shorter path algorith and is sufficient algorith and is sufficient to actually obser from the previous kind to actually obser from the previous kind of discussion that in many cases that", "image_path": "img_data/video_30_chunk_55.jpg"}
{"video": "video_30", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "to actually obser from the previous kind of discussion that in many cases that of discussion that in many cases that kind of priority c let's say a lifeo did kind of priority c let's say a lifeo did not result into an optimal kind of path not result into an optimal kind of path and maybe the bre first search also and maybe the bre first search also didn't it's just that in this didn't it's just that in this specific case where the graph was too specific case where the graph was too simple did dias though is has simple did dias though is has been shown that it is going to find been shown that it is going to find let's say the optimal paths let's say the optimal paths throughout if you a graph and throughout if you a graph and that's why it is a very i would call it that's why it is a very i would call it popular algorithm in many of these popular algorithm in many of these settings that we are dealing with to", "image_path": "img_data/video_30_chunk_56.jpg"}
{"video": "video_30", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "popular algorithm in many of these settings that we are dealing with to settings that we are dealing with to motivate that kind of algorithm we will motivate that kind of algorithm we will be dealing with a graph which is be dealing with a graph which is slightly different this graph will actually different this graph will actually also have edges which are i will also have edges which are i will call it directional also and also call it directional also and also have a cost associated with it let me have a cost associated with it let me just draw the graph over here i have just draw the graph over here i have this kind of a starting state this kind of a starting state s we have a with cost", "image_path": "img_data/video_30_chunk_57.jpg"}
{"video": "video_30", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "s we have a with cost one a b is from a with cost six then we one a b is from a with cost six then we have some target or goal node which is some target or goal node which is has a cost of two from b to has a cost of two from b to e this is the graph over here", "image_path": "img_data/video_30_chunk_58.jpg"}
{"video": "video_30", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "has a cost of two from b to e this is the graph over here this is the problem statement here is calculate the minimum cost path", "image_path": "img_data/video_30_chunk_59.jpg"}
{"video": "video_30", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "calculate the minimum cost path from s to all other states that's one kind of problem statement that we need to problem statement that we need to solve here and one we have solve here and one we have the graph has this vert", "image_path": "img_data/video_30_chunk_60.jpg"}
{"video": "video_30", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "solve here and one we have the graph has this vert is this is s c a b d and e and obviously it has some kind of e and obviously it has some kind of edges the first thing that we are edges the first thing that we are doing is to linear linearize the what is called d ed a cyclic", "image_path": "img_data/video_30_chunk_61.jpg"}
{"video": "video_30", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "linearize the what is called d ed a cyclic the what is called d ed a cyclic graph probably notice we have no loops graph probably notice we have no loops over here we have a directed a cyclic over here we have a directed a cyclic graph we linearize it in a sense that graph we linearize it in a sense that we are going to create a new graph that we are going to create a new graph that is going to have only edges that are is going to have only edges that are moving from left to right the moving from left to right the graph is this s c a [music]", "image_path": "img_data/video_30_chunk_62.jpg"}
{"video": "video_30", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "c a [music] [music] b d e and from b to e we have one more d e and from b to e we have one more edge we have from c to d one more edge we have from c to d one more edge and we have from s to way another edge and we have from s to way another edge this one has cost three cost this one has cost three cost one first two and this is 2 4", "image_path": "img_data/video_30_chunk_63.jpg"}
{"video": "video_30", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "one first two and this is 2 4 six and these guys are one the in this kind of linearization exercise that we have linearization exercise that we have just done we got if you a graph just done we got if you a graph that is sort of going to be far more that is sort of going to be far more helpful than the graph over here to come helpful than the graph over here to come up with a an optimal if you scheme up with a an optimal if you scheme to solve this kind of problem what we to solve this kind of problem what we are actually going to do is we will are actually going to do is we will be designating as u all costs with the", "image_path": "img_data/video_30_chunk_64.jpg"}
{"video": "video_30", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "be designating as u all costs with the designating as u all costs with the capital c to initial the very capital c to initial the very first step is to initialize all costs with the letter that designated with the letter c and this is from cost with the letter c and this is from cost from state s to all other states as", "image_path": "img_data/video_30_chunk_65.jpg"}
{"video": "video_30", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "with the letter c and this is from cost from state s to all other states as from state s to all other states as zero and evidently the let's say css is evidently the let's say css is equal to zero and then the second stage is for all it's a for loop for all it's a for loop for all vertic is v that belongs to the set all vertic is v that belongs to the set of vertices capital", "image_path": "img_data/video_30_chunk_66.jpg"}
{"video": "video_30", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "all vertic is v that belongs to the set of vertices capital v except from the starting vertex s this is back slash kly s this is back slash kly brackets s means except from s brackets s means except from s in the linearized order calculate the following", "image_path": "img_data/video_30_chunk_67.jpg"}
{"video": "video_30", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "calculate the following equation g not g c sv where v is this vertex that c sv where v is this vertex that we have in the for loop calculate this as a minimum of u comma v which are the set of", "image_path": "img_data/video_30_chunk_68.jpg"}
{"video": "video_30", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "u comma v which are the set of which are the set of vertices that the pair of vertices that the pair of vertices which i need to define which are belong to the set of define which are belong to the set of edges i forgot that there's also a set edges i forgot that there's also a set of edges over here with all the edges of edges over here with all the edges that we have in the graph for all the that we have in the graph for all the edges if you we take the minimum edges if you we take the minimum of quantities that are associated as the quantities that are associated as the cost let me call that cost let me call that edge", "image_path": "img_data/video_30_chunk_69.jpg"}
{"video": "video_30", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "cost let me call that edge cost plus total cost until node u total cost until node u let u total cost until node u let me replace them with some kind of let me replace them with some kind of variables we can associate a bit better the we can associate a bit better the equations u comma", "image_path": "img_data/video_30_chunk_70.jpg"}
{"video": "video_30", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "we can associate a bit better the equations u comma v belongs to edge set of edges the edge cost i will edge set of edges the edge cost i will call it a c uv and the total cost i'll call it the c uv and the total cost i'll call it the c suu until the node u i will call it as suu until the node u i will call it as csu and this formula makes a complete csu and this formula makes a complete intuitive sense to me as it basically intuitive sense to me as it basically says to us that the cost from s says to us that the cost from s to v let's say the cost from s let's say", "image_path": "img_data/video_30_chunk_71.jpg"}
{"video": "video_30", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "says to us that the cost from s to v let's say the cost from s let's say to v let's say the cost from s let's say to d is the minimum of the cost from s to d is the minimum of the cost from s to b plus the edge cost is the minimum of b plus the edge cost is the minimum of the total cost until we reach the node b plus cost until we reach the node b plus whatever that cost is plus the edge whatever that cost is plus the edge from b to d this is the minimum from b to d this is the minimum over all of", "image_path": "img_data/video_30_chunk_72.jpg"}
{"video": "video_30", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "from b to d this is the minimum over all of the minimum over all of this will actually be determined this will actually be determined now step by step we will start from a now step by step we will start from a problem that we already know the problem that we already know the solution and the problem we know solution and the problem we know solution is this one we can just solution is this one we can just repeat it here css is equal to zero and the next node over is the zero and the next node over is the c s c which according to this kind of c s c which according to this kind of formula is the", "image_path": "img_data/video_30_chunk_73.jpg"}
{"video": "video_30", "start": "0:37:00", "end": "0:37:30", "timestamp": "0:37:00 - 0:37:30", "text": "formula is the minimum it's there not really a minimum it's there not really a choice here because there's only one choice here because there's only one edge that goes from s to c of what edge that goes from s to c of what we have here as just we have here as just c css plus let me actually write the edge since i wrote the edge first in the since i wrote the edge first in the equation it is", "image_path": "img_data/video_30_chunk_74.jpg"}
{"video": "video_30", "start": "0:37:30", "end": "0:38:00", "timestamp": "0:37:30 - 0:38:00", "text": "equation it is csc plus css there only one term here this is basically two all right because css is zero two all right because css is zero and then we have the other note which is and then we have the other note which is the cost to a csa is the minimum of the edge from c to", "image_path": "img_data/video_30_chunk_75.jpg"}
{"video": "video_30", "start": "0:38:00", "end": "0:38:30", "timestamp": "0:38:00 - 0:38:30", "text": "minimum of the edge from c to a plus the c sc comma the other one is there's a comma the other one is there's a direct path from s to a this is direct path from s to a this is basically c sa", "image_path": "img_data/video_30_chunk_76.jpg"}
{"video": "video_30", "start": "0:38:30", "end": "0:39:00", "timestamp": "0:38:30 - 0:39:00", "text": "basically c sa a plus c ss we have here the course c ca is a four the sc is two this is minimum and then the other one is one minimum and then the other one is one and therefore the answer is one and the same thing i can actually go", "image_path": "img_data/video_30_chunk_77.jpg"}
{"video": "video_30", "start": "0:39:00", "end": "0:39:30", "timestamp": "0:39:00 - 0:39:30", "text": "and the same thing i can actually go into repeat this exercise that we have into repeat this exercise that we have done for all other notes let me write done for all other notes let me write one more this is b and as you can understand we are b and as you can understand we are actually reusing earlier results actually reusing earlier results that's the sort of value of dynamic that's the sort of value of dynamic programming if you to reuse earli programming if you to reuse earli results that we have obtained in other results that we have obtained in other words we are converting if you a words we are converting if you a global problem of calculating the", "image_path": "img_data/video_30_chunk_78.jpg"}
{"video": "video_30", "start": "0:39:30", "end": "0:40:00", "timestamp": "0:39:30 - 0:40:00", "text": "words we are converting if you a global problem of calculating the global problem of calculating the minimum cost let's say path from m to minimum cost let's say path from m to other nodes into a sort of many other nodes into a sort of many local problems that we actually know local problems that we actually know the solution either we know the solution either we know the solution already or we can it's very easy or already or we can it's very easy or trivial to calculate the solution and trivial to calculate the solution and then we are gradually expanding into then we are gradually expanding into more and more complicated problems as we more and more complicated problems as we move along the this linearized kind of move along the this linearized kind of graph reusing early results we have", "image_path": "img_data/video_30_chunk_79.jpg"}
{"video": "video_30", "start": "0:40:00", "end": "0:40:30", "timestamp": "0:40:00 - 0:40:30", "text": "move along the this linearized kind of graph reusing early results we have graph reusing early results we have seen this reuse over seen this reuse over here the two was reused over here the two was reused over here without really having to solve anything without really having to solve anything else we can just replace it and else we can just replace it and now we will see it also over here this now we will see it also over here this is the edge from a to b plus the cost from sa there's no nothing else going into b sa there's no nothing else going into b this is now that we have that is the", "image_path": "img_data/video_30_chunk_80.jpg"}
{"video": "video_30", "start": "0:40:30", "end": "0:41:00", "timestamp": "0:40:30 - 0:41:00", "text": "sa there's no nothing else going into b this is now that we have that is the this is now that we have that is the minimum of a and b the a and b a to b minimum of a and b the a and b a to b is 6 plus one is the final answer is seven and on we can actually continue to calculate all of actually continue to calculate all of the costs associated with it and the costs associated with it and reach if you nodes which are fairly reach if you nodes which are fairly far away from s by effectively", "image_path": "img_data/video_30_chunk_81.jpg"}
{"video": "video_30", "start": "0:41:00", "end": "0:41:30", "timestamp": "0:41:00 - 0:41:30", "text": "reach if you nodes which are fairly far away from s by effectively far away from s by effectively calculating the cost to calculating the cost to d the csd in other words and then d the csd in other words and then adding if you the one over here adding if you the one over here which is the edge and then and which is the edge and then and that compare that and taking the minimum that compare that and taking the minimum between the csb and adding that edge of two and csb and adding that edge of two and which one is going to win that kind of which one is going to win that kind of minimum comparison also will give us the minimum comparison also will give us the c sc at the end of the day c sc at the end of the day this is exactly identical to", "image_path": "img_data/video_30_chunk_82.jpg"}
{"video": "video_30", "start": "0:41:30", "end": "0:42:00", "timestamp": "0:41:30 - 0:42:00", "text": "c sc at the end of the day this is exactly identical to this is exactly identical to what all the calculations we have done what all the calculations we have done earlier how this is related to what earlier how this is related to what we have met before this priority q and we have met before this priority q and the selection of the and the election of the selection of the and the election of the new states s primes out of the new states s primes out of prioritization of the queue well it is prioritization of the queue well it is related as follows the priority metric related as follows the priority metric in the que is the pass cost as it's in the que is the pass cost as it's called or cost from the beginning of the called or cost from the beginning of the initial start starting state if you initial start starting state if you to the specific node that we have", "image_path": "img_data/video_30_chunk_83.jpg"}
{"video": "video_30", "start": "0:42:00", "end": "0:42:30", "timestamp": "0:42:00 - 0:42:30", "text": "initial start starting state if you to the specific node that we have to the specific node that we have there in the graph we are actually there in the graph we are actually prioritizing the q and we are ranking if prioritizing the q and we are ranking if you or sorting the elements of the you or sorting the elements of the que which are effectively nodes in the que which are effectively nodes in the graph according to this metric over here graph according to this metric over here we're always effectively picking up we're always effectively picking up from the queue the one which has the from the queue the one which has the minimum possible cost from the minimum possible cost from the beginning of the problem until the point beginning of the problem until the point that we are right now and this is that we are right now and this is basically what we are sort of mark basically what we are sort of mark them as visited nodes and then how we", "image_path": "img_data/video_30_chunk_84.jpg"}
{"video": "video_30", "start": "0:42:30", "end": "0:43:00", "timestamp": "0:42:30 - 0:43:00", "text": "basically what we are sort of mark them as visited nodes and then how we them as visited nodes and then how we actually proceeding is we're proceeding actually proceeding is we're proceeding in a very similar way to that general in a very similar way to that general flow that we have seen earlier on flow that we have seen earlier on in this discussion that's how the in this discussion that's how the dix kind of algorithm with the priority dix kind of algorithm with the priority q are kind of connected in fact it's q are kind of connected in fact it's the priority q is really the priority q is really the implementation of the tix algorith where implementation of the tix algorith where priority is the that kind of a pass priority is the that kind of a pass cost as we see in a moment there cost as we see in a moment there are", "image_path": "img_data/video_30_chunk_85.jpg"}
{"video": "video_30", "start": "0:43:00", "end": "0:43:28.266667", "timestamp": "0:43:00 - 0:43:28.266667", "text": "cost as we see in a moment there are two things that we should be two things that we should be concerned one is the short go of the concerned one is the short go of the pass cost and we have versions of the pass cost and we have versions of the algorithms that we actually be calling algorithms that we actually be calling acr that are actually dealing with acr that are actually dealing with estimates of future costs in other words estimates of future costs in other words estimates from the note that we are estimates from the note that we are right now to the sort of let's say right now to the sort of let's say a goal node if we have that in our a goal node if we have that in our problem and that's what we will be problem and that's what we will be dealing with right now", "image_path": "img_data/video_30_chunk_86.jpg"}
{"video": "video_31", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "according to the planning domain a definition language or pddl we need actually to specify pddl we need actually to specify two files the first one was the two files the first one was the domain model which is going to be discussed model which is going to be discussed as part of a file c", "image_path": "img_data/video_31_chunk_0.jpg"}
{"video": "video_31", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "model which is going to be discussed as part of a file c domain to pddl and the other is the -called pddl and the other is the -called problem which is descri problem which is descri describes the certain things that are describes the certain things that are associated with the specific instance of associated with the specific instance of that of the problem we try to solve that of the problem we try to solve this one is the general description this one is the general description of a domain and the other is the of a domain and the other is the specific instance of that planning specific instance of that planning problem we are facing as we said we problem we are facing as we said we are going to be discuss discing a", "image_path": "img_data/video_31_chunk_1.jpg"}
{"video": "video_31", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "problem we are facing as we said we are going to be discuss discing a are going to be discuss discing a domain initially it was presented to be domain initially it was presented to be a domain of logistics for this a domain of logistics for this specific discussion we have here we need specific discussion we have here we need something which is a bit simpler something which is a bit simpler we'll be using the -called blocks word because it's actually simpler to understand what is happening over understand what is happening over there and to have too many object there and to have too many object types involved in this kind of plan types involved in this kind of plan blocks word is going to involve a number blocks word is going to involve a number of it's actually one of the classical", "image_path": "img_data/video_31_chunk_2.jpg"}
{"video": "video_31", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "blocks word is going to involve a number of it's actually one of the classical of it's actually one of the classical examples in planning and it will involve examples in planning and it will involve a number of blocks and how we arrange a number of blocks and how we arrange them on a table that was them on a table that was basically the domain and basically the domain and as far as the domain now is as far as the domain now is concerned we actually have you concerned we actually have a couple of things that we need to know a couple of things that we need to mention here the domain is involving the", "image_path": "img_data/video_31_chunk_3.jpg"}
{"video": "video_31", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "domain is involving the specification of types these are obviously the usual types these are obviously the usual types with we familiar with the types with we familiar with the equivalent of classes if you in equivalent of classes if you in object oriented design let me just write it design let me just write it down we need to introduce predicates that effectively", "image_path": "img_data/video_31_chunk_4.jpg"}
{"video": "video_31", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "predicates that effectively state something about the subject of a sentence generically that's something we will introduce a bit know something we will introduce a bit more formally in a moment and then we more formally in a moment and then we have the -cal", "image_path": "img_data/video_31_chunk_5.jpg"}
{"video": "video_31", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "more formally in a moment and then we have the -cal actions schemas also known as operators and actually we call them action schemas because they are they because they are they describes they describe actions fairly describes they describe actions fairly generically that's why we call it generically that's why we call it schemas the schema consist schemas the schema consist of the name of the", "image_path": "img_data/video_31_chunk_6.jpg"}
{"video": "video_31", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "schemas the schema consist of the name of the action the name of the action and action the name of the action and parameters action name and with some kind of parameters also known as variables and this will actually be the signature that we will see this signature that we will see this action scheme has been", "image_path": "img_data/video_31_chunk_7.jpg"}
{"video": "video_31", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "signature that we will see this action scheme has been action scheme has been discussed and of course it will be discussed and of course it will be introduced a bit more formally in a introduced a bit more formally in a moment because we have certain moment because we have certain constraints to satisfy about both the constraints to satisfy about both the action schemas and also the predicates action schemas and also the predicates we also have the concept of a precondition which is associated with the action schema and the action schema and discuss describes what are all the discuss describes what are all the prerequisites that we actually need", "image_path": "img_data/video_31_chunk_8.jpg"}
{"video": "video_31", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "discuss describes what are all the prerequisites that we actually need prerequisites that we actually need to have in place in order for us to have in place in order for us to trigger that kind of ac for that action trigger that kind of ac for that action to be able to be triggered and then of course triggered and then of course we have the -cal effects which are as you can understand the results of those actions which is the results of those actions which is effectively taking the system from one effectively taking the system from one state s to another state s prime state s to another state s prime this is associated with a state this is associated with a state transition", "image_path": "img_data/video_31_chunk_9.jpg"}
{"video": "video_31", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "this is associated with a state transition that we have seen in other videos in that we have seen in other videos in this lecture series and in terms of this lecture series and in terms of specific specifying a problem a specific specifying a problem a planning problem in the -call blocks planning problem in the -call blocks world domain here is an example of world domain here is an example of a problem we have a table a surface a problem we have a table a surface and we are going to be specifying and we are going to be specifying here we just have three blocks abc as here we just have three blocks abc as actually shown here that's going to be actually shown here that's going to be the specification requires let's say the specification requires let's say the initial state and this will actually", "image_path": "img_data/video_31_chunk_10.jpg"}
{"video": "video_31", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "the specification requires let's say the initial state and this will actually initial state and this will actually be the goal state c b a this is this will be the c b a this is this will be the goal state and here is what is a pddl state consist of a specific flavor of state in of a specific flavor of state in this kind of representation language this kind of representation language a state in a pedial state is a state in a pedial state is a conjunction", "image_path": "img_data/video_31_chunk_11.jpg"}
{"video": "video_31", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "a state in a pedial state is a conjunction of what is called ground atomic fluence and this basically looks a fluence and this basically looks a bit more complicated than it is but bit more complicated than it is but what it ends up being is let's say a what it ends up being is let's say a specific grounded if you a specific", "image_path": "img_data/video_31_chunk_12.jpg"}
{"video": "video_31", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "what it ends up being is let's say a specific grounded if you a specific grounded that's where the ground comes from the grounded let's say arrangement of objects that can also involve", "image_path": "img_data/video_31_chunk_13.jpg"}
{"video": "video_31", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "objects that can also involve predicates with parameters or arguments that are constants definitely states can change and but in a specific instance change and but in a specific instance in time we have let's say a specific in time we have let's say a specific arrangement of blocks let's say on the arrangement of blocks let's say on the table and we cannot have", "image_path": "img_data/video_31_chunk_14.jpg"}
{"video": "video_31", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "arrangement of blocks let's say on the table and we cannot have table and we cannot have variables we cannot have variables as arguments in a state specification for example this specific state that we have seen the specific state that we have seen the kind of the state at which we", "image_path": "img_data/video_31_chunk_15.jpg"}
{"video": "video_31", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "specific state that we have seen the kind of the state at which we the kind of the state at which we started with we have the block the started with we have the block the initial kind of state we have the initial kind of state we have the block a sorry this was block a sorry this was b and if i see here it is b this was b and if i see here it is a and c this is basically the a and c this is basically the other two blocks on the table and the other two blocks on the table and the initial kind of state is the initial kind of state is described as follows we have the described as follows we have the predicate on that takes the arguments the specific", "image_path": "img_data/video_31_chunk_16.jpg"}
{"video": "video_31", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "predicate on that takes the arguments the specific that takes the arguments the specific object let's say a comma table a is on top of the table a is on top of the table and b is also on top of the and b is also on top of the table and c is on a", "image_path": "img_data/video_31_chunk_17.jpg"}
{"video": "video_31", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "and c is on a this is a grounded atomic in other words grounded atomic in other words there is only one predicate there is only one predicate involved conjunction of fluence and fluent are conjunction of fluence and fluent are effectively have the notion of things effectively have the notion of things that actually can change over time as that actually can change over time as the system kind of operates this the system kind of operates this is basically the", "image_path": "img_data/video_31_chunk_18.jpg"}
{"video": "video_31", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "the system kind of operates this is basically the is this is basically the whole specification of a pddl the whole specification of a pddl state that's what a pddl state is which state that's what a pddl state is which is kind of fairly intuitive i don't is kind of fairly intuitive i don't think you have a kind of problem kind of think you have a kind of problem kind of absorbing it let's look now at the absorbing it let's look now at the acttion what we call earlier as action schema and again the name is of a schema may be a bit kind of confusing schema may be a bit kind of confusing but think about the schema that you but think about the schema that you have let's say in databases", "image_path": "img_data/video_31_chunk_19.jpg"}
{"video": "video_31", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "but think about the schema that you have let's say in databases have let's say in databases where we have let's say type where we have let's say type information associated let's say with information associated let's say with the columns let's say of a table and the columns let's say of a table and the columns of the table represent the columns of the table represent the sort of types that you the sort of types that you have and the specific rows the have and the specific rows the specific grounded objects that you specific grounded objects that you have the specific if you have the specific if you instances of those types instances of those types and similar analogies we actually and similar analogies we actually can draw here about actions in the", "image_path": "img_data/video_31_chunk_20.jpg"}
{"video": "video_31", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "and similar analogies we actually can draw here about actions in the can draw here about actions in the actions you would expect to see actions you would expect to see variables an action schema variables an action schema effectively represents a family of ground actions", "image_path": "img_data/video_31_chunk_21.jpg"}
{"video": "video_31", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "family of ground actions for example let me provide an example of an action schema from the an example of an action schema from the block wordss we have an action let's block wordss we have an action let's say of move and in as an arguments you would expect to see variables we have the expect to see variables we have the variable b and we have another variable x b and we have another variable x represents that represents let's say a represents that represents let's say a specific", "image_path": "img_data/video_31_chunk_22.jpg"}
{"video": "video_31", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "represents that represents let's say a specific location and another variable y location and another variable y that represents another location move that represents another location move a block of something which is of a block of something which is of type block p from location x to type block p from location x to location y these all of them all location y these all of them all three of them are kind of variables three of them are kind of variables that is going to be sort of the that is going to be sort of the name if you of the action name if you of the action parameterized by this specific parameterized by this specific param arameters which are variables and", "image_path": "img_data/video_31_chunk_23.jpg"}
{"video": "video_31", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "parameterized by this specific param arameters which are variables and param arameters which are variables and the action schema will involve also two the action schema will involve also two other things the other the one thing other things the other the one thing that it needs to involve the -cal that it needs to involve the -cal precondition let me write it this a precondition let me write it this a precondition these are obviously not precondition these are obviously not very formal the actual language is very formal the actual language is varies you differs quite significantly varies you differs quite significantly from what i'm actually writing over here from what i'm actually writing over here but this is also the notation that your but this is also the notation that your textbook kind of absorbs and recommends just to absorbs and recommends just to understand what is happening and then of", "image_path": "img_data/video_31_chunk_24.jpg"}
{"video": "video_31", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "absorbs and recommends just to understand what is happening and then of understand what is happening and then of course the syntax is a bit more course the syntax is a bit more complicated than that and it actually complicated than that and it actually takes inspiration from a language takes inspiration from a language that was actually called lisp is that was actually called lisp is still was actually one of the first still was actually one of the first ai kind of languages and is still ai kind of languages and is still being used to some degree from kind being used to some degree from kind of some kind of legacy applications of some kind of legacy applications but the coming back to that we but the coming back to that we have this kind of a precondition and what is really a precondition and what is really a precondition as we discussed earlier the", "image_path": "img_data/video_31_chunk_25.jpg"}
{"video": "video_31", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "precondition and what is really a precondition as we discussed earlier the precondition as we discussed earlier the precondition is the lists all of the precondition is the lists all of the conditions that need to be satisfied in conditions that need to be satisfied in order for that action to be able to be order for that action to be able to be triggered let me provide an example of precondition here provide an example of precondition here in order to move to be able to execute in order to move to be able to execute this kind of action definitely you this kind of action definitely you have to have this being true", "image_path": "img_data/video_31_chunk_26.jpg"}
{"video": "video_31", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "have to have this being true the block must be on something x which on something x which actually called x could be let say the actually called x could be let say the table the other condition is that there the other condition is that there is b is clear and clear means here that clear and clear means here that there's nothing else that it is on top", "image_path": "img_data/video_31_chunk_27.jpg"}
{"video": "video_31", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "clear and clear means here that there's nothing else that it is on top there's nothing else that it is on top of b it actually can is clear on of b it actually can is clear on that respect also we have to have that respect also we have to have in order to move b into another in order to move b into another location y then definitely we have location y then definitely we have going to have b tob of type going to have b tob of type block maybe this is what we should have block maybe this is what we should have written first now that i'm thinking written first now that i'm thinking about it we also", "image_path": "img_data/video_31_chunk_28.jpg"}
{"video": "video_31", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "written first now that i'm thinking about it we also have y to be a block and another one condition is block and another one condition is that the b is not equal to x another that b is not equal to x another that b is not equal to y and another that x and y are y and another that x and y are different this is a specification of precondition which", "image_path": "img_data/video_31_chunk_29.jpg"}
{"video": "video_31", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "different this is a specification of precondition which specification of precondition which kind of makes sense in the case where kind of makes sense in the case where we can move a block let's say from we can move a block let's say from the table to put it let's say on top the table to put it let's say on top of the c these preconditions needs to be c these preconditions needs to be this conjunction needs to be satisfied this conjunction needs to be satisfied needs to evaluate to true and of course needs to evaluate to true and of course it will evaluate to true when all of it will evaluate to true when all of these conditions evaluate to true these conditions evaluate to true definitely be is a block this is", "image_path": "img_data/video_31_chunk_30.jpg"}
{"video": "video_31", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "these conditions evaluate to true definitely be is a block this is definitely be is a block this is valuation to true definitely the y the c true definitely the y the c is a block as well y is c in this is a block as well y is c in this kind of case b is different than kind of case b is different than the b is different than the table the x is the table over table the x is the table over here and let me write this down here and let me write this down just to understand the b here is", "image_path": "img_data/video_31_chunk_31.jpg"}
{"video": "video_31", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "here and let me write this down just to understand the b here is just to understand the b here is the block b can be assigned the block b can be assigned the block b the x would be assigned the x would be assigned the table the u y would be assigned table the u y would be assigned c and that's basically what the c and that's basically what the assignments we have done in order to assignments we have done in order to check let's say an example of when this check let's say an example of when this precondition is satisfied the evidently precondition is satisfied the evidently all this inequalities called as true all this inequalities called as true and all of them are true b", "image_path": "img_data/video_31_chunk_32.jpg"}
{"video": "video_31", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "all this inequalities called as true and all of them are true b and all of them are true b has to be clear in order for it to be has to be clear in order for it to be able to move in the blocks world domain able to move in the blocks world domain we will see some of the rules of the we will see some of the rules of the blocks world in the domain. pdl file blocks world in the domain. pdl file that we will introduce that's that we will introduce that's basically an example of a precondition basically an example of a precondition and the other thing i we would to and the other thing i we would to sort of specify in this action sort of specify in this action kind of schema is the sort of kind of schema is the sort of effect this is the last thing the", "image_path": "img_data/video_31_chunk_33.jpg"}
{"video": "video_31", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "kind of schema is the sort of effect this is the last thing the effect this is the last thing the effect i'm going to write it down on u effect i'm going to write it down on u the last space last line over here the last space last line over here the effect let me give you an example of the effect let me give you an example of an effect we have a predicate an effect we have a predicate that if the let's say the block that if the let's say the block b is going to be on top of the b is going to be on top of the y definitely with effect should y definitely with effect should say that on b comma", "image_path": "img_data/video_31_chunk_34.jpg"}
{"video": "video_31", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "y definitely with effect should say that on b comma y also since we moved it and if b was also since we moved it and if b was on top of x let's say on top of the on top of x let's say on top of the table then we have a clear x this means that since the b was moved the that since the b was moved the table now is clear x and also", "image_path": "img_data/video_31_chunk_35.jpg"}
{"video": "video_31", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "that since the b was moved the table now is clear x and also table now is clear x and also we have a negative condition over we have a negative condition over here that this is no longer true that is fairly intuitive and true that is fairly intuitive and i'll try to squeeze another last i'll try to squeeze another last effect over here maybe over here effect over here maybe over here and the clear is not of y is not true", "image_path": "img_data/video_31_chunk_36.jpg"}
{"video": "video_31", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "effect over here maybe over here and the clear is not of y is not true and the clear is not of y is not true anymore evidently because now b went on anymore evidently because now b went on top of it and therefore stopped you top of it and therefore stopped being the why to be clear know being the why to be clear that's basically the u sort of effect that's basically the u sort of effect and this is the whole specification of and this is the whole specification of an action scheme fairly generic it an action scheme fairly generic it does allow us to very compactly kind of does allow us to very compactly kind of discuss and describe what it is discuss and describe what it is the precondition and the effects and the precondition and the effects and therefore deter what will be the state of the system", "image_path": "img_data/video_31_chunk_37.jpg"}
{"video": "video_31", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "therefore deter what will be the state of the system what will be the state of the system after this action is taken in another after this action is taken in another video we have seen how we can actually video we have seen how we can actually launch the repository of the course launch the repository of the course and the associated if you site in a and the associated if you site in a doer container and if you have not container and if you have not seen this kind of video please review it seen this kind of video please review it as it actually shows you how to as it actually shows you how to effectively get vs cod to be able to effectively get vs cod to be able to launch it in a container launch it in a container in the container and this is", "image_path": "img_data/video_31_chunk_38.jpg"}
{"video": "video_31", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "launch it in a container in the container and this is container in the container and this is basically what you should be able to see basically what you should be able to see when the vs code is launched when the vs code is launched there inside that container as a there inside that container as a reminder this container the reminder this container the specification of that container is specification of that container is discussed here in this file and discussed here in this file and this a container which can be launched this a container which can be launched either in the py or in tens of flow either in the py or in tens of flow there are two do files one for py and there are two do files one for py and the other for tens of flow and the", "image_path": "img_data/video_31_chunk_39.jpg"}
{"video": "video_31", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "there are two do files one for py and the other for tens of flow and the other for tens of flow and the purpose of this kind of know purpose of this kind of know discussion is to allow everyone to be discussion is to allow everyone to be able to see the pddl in terms of do the synx pddl in terms of do the synx highlighting which is actually needed highlighting which is actually needed for us to be able to visualize both the for us to be able to visualize both the domain and also the problem pddl files domain and also the problem pddl files as well also to connect on a remote as well also to connect on a remote solver or planner that the authors of solver or planner that the authors of specific extensions have connected to specific extensions have connected to in order for that problem to be", "image_path": "img_data/video_31_chunk_40.jpg"}
{"video": "video_31", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "specific extensions have connected to in order for that problem to be in order for that problem to be solved and to see actually the actual solved and to see actually the actual plan the actual suggested plan out of plan the actual suggested plan out of that solver in order for us to that solver in order for us to do we need to install an extension do we need to install an extension which is actually called pddl which is actually called pddl unsurprisingly this extension is shown here and if you typed pddl when you click on this kind of pddl when you click on this kind of extensions icon then you will be", "image_path": "img_data/video_31_chunk_41.jpg"}
{"video": "video_31", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "pddl when you click on this kind of extensions icon then you will be extensions icon then you will be prompted to install in the dev container prompted to install in the dev container we will install it in the dev we will install it in the dev container and after a few seconds container and after a few seconds then you will be able also to get then you will be able also to get that extension to install as that extension to install as it's actually shown here and this it's actually shown here and this extension is actually a very neat extension is actually a very neat extension to support our planning extension to support our planning kind of training over here because it kind of training over here because it does allow you to connect to as does allow you to connect to as i said to a remote planner without", "image_path": "img_data/video_31_chunk_42.jpg"}
{"video": "video_31", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "does allow you to connect to as i said to a remote planner without i said to a remote planner without really installing anything which is really installing anything which is basically the algorith that we are going basically the algorith that we are going to look in a bit more detail a bit to look in a bit more detail a bit later and then solve that planning later and then solve that planning problem the u directory problem the u directory lectures blogs world pddl and the lectures blogs world pddl and the demo. bdl are the corresponding under demo. bdl are the corresponding under the planning directory are the corres the planning directory are the corres oning domain file and the corresponding", "image_path": "img_data/video_31_chunk_43.jpg"}
{"video": "video_31", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "oning domain file and the corresponding domain file and the corresponding problem file the problem. bdl file problem file the problem. bdl file the specific domain as you click the specific domain as you click on this pddl then you will actually on this pddl then you will actually be prompted by you prbly to actually be prompted by you prbly to actually configure some of the things that are configure some of the things that are needed for this exercise to be needed for this exercise to be completed i would suggest that you completed i would suggest that you do install the special kind of icons do install the special kind of icons that actually are very useful for that actually are very useful for visualization purposes and you are also", "image_path": "img_data/video_31_chunk_44.jpg"}
{"video": "video_31", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "that actually are very useful for visualization purposes and you are also visualization purposes and you are also download the tools which are download the tools which are needed for properly passing pddl needed for properly passing pddl and validating the plans you will be and validating the plans you will be prompted with some popup windows i prompted with some popup windows i suggest that you type that you click on suggest that you type that you click on this override kind of this override kind of button and you will be able to then button and you will be able to then proceed you need to select the solver do proceed you need to select the solver do planning domains that is the solver", "image_path": "img_data/video_31_chunk_45.jpg"}
{"video": "video_31", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "proceed you need to select the solver do planning domains that is the solver planning domains that is the solver we will be using in this exercise and we will be using in this exercise and you will can select either the output you will can select either the output window or the search debugger i selected window or the search debugger i selected the output window as the default the output window as the default window that the plan will actually be window that the plan will actually be shown that's all with respect to shown that's all with respect to this kind of configuration and now that we have configuration and now that we have configured let's go into the configured let's go into the extension let's go into understanding a extension let's go into understanding a couple and see actually the actual synex", "image_path": "img_data/video_31_chunk_46.jpg"}
{"video": "video_31", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "extension let's go into understanding a couple and see actually the actual synex couple and see actually the actual synex earlier when we were writing earlier when we were writing things down we said that we things down we said that we are going to have a simplified kind of are going to have a simplified kind of synex but here is the actual pddl synex but here is the actual pddl synex here we have we're starting with a here we have we're starting with a domain that is actually called blocks domain that is actually called blocks world and we also have the world and we also have the various types we define the various types we define the type of type block that's the type of type block that's the class we have blocks and we define", "image_path": "img_data/video_31_chunk_47.jpg"}
{"video": "video_31", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "type of type block that's the class we have blocks and we define class we have blocks and we define predicates we have the on predicate predicates we have the on predicate that takes us parameters a and b that takes us parameters a and b of type block we have clear which we effectively as we clear which we effectively as we discussed was when there's nothing on discussed was when there's nothing on top of that block a we have the top of that block a we have the predicate on table which is means predicate on table which is means that evidently there is a block on that evidently there is a block on top of a table we have", "image_path": "img_data/video_31_chunk_48.jpg"}
{"video": "video_31", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "that evidently there is a block on top of a table we have top of a table we have holding in this specific blocks word holding in this specific blocks word we have also an actor which is a we have also an actor which is a gripper that it will actually be the gripper that it will actually be the entity that will actually lift the block entity that will actually lift the block from and move that block from one from and move that block from one position to another and had empty which position to another and had empty which in effectively indicates the state of in effectively indicates the state of the gripper that it is very the gripper that it is very specific state the grier is not holding specific state the grier is not holding any block this is these are all the any block this is these are all the predicates that we have defined and", "image_path": "img_data/video_31_chunk_49.jpg"}
{"video": "video_31", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "predicates that we have defined and predicates that we have defined and we have a number of actions that we have a number of actions that follow action schemas that is the follow action schemas that is the action schema pickup as we discussed action schema pickup as we discussed an action schema is defined by the name an action schema is defined by the name in many instances have you the name in many instances have know parameters or the parameters here is actually shown in the second here is actually shown in the second line the parameter pick up for the line the parameter pick up for the parameter for pickup is a block and a", "image_path": "img_data/video_31_chunk_50.jpg"}
{"video": "video_31", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "line the parameter pick up for the parameter for pickup is a block and a parameter for pickup is a block and a precondition and an effect we precondition and an effect we have action for pickup an action for have action for pickup an action for that's called unstack and is in this case we have unstack and is in this case we have effectively two blocks and we are unstacking blocks and we are unstacking when we are picking up from a block when we are picking up from a block from on top of another block a from on top of another block a block from that is on top of that block from that is on top of that another block and while the ver pick", "image_path": "img_data/video_31_chunk_51.jpg"}
{"video": "video_31", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "block from that is on top of that another block and while the ver pick another block and while the ver pick up the action pickup is being used up the action pickup is being used to when we pick up from a table we to when we pick up from a table we have the action put down with its corresponding down with its corresponding preconditions and effects the preconditions and effects the action stack which as you can guess it action stack which as you can guess it involves two blocks and that's basically our blocks and that's basically our domain fairly compact obviously domain fairly compact obviously it's a very simple application over it's a very simple application over here and that's basically the here and that's basically the domain and the domain consist of", "image_path": "img_data/video_31_chunk_52.jpg"}
{"video": "video_31", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "here and that's basically the domain and the domain consist of domain and the domain consist of the action schemas the predicates and the action schemas the predicates and the types that we have and the demo. pddl is the act have and the demo. pddl is the act the grounded specification of what we the grounded specification of what we face right now we have face right now we have an initial state we have first of all an initial state we have first of all the specific objects that the specific objects that correspond to the types that we have correspond to the types that we have defined we have red green blue yellow defined we have red green blue yellow brown pink of type u of that are", "image_path": "img_data/video_31_chunk_53.jpg"}
{"video": "video_31", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "defined we have red green blue yellow brown pink of type u of that are brown pink of type u of that are of type block and we have the initial of type block and we have the initial state and we have the gold state and we have the gold state this our initial state as we this our initial state as we discussed the initial state consist of discussed the initial state consist of no we don't see any variables here we no we don't see any variables here we have the specific grounded or objects that we have grounded or objects that we have see grounded fluence atomic fluence and", "image_path": "img_data/video_31_chunk_54.jpg"}
{"video": "video_31", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "grounded or objects that we have see grounded fluence atomic fluence and see grounded fluence atomic fluence and that's basically the two states si that's basically the two states si and sg is the term that we'll be using and sg is the term that we'll be using for the initial and a goal state a bit for the initial and a goal state a bit later to discussed and this is what we later to discussed and this is what we see here that is basically see here that is basically the two essential kind of files and the two essential kind of files and how we are actually going to exercise how we are actually going to exercise the plan to the solver or the plan to the solver or the planner we're going to the planner we're going to effectively right mouse click", "image_path": "img_data/video_31_chunk_55.jpg"}
{"video": "video_31", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "the planner we're going to effectively right mouse click and select run the planner and display the plan if we actually display the plan if we actually do that we don't need to specify any do that we don't need to specify any options for the planner then we options for the planner then we actually see the planner being used actually see the planner being used the planning algorithm being exercised the planning algorithm being exercised and very quickly as we expect the and very quickly as we expect the problem is very small the plan to be problem is very small the plan to be produced on a separate kind of window produced on a separate kind of window the plan evidently does is", "image_path": "img_data/video_31_chunk_56.jpg"}
{"video": "video_31", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "produced on a separate kind of window the plan evidently does is the plan evidently does is optimal in what sense the it is optimal in what sense the it sort of satisfies some kind of a sort of satisfies some kind of a cost metric in this case it is the cost metric in this case it is the minimum number of potentially minimum number of potentially minimum number of actions that we had to take in number of actions that we had to take in order to go from the initial state to order to go from the initial state to the goal state as you can see here we the goal state as you can see here we have far more number of blocks that we have far more number of blocks that we have with drawn earlier we have 1 2 3 4", "image_path": "img_data/video_31_chunk_57.jpg"}
{"video": "video_31", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "have far more number of blocks that we have with drawn earlier we have 1 2 3 4 have with drawn earlier we have 1 2 3 4 five six blocks and this is the first blocks and this is the first action the -cal unstuck action the -cal unstuck action the put down action the and on as we can put down action the and on as we can see and at the end of the day we have see and at the end of the day we have u this plan which involves i can u this plan which involves i can count right now but there's a number of count right now but there's a number of potentially finite number of actions potentially finite number of actions until we get into the gold", "image_path": "img_data/video_31_chunk_58.jpg"}
{"video": "video_31", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "potentially finite number of actions until we get into the gold until we get into the gold state what will be the gold state what will be the gold state that the red is on top of the red is on top of the brown the green is on top of the red brown the green is on top of the red and the gripper is holding the yellow and the gripper is holding the yellow that will actually be the that will actually be the goal state and as we see here the goal state and as we see here we satisfy the goal state with this we satisfy the goal state with this planner if we do that exercise", "image_path": "img_data/video_31_chunk_59.jpg"}
{"video": "video_31", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "we satisfy the goal state with this planner if we do that exercise with this planner if we do that exercise by hand which i'm not going to do right by hand which i'm not going to do right now but please feel free to do that now but please feel free to do that yourselves yeah also on the output you yourselves yeah also on the output you can actually see the value of the can actually see the value of the metric the number of seconds that it metric the number of seconds that it actually took for that plan to be actually took for that plan to be executed and evidently this plan is executed and evidently this plan is the optimal one in terms of number of the optimal one in terms of number of actions that we have executed here actions that we have executed here the there are various kind of options", "image_path": "img_data/video_31_chunk_60.jpg"}
{"video": "video_31", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "actions that we have executed here the there are various kind of options the there are various kind of options for you to export that plan into a for you to export that plan into a dot plan file if you click on that dot plan file if you click on that then you can get a plan file that you then you can get a plan file that you can actually sort of save for that can actually sort of save for that specific exercise and this plan can specific exercise and this plan can actually be read by another software actually be read by another software entity let's say a simulator as we will see a bit simulator as we will see a bit later we may actually pair this", "image_path": "img_data/video_31_chunk_61.jpg"}
{"video": "video_31", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "simulator as we will see a bit later we may actually pair this later we may actually pair this planning capability that the tools planning capability that the tools such as pddl allows us to develop such as pddl allows us to develop with a roboting operating system perhaps with a roboting operating system perhaps this merging can be done as part this merging can be done as part of a project and we have of a project and we have already a project on a warehouse that already a project on a warehouse that works this and saving or pass passing on these kind of saving or pass passing on these kind of plans despite the limitation ations of plans despite the limitation ations of those plans being deterministic and sort of assuming", "image_path": "img_data/video_31_chunk_62.jpg"}
{"video": "video_31", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "those plans being deterministic and sort of assuming deterministic and sort of assuming close worlds or full knowledge of the close worlds or full knowledge of the environment nevertheless they are environment nevertheless they are very good for training and very good for training and tutorial kind of purposes that is tutorial kind of purposes that is the capability that we will be using the capability that we will be using this plan files for the other u i this plan files for the other u i think important to recognize is that think important to recognize is that evidently this is a closed tool right evidently this is a closed tool right now it is accepting pddl as inputs and", "image_path": "img_data/video_31_chunk_63.jpg"}
{"video": "video_31", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "evidently this is a closed tool right now it is accepting pddl as inputs and now it is accepting pddl as inputs and spits out this kind of plan file but spits out this kind of plan file but it's kind of quite important to be it's kind of quite important to be able to have if you a tool that able to have if you a tool that it is in a programming language that we it is in a programming language that we all familiar with and closing this all familiar with and closing this kind of discussion about tooling for kind of discussion about tooling for planning i think it's worthwhile planning i think it's worthwhile visiting a library which has lately has visiting a library which has lately has appeared trying to unify many of appeared trying to unify many of these planners which are around and", "image_path": "img_data/video_31_chunk_64.jpg"}
{"video": "video_31", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "appeared trying to unify many of these planners which are around and these planners which are around and be able to intelligently select the be able to intelligently select the right planning planner for the right planning planner for the right job not all planners are right job not all planners are appropriate for all problems that we appropriate for all problems that we face doing that kind of filtering for face doing that kind of filtering for is quite kind of important as well is quite kind of important as well that library is called unified planning that library is called unified planning and it is available inside the docker and it is available inside the docker container and here we actually have container and here we actually have authored a notebook that allows to solve the notebook that allows to solve the blocks world domain in pure python", "image_path": "img_data/video_31_chunk_65.jpg"}
{"video": "video_31", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "notebook that allows to solve the blocks world domain in pure python blocks world domain in pure python and this is something that perhaps and this is something that perhaps you will feel a bit more comfortable in you will feel a bit more comfortable in actually using and also offers because actually using and also offers because it's in python offers a much tighter it's in python offers a much tighter integration with other the other integration with other the other software entity such as the roboting software entity such as the roboting operating system on that's operating system on that's that is an example of the block that is an example of the block wordss again and i just want to wordss again and i just want to highlight briefly some of the apis that highlight briefly some of the apis that are being defined over here that are being defined over here the we have the usual types", "image_path": "img_data/video_31_chunk_66.jpg"}
{"video": "video_31", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "that are being defined over here the we have the usual types the we have the usual types that we are have also met in the that we are have also met in the blocks world domain we have the blocks world domain we have the type of block that's the equivalent type of block that's the equivalent of how you define the block as a type of how you define the block as a type we have as fluence in that api we are we have as fluence in that api we are absorbing all the predicates under that absorbing all the predicates under that kind of api effectively what we have kind of api effectively what we have seen earlier as let's say a predicate seen earlier as let's say a predicate called clear that takes as an argument an object", "image_path": "img_data/video_31_chunk_67.jpg"}
{"video": "video_31", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "called clear that takes as an argument an object that takes as an argument an object of type block this is how you define exactly block this is how you define exactly the same predicate in this the same predicate in this tool the other and on you can tool the other and on you can actually see here the on table predicate actually see here the on table predicate the r empty holding and on all of the r empty holding and on all of these are exactly included on these are exactly included on as predicates in the pddl on as predicates in the pddl specification and we also have the specification and we also have the specification of action", "image_path": "img_data/video_31_chunk_68.jpg"}
{"video": "video_31", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "specification and we also have the specification of action specification of action schemas in the python api in that in schemas in the python api in that in this library we have the concept of this library we have the concept of instantaneous action and instantaneous action and instantaneous action kind of makes sense because we action kind of makes sense because we never talked about duration of actions never talked about duration of actions these are all enhancements of in the these are all enhancements of in the pddl specification that we can actually pddl specification that we can actually people have actually done and this people have actually done and this is and let's say the pickup action is and let's say the pickup action which takes as a parameter the object which takes as a parameter the object block and for this specific action schema", "image_path": "img_data/video_31_chunk_69.jpg"}
{"video": "video_31", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "block and for this specific action schema and for this specific action schema obviously we need to start adding obviously we need to start adding prec preconditions in a similar way as prec preconditions in a similar way as we have seen them and also define we have seen them and also define effects are the conditions that are going to be the conditions that are going to be met at when in the new state of met at when in the new state of the environment these are all the environment these are all the preconditions and the effects for the preconditions and the effects for the pickup action the put down action the pickup action the put down action the stack and the unstack actions", "image_path": "img_data/video_31_chunk_70.jpg"}
{"video": "video_31", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "the pickup action the put down action the stack and the unstack actions the stack and the unstack actions and these are all kind of matching and these are all kind of matching the corresponding actions that we the corresponding actions that we have seen earlier and obviously we are able to earlier and obviously we are able to model also the problem using this model also the problem using this python api we have here an initial python api we have here an initial state and a goal state which is state and a goal state which is obviously we need to specify in this obviously we need to specify in this cell the objects are the ones that", "image_path": "img_data/video_31_chunk_71.jpg"}
{"video": "video_31", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "cell the objects are the ones that objects are the ones that we are actually creating as of type we are actually creating as of type block they are being created in this block they are being created in this kind of list comprehension here kind of list comprehension here structure and the we adding as structure and the we adding as the objects and very similarly to what the objects and very similarly to what we have been adding the objects in this we have been adding the objects in this demo. pddl we have here demo. pddl we have here this one two 3 four five six objects this one two 3 four five six objects what we have as well over here and what we have as well over here and then we have the initial values", "image_path": "img_data/video_31_chunk_72.jpg"}
{"video": "video_31", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "what we have as well over here and then we have the initial values then we have the initial values of all the objects the initial values of all the objects involved and the actors involved and the actors involved these are all let's say the arm is these are all let's say the arm is empty and for each of the objects we empty and for each of the objects we actually have defined we are able to assign initial defined we are able to assign initial values for those which were values for those which were which are on the table each one of which are on the table each one of them is on the table and we have the each one table and we have the each one of them is clear this is our initial", "image_path": "img_data/video_31_chunk_73.jpg"}
{"video": "video_31", "start": "0:37:00", "end": "0:37:30", "timestamp": "0:37:00 - 0:37:30", "text": "table and we have the each one of them is clear this is our initial of them is clear this is our initial state and this is obviously this is a state and this is obviously this is a bit different than the initial state we bit different than the initial state we have seen over there in this in have seen over there in this pddl but it's one and the same thing pddl but it's one and the same thing we have the goal state which we have the goal state which follows the goal state is that we follows the goal state is that we have the on top of another object have the on top of another object each one of those object objects we each one of those object objects we have defined in this problem statement", "image_path": "img_data/video_31_chunk_74.jpg"}
{"video": "video_31", "start": "0:37:30", "end": "0:38:00", "timestamp": "0:37:30 - 0:38:00", "text": "each one of those object objects we have defined in this problem statement have defined in this problem statement and that's basically that is our and that's basically that is our specification about our problem and then specification about our problem and then what we do is we are able what we do is we are able to exercise for this specific kind of to exercise for this specific kind of problem we are able to select problem we are able to select intelligently the planner or the solver intelligently the planner or the solver that it will actually solve it for us that it will actually solve it for us this is what's happening in this cell this is what's happening in this cell and with a dot solve method is and with a dot solve method is actually being used here to provide", "image_path": "img_data/video_31_chunk_75.jpg"}
{"video": "video_31", "start": "0:38:00", "end": "0:38:30", "timestamp": "0:38:00 - 0:38:30", "text": "and with a dot solve method is actually being used here to provide actually being used here to provide the sort of solution to print the sort of solution to print the solution for us over here now the solution for us over here now the solutions are of two types one is a the solutions are of two types one is a valid solution which is actually called valid solution which is actually called here solve satisfying and the other is here solve satisfying and the other is the optimal solution and with the optimal solution and with guarantees and in this case we have a guarantees and in this case we have a satisfying solution as to how you can satisfying solution as to how you can actually stack this object on top actually stack this object on top of that on top of each other perir", "image_path": "img_data/video_31_chunk_76.jpg"}
{"video": "video_31", "start": "0:38:30", "end": "0:39:00", "timestamp": "0:38:30 - 0:39:00", "text": "actually stack this object on top of that on top of each other perir of that on top of each other perir the specification of the gold state the specification of the gold state and we're actually able to and we're actually able to see some of the graphs that are being see some of the graphs that are being considered by the solver but being considered by the solver but being constructed internally to the solver to constructed internally to the solver to come up to this solution we will come up to this solution we will come back into those graphs when the back into those graphs when the time comes to look at the algorithms time comes to look at the algorithms which are powering this kind of planners which are powering this kind of planners and the u the last graph over here and the u the last graph over here the last plot which is supported by", "image_path": "img_data/video_31_chunk_77.jpg"}
{"video": "video_31", "start": "0:39:00", "end": "0:39:14.266667", "timestamp": "0:39:00 - 0:39:14.266667", "text": "and the u the last graph over here the last plot which is supported by the last plot which is supported by the api is the graph is view of the api is the graph is view of how the plan and each of the sequence of how the plan and each of the sequence of actions that we have to execute in a actions that we have to execute in a graphical form", "image_path": "img_data/video_31_chunk_78.jpg"}
{"video": "video_32", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we will introduce a new topic called automatic planning that topic called automatic planning that combines two main areas of ai the combines two main areas of ai the first is the problem solving area which first is the problem solving area which is implemented with a search type of is implemented with a search type of algorithms and the second is the area we algorithms and the second is the area we have seen in a different video that have seen in a different video that it was associated with logical agents it was associated with logical agents and propositional logic in this and propositional logic in this automated kind of planning the use case automated kind of planning the use case that we actually can see and in imp", "image_path": "img_data/video_32_chunk_0.jpg"}
{"video": "video_32", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "automated kind of planning the use case that we actually can see and in imp that we actually can see and in imp is a use case associated with is a use case associated with determining a sequence of actions that determining a sequence of actions that tim will actually result into as the tim will actually result into as the agent reaching if you a desired agent reaching if you a desired goal state starting some initial state goal state starting some initial state it's a type of planning that is it's a type of planning that is going to be called planning without going to be called planning without interactions because later in another interactions because later in another video in a different if you video in a different if you topic we will actually see planning topic we will actually see planning but with interaction with the", "image_path": "img_data/video_32_chunk_1.jpg"}
{"video": "video_32", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "topic we will actually see planning but with interaction with the but with interaction with the environment right now we are not environment right now we are not having interactions and things will having interactions and things will become a bit more complicated with those become a bit more complicated with those interactions being in place one of interactions being in place one of the things that we can actually start is the things that we can actually start is to just go to the site and to just go to the site and actually see what is effectively a actually see what is effectively a good initial kind of application or a good initial kind of application or use case that we can actually motivate use case that we can actually motivate the discussion and here it is you have a discussion and here it is you have a planning problem to solve where you planning problem to solve where you need to determine the sequence of", "image_path": "img_data/video_32_chunk_2.jpg"}
{"video": "video_32", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "planning problem to solve where you need to determine the sequence of need to determine the sequence of actions that it will take the system actions that it will take the system from the initial state where effectively from the initial state where effectively you have a crane that it is handling you have a crane that it is handling the containers c1 and c2 and is using the containers c1 and c2 and is using for the transportation of those for the transportation of those containers track r1 the containers track r1 the containers are in some kind of position p1 and this are in some kind of position p1 and this is the initial kind of state and the", "image_path": "img_data/video_32_chunk_3.jpg"}
{"video": "video_32", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "are in some kind of position p1 and this is the initial kind of state and the is the initial kind of state and the final kind of state is that the tru final kind of state is that the tru should be on location should be on location two the containers will actually be two the containers will actually be stacked the way they are shown c1 c2 stacked the way they are shown c1 c2 will be on top of c1 in position will be on top of c1 in position p2 and that's basically the p2 and that's basically the goal and the plan will actually be goal and the plan will actually be the sequence of actions from one to 7 the sequence of actions from one to 7 that you actually see here on the left that you actually see here on the left side and obviously this sequence of", "image_path": "img_data/video_32_chunk_4.jpg"}
{"video": "video_32", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "that you actually see here on the left side and obviously this sequence of side and obviously this sequence of functions will actually be optimal in a functions will actually be optimal in a sense that they need to satisfy some sense that they need to satisfy some kind of constraint or some kind of a kind of constraint or some kind of a cost minimize some kind of a cost and cost minimize some kind of a cost and this will actually determine their this will actually determine their optimality we will actually optimality we will actually need to understand how a machine will need to understand how a machine will be able to determine those kind of be able to determine those kind of actions from 1 to seven evidently each actions from 1 to seven evidently each one of you will be able to very one of you will be able to very easily figure out this sequence of easily figure out this sequence of those actions but the question is how", "image_path": "img_data/video_32_chunk_5.jpg"}
{"video": "video_32", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "easily figure out this sequence of those actions but the question is how those actions but the question is how we can actually teach the machine how to we can actually teach the machine how to do that we'll attack this problem this that we'll attack this problem this planning problem and similar planning problem and similar planning problem such as this with a new problem such as this with a new representation language is actually representation language is actually called planning domain definition called planning domain definition language or p pddl and pddl has kind of two key pddl and pddl has kind of two key attributes the first is that it allows attributes the first is that it allows us to express the a domain let's us to express the a domain let's say logistics in this case or other", "image_path": "img_data/video_32_chunk_6.jpg"}
{"video": "video_32", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "us to express the a domain let's say logistics in this case or other say logistics in this case or other domains in a very i will call it a domains in a very i will call it a compact way and this is in compact way and this is in contrast to what we have seen earlier contrast to what we have seen earlier where with let's say logical agents where with let's say logical agents we have seen an agent trying to behave we have seen an agent trying to behave optimally let's say retrieve the gold in optimally let's say retrieve the gold in a rus kind of world but this a rus kind of world but this behavior was also accompanied with a behavior was also accompanied with a combinatorial explosion of", "image_path": "img_data/video_32_chunk_7.jpg"}
{"video": "video_32", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "behavior was also accompanied with a combinatorial explosion of combinatorial explosion of as the agent actually was moving as the agent actually was moving we are actually seeing the knowledge we are actually seeing the knowledge base being constantly kind of expanding base being constantly kind of expanding and the whole solution is be became and the whole solution is be became sort of fairly difficult as sort of fairly difficult as the time was going by that's one the time was going by that's one problem that the pddl actually provide problem that the pddl actually provide to us and solve for us the second is to us and solve for us the second is the type of planning solutions that the type of planning solutions that we are going to be interested in is", "image_path": "img_data/video_32_chunk_8.jpg"}
{"video": "video_32", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "the type of planning solutions that we are going to be interested in is we are going to be interested in is the one is actually shown in the one is actually shown in the figure where we can express the problem figure where we can express the problem the to called domain u model in the to called domain u model in pddl as well also an instance of that pddl as well also an instance of that domain which is let's say the specific domain which is let's say the specific instance that we have seen the instance that we have seen the logistics problems in the previous logistics problems in the previous figure with the crates and the figure with the crates and the containers and solve and the containers and solve these provide those input and solve", "image_path": "img_data/video_32_chunk_9.jpg"}
{"video": "video_32", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "and the containers and solve these provide those input and solve these provide those input and solve this planning problem in a domain this planning problem in a domain independent way which means that we independent way which means that we don't have to go ahead and specify don't have to go ahead and specify algorithms that are very much domain algorithms that are very much domain dependent they are sort of make usage of dependent they are sort of make usage of a domain expertise but they are a domain expertise but they are algorithms which are going to be algorithms which are going to be decoupled from the domain and therefore decoupled from the domain and therefore they are fairly general and kind of this they are fairly general and kind of this kind of some somehow rhymes with the kind of some somehow rhymes with the neural networks we have seen in that neural networks we have seen in that earlier kind of disc discussion in the", "image_path": "img_data/video_32_chunk_10.jpg"}
{"video": "video_32", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "neural networks we have seen in that earlier kind of disc discussion in the earlier kind of disc discussion in the perception where we have seen that perception where we have seen that architectures of neural networks the architectures of neural networks the connection is kind of a if you connection is kind of a if you paradigm allows for some kind of a paradigm allows for some kind of a generic machine that will actually learn generic machine that will actually learn patterns from data it's kind of patterns from data it's kind of similar kind of analogy we have here but similar kind of analogy we have here but in the kind of a planning space in the kind of a planning space obviously there is a tradeoff if you obviously there is a tradeoff if you have domain expertise and you are able have domain expertise and you are able to use domain expertise potentially you to use domain expertise potentially you can actually come up with a much more can actually come up with a much more efficient and plans but at this at", "image_path": "img_data/video_32_chunk_11.jpg"}
{"video": "video_32", "start": "0:06:00", "end": "0:06:20.766667", "timestamp": "0:06:00 - 0:06:20.766667", "text": "can actually come up with a much more efficient and plans but at this at efficient and plans but at this at the same time you are losing that kind the same time you are losing that kind of generality we actually seeking here of generality we actually seeking here that is the a system that we would that is the a system that we would to sort of understand and to sort of understand and put in some kind of a fashion in motion put in some kind of a fashion in motion and we will also see how in terms of and we will also see how in terms of tooling that is system is implemented tooling that is system is implemented today", "image_path": "img_data/video_32_chunk_12.jpg"}
{"video": "video_33", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "the last application that we're going to see where lstms will actually be used see where lstms will actually be used is the neurom machine is the neurom machine translation and in this kind of translation and in this kind of application i mean you probably have application i mean you probably have used machine translation used machine translation extensively for example today we have extensively for example today we have real time or almost real time kind of real time or almost real time kind of neur machine translation in every neur machine translation in every smartphone where there is an application smartphone where there is an application where you record your voice in where you record your voice in english and you can translate it to any", "image_path": "img_data/video_33_chunk_0.jpg"}
{"video": "video_33", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "where you record your voice in english and you can translate it to any english and you can translate it to any other language and on but in general other language and on but in general the murine translation is one of the murine translation is one of the many applications of what is a more many applications of what is a more generally called sequence to sequence generally called sequence to sequence models in the sequence to sequence models in the sequence to sequence models we are going to be dealing models we are going to be dealing with here is we're going to start with here is we're going to start with the lsdm kind of treatment of the lsdm kind of treatment of the topic and then we are going to be topic and then we are going to be introducing sequence to sequence with introducing sequence to sequence with attention and this is actually going attention and this is actually going to will be giving us quite a lot of", "image_path": "img_data/video_33_chunk_1.jpg"}
{"video": "video_33", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "attention and this is actually going to will be giving us quite a lot of to will be giving us quite a lot of visibility into what we are calling visibility into what we are calling transformer architectures because transformer architectures because transformers as we said started from transformers as we said started from sort of sequence to sequence models the sort of sequence to sequence models the -cal encoder decoder models and then -cal encoder decoder models and then evolved those models to u to evolved those models to u to create the revolution we have create the revolution we have are seeing today in that in the are seeing today in that in the field of natural language processing field of natural language processing i will be motivating the discussion i will be motivating the discussion from the translation kind of perspective from the translation kind of perspective persective but sequence to sequence", "image_path": "img_data/video_33_chunk_2.jpg"}
{"video": "video_33", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "from the translation kind of perspective persective but sequence to sequence persective but sequence to sequence models are a bit more generic in fact models are a bit more generic in fact you can have sort of any sort of you can have sort of any sort of the applications quoted over here of the applications quoted over here let's say the summarization where we have a let's say summarization where we have a let's say a body of text that we would to a body of text that we would to create a coincide summary of it we create a coincide summary of it we can actually have u any sort of sequence can actually have u any sort of sequence or sequence model to take some kind or sequence model to take some kind of code as input", "image_path": "img_data/video_33_chunk_3.jpg"}
{"video": "video_33", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "of code as input sorry i take natural language as sorry i take natural language as input and converting into code input and converting into code that's another sequence to sequence task that's another sequence to sequence task and on all right in this and on all right in this respect what we are going to be doing respect what we are going to be doing is we are going to start is we are going to start with the u neuromin translation with a with the u neuromin translation with a sentence which is in the in greek sentence which is in the in greek let's let me write this sentence in let's let me write this sentence in greek this sentence we will be sort", "image_path": "img_data/video_33_chunk_4.jpg"}
{"video": "video_33", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "this sentence we will be sort of symbolizing this sentence with of symbolizing this sentence with x we have here let's assume the x we have here let's assume the simple case where we have a word level simple case where we have a word level tokenization to avoid any complications tokenization to avoid any complications x1 x2 x3 and the translation which is x3 and the translation which is i love you that's basically and i love you that's basically and here we actually seeing also the here we actually seeing also the simplest case where we have exactly the", "image_path": "img_data/video_33_chunk_5.jpg"}
{"video": "video_33", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "here we actually seeing also the simplest case where we have exactly the simplest case where we have exactly the same tokens at the input and the output although obviously this is not going to be the case in general the going to be the case in general the sequence to sequence model consist of sequence to sequence model consist of the -call encoder decoder the -call encoder decoder architectures in other words we are architectures in other words we are going to be creating a probabilistic going to be creating a probabilistic machine again that will consist of these machine again that will consist of these two parts in encoding part where we are two parts in encoding part where we are going to take the input sequence x1 x2 going to take the input sequence x1 x2 and three and create a representation of", "image_path": "img_data/video_33_chunk_6.jpg"}
{"video": "video_33", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "going to take the input sequence x1 x2 and three and create a representation of and three and create a representation of that sequence and a decoder part it will that sequence and a decoder part it will start from that representation that the start from that representation that the encoder built and we are going to encoder built and we are going to translate it to create if you the translate it to create if you the individual the translated tokens which individual the translated tokens which in this case is x primes in this case is x primes in terms of architecture it will look terms of architecture it will look this we start from some initial this we start from some initial hidden state let's say all zeros x h0", "image_path": "img_data/video_33_chunk_7.jpg"}
{"video": "video_33", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "this we start from some initial hidden state let's say all zeros x h0 hidden state let's say all zeros x h0 again i'm not going to be drawing the again i'm not going to be drawing the full lstm cell here but in practice this full lstm cell here but in practice this is an lstm cell and we are going is an lstm cell and we are going to also introduce some kind of a to also introduce some kind of a enhancement to the simple lsdm cell enhancement to the simple lsdm cell not the cell itself but some not the cell itself but some enhancement the overall kind of enhancement the overall kind of architecture we have seen earlier in a architecture we have seen earlier in a moment here we have the sort of moment here we have the sort of some input x we'll see exactly with what some input x we'll see exactly with what order this x is ared", "image_path": "img_data/video_33_chunk_8.jpg"}
{"video": "video_33", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "some input x we'll see exactly with what order this x is ared that's the third token and this is what the representation fi we'll call what the representation fi we'll call this representation fi what the encoder this representation fi what the encoder is generating and here's the decoder", "image_path": "img_data/video_33_chunk_9.jpg"}
{"video": "video_33", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "this representation fi what the encoder is generating and here's the decoder is starts from u a token which will call a start of sentence and during the training", "image_path": "img_data/video_33_chunk_10.jpg"}
{"video": "video_33", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "sentence and during the training we are using the ground truth and we have here the prediction is the prediction of the translated token the prediction of the translated token in this case is i in this case is in this instance is love and this is u and this instance is love and this is u and when u comes in what we are when u comes in what we are predicting is another token which is", "image_path": "img_data/video_33_chunk_11.jpg"}
{"video": "video_33", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "when u comes in what we are predicting is another token which is predicting is another token which is stands for end of sentence token or stands for end of sentence token or eos effectively the decoder is eos effectively the decoder is triggered by the arrival of the start of triggered by the arrival of the start of sentence token there is certain sentence token there is certain things that need to happen during the things that need to happen during the tokenization step in order for us to tokenization step in order for us to insert this actually tokens and prepare insert this actually tokens and prepare our data set appropriately and it ends with the appropriately and it ends with the end of sentence as the end of sentence as the prediction that's the whole process prediction that's the whole process thisi is going will be called the", "image_path": "img_data/video_33_chunk_12.jpg"}
{"video": "video_33", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "prediction that's the whole process thisi is going will be called the thisi is going will be called the thought vector and one trick that we are actually doing in sequence to actually doing in sequence to sequence modeling is the fact that we sequence modeling is the fact that we are feeding the input sentence in are feeding the input sentence in kind of a reverse order and in many l", "image_path": "img_data/video_33_chunk_13.jpg"}
{"video": "video_33", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "order and in many l languages we are doing because in languages we are doing because in many languages the very first word many languages the very first word that is being translated is very much that is being translated is very much the word that it is the very first the word that it is the very first token of the source language the token of the source language the source and the target are sort of are source and the target are sort of are kind of aligned in many language not all kind of aligned in many language not all language but in many languages that's language but in many languages that's the case and what we want to achieve the case and what we want to achieve here is that we would to bootstrap here is that we would to bootstrap the decoding kind of process in other the decoding kind of process in other words this x one to be a recent addition", "image_path": "img_data/video_33_chunk_14.jpg"}
{"video": "video_33", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "the decoding kind of process in other words this x one to be a recent addition words this x one to be a recent addition into the hidden state and of that of into the hidden state and of that of the at this point in time of the rnn in the at this point in time of the rnn in order for it to get a good start order for it to get a good start from the very first token which is in from the very first token which is in the target language the target in the target language in terms of this is one in terms of this is one sort of point that i want to sort of point that i want to explicitly do the other thing is let me", "image_path": "img_data/video_33_chunk_15.jpg"}
{"video": "video_33", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "explicitly do the other thing is let me do the other thing is let me write it down we typically feed the tokens in into the decoder with the reverse", "image_path": "img_data/video_33_chunk_16.jpg"}
{"video": "video_33", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "decoder with the reverse order another thing we actually do is we are implementing what is actually called are implementing what is actually called a by direction rnn the let me write down b directional by direction are used", "image_path": "img_data/video_33_chunk_17.jpg"}
{"video": "video_33", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "directional by direction are used and the way to actually understand that is the following and let me give you an is the following and let me give you an example to let's assume that we example to let's assume that we have let's say two sentences the troops were loyal to george washington and the second sentence is", "image_path": "img_data/video_33_chunk_18.jpg"}
{"video": "video_33", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "washington and the second sentence is george washington is a parking lot is it's is closed to traffic today", "image_path": "img_data/video_33_chunk_19.jpg"}
{"video": "video_33", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "it's is closed to traffic today in many instances we have the following kind of problem the meaning of let's say a problem the meaning of let's say a term let's say as this sequence of tokens as this sequence of tokens that corresponds to a name that corresponds to a name george washington can be george washington can be sometimes is a function of what follows sometimes is a function of what follows or what it proceeds it in this or what it proceeds it in this case what we trying to do with the", "image_path": "img_data/video_33_chunk_20.jpg"}
{"video": "video_33", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "or what it proceeds it in this case what we trying to do with the case what we trying to do with the bidirectional rnn in order for us to bidirectional rnn in order for us to achieve that thought vector to contain achieve that thought vector to contain both types of sort of both types of sort of representations we have effectively representations we have effectively forming two hidden states forming two hidden states one by looking at the tokens in one by looking at the tokens in this order and the other by looking at this order and the other by looking at the tokens in that other order the tokens in that other order the forward and the reverse i'll be forward and the reverse i'll be calling the f the first one and the f calling the f the first one and the f r the second", "image_path": "img_data/video_33_chunk_21.jpg"}
{"video": "video_33", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "calling the f the first one and the f r the second one we have two vectors and one we have two vectors and we are concatenating them together we are concatenating them together needless to say that the vf is the last needless to say that the vf is the last hidden state is the last hidden state of this encoder rnn this is my of this encoder rnn this is my encoder rnn and the moment we have this representation built representation built we are effectively feeding in into the", "image_path": "img_data/video_33_chunk_22.jpg"}
{"video": "video_33", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "representation built we are effectively feeding in into the we are effectively feeding in into the decoder that's my decoder that's basically the second thing i wanted to mention and second thing i wanted to mention and the third thing is we wanted the third thing is we wanted to distinguish between two things to distinguish between two things what we do during training during the training protocol training during the training protocol if you and what we do for inference", "image_path": "img_data/video_33_chunk_23.jpg"}
{"video": "video_33", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "training during the training protocol if you and what we do for inference if you and what we do for inference we use the term teacher we use the term teacher [music] forcing to describe the process that during training where we feed the ground truth via the decoder input", "image_path": "img_data/video_33_chunk_24.jpg"}
{"video": "video_33", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "truth via the decoder input but during inference and this is actually what we are going to be doing is we are going to be doing is we are going to feed and this is how i show it going to feed and this is how i show it with this kind of dotted with this kind of dotted line we are feeding the previously line we are feeding the previously predictive token predicted token as input to the decoder", "image_path": "img_data/video_33_chunk_25.jpg"}
{"video": "video_33", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "input to the decoder during inference feed the previous predicted token as input to the decoder and obviously we have to be dealing with all these kind of dealing with all these kind of complications and we'll complications and we'll we have an kind of an extensive notebook", "image_path": "img_data/video_33_chunk_26.jpg"}
{"video": "video_33", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "complications and we'll we have an kind of an extensive notebook we have an kind of an extensive notebook on implementation of this on implementation of this sequence to sequence neural machine sequence to sequence neural machine translation with lstms and evidently in this lstms and evidently in this notebook you will also come across the notebook you will also come across the fact that the cardinality of the fact that the cardinality of the source is a vocabulary is not obviously source is a vocabulary is not obviously the same as the cardinality of the same as the cardinality of the target and this is basically our sort", "image_path": "img_data/video_33_chunk_27.jpg"}
{"video": "video_33", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "and this is basically our sort of normal situation we're going to need of normal situation we're going to need see exactly how we're going to address see exactly how we're going to address let's now that we understood the let's now that we understood the block diagram we have if you block diagram we have if you the block diagram in front of us let's the block diagram in front of us let's see a little bit more mathematically see a little bit more mathematically what it is really this p model what it is really this p model what is the probabilistic what is the probabilistic machine that we have machine that we have built the encoding", "image_path": "img_data/video_33_chunk_28.jpg"}
{"video": "video_33", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "machine that we have built the encoding process is effectively solves the u calculates a thought vector calculates a thought vector f and it effectively models via the f and it effectively models via the encoder rnn i'll mention this in the encoder rnn i'll mention this in the literature you this come across as q the literature you this come across as q the function of the encoding rnn the function of the encoding rnn evidently this there is a p model be evidently this there is a p model be behind it there's a probably the", "image_path": "img_data/video_33_chunk_29.jpg"}
{"video": "video_33", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "evidently this there is a p model be behind it there's a probably the behind it there's a probably the distribution behind it but the distribution behind it but the here we write if you the sort of here we write if you the sort of simplifying this kind of notation and we simplifying this kind of notation and we have a function that effectively have a function that effectively takes us input this hidden stage h1 h2 takes us input this hidden stage h1 h2 dot htx we met this capital t notation htx we met this capital t notation in the when we looked at the language in the when we looked at the language kind of models and over here we can tx", "image_path": "img_data/video_33_chunk_30.jpg"}
{"video": "video_33", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "kind of models and over here we can tx models and over here we can tx is the length of the source sentence all right that's basically what the five vector is calculated what the five vector is calculated and we've seen that how it is also and we've seen that how it is also catenated with 4 than reverse passes", "image_path": "img_data/video_33_chunk_31.jpg"}
{"video": "video_33", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "and we've seen that how it is also catenated with 4 than reverse passes catenated with 4 than reverse passes and the decoder and that's basically the where this name came from the -called this name came from the -called conditional language model is the is conditional language model is the is from the decoder perspective the y from the decoder perspective the y hat which is being calculated which is hat which is being calculated which is the sequence of words but why hat is the sequence of words but why hat is being written here as a being written here as a general a sequence of general a sequence of predictions we have here an", "image_path": "img_data/video_33_chunk_32.jpg"}
{"video": "video_33", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "general a sequence of predictions we have here an argmax over all sequences y of p of y given 5 comma theta this is the p model and in fact i think that just", "image_path": "img_data/video_33_chunk_33.jpg"}
{"video": "video_33", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "model and in fact i think that just because the literature uses q we can because the literature uses q we can actually for notational simplicity we actually for notational simplicity we can also call this the p model for u can also call this the p model for u this p model is for the encoder and this is the decoder p model that's these are model that's these are architecturally ent identical in a architecturally ent identical in a sense that both are based on the lstm sense that both are based on the lstm kind of neural technology all right kind of neural technology all right let's write this down a bit more let's write this down a bit more explicitly i do not want necessarily explicitly i do not want necessarily to confuse this y under bar with a", "image_path": "img_data/video_33_chunk_34.jpg"}
{"video": "video_33", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "explicitly i do not want necessarily to confuse this y under bar with a to confuse this y under bar with a vector maybe we can just u call it vector maybe we can just u call it this kind of a sequence of this kind of a sequence of output the tokens of target tokens output the tokens of target tokens and the same thing can go and the same thing can go here we have a here an argmax over y", "image_path": "img_data/video_33_chunk_35.jpg"}
{"video": "video_33", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "argmax over y of a p model of y1 y 2 y t y given 5 comma theta where p y now in this case theta where p y now in this case is the length of the target sentence of the target sentence all", "image_path": "img_data/video_33_chunk_36.jpg"}
{"video": "video_33", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "sentence of the target sentence all right what we actually can see here right what we actually can see here is that we have a we can actually that we have a we can actually make a similar simplification to the make a similar simplification to the calculating of this model calculating of this model that we have done earlier in that we have done earlier in when we're discussing language models when we're discussing language models or actually when we're discussing word or actually when we're discussing word to vex in that kind of video we to vex in that kind of video we can actually write it as", "image_path": "img_data/video_33_chunk_37.jpg"}
{"video": "video_33", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "to vex in that kind of video we can actually write it as argmax over y in this case now of p model of yt y given y1 y2 yt y -1 comma y comma theta", "image_path": "img_data/video_33_chunk_38.jpg"}
{"video": "video_33", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "y2 yt y -1 comma y comma theta in fact we don't need to make in this case any simplifying assumption this is case any simplifying assumption this is just a factorization from first just a factorization from first principles of that kind of a joint principles of that kind of a joint propability distribution we can always propability distribution we can always factorize a joint probability factorize a joint probability distribution as a product of the distribution as a product of the individual conditionals for example to individual conditionals for example to open a parenthesis here you can actually open a parenthesis here you can actually factorize p of x1 let's say y1 comma factorize p of x1 let's say y1 comma y2", "image_path": "img_data/video_33_chunk_39.jpg"}
{"video": "video_33", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "factorize p of x1 let's say y1 comma y2 as p of y2 given y1 * p of y1 and this was the case i mean this was the case i mean this was met in another video as kind of the met in another video as kind of the product rule we're not really product rule we're not really breaking any new ground here we're just breaking any new ground here we're just rewriting this as a product of the rewriting this as a product of the individual probabilities this is now individual probabilities this is now p model times dot p model finally we arrive model finally we arrive at y2 given", "image_path": "img_data/video_33_chunk_40.jpg"}
{"video": "video_33", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "model finally we arrive at y2 given y1 comma 5 comma theta times p model of y1 comma 5 comma model of y1 comma 5 comma theta this can be simplified as ar max in terms of simplified as ar max in terms of notation at least over y the product notation at least over y the product from t p is equal to 1 to t of", "image_path": "img_data/video_33_chunk_41.jpg"}
{"video": "video_33", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "notation at least over y the product from t p is equal to 1 to t of from t p is equal to 1 to t of y p of yt given y1 yt minus1 comma 5 comma theta and this is the reason why the num translation is actually called a conditional language model because we are actually conditioning on the thought", "image_path": "img_data/video_33_chunk_42.jpg"}
{"video": "video_33", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "language model because we are actually conditioning on the thought vector the now that we actually have seen the in terms of we actually have seen the in terms of equations what is been done if you equations what is been done if you in this kind of block diagram in this kind of block diagram with this kind of block with this kind of block diagram let's look at an implementation diagram let's look at an implementation and see how this two evidently the and see how this two evidently the decoder is going to involve some form of decoder is going to involve some form of a loss that in the decoding part a loss that in the decoding part we have to attach some form of a of loss we have to attach some form of a of loss now we'll see exactly what are these", "image_path": "img_data/video_33_chunk_43.jpg"}
{"video": "video_33", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "we have to attach some form of a of loss now we'll see exactly what are these now we'll see exactly what are these loss functions but i think it's better loss functions but i think it's better to see that in an implementation rather to see that in an implementation rather than continue to write it in a math than continue to write it in a math notation here before going into the notation here before going into the notebook i think it's worthwhile kind of notebook i think it's worthwhile kind of highlighting a couple of things here highlighting a couple of things here we need to explain a couple of things we need to explain a couple of things because it's not entirely because it's not entirely straightforward exactly how we will be straightforward exactly how we will be implementing this kind of argmax implementing this kind of argmax operator which we have seen quoted in operator which we have seen quoted in the equations of the decoder p model the equations of the decoder p model the first thing that we wanted to", "image_path": "img_data/video_33_chunk_44.jpg"}
{"video": "video_33", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "the equations of the decoder p model the first thing that we wanted to the first thing that we wanted to highlight is the fact that we are going highlight is the fact that we are going to have deep rnn architectures here to have deep rnn architectures here deep lsdm architecture we have deep lsdm architecture we have a video on rnn and lsdm for those who a video on rnn and lsdm for those who want to review that topic then we want to review that topic then we explained there what is a deep explained there what is a deep architecture is we are going to be architecture is we are going to be gradually building up hidden state gradually building up hidden state representations over multiple layers representations over multiple layers that's one thing the second is that's one thing the second is that which is a bit more nuanced is", "image_path": "img_data/video_33_chunk_45.jpg"}
{"video": "video_33", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "that's one thing the second is that which is a bit more nuanced is that which is a bit more nuanced is actually going to be described using actually going to be described using this picture that we have offered this picture that we have offered earlier the decoding during this earlier the decoding during this kind of a decoding process we are kind of a decoding process we are going to pick up losses at every part of going to pick up losses at every part of the translation process for every the translation process for every stage of the translation process there's stage of the translation process there's going to be some loss function going to be some loss function obviously it's going to be cross entropy obviously it's going to be cross entropy loss because we're making a prediction loss because we're making a prediction over discrete set of classes which is of", "image_path": "img_data/video_33_chunk_46.jpg"}
{"video": "video_33", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "loss because we're making a prediction over discrete set of classes which is of over discrete set of classes which is of course the vocabulary size the target course the vocabulary size the target vocabulary size the v target vocabulary size the v target over here we are for every loss over here we are for every loss that we are calculating we are going to that we are calculating we are going to be doing back propagation through time be doing back propagation through time and u in this back propagation through and u in this back propagation through time we are going to pass the time we are going to pass the gradient over into the encoder as well gradient over into the encoder as well the trainable parameters of the trainable parameters of the encoder and by extension the produced", "image_path": "img_data/video_33_chunk_47.jpg"}
{"video": "video_33", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "the trainable parameters of the encoder and by extension the produced encoder and by extension the produced thought vector is a function of how the thought vector is a function of how the decoder is doing in other words we don't decoder is doing in other words we don't have decoupling here we have if you have decoupling here we have if you a joint parameter optimization as a joint parameter optimization as you might have expected in this kind of you might have expected in this kind of architecture but i thought to mention it architecture but i thought to mention it explicitly that's let me write explicitly that's let me write it down joint parameter optimization", "image_path": "img_data/video_33_chunk_48.jpg"}
{"video": "video_33", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "parameter optimization of encoder and decoder trainable parameters that's the other thing of course i wanted to mention is thing of course i wanted to mention is that which is a bit more nuanced is that which is a bit more nuanced is the following at every step of is the following at every step of away what we do we are actually doing away what we do we are actually doing a kind of a greedy decoding", "image_path": "img_data/video_33_chunk_49.jpg"}
{"video": "video_33", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "away what we do we are actually doing a kind of a greedy decoding doing a kind of a greedy decoding process remember that this is an process remember that this is an unrolled diagram physically we have unrolled diagram physically we have this step at this point in time we this step at this point in time we are selecting the next token the are selecting the next token the translated token based on the fact that translated token based on the fact that we are taking if you a soft we are taking if you a soft max over all the probabilities over the max over all the probabilities over the pro posterior probability distribution pro posterior probability distribution we have at the output that we have at the output that softmax which is softer version of", "image_path": "img_data/video_33_chunk_50.jpg"}
{"video": "video_33", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "we have at the output that softmax which is softer version of softmax which is softer version of argmax is the reason why i actually call argmax is the reason why i actually call that kind of a gritty process we may that kind of a gritty process we may we do not consider for the we do not consider for the translation of this token any other translation of this token any other token which may come next to it or for token which may come next to it or for the translation of this token any other the translation of this token any other token that it was produced before that token that it was produced before that we have some form of kind of a we have some form of kind of a memoryless greedy process going on in memoryless greedy process going on in the decoder process that is has the decoder process that is has been shown it is not really the best", "image_path": "img_data/video_33_chunk_51.jpg"}
{"video": "video_33", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "the decoder process that is has been shown it is not really the best been shown it is not really the best thing we can do in fact if you take a thing we can do in fact if you take a note on in this actually wrote down the note on in this actually wrote down the sort of calculations here in a bit sort of calculations here in a bit more conditional probability more conditional probability distributions here what we actually distributions here what we actually can see is that we need to be taking a can see is that we need to be taking a an argmax over the sequences in order for us to solve sequences in order for us to solve optimally the problem what we need to optimally the problem what we need to do is we need to have an addon algorithm", "image_path": "img_data/video_33_chunk_52.jpg"}
{"video": "video_33", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "optimally the problem what we need to do is we need to have an addon algorithm do is we need to have an addon algorithm that it will be correcting some of the predictions correcting some of the predictions that we actually are making and we that we actually are making and we this algorithm is actually called beam this algorithm is actually called beam search and correcting is actually a search and correcting is actually a strong term basically is path selection strong term basically is path selection at the end and now we will need to at the end and now we will need to discuss this algorithm because this discuss this algorithm because this basically what is extensively used in basically what is extensively used in practice let's now spend some time practice let's now spend some time explaining the beam search and then we explaining the beam search and then we can go to the", "image_path": "img_data/video_33_chunk_53.jpg"}
{"video": "video_33", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "explaining the beam search and then we can go to the notebook this beam search has it its root on a very famous algorith called root on a very famous algorith called the maximum likelihood sequence the maximum likelihood sequence estimator also known as the vby estimator also known as the vby algorithm from inventor vby and in fact algorithm from inventor vby and in fact this algorith is being implemented in this algorith is being implemented in many places including all your many places including all your smartphones to receive radio signals", "image_path": "img_data/video_33_chunk_54.jpg"}
{"video": "video_33", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "many places including all your smartphones to receive radio signals smartphones to receive radio signals let's start with an example let's start with an example in this example we have the following in this example we have the following kind of a decoding process as we kind of a decoding process as we discussed we start with a start of discussed we start with a start of sentence token and we sentence token and we probability at the output of the soft probability at the output of the soft max in the very first decoding step we max in the very first decoding step we produce the token we predict the produce the token we predict the token v because it's a probability token v because it's a probability one we have no other tokens all of the one we have no other tokens all of the other tokens are zero because the output other tokens are zero because the output is a posterior probability", "image_path": "img_data/video_33_chunk_55.jpg"}
{"video": "video_33", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "other tokens are zero because the output is a posterior probability is a posterior probability distribution then we go to the second distribution then we go to the second decoding step and again with probability decoding step and again with probability one let's say we decode the we one let's say we decode the we predict the word ship all right predict the word ship all right and then we have here in the next and then we have here in the next decoding step however we have decoding step however we have a sort of a token that is predicted with a token that is predicted with probability 0.6 has and with probability", "image_path": "img_data/video_33_chunk_56.jpg"}
{"video": "video_33", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "a token that is predicted with probability 0.6 has and with probability 0.6 has and with probability 0.4 let's say sailed and over here after has we're going to the kind of a final step which is the word docked and the word sunk this was predicted with word sunk this was predicted with 0.55 and as you might expect 1us 0.55 is", "image_path": "img_data/video_33_chunk_57.jpg"}
{"video": "video_33", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "word sunk this was predicted with 0.55 and as you might expect 1us 0.55 is 0.55 and as you might expect 1us 0.55 is 0.45 for the world s and of course there 0.45 for the world s and of course there is this end of sentence kind of token is this end of sentence kind of token after that we are not going to after that we are not going to draw that we have sailed over here after s in this branch we were predicting yeah some a home and away", "image_path": "img_data/video_33_chunk_58.jpg"}
{"video": "video_33", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "yeah some a home and away the home here was 0.9 and the way 0.1 obviously during the beam search algorith is know the beam search algorith is actually happens in inference time where actually happens in inference time where we have if you a model we are we have if you a model we are forming because obviously", "image_path": "img_data/video_33_chunk_59.jpg"}
{"video": "video_33", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "we have if you a model we are forming because obviously forming because obviously during training we have the ground truth during training we have the ground truth we don't have any anything to sort of we don't have any anything to sort of warriers at this point in time but in warriers at this point in time but in the during inference what is happening the during inference what is happening is the following we as we discussed we adopted a kind we as we discussed we adopted a kind of a greedy approach this one would of a greedy approach this one would actually have been selected greedily actually have been selected greedily out of the two h will have been produced out of the two h will have been produced at the output and over here dock would have", "image_path": "img_data/video_33_chunk_60.jpg"}
{"video": "video_33", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "at the output and over here dock would have output and over here dock would have been produced at the output in the gitty been produced at the output in the gitty approach because between the two approach because between the two zero point the path that have been zero point the path that have been produced at the output is this one let me write this down in the greedy decoding gy decoding does not", "image_path": "img_data/video_33_chunk_61.jpg"}
{"video": "video_33", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "decoding does not maximize the likelihood actually i'm writing the wrong conclusion we will we'll come to wrong conclusion we will we'll come to this conclusion a bit this conclusion a bit later but what i wanted to write here is later but what i wanted to write here is that according to the greedy", "image_path": "img_data/video_33_chunk_62.jpg"}
{"video": "video_33", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "later but what i wanted to write here is that according to the greedy approach of selecting the token with the maximum posterior probability at any instant in time the", "image_path": "img_data/video_33_chunk_63.jpg"}
{"video": "video_33", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "the decoder produces the ship has docked and this is actually going to be not and this is actually going to be not necessarily the decoder the decoding", "image_path": "img_data/video_33_chunk_64.jpg"}
{"video": "video_33", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "and this is actually going to be not necessarily the decoder the decoding necessarily the decoder the decoding that would have actually resulted how that would have actually resulted how we selected what the decoder the we selected what the decoder the actual equation is telling us to do the actual equation is telling us to do because the actual equation is telling because the actual equation is telling us to select the most likely sequence us to select the most likely sequence over the to take the r max over all over the to take the r max over all the sequences what we actually need the sequences what we actually need to do here is calculate the probability to do here is calculate the probability or the likelihood of every sequence or the likelihood of every sequence if we are to do that then this if we are to do that then this problem ability will end up being 0.55 *", "image_path": "img_data/video_33_chunk_65.jpg"}
{"video": "video_33", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "if we are to do that then this problem ability will end up being 0.55 * problem ability will end up being 0.55 * 0.6 this will actually be 0.33 0.6 this will actually be 0.33 let me write it down as the let me write it down as the let me write it down over let me write it down over here with maybe a red thing this is here with maybe a red thing this is 0.33 and out of the many others we 0.33 and out of the many others we actually can see here that this actually can see here that this probability will actually be zero probability will actually be zero .36 but we never produced", "image_path": "img_data/video_33_chunk_66.jpg"}
{"video": "video_33", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "probability will actually be zero .36 but we never produced .36 but we never produced this what we need to do now is we this what we need to do now is we need to come up with an algorithm need to come up with an algorithm that will actually allow us to select that will actually allow us to select this path in fact this is the most this path in fact this is the most likely path that we have to select this is achieved with a beam search with is achieved with a beam search with beam sears let me write down the fact sears let me write down the fact first that i was writing earlier first that i was writing earlier greedy [music]", "image_path": "img_data/video_33_chunk_67.jpg"}
{"video": "video_33", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "greedy [music] decoding does not maximize the likelihood of the sequence of translated", "image_path": "img_data/video_33_chunk_68.jpg"}
{"video": "video_33", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "of the sequence of translated toin and the second is that the solution which is beam search achieves that by maintaining the log", "image_path": "img_data/video_33_chunk_69.jpg"}
{"video": "video_33", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "maintaining the log probability of course over here i didn't use the log but i use the absolute use the log but i use the absolute probabilities here but the path is very probabilities here but the path is very small and it's very short and we don't small and it's very short and we don't have any problem with underflow the have any problem with underflow the maintaining the log probability of each maintaining the log probability of each branch we have here 1 2 3 four branch we have here 1 2 3 four branches two of them have been branches two of them have been calculated and soon course this is calculated and soon course this is enough to make the point of each branch and by of each branch", "image_path": "img_data/video_33_chunk_70.jpg"}
{"video": "video_33", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "of each branch and by of each branch and maintains capital k branches for capital k is equal to 2 in fact we are going to 2 in fact we are going to be as we discussed selected here docked and home will actually have", "image_path": "img_data/video_33_chunk_71.jpg"}
{"video": "video_33", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "selected here docked and home will actually have docked and home will actually have been kept in this case and finally been kept in this case and finally with k is equal to two the most likely sequence of the two corresponds to k is the ship shape bu", "image_path": "img_data/video_33_chunk_72.jpg"}
{"video": "video_33", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "ship shape bu home and this will actually what we produced at the translation that is produced at the translation that is what i mentioned about the correction what i mentioned about the correction step to obviously according to the step to obviously according to the v der kind of algorith help us translate v der kind of algorith help us translate the right way the target the right way the target sentences and to close the discussion on sentences and to close the discussion on the neurom machine translation at least the neurom machine translation at least in its basic form without intention i in its basic form without intention i think it's worthwhile covering the topic", "image_path": "img_data/video_33_chunk_73.jpg"}
{"video": "video_33", "start": "0:37:00", "end": "0:37:30", "timestamp": "0:37:00 - 0:37:30", "text": "in its basic form without intention i think it's worthwhile covering the topic think it's worthwhile covering the topic of metrics because we will see them of metrics because we will see them see those metrics in front of see those metrics in front of us when we look at the us when we look at the implementations and there are implementations and there are obviously many metrics that we can obviously many metrics that we can discuss one of them is called discuss one of them is called blue and stands for bilingual blue and stands for bilingual evaluation under study it was invented evaluation under study it was invented by ibm many decades ago 20 years ago 2020 2002 ago 20 years ago 2020 2002 and", "image_path": "img_data/video_33_chunk_74.jpg"}
{"video": "video_33", "start": "0:37:30", "end": "0:38:00", "timestamp": "0:37:30 - 0:38:00", "text": "ago 20 years ago 2020 2002 and obviously it has many variants but it obviously it has many variants but it is very frequently kind of quoted that's is very frequently kind of quoted that's why we selected to actually cover that why we selected to actually cover that and this metric is fairly intuitive and this metric is fairly intuitive and it has to do a lot as many other and it has to do a lot as many other metrics we've seen in the past with the metrics we've seen in the past with the original metrics we've seen for original metrics we've seen for classification because at the end of the classification because at the end of the day we have a classification task in day we have a classification task in front of us here but in neurom front of us here but in neurom translation let me motivate that translation let me motivate that discussion with this discussion with an", "image_path": "img_data/video_33_chunk_75.jpg"}
{"video": "video_33", "start": "0:38:00", "end": "0:38:30", "timestamp": "0:38:00 - 0:38:30", "text": "translation let me motivate that discussion with this discussion with an discussion with this discussion with an example u blue involves example u blue involves the calculation of precision using reference precision using reference sentences that are came from a human sentences that are came from a human translator and human expert translation translator and human expert translation and of course the predictor that we have and of course the predictor that we have built over here and that will be built over here and that will be called candidate sentences let called candidate sentences let me give you an example let's assume that me give you an example let's assume that we have the candidate", "image_path": "img_data/video_33_chunk_76.jpg"}
{"video": "video_33", "start": "0:38:30", "end": "0:39:00", "timestamp": "0:38:30 - 0:39:00", "text": "me give you an example let's assume that we have the candidate we have the candidate sentence that it is the sentence that it is the plane this is the translation the plane fluing from let's say athens and we have u two human experts that they receive the source experts that they receive the source sentence and they offered their two", "image_path": "img_data/video_33_chunk_77.jpg"}
{"video": "video_33", "start": "0:39:00", "end": "0:39:30", "timestamp": "0:39:00 - 0:39:30", "text": "experts that they receive the source sentence and they offered their two sentence and they offered their two reference sentences this reference sentences this is the plane the first expert plane the first expert translated the plane took off from athens obviously this one does not spell good news for our predictor and the good news for our predictor and the other sentence is the", "image_path": "img_data/video_33_chunk_78.jpg"}
{"video": "video_33", "start": "0:39:30", "end": "0:40:00", "timestamp": "0:39:30 - 0:40:00", "text": "good news for our predictor and the other sentence is the plane departed from athens actually i'll take that back what i just said about not spelling good news i just said about not spelling good news we'll see exactly how the blue we'll see exactly how the blue is sort of working and then we can is sort of working and then we can make some comment in summary what blue is to", "image_path": "img_data/video_33_chunk_79.jpg"}
{"video": "video_33", "start": "0:40:00", "end": "0:40:30", "timestamp": "0:40:00 - 0:40:30", "text": "make some comment in summary what blue is to in summary what blue is to make the definition blue measures sim similarity using the -cal n gs nr overlaps let's see exactly what an end gram is and look at this sort end gram is and look at this sort of a bit more let's look at the", "image_path": "img_data/video_33_chunk_80.jpg"}
{"video": "video_33", "start": "0:40:30", "end": "0:41:00", "timestamp": "0:40:30 - 0:41:00", "text": "end gram is and look at this sort of a bit more let's look at the of a bit more let's look at the byrams as an example let's form all the diagrams that example let's form all the diagrams that are associated with the sort of a are associated with the sort of a candidate sentence all right diagrams sentence all right diagrams from the kinded sentence we have a from the kinded sentence we have a bagram stand of by stands for bagram stand of by stands for two and u we have gram stands for two and u we have gram stands for the corresponding grst from the greek", "image_path": "img_data/video_33_chunk_81.jpg"}
{"video": "video_33", "start": "0:41:00", "end": "0:41:30", "timestamp": "0:41:00 - 0:41:30", "text": "two and u we have gram stands for the corresponding grst from the greek the corresponding grst from the greek word grma which means letter but in this word grma which means letter but in this case they are words we have case they are words we have the first bagram the plane the other diagram is plain flu the other one is let's say fluin the other one is i forgot to mention that diagrams", "image_path": "img_data/video_33_chunk_82.jpg"}
{"video": "video_33", "start": "0:41:30", "end": "0:42:00", "timestamp": "0:41:30 - 0:42:00", "text": "fluin the other one is i forgot to mention that diagrams i forgot to mention that diagrams are consecutive that's why i are consecutive that's why i don't have all the permutations here flu don't have all the permutations here flu in from and from aens now that we have form if you this diagrams we will be forming this diagrams we will be forming the usual kind of precision metric which if you recall from the kind of binary classification radar", "image_path": "img_data/video_33_chunk_83.jpg"}
{"video": "video_33", "start": "0:42:00", "end": "0:42:30", "timestamp": "0:42:00 - 0:42:30", "text": "metric which if you recall from the kind of binary classification radar kind of binary classification radar video we know it is a true positive video we know it is a true positive divided by true positive plus false divided by true positive plus false positives in order for us to positives in order for us to understand kind of what blue understand kind of what blue represents we are going to need to represents we are going to need to translate the something to correspond translate the something to correspond to thr positives from here and something to thr positives from here and something else to corresponds to false positive else to corresponds to false positive events we have to count ev events we have to count ev to create this kind of precision metric", "image_path": "img_data/video_33_chunk_84.jpg"}
{"video": "video_33", "start": "0:42:30", "end": "0:43:00", "timestamp": "0:42:30 - 0:43:00", "text": "events we have to count ev to create this kind of precision metric to create this kind of precision metric the true positives are going to the true positives are going to correspond to the following there are in correspond to the following there are in general engrams in the candidate sentences we only have here one in sentences we only have here one in candidate that are also in reference sentences how many also in reference sentences how many references we have and the false references we have and the false positive over here is going to be", "image_path": "img_data/video_33_chunk_85.jpg"}
{"video": "video_33", "start": "0:43:00", "end": "0:43:30", "timestamp": "0:43:00 - 0:43:30", "text": "references we have and the false positive over here is going to be positive over here is going to be engrams probably you have guessed engrams probably you have guessed it in candidate that are not in reference that is the false positive let's look at the bams of positive let's look at the bams of the candidate evidently the plane corresponds to a true positive the", "image_path": "img_data/video_33_chunk_86.jpg"}
{"video": "video_33", "start": "0:43:30", "end": "0:44:00", "timestamp": "0:43:30 - 0:44:00", "text": "evidently the plane corresponds to a true positive the plane corresponds to a true positive the plane corresponds to true positive plane corresponds to true positive because it's part of the because it's part of the reference plain flu well no reference flu plain flu well no reference has it that's a false has it that's a false positive flu in that is also false positive in from also forse positive and from mth that is definitely a true", "image_path": "img_data/video_33_chunk_87.jpg"}
{"video": "video_33", "start": "0:44:00", "end": "0:44:30", "timestamp": "0:44:00 - 0:44:30", "text": "positive and from mth that is definitely a true positive in terms of precision over here this kind of metric we have here this kind of metric we have two true positives divided by two true positives divided by two true positives plus three and therefore true positives plus three and therefore the precision is 2 fths the actual blue metric is more fths the actual blue metric is more complicated than this kind of discussion complicated than this kind of discussion we just had because we need to we just had because we need to calculate precisions for many types of calculate precisions for many types of engrams not only the diagrams we see", "image_path": "img_data/video_33_chunk_88.jpg"}
{"video": "video_33", "start": "0:44:30", "end": "0:45:00", "timestamp": "0:44:30 - 0:45:00", "text": "calculate precisions for many types of engrams not only the diagrams we see engrams not only the diagrams we see here but trigrams and on or unigrams here but trigrams and on or unigrams and we need to calculate the and we need to calculate the geometric mean of those and also account geometric mean of those and also account for the -called brevity for the -called brevity penalty that is actually called here penalty that is actually called here a beta and for that kind of reason you a beta and for that kind of reason this description is going know this description is going to be fairly involved as to what to be fairly involved as to what exactly is being calculated here and exactly is being calculated here and i in the references in your notes we are i in the references in your notes we are pointing to the -cal evaluate library", "image_path": "img_data/video_33_chunk_89.jpg"}
{"video": "video_33", "start": "0:45:00", "end": "0:45:20.533333", "timestamp": "0:45:00 - 0:45:20.533333", "text": "i in the references in your notes we are pointing to the -cal evaluate library pointing to the -cal evaluate library from hagging face also as well also from hagging face also as well also offering some other python offering some other python code to from n ldk to actually see code to from n ldk to actually see exactly how the sort of precision exactly how the sort of precision metrics are being calculated and what metrics are being calculated and what will be the corresponding blue will be the corresponding blue metric", "image_path": "img_data/video_33_chunk_90.jpg"}
{"video": "video_34", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we actually start discussing language models and as an discussing language models and as an example for those who are not familiar example for those who are not familiar with the term i guess most of with the term i guess most of you used a language model in the past you used a language model in the past every time you are trying to enter a every time you are trying to enter a search query in the google search bar search query in the google search bar you're using a language model you can you're using a language model you can actually try it right now you can start actually try it right now you can start typing what is the and then typing what is the and then immediately you can notice that google immediately you can notice that google is suggesting", "image_path": "img_data/video_34_chunk_0.jpg"}
{"video": "video_34", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "immediately you can notice that google is suggesting the top guesses or predictions of the top guesses or predictions of what google believes that you are what google believes that you are intending to type next that is a intending to type next that is a specific version of a language model specific version of a language model that is actually called auto regressive that is actually called auto regressive model the prediction of what's comes model the prediction of what's comes next the next token is what we'll be next the next token is what we'll be discussing in this video a discussing in this video a bit more sort of mathematically we actually going to be mathematically we actually going to be dealing with the modeling of the", "image_path": "img_data/video_34_chunk_1.jpg"}
{"video": "video_34", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "mathematically we actually going to be dealing with the modeling of the dealing with the modeling of the following sort of probability which is the center not the center word the word that the center word the word that actually coming next let's call this actually coming next let's call this word wt given the wt minus one wt minus wt given the wt minus one wt minus 2 and on let's say wt minus n comma 2 and on let's say wt minus n comma theta this is the model that we are", "image_path": "img_data/video_34_chunk_2.jpg"}
{"video": "video_34", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "2 and on let's say wt minus n comma theta this is the model that we are theta this is the model that we are going to sort of discuss and this is going to sort of discuss and this is let me call this a context the sequence of words that precede the word that we are trying to precede the word that we are trying to predict is going to be called the predict is going to be called the context and what we are going to context and what we are going to be using for this is initially we using for this is initially we will be dealing with this p model will be dealing with this p model construction using a neural", "image_path": "img_data/video_34_chunk_3.jpg"}
{"video": "video_34", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "will be dealing with this p model construction using a neural construction using a neural architecture we have seen in the past architecture we have seen in the past and we will be doing dealing with this and we will be doing dealing with this recurrent neural network the curr unit architectures we have seen them in an earlier we have seen them in an earlier video and more specifically out of the video and more specifically out of the many recurrent architectures we will be assuming", "image_path": "img_data/video_34_chunk_4.jpg"}
{"video": "video_34", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "many recurrent architectures we will be assuming architectures we will be assuming that we are going to be using the that we are going to be using the -called lstm or long short-term memory -called lstm or long short-term memory architecture and today we are architecture and today we are language models are dominated by what is language models are dominated by what is called transformer architectures and but a natural architectures and but a natural progression to reach the architecture progression to reach the architecture and understand them it has to go through and understand them it has to go through understanding first language understanding first language models for the which are rnn models for the which are rnn based and let us try to use", "image_path": "img_data/video_34_chunk_5.jpg"}
{"video": "video_34", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "models for the which are rnn based and let us try to use based and let us try to use this lstm and for those who have not this lstm and for those who have not heard about this term before then i heard about this term before then i suggest that you go and review that suggest that you go and review that video on rnn and lstm video on rnn and lstm architectures but if you recall an architectures but if you recall an rnn has this and i'll be using the rnn has this and i'll be using the rnn lstm interchangeably when i say rnn lstm interchangeably when i say rnn i mean lstm i will be if you i mean lstm i will be if you remember from the rnn architecture the remember from the rnn architecture the rnn had a feedback", "image_path": "img_data/video_34_chunk_6.jpg"}
{"video": "video_34", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "remember from the rnn architecture the rnn had a feedback a path because it was modeling the a path because it was modeling the an equation that involved the previous an equation that involved the previous hidden state and we actually went hidden state and we actually went ahead and we constructed the unrolled ahead and we constructed the unrolled version of the rnn in that version of the rnn in that video we discussed it that helps us kind video we discussed it that helps us kind of understand what is happening in this of understand what is happening in this model the rnn's in this model the rnn's let me call this rnn is a trans sing is accepting some", "image_path": "img_data/video_34_chunk_7.jpg"}
{"video": "video_34", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "let me call this rnn is a trans sing is accepting some rnn is a trans sing is accepting some initial hidden state let's call this ht initial hidden state let's call this ht minus n + 1 this minus one sorry this minus n + 1 this minus one sorry this hidden state and we have here we are receiving this xt here we are receiving this xt minus n and i kind of change a little bit n and i kind of change a little bit the notation because we back", "image_path": "img_data/video_34_chunk_8.jpg"}
{"video": "video_34", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "n and i kind of change a little bit the notation because we back the notation because we back then we've been dealing with x as an then we've been dealing with x as an input to the rnn and these input to the rnn and these two will be finally consistent because two will be finally consistent because this x is produced via an edding this x is produced via an edding transformation or embedding and coding transformation or embedding and coding one of the embeddings we have seen in one of the embeddings we have seen in another video is called war to vac another video is called war to vac obviously there are many other obviously there are many other embeddings possible but let me call it em for possible but let me call it em for short and the embedding is let's say", "image_path": "img_data/video_34_chunk_9.jpg"}
{"video": "video_34", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "possible but let me call it em for short and the embedding is let's say short and the embedding is let's say the one hot encoded vector wt minus one this is this vector wt minus one this is the input and finally a hidden is the input and finally a hidden state is being generated and of course i understand generated and of course i understand that in the lsdm discussion we had used that in the lsdm discussion we had used both h and s as the hidden state to both h and s as the hidden state to describe what is happening there one was describe what is happening there one was called the cell state and the other was called the cell state and the other was the s of a called hidden state over", "image_path": "img_data/video_34_chunk_10.jpg"}
{"video": "video_34", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "called the cell state and the other was the s of a called hidden state over the s of a called hidden state over here i have combined both into this here i have combined both into this letter h to simplify little bit the letter h to simplify little bit the notation and of course we have here notation and of course we have here the head that and takes this kind of hidden state and takes this kind of hidden state and i'll just call it head for and i'll just call it head for lack of a better term and produces lack of a better term and produces the y hat of t minus", "image_path": "img_data/video_34_chunk_11.jpg"}
{"video": "video_34", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "lack of a better term and produces the y hat of t minus the y hat of t minus n + 1 it's actually important to it's actually important to understand the indices maybe i can just use an indices maybe i can just use an example we have a p model of w let's say five given w4 w3 w2 w1 the", "image_path": "img_data/video_34_chunk_12.jpg"}
{"video": "video_34", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "w1 the first word is actually coming in then first word is actually coming in then the subsequent word is going to be the subsequent word is going to be predicted at the output this is predicted at the output this is repeated here with evidently different repeated here with evidently different indices let me just do that quickly and this is the edding layer again xt minus n + one the di of this is d", "image_path": "img_data/video_34_chunk_13.jpg"}
{"video": "video_34", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "one the di of this is d pi1 and the dimensions of this pi1 and the dimensions of this is cardinality of the vocabulary times is cardinality of the vocabulary times one and for those who have not familiar one and for those who have not familiar with this term please look at the edding with this term please look at the edding video and here we have the head that is producing the y hat of t - head that is producing the y hat of t - n + 2", "image_path": "img_data/video_34_chunk_14.jpg"}
{"video": "video_34", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "head that is producing the y hat of t - n + 2 this is going to continue until we have produced all continue until we have produced all of the words in this kind of a context or all the words of the of all the words of the of what is a corpus is actually giving", "image_path": "img_data/video_34_chunk_15.jpg"}
{"video": "video_34", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "all the words of the of what is a corpus is actually giving us and the output of the head is the y hat of t w1 w2 w3 4 and then it will be producing the y had five in this case", "image_path": "img_data/video_34_chunk_16.jpg"}
{"video": "video_34", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "t w1 w2 w3 4 and then it will be producing the y had five in this case be producing the y had five in this case and evidently it is going to and evidently it is going to be a posterior probability that is associated with the prediction of the at this moment in time prediction of the at this moment in time given the hidden state that it was built given the hidden state that it was built by the rnn the hidden state is by the rnn the hidden state is adjusting is adjusted for every single adjusting is adjusted for every single word is actually coming in and evidently word is actually coming in and evidently as we discussed in rnn we do not", "image_path": "img_data/video_34_chunk_17.jpg"}
{"video": "video_34", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "word is actually coming in and evidently as we discussed in rnn we do not as we discussed in rnn we do not necessarily need to predict at every necessarily need to predict at every moment in time but the model is moment in time but the model is able to do but at the end of the able to do but at the end of the day what you see on the google search day what you see on the google search bar in that kind of use case we just bar in that kind of use case we just looked at is the predictions show looked at is the predictions show up at the when you stop typing but you up at the when you stop typing but you could produce predictions as you're could produce predictions as you're typing along and this is the reason typing along and this is the reason why we actually had this heads", "image_path": "img_data/video_34_chunk_18.jpg"}
{"video": "video_34", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "typing along and this is the reason why we actually had this heads this is yeah that's basically the block diagram a very block diagram a very straightforward for the case of straightforward for the case of rnn and it's over here rnn and it's over here the head probably you have seen this the head probably you have seen this prediction has to be at the this prediction has to be at the granularity of the vocabulary right size", "image_path": "img_data/video_34_chunk_19.jpg"}
{"video": "video_34", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "this prediction has to be at the granularity of the vocabulary right size granularity of the vocabulary right size here the head is expanding the head is expanding the head is expanding the dimensions from whatever dimensions we dimensions from whatever dimensions we have decided to construct the hidden have decided to construct the hidden state from to the potentially a million state from to the potentially a million dimensions that could correspond to the dimensions that could correspond to the vocabulary of the of that vocabulary of the of that contains all the tokens and this of contains all the tokens and this of course the number of tokens and course the number of tokens and therefore the size of the vocabulary is therefore the size of the vocabulary is a function of your tokenizer a function of your tokenizer that's basically the initial", "image_path": "img_data/video_34_chunk_20.jpg"}
{"video": "video_34", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "a function of your tokenizer that's basically the initial that's basically the initial kind of block diagram and in the core side you see if diagram and in the core side you see if you a simple example which is you a simple example which is exactly identical what we just threw exactly identical what we just threw over here where we have some in kind of over here where we have some in kind of state we have a sentence let's say the state we have a sentence let's say the student open there and then as the student open there and then as the hidden state is being hidden state is being propagated forward at the very end propagated forward at the very end we are able we are ahead to predict we are able we are ahead to predict the what's com next we are going to", "image_path": "img_data/video_34_chunk_21.jpg"}
{"video": "video_34", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "we are able we are ahead to predict the what's com next we are going to the what's com next we are going to be producing a v dimensional posterior be producing a v dimensional posterior probability distribution and out of them in this distribution and out of them in this specific case we will expect to see specific case we will expect to see books laptops and on these books laptops and on these are this is basically very simple are this is basically very simple diagram and in terms of training we will diagram and in terms of training we will be training the rnns in a very similar way as we have rnns in a very similar way as we have seen earlier we are going to have a seen earlier we are going to have a classification use case there because we classification use case there because we are trying to predict are trying to predict a specific a discrete word out of", "image_path": "img_data/video_34_chunk_22.jpg"}
{"video": "video_34", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "are trying to predict a specific a discrete word out of a specific a discrete word out of the specific token id out of the v the specific token id out of the v possible token ids v classes in other possible token ids v classes in other words the number of classes here is quite sizable and classes here is quite sizable and we are going to be using again cross we are going to be using again cross entropy for everything we have entropy for everything we have discussed in the maximum likelihood discussed in the maximum likelihood section and in this case the cross section and in this case the cross entropy takes a fairly simple form entropy takes a fairly simple form and we going to have to we're going and we going to have to we're going to have a", "image_path": "img_data/video_34_chunk_23.jpg"}
{"video": "video_34", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "and we going to have to we're going to have a loss for every head that we are able loss for every head that we are able to sort of form at every step on the to sort of form at every step on the way we are going to for every word way we are going to for every word which is actually coming in we are going which is actually coming in we are going to have a prediction and we are going to have a prediction and we are going to calculate based on that prediction to calculate based on that prediction the cross entropy loss for this specific the cross entropy loss for this specific time point of time and then we time point of time and then we are going to have another loss and are going to have another loss and at the end of the day another loss and at the end of the day our total loss is going to be the", "image_path": "img_data/video_34_chunk_24.jpg"}
{"video": "video_34", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "another loss and at the end of the day our total loss is going to be the our total loss is going to be the summation the sample mean of all the summation the sample mean of all the losses where capital t here corresponds losses where capital t here corresponds to the small letter n which i drew to the small letter n which i drew over there is the number of context over there is the number of context words that we are going to be words that we are going to be considering in our u in our prediction considering in our u in our prediction and of course there are certain ocares and of course there are certain ocares in the construction of the in the construction of the language models in terms of language models in terms of implementation and what i think we implementation and what i think we should be a good idea to do now is to should be a good idea to do now is to go through the python code of a", "image_path": "img_data/video_34_chunk_25.jpg"}
{"video": "video_34", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "should be a good idea to do now is to go through the python code of a to go through the python code of a very simple but i would call it this very simple but i would call it this actually a very popular language actually a very popular language model called language le character model called language le character level rnn and it was constructed many level rnn and it was constructed many years ago but it's actually very years ago but it's actually very instructive to go through and understand instructive to go through and understand all of these kind of oses all of these kind of oses associated with how we are making associated with how we are making this next character prediction we are this next character prediction we are not going to have an elaborate not going to have an elaborate tokenization or on elaborate kind of a", "image_path": "img_data/video_34_chunk_26.jpg"}
{"video": "video_34", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "not going to have an elaborate tokenization or on elaborate kind of a tokenization or on elaborate kind of a bedding scheme but we're just going to bedding scheme but we're just going to focus on the on this kind of blog focus on the on this kind of blog diagram and see how it is implemented diagram and see how it is implemented in python back to this in python back to this implementation in this in our core implementation in this in our core site we have this simple rnn language site we have this simple rnn language model notebook and you can actually go model notebook and you can actually go ahead and open this in collab by ahead and open this in collab by clicking that kind of button and in this clicking that kind of button and in this notebook if i make it a bit larger notebook if i make it a bit larger you can actually see that we have some you can actually see that we have some kind of data that corresponds to some", "image_path": "img_data/video_34_chunk_27.jpg"}
{"video": "video_34", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "you can actually see that we have some kind of data that corresponds to some kind of data that corresponds to some kind of wikipedia paragraph about a an kind of wikipedia paragraph about a an island in greece called hios and just island in greece called hios and just describes if you the island and describes if you the island and on it's just a very small on it's just a very small sentence that it is given as input sentence that it is given as input and u the notebook itself was written by and u the notebook itself was written by andre karpathy when he was in andre karpathy when he was in stanford many years ago and stanford many years ago and nevertheless what is happening over", "image_path": "img_data/video_34_chunk_28.jpg"}
{"video": "video_34", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "stanford many years ago and nevertheless what is happening over nevertheless what is happening over here is that initially we have the here is that initially we have the conversion from u hidden states sorry conversion from u hidden states sorry from the conversion from the characters from the conversion from the characters it's a character level kind of model it's a character level kind of model into tokens and we initially also into tokens and we initially also have the definition of all the have the definition of all the parameters of an rnn now the convention parameters of an rnn now the convention that is being followed over here in that is being followed over here in terms of notation is different from the terms of notation is different from the one we have seen in the notes", "image_path": "img_data/video_34_chunk_29.jpg"}
{"video": "video_34", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "terms of notation is different from the one we have seen in the notes one we have seen in the notes but the indices every single matrix but the indices every single matrix that we have seen in the rnn video that we have seen in the rnn video the -cal u matrices for the -cal u matrices for the input and this corresponds to the input and this corresponds to the w xh matrix the w matrix we have w xh matrix the w matrix we have seen corresponds to whh matrix here and seen corresponds to whh matrix here and the w and the uv matrix for the head the w and the uv matrix for the head corresponds to the w h1 matrix and also", "image_path": "img_data/video_34_chunk_30.jpg"}
{"video": "video_34", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "the w and the uv matrix for the head corresponds to the w h1 matrix and also corresponds to the w h1 matrix and also we have some bias terms we have a bias we have some bias terms we have a bias term called b for the input and this is term called b for the input and this is the bh and the we also had a bias for the bh and the we also had a bias for the that was called c for the head the that was called c for the head and this is called b y over here and this is called b y over here these are the mappings that you need to these are the mappings that you need to be aware of as you are browsing if you be aware of as you are browsing if you the this kind of original kind of the this kind of original kind of code but if you can absorb this kind of code but if you can absorb this kind of mappings then you i think we are in mappings then you i think we are in good shape to proceed our", "image_path": "img_data/video_34_chunk_31.jpg"}
{"video": "video_34", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "mappings then you i think we are in good shape to proceed our good shape to proceed our dimensionality for the hidden state is dimensionality for the hidden state is 100 and we have now a sequence 100 and we have now a sequence length of 25 now we have not really length of 25 now we have not really quoted how the sequence length in the quoted how the sequence length in the language model which we just discussed language model which we just discussed how this sequence length is going to be how this sequence length is going to be used but it's actually a key component used but it's actually a key component of the implementation we have selected a of the implementation we have selected a hyperparameter called learning rate for hyperparameter called learning rate for our stochastic grade descent or some cin", "image_path": "img_data/video_34_chunk_32.jpg"}
{"video": "video_34", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "hyperparameter called learning rate for our stochastic grade descent or some cin our stochastic grade descent or some cin of stochastic grade descent that we are of stochastic grade descent that we are going to be using here and if we going to be using here and if we actually run this notebook this actually run this notebook this specific cell we recognize that we have specific cell we recognize that we have 8 characters which are coming at the 8 characters which are coming at the input and out of these 58 we have 43 input and out of these 58 we have 43 characters which are characters which are unique the unique characters evidently corresponds unique characters evidently corresponds to the tokens here we have a to the tokens here we have a character based model effectively a token is", "image_path": "img_data/video_34_chunk_33.jpg"}
{"video": "video_34", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "character based model effectively a token is based model effectively a token is corresponds to the unique characters corresponds to the unique characters in the text we have here in the text we have here another function called sample and another function called sample and in this function what actually we are in this function what actually we are doing this sample function is being doing this sample function is being called in the main function which is called in the main function which is called the loss function the loss called the loss function the loss we'll see that what it does by looking we'll see that what it does by looking at the loss function the loss at the loss function accepts inputs accepts function accepts inputs accepts evidently the y's that targets and", "image_path": "img_data/video_34_chunk_34.jpg"}
{"video": "video_34", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "function accepts inputs accepts evidently the y's that targets and evidently the y's that targets and accepts also the previous hidden state accepts also the previous hidden state the h previous and it is basically the h previous and it is basically calculating another loss for based on calculating another loss for based on this if you see back to our notes this if you see back to our notes over here if you see the block over here if you see the block diagram then the inputs are this ones diagram then the inputs are this ones we have the previous hidden state which we have the previous hidden state which is this one and at the top based on the is this one and at the top based on the y cut and the ground truth y will be y cut and the ground truth y will be calculating a loss this is calculating a loss this is typically put after this kind of why", "image_path": "img_data/video_34_chunk_35.jpg"}
{"video": "video_34", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "calculating a loss this is typically put after this kind of why typically put after this kind of why hat is estimated all right hat is estimated all right let's see the elements of the l kind let's see the elements of the l kind of function we have the forward pass here and the forward pass here and the in the for part equations are the in the for part equations are the exactly the same equations as we have exactly the same equations as we have seen in the rnn video we have a seen in the rnn video we have a hyperbolic time nonlinearity mind you hyperbolic time nonlinearity mind you this is not the lsdm implementation this is not the lsdm implementation that's a simple rnn layer that's a simple rnn layer implementation it's even the forward", "image_path": "img_data/video_34_chunk_36.jpg"}
{"video": "video_34", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "that's a simple rnn layer implementation it's even the forward implementation it's even the forward pass equations are extremely pass equations are extremely straightforward to follow and we straightforward to follow and we have first the inputs been have first the inputs been computing the one hot en computing the one hot en coding and we don't have any a coding and we don't have any a bedding layer over here we do not bedding layer over here we do not have any war tove or any other more have any war tove or any other more complicated kind of a bedding scheme complicated kind of a bedding scheme we just use the one hoten codings we just use the one hoten codings straight into the rnn all right", "image_path": "img_data/video_34_chunk_37.jpg"}
{"video": "video_34", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "we just use the one hoten codings straight into the rnn all right straight into the rnn all right then we calculate the loss it's then we calculate the loss it's exactly the same formula we just saw exactly the same formula we just saw earlier and the t index here is earlier and the t index here is the range over the from one to the range over the from one to the number of inputs and we need to number of inputs and we need to understand this number of inputs that we understand this number of inputs that we have what kind of inputs we have where have what kind of inputs we have where they're coming from a little bit in a they're coming from a little bit in a more detailed way because it's quite more detailed way because it's quite important for that what we need to do is important for that what we need to do is we need to look at this cell which is", "image_path": "img_data/video_34_chunk_38.jpg"}
{"video": "video_34", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "important for that what we need to do is we need to look at this cell which is we need to look at this cell which is obviously running still because it is obviously running still because it is running forever in the specific version running forever in the specific version of the code and here you can actually of the code and here you can actually see that we are going to be forming some see that we are going to be forming some windows and there are two versions of windows and there are two versions of rnns that we typically we implement and rnns that we typically we implement and each version has certain advantages each version has certain advantages and disadvantages let's look at the and disadvantages let's look at the sort of these two versions let's sort of these two versions let's look at let me titly look at let me titly training of", "image_path": "img_data/video_34_chunk_39.jpg"}
{"video": "video_34", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "look at let me titly training of a character based rnn this kind of discussion we have a typically as discussion we have a typically as input let's say this the sentence touch input let's say this the sentence touch heos is a island of xyz i don't know square island of xyz i don't know square kilometers and on all right the", "image_path": "img_data/video_34_chunk_40.jpg"}
{"video": "video_34", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "island of xyz i don't know square kilometers and on all right the kilometers and on all right the first version of rnn that we first version of rnn that we typically implement is the -called typically implement is the -called stateless rnn and in the stateless rnn and in the stateless rnn we are forming a window and this is the we are forming a window and this is the sequence length if you we sequence length if you we need to sort of form the training need to sort of form the training data from using the", "image_path": "img_data/video_34_chunk_41.jpg"}
{"video": "video_34", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "need to sort of form the training data from using the obviously the text that we have obviously the text that we have the autoaggressive kind of language the autoaggressive kind of language models that we are dealing with here are models that we are dealing with here are extremely straightforward to create extremely straightforward to create evidently training data from because evidently training data from because the label is in the actual text itself the label is in the actual text itself we have here our x's let's assume that this is my x's let's assume that this is my x the sequence of tokens i'm going to", "image_path": "img_data/video_34_chunk_42.jpg"}
{"video": "video_34", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "x's let's assume that this is my x the sequence of tokens i'm going to this is my sequence of tokens and for every token that can form another window which is that can form another window which is going to be offsetted by one relative to going to be offsetted by one relative to the x and this will be the sequence of the x and this will be the sequence of my y the target variables and my y the target variables and this is going to be let's this is going to be let's say part of a specific say let's say part of a specific batch this is the", "image_path": "img_data/video_34_chunk_43.jpg"}
{"video": "video_34", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "say let's say part of a specific batch this is the first example and this will be let's say example and this will be let's say another example will actually be from another example will actually be from u the o whatever is this is the sequence length also we have seen this as capital t in the formulas in lang language t in the formulas in lang language modeling and this is by offsetted by", "image_path": "img_data/video_34_chunk_44.jpg"}
{"video": "video_34", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "t in the formulas in lang language modeling and this is by offsetted by one this will be my corresponding label y this is x1 y1 x2 y2 and on we are going to y1 x2 y2 and on we are going to be able to form batches let's say a buet that corresponds to b one will correspond to", "image_path": "img_data/video_34_chunk_45.jpg"}
{"video": "video_34", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "batches let's say a buet that corresponds to b one will correspond to corresponds to b one will correspond to example one example two example 35 you example one example two example 35 the typical thing we do to form know the typical thing we do to form a batch is to sample from the number of a batch is to sample from the number of examples that we have and we form a batches we are forming batches we are forming batches based on the kind of data set whatever based on the kind of data set whatever batches the number of mini batches the number of mini batches that we have is let's say batches that we have is let's say capital k that is this is the approach in", "image_path": "img_data/video_34_chunk_46.jpg"}
{"video": "video_34", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "capital k that is this is the approach in that is this is the approach in this kind of a stateless rnn and in this kind of a stateless rnn and in the in this kind of approach we are not the in this kind of approach we are not maintaining anything as to what will maintaining anything as to what will be the hidden state from this be the hidden state from this window and the corresponding x to that window and the corresponding x to that window which is another x which is window which is another x which is offset it to compare to this window offset it to compare to this window we don't maintain the state and we don't maintain the state and that's why we call it stateless we don't that's why we call it stateless we don't store the state anywhere there's another store the state anywhere there's another category of rnns that is implementation category of rnns that is implementation is called stateful", "image_path": "img_data/video_34_chunk_47.jpg"}
{"video": "video_34", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "category of rnns that is implementation is called stateful annn stateful rnn and your kind of annn stateful rnn and your kind of python textbook the one that we are python textbook the one that we are recommending is the hands on machine learning of intens of hands on machine learning of intens of flow in k using intens of flow k book is flow in k using intens of flow k book is has some kind of a extensive discussion has some kind of a extensive discussion about this in the in this about this in the in this chapter where they discuss rnns in the chapter where they discuss rnns in the state fold rnn we do we are state fold rnn we do we are forming again the window let me see if i", "image_path": "img_data/video_34_chunk_48.jpg"}
{"video": "video_34", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "state fold rnn we do we are forming again the window let me see if i forming again the window let me see if i can draw the window exactly the same can draw the window exactly the same extent this is my first window extent this is my first window this is a second offset it by one this is a second offset it by one this is going to be my x one and this is going to be my x one and y1 but the second window is offset it y1 but the second window is offset it is actually for exactly where the is actually for exactly where the other window is", "image_path": "img_data/video_34_chunk_49.jpg"}
{"video": "video_34", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "is actually for exactly where the other window is ends the we maintain if you gradient flow and the back propagation flow and the back propagation over that kind of sequence length that's over that kind of sequence length that's one thing and but in the one case we one thing and but in the one case we are forgetting the hidden state we are forgetting the hidden state we start from scratch in this in another", "image_path": "img_data/video_34_chunk_50.jpg"}
{"video": "video_34", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "are forgetting the hidden state we start from scratch in this in another start from scratch in this in another window but over here we actually window but over here we actually maintain that state and the advantage of a stateful state and the advantage of a stateful rnn is that we do maintain by rnn is that we do maintain by maintaining you this state we can maintaining you this state we can actually capture longer term kind of actually capture longer term kind of dependencies although the back dependencies although the back propagation is still happening over propagation is still happening over the 15 or whatever the size of our the 15 or whatever the size of our sequence length is and therefore we sequence length is and therefore we see an advantage on the other hand", "image_path": "img_data/video_34_chunk_51.jpg"}
{"video": "video_34", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "sequence length is and therefore we see an advantage on the other hand we see an advantage on the other hand there are some other disadvantages for there are some other disadvantages for example we cannot really easily do example we cannot really easily do batching and we are limiting our case batching and we are limiting our case to bat sizes of one back to our code to bat sizes of one back to our code over here you can actually see the over here you can actually see the preparation of the input is the preparation of the input is the generation of those windows and we are able to as you windows and we are able to as you can see here the windows are the window can see here the windows are the window from that it is involved sequence length from that it is involved sequence length size of sequence length windows and we", "image_path": "img_data/video_34_chunk_52.jpg"}
{"video": "video_34", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "from that it is involved sequence length size of sequence length windows and we size of sequence length windows and we have inputs the x's and the y's the have inputs the x's and the y's the targets y's are the inputs offset it by targets y's are the inputs offset it by one and we are going one and we are going to then create if you call to then create if you call this kind of function sample which will this kind of function sample which will u effectively bring in one example of u effectively bring in one example of this structure over here into the of this structure over here into the network and we will u do the", "image_path": "img_data/video_34_chunk_53.jpg"}
{"video": "video_34", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "of this structure over here into the network and we will u do the network and we will u do the forward propagation and the back forward propagation and the back propagation by calling the loss propagation by calling the loss function and we will be u averaging the function and we will be u averaging the loss function over i think this size loss function over i think this size is 1,000 time intervals over here and we is 1,000 time intervals over here and we will print it at the output at the will print it at the output at the end and we will maintain also some of end and we will maintain also some of the gradients into a tape in order the gradients into a tape in order for us to potentially observe them for us to potentially observe them later that's basically also", "image_path": "img_data/video_34_chunk_54.jpg"}
{"video": "video_34", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "for us to potentially observe them later that's basically also later that's basically also we have on this front over here we have on this front over here we have the learning rate that it's have the learning rate that it's been adjusted with respect to the u adjusted with respect to the u typical adr that we have over here typical adr that we have over here as compared to other to the typical as compared to other to the typical adrad we actually see the what adrad we actually see the what happens at the as the number of", "image_path": "img_data/video_34_chunk_55.jpg"}
{"video": "video_34", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "adrad we actually see the what happens at the as the number of happens at the as the number of iterations are progressing i think iterations are progressing i think if we stop this over here and start if we stop this over here and start from the beginning start these iterations beginning start these iterations from the code in the beginning over from the code in the beginning over here you can see at the initial now i've here you can see at the initial now i've been running it for some time but been running it for some time but initially we've been initially we've been i guess you can always go here and press the", "image_path": "img_data/video_34_chunk_56.jpg"}
{"video": "video_34", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "i guess you can always go here and press the you can always go here and press the play button and you can play button and you can see that at iteration one initially we're actually getting initially we're actually getting some kind of a garbage but as the iterations are garbage but as the iterations are progressing the first one is not progressing the first one is not valid but the second one is the one we", "image_path": "img_data/video_34_chunk_57.jpg"}
{"video": "video_34", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "progressing the first one is not valid but the second one is the one we valid but the second one is the one we start getting the actual start getting the actual output as the iterations are progressing output as the iterations are progressing as we have seen earlier then the as we have seen earlier then the model is able to learn the model is able to learn the dependencies time dependencies and this dependencies time dependencies and this patterns the character level language patterns the character level language patterns and we're able to do fairly patterns and we're able to do fairly good prediction on the other hand you good prediction on the other hand you can see that the training process lasts can see that the training process lasts for some time and the reason why it for some time and the reason why it lasts for some time is because the rnn", "image_path": "img_data/video_34_chunk_58.jpg"}
{"video": "video_34", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "for some time and the reason why it lasts for some time is because the rnn lasts for some time is because the rnn as we discussed are sequential and as we discussed are sequential and therefore we expect to see we therefore we expect to see we have to wait until all the tokens are have to wait until all the tokens are coming in over the sequence length until coming in over the sequence length until we get an update and then we reset we get an update and then we reset the grade and we do another window and the grade and we do another window and on this is the sort of stateless on this is the sort of stateless rnn implementation and what we need to do implementation and what we need to do next is we need to look at other tasks next is we need to look at other tasks that are a bit more advanced than the", "image_path": "img_data/video_34_chunk_59.jpg"}
{"video": "video_34", "start": "0:30:00", "end": "0:30:10.400000", "timestamp": "0:30:00 - 0:30:10.400000", "text": "next is we need to look at other tasks that are a bit more advanced than the that are a bit more advanced than the simple task of language model we have simple task of language model we have here and the next task for us is to look here and the next task for us is to look at -called sequence to sequence at -called sequence to sequence models", "image_path": "img_data/video_34_chunk_60.jpg"}
{"video": "video_35", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video now we will see how we will be generating this will be generating this embeddings the conversion if you embeddings the conversion if you from the tokens into vectors let me from the tokens into vectors let me just draw this the final kind of go goal just draw this the final kind of go goal this kind of an edding encoder that will take as input the token", "image_path": "img_data/video_35_chunk_0.jpg"}
{"video": "video_35", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "input the token and we'll create a vector of some length that is vector of some length that is going to represent that kind of token going to represent that kind of token we've seen from the an earlier video on we've seen from the an earlier video on tokenization that at the end of the day tokenization that at the end of the day we are getting a sequence of we are getting a sequence of tokens for each that corresponds to tokens for each that corresponds to our text let's say 1,535 things that are", "image_path": "img_data/video_35_chunk_1.jpg"}
{"video": "video_35", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "1,535 things that are coming at the input of this edding coming at the input of this edding encoder and our vectors will encoder and our vectors will be d dimensional we will be using be d dimensional we will be using the letter d to represent the vector the letter d to represent the vector vectors will be represented with the vectors will be represented with the usual letter x that we represent also usual letter x that we represent also data x is going to be this is x data x is going to be this is x that corresponds to 5 35 token and on and x", "image_path": "img_data/video_35_chunk_2.jpg"}
{"video": "video_35", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "that corresponds to 5 35 token and on and x to 5 35 token and on and x is going to be d dimensional is going to be d dimensional that is the ro the role of the that is the ro the role of the bedding kind of a layer as we have bedding kind of a layer as we have seen every neural architecture requires seen every neural architecture requires a vector at the input in order for it to a vector at the input in order for it to be able to produce a prediction that be able to produce a prediction that corresponds to the task that we have at corresponds to the task that we have at hand and an obvious way to hand and an obvious way to actually create a vector that as", "image_path": "img_data/video_35_chunk_3.jpg"}
{"video": "video_35", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "hand and an obvious way to actually create a vector that as actually create a vector that as it turns out it may not actually work it turns out it may not actually work very well is to encode these vectors very well is to encode these vectors in the -cal one hot encoding in the -cal one hot encoding let's assume that we have a word that let's assume that we have a word that is corresponds also to a token is corresponds also to a token that is called the hotel this is that is called the hotel this is our token over here and this our token over here and this indeed corresponds to the in a indeed corresponds to the in a token of 1,53", "image_path": "img_data/video_35_chunk_4.jpg"}
{"video": "video_35", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "indeed corresponds to the in a token of 1,53 5 that's a token id one way to encode 5 that's a token id one way to encode the hotel into a vector is to we the hotel into a vector is to we have a vocabulary size which is the have a vocabulary size which is the total number of tokens that the total number of tokens that the tokenizer cannot produced for our corpus tokenizer cannot produced for our corpus and we have all zeros and at the 1,535 location index", "image_path": "img_data/video_35_chunk_5.jpg"}
{"video": "video_35", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "1,535 location index of this kind of vector we have a one this is d dimensional where d is the dimensional where d is the vocabulary size in this case we have vocabulary size in this case we have the cardinality of our vocabulary now the cardinality of our vocabulary now we'll be symbolizing the vocabulary a set of all the tokens in other words of the tokenizer produ with", "image_path": "img_data/video_35_chunk_6.jpg"}
{"video": "video_35", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "vocabulary a set of all the tokens in other words of the tokenizer produ with other words of the tokenizer produ with capital v and this sometimes in the capital v and this sometimes in the literature you use that to represent the -called that to represent the -called cardinality operator in other words how many elements do we have here sometimes we'll elements do we have here sometimes we'll use this cardinality operator sometimes use this cardinality operator sometimes we'll skip it for kind of keeping the we'll skip it for kind of keeping the notation a bit simply simple notation a bit simply simple that's basically my", "image_path": "img_data/video_35_chunk_7.jpg"}
{"video": "video_35", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "notation a bit simply simple that's basically my the representation of the hotel the representation of the hotel and it's not really difficult to and it's not really difficult to see that if we are to use this see that if we are to use this representation we are going to have some representation we are going to have some issues with every processing issues with every processing that involves some form of extraction that involves some form of extraction of correlations or patterns in a text of correlations or patterns in a text of corpus and the way to one way", "image_path": "img_data/video_35_chunk_8.jpg"}
{"video": "video_35", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "of correlations or patterns in a text of corpus and the way to one way of corpus and the way to one way to actually show it is which always to actually show it is which always involves some form of a dot product involves some form of a dot product we've seen in earlier videos that most we've seen in earlier videos that most neural architectures will involve neural architectures will involve projections and projection projections and projection represented by dot product is going to represented by dot product is going to create the following kind of issue create the following kind of issue let's assume that i have another let's assume that i have another word that it is the x is one let's word that it is the x is one let's say 2010 and this word is corresponds to 2010 and this word is corresponds to the word", "image_path": "img_data/video_35_chunk_9.jpg"}
{"video": "video_35", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "2010 and this word is corresponds to the word model following if you the same kind of approach we will be same kind of approach we will be positioning a one at location 2010 and on we see that by construction the two vectors are construction the two vectors are orthogonal and which means that their orthogonal and which means that their dot product if we write the dot product if we write the dot product over here will be zero and which means", "image_path": "img_data/video_35_chunk_10.jpg"}
{"video": "video_35", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "dot product if we write the dot product over here will be zero and which means over here will be zero and which means that it causes the following kind of that it causes the following kind of issue the two words are very similar issue the two words are very similar what we expect to see from their do what we expect to see from their do product kind of operation that they do prodct of operation that they do prodct of similar semantically similar u words to similar semantically similar u words to be the maximum possible that kind be the maximum possible that kind of requirement is not satisfied from of requirement is not satisfied from the kind of a one hod and coding repres the kind of a one hod and coding repres presentation which is obviously a kind", "image_path": "img_data/video_35_chunk_11.jpg"}
{"video": "video_35", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "the kind of a one hod and coding repres presentation which is obviously a kind presentation which is obviously a kind of a trivial representation our task of a trivial representation our task here is to come up with representations here is to come up with representations beddings that we are that are beddings that we are that are semantically similar words are in some d semantically similar words are in some d dimensional space very close to each dimensional space very close to each other let us try to see an example other let us try to see an example now of how this will look kind now of how this will look kind of a demo and if you go to the site then demo and if you go to the site then actually can see here let me sort", "image_path": "img_data/video_35_chunk_12.jpg"}
{"video": "video_35", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "demo and if you go to the site then actually can see here let me sort actually can see here let me sort of bring it up a little bit actually of bring it up a little bit actually can see here the representation we can see here the representation we are actually after the representation that after the representation that we're going to be dealing is called we're going to be dealing is called word to v that construction from tokens word to v that construction from tokens ids into vectors is going is called ids into vectors is going is called war tove and it was it is in the war tove and it was it is in the category of representation actually category of representation actually called context free", "image_path": "img_data/video_35_chunk_13.jpg"}
{"video": "video_35", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "category of representation actually called context free representations and we'll clarify now representations and we'll clarify now what is context free and what is context free and what is contextual kind of representations in a contextual kind of representations in a moment but let's see exactly what we see moment but let's see exactly what we see here a cloud of points here we see here a cloud of points and each of these points represents a and each of these points represents a word and the construction is such word and the construction is such that let's say that we search here for that let's say that we search here for the word by the way the vocabulary size is 10,000 and the", "image_path": "img_data/video_35_chunk_14.jpg"}
{"video": "video_35", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "the word by the way the vocabulary size is 10,000 and the vocabulary size is 10,000 and the number of dimensions the d that we number of dimensions the d that we have actually specified earlier is 200 have actually specified earlier is 200 we are these words live in a 200 we are these words live in a 200 dimensional space and obviously we have dimensional space and obviously we have here a three dimensional space what here a three dimensional space what we have done is we have did a we have done is we have did a dimensionality reduction this dimensionality reduction this dimensionality reduction is achieved dimensionality reduction is achieved with a technical principal component with a technical principal component analysis and pca is very similar to", "image_path": "img_data/video_35_chunk_15.jpg"}
{"video": "video_35", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "with a technical principal component analysis and pca is very similar to analysis and pca is very similar to the singular value decomposition operation that you decomposition operation that you may be familiar from linear algebra and may be familiar from linear algebra and we went down from 200 dimensions to we went down from 200 dimensions to three dimensions one obvious three dimensions one obvious thing that we cannot really do here we thing that we cannot really do here we cannot really extrapolate the geometric extrapolate the geometric similarity the geometric distance similarity the geometric distance in the threedimensional space back to in the threedimensional space back to 200 dimensional space that we cannot do 200 dimensional space that we cannot do that but what we can do is we can", "image_path": "img_data/video_35_chunk_16.jpg"}
{"video": "video_35", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "200 dimensional space that we cannot do that but what we can do is we can that but what we can do is we can look at the geometrical distance at the look at the geometrical distance at the original 200 dimensional space by original 200 dimensional space by let's say searching for a word let's say searching for a word let's say car over here and we can actually see how the here and we can actually see how the representation that war the representation that war tove has generated associates very tove has generated associates very similar meanings to the other words in similar meanings to the other words in our vocab ulary for example here", "image_path": "img_data/video_35_chunk_17.jpg"}
{"video": "video_35", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "similar meanings to the other words in our vocab ulary for example here our vocab ulary for example here driver cars automobile race racing driver cars automobile race racing vehicle and on are at the top vehicle and on are at the top positions and at the bottom this positions and at the bottom this could be a horse businessman and could be a horse businessman and on let's see how this will be achieved on let's see how this will be achieved how we can construct this representation how we can construct this representation and we need some kind of a guiding and we need some kind of a guiding principle in this kind of principle in this kind of construction and this came from l ist", "image_path": "img_data/video_35_chunk_18.jpg"}
{"video": "video_35", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "principle in this kind of construction and this came from l ist construction and this came from l ist f who's actually said the following f who's actually said the following many decades ago a words meaning is many decades ago a words meaning is given by words that frequently appear given by words that frequently appear close by all of the i mean the close by all of the i mean the keywords here that are the following keywords here that are the following frequently and close by and the moment frequently and close by and the moment you hear the keyword frequence you hear the keyword frequence frequency then in your mind should frequency then in your mind should actually come the sort of frequency actually come the sort of frequency frequen is view of probability first", "image_path": "img_data/video_35_chunk_19.jpg"}
{"video": "video_35", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "actually come the sort of frequency frequen is view of probability first frequen is view of probability first pointed to a kind of a probabilistic way pointed to a kind of a probabilistic way that we are going to do this kind of that we are going to do this kind of construction that's point number one and construction that's point number one and also pointed into some form of windowing also pointed into some form of windowing that we are need to implement in that we are need to implement in order for us to do to extract the order for us to do to extract the meaning of that of a certain meaning of that of a certain word all right they we will be calling this they we will be calling this window the context and despite the window the context and despite the fact that we are mentioning here the", "image_path": "img_data/video_35_chunk_20.jpg"}
{"video": "video_35", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "window the context and despite the fact that we are mentioning here the fact that we are mentioning here the word context at the end of this word context at the end of this discussion we will all realize that this discussion we will all realize that this is a context free representation let's wait until representation let's wait until that point and here is what is going to that point and here is what is going to what we are going to do we are going what we are going to do we are going to be given a corpus of a corpor of documents corpus of a corpor of documents for example let's see over here we for example let's see over here we have a corpus of documents that have to have a corpus of documents that have to do with let's say financial news", "image_path": "img_data/video_35_chunk_21.jpg"}
{"video": "video_35", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "have a corpus of documents that have to do with let's say financial news do with let's say financial news and here we have the word banking and here we have the word banking being quoted and in for this specific being quoted and in for this specific representation of the world kind of representation of the world kind of banking what we'll be looking we'll be banking what we'll be looking over a context of size let's looking over a context of size let's say two wordss before it say two let's say two wordss before it and two wordss let's say after it and we and two wordss let's say after it and we will be counting effectively therefore will be counting effectively therefore estimating probabilities estimating probabilities therefore estimation of the therefore estimation of the probabilities are going to be probabilities are going to be associated with some kind of", "image_path": "img_data/video_35_chunk_22.jpg"}
{"video": "video_35", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "probabilities are going to be associated with some kind of associated with some kind of prediction of the words of the prediction of the words of the occurrence of the words that are occurrence of the words that are appearing next to it let's see appearing next to it let's see a bit more specifically let's write a bit more specifically let's write this kind of down and see how these this kind of down and see how these probabilities will look and probabilities will look and evidently we'll take the form of evidently we'll take the form of posterior probabilities let's draw posterior probabilities let's draw we have the concept of let's say the we have the concept of let's say the center word that it was the word bank", "image_path": "img_data/video_35_chunk_23.jpg"}
{"video": "video_35", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "we have the concept of let's say the center word that it was the word bank the center word is actually coming in we'll be representing the words with we'll be representing the words with let's say the letter w i mean it let's say the letter w i mean it is definitely a deviation from u the is definitely a deviation from u the what we have seen earlier but we will be what we have seen earlier but we will be representing these words with w in representing these words with w in of notation this is wt we'll be", "image_path": "img_data/video_35_chunk_24.jpg"}
{"video": "video_35", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "representing these words with w in of notation this is wt we'll be of notation this is wt we'll be using now the index also to indicate using now the index also to indicate that this is basically the center word that this is basically the center word and as compared to earlier words we have and as compared to earlier words we have t minus indices and as compared to t minus indices and as compared to future words we have t+ indices we future words we have t+ indices we have wt coming in and this is going have wt coming in and this is going to be a vector that it is associated to be a vector that it is associated with the one hot and coding vector that with the one hot and coding vector that we have seen earlier we will be doing we have seen earlier we will be doing the one hod encoding at the input of our", "image_path": "img_data/video_35_chunk_25.jpg"}
{"video": "video_35", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "we have seen earlier we will be doing the one hod encoding at the input of our the one hod encoding at the input of our encoder vector in vector encoder vector in vector output in other words and we are output in other words and we are going to over here predicting the let me draw it this for let's say a", "image_path": "img_data/video_35_chunk_26.jpg"}
{"video": "video_35", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "this for let's say a probabilities the y hat of t - 2 the y hat of t minus1 the y hat of t + 2 the y hat of t minus1 the y hat of t + one and the y of t + one and the y of t + 2 in other words if we are to go back 2 in other words if we are to go back in into the site what we will be in into the site what we will be doing the following doing we will be doing the following we will be predicting we look at the", "image_path": "img_data/video_35_chunk_27.jpg"}
{"video": "video_35", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "doing we will be doing the following we will be predicting we look at the we will be predicting we look at the center word let's say into and we will be word let's say into and we will be built a machine that will predict built a machine that will predict with high probability of being with high probability of being correct in other words the words are correct in other words the words are appearing next week in two we appearing next week in two we have a context window of two as we have a context window of two as we discussed let's say in this case discussed let's say in this case the words are appearing next to we in the words are appearing next to we in this case is problems and turning this case is problems and turning banking and crisis for banking it is turning and crisis", "image_path": "img_data/video_35_chunk_28.jpg"}
{"video": "video_35", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "banking and crisis for banking it is turning and crisis for banking it is turning and crisis we have a whole bunch of these kind we have a whole bunch of these kind of sentences and each sentence kind of sentences and each sentence kind of is going to give us an opportunity of is going to give us an opportunity to each word in fact is going to give to each word in fact is going to give us an opportunity to create if you us an opportunity to create if you a data set we'll see that in a moment a data set we'll see that in a moment how we will do that without really much how we will do that without really much of an issue and our kind of an issue and our kind of will be the neural network that it will be the neural network that it will give us at the output white hats", "image_path": "img_data/video_35_chunk_29.jpg"}
{"video": "video_35", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "will be the neural network that it will give us at the output white hats give us at the output white hats these white hats have always been for us these white hats have always been for us posterior probabilities they will be posterior probabilities they will be predicting the word that is appearing the word that is appearing immediately before the word of interest immediately before the word of interest the center w as we call it given the center w as we call it given the presence of the center w that is presence of the center w that is really the sort of aim of our really the sort of aim of our construction and to do we need this construction and to do we need this kind of a training data and this is a kind of a training data and this is a table that kind of explains how the", "image_path": "img_data/video_35_chunk_30.jpg"}
{"video": "video_35", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "kind of a training data and this is a table that kind of explains how the table that kind of explains how the train data can be obtained very easily train data can be obtained very easily from our text and we don't have any from our text and we don't have any problem generating this data from problem generating this data from existing text as many as much data existing text as many as much data as we want let's assume that we have as we want let's assume that we have an input sentence let's say in our an input sentence let's say in our corpus let's say cla monet painted the corpus let's say cla monet painted the grand canal of venice in 1908 and as we grand canal of venice in 1908 and as we can actually see here it's very easy to can actually see here it's very easy to generate a data set that will have as generate a data set that will have as ground truths the words that are", "image_path": "img_data/video_35_chunk_31.jpg"}
{"video": "video_35", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "generate a data set that will have as ground truths the words that are ground truths the words that are preceding let's say cloud is not preceding let's say cloud is not preceded by anything but it is preceded by anything but it is followed by monet and painted and on followed by monet and painted and on painted is followed by cla and on painted is followed by cla monet and is preceded by cla monet and is preceded by cla monet and followed by the grand and monet and followed by the grand and we can actually build quite a lot of we can actually build quite a lot of data that can construct a lot of data that can construct a lot of training data for our purpose over here training data for our purpose over here train data is not going to be a train data is not going to be a problem and that's what we will do problem and that's what we will do and at the same time we will need to go", "image_path": "img_data/video_35_chunk_32.jpg"}
{"video": "video_35", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "problem and that's what we will do and at the same time we will need to go and at the same time we will need to go back in an earlier video where we have back in an earlier video where we have seen the concept of maximum likelihood i seen the concept of maximum likelihood i kind of construct that machine that is kind of construct that machine that is going to take the center word and going to take the center word and predict the posterior probabilities of predict the posterior probabilities of the corresponding contextual words and corresponding contextual words and we have here a context that will be we have here a context that will be let's call this context capital c let's call this context capital c is in this case let's", "image_path": "img_data/video_35_chunk_33.jpg"}
{"video": "video_35", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "let's call this context capital c is in this case let's capital c is in this case let's say the context it's of let's say size four let's call it plus or minus size four let's call it plus or minus two and that's basically what we will two and that's basically what we will be doing we will now see the be doing we will now see the elements of that machine but how this elements of that machine but how this machine get constructed i think it machine get constructed i think it should be motivated from maximum should be motivated from maximum likelihood as we always have done in the likelihood as we always have done in the past let's write down past let's write down poster we always had in u in", "image_path": "img_data/video_35_chunk_34.jpg"}
{"video": "video_35", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "past let's write down poster we always had in u in poster we always had in u in two probably distributions if you two probably distributions if you revisit that video on maximum likelihood revisit that video on maximum likelihood one was called p data hatut and the one was called p data hatut and the other was actually called the p model other was actually called the p model the table that we have just show had the table that we have just show had over here will actually give us u the over here will actually give us u the p data hat because it has the sort of p data hat because it has the sort of training data and let's write down training data and let's write down the nature of the speed data hat as a", "image_path": "img_data/video_35_chunk_35.jpg"}
{"video": "video_35", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "training data and let's write down the nature of the speed data hat as a the nature of the speed data hat as a posterior probability in this speed posterior probability in this speed data hat we i have u the let's say wt data hat we i have u the let's say wt minus c in fact c in this case will be just two because we will insert this minus over here t minus c wt minus c minus over here t minus c wt minus c + 1 dot do dot wt + c these are all the", "image_path": "img_data/video_35_chunk_36.jpg"}
{"video": "video_35", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "minus over here t minus c wt minus c + 1 dot do dot wt + c these are all the + 1 dot do dot wt + c these are all the words that are in the context of the words that are in the context of the center word wt given wt no surprises center word wt given wt no surprises there that the speed data hat is a there that the speed data hat is a posterior distribution let me call posterior distribution let me call this center word and this will be the context words", "image_path": "img_data/video_35_chunk_37.jpg"}
{"video": "video_35", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "words context words and similar i have seen earlier we need to have now a p model we need to have now a p model we need to construct a hypothesis our p model is going to be of some similar nature to our p data hat because similar nature to our p data hat because we as we discussed we had to produce a p we as we discussed we had to produce a p model that is going to be a model that is going to be a parameterized by parameter set theta to parameterized by parameter set theta to be able to appro approximate that p data be able to appro approximate that p data hat as closely as possible our p", "image_path": "img_data/video_35_chunk_38.jpg"}
{"video": "video_35", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "be able to appro approximate that p data hat as closely as possible our p hat as closely as possible our p model not surprises that will be of a model not surprises that will be of a similar nature let me put some dots here to make nature let me put some dots here to make the notation a bit simpler this the notation a bit simpler this will be given wt comma theta where theta will be given wt comma theta where theta is the set of parameters obviously the is the set of parameters obviously the set of trainable parameters as we as set of trainable parameters as we have used this letter before and we have used this letter before and now we're going to make an", "image_path": "img_data/video_35_chunk_39.jpg"}
{"video": "video_35", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "we have used this letter before and now we're going to make an now we're going to make an assumption that is actually assumption that is actually called let me write it down a bit called let me write it down a bit more specifically here this is called more specifically here this is called naive base assumption in order for us to be able to simplify the architecture of this kind simplify the architecture of this kind of neural network this assumption helps of neural network this assumption helps quite a bit and what is this assumption", "image_path": "img_data/video_35_chunk_40.jpg"}
{"video": "video_35", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "of neural network this assumption helps quite a bit and what is this assumption quite a bit and what is this assumption is actually saying is something pretty is actually saying is something pretty straightforward let me write down straightforward let me write down the naive kind of base assumption over the naive kind of base assumption over here the naive base assumption is here the naive base assumption is suggesting that given and it's not being used only in given and it's not being used only in this kind of application naive base is a this kind of application naive base is a sort of well-known sort of predict prediction well-known sort of predict prediction kind of assumption that can be used for kind of assumption that can be used for many other kind of data sets but here it", "image_path": "img_data/video_35_chunk_41.jpg"}
{"video": "video_35", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "kind of assumption that can be used for many other kind of data sets but here it many other kind of data sets but here it means that given the center word all context words are independent of each other this is the this are this is the assumption behind naive base and it", "image_path": "img_data/video_35_chunk_42.jpg"}
{"video": "video_35", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "other this is the this are this is the assumption behind naive base and it assumption behind naive base and it makes some sense it's is a simplifying makes some sense it's is a simplifying assumption and how successful this assumption and how successful this will be of course it will be judged at will be of course it will be judged at the end from the results over here the end from the results over here given this assumption and remembering given this assumption and remembering the -called conditional the -called conditional independence that we have reviewed independence that we have reviewed much earlier in another much earlier in another video we had the we can actually video we had the we can actually write this condition probability as a", "image_path": "img_data/video_35_chunk_43.jpg"}
{"video": "video_35", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "video we had the we can actually write this condition probability as a write this condition probability as a product of other conditional product of other conditional probabilities we can actually probabilities we can actually factorize this in other words times p model- c +1 given t comma theta times and", "image_path": "img_data/video_35_chunk_44.jpg"}
{"video": "video_35", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "times and on finally we have this last one let me put some dots here to make one let me put some dots here to make the notation a bit more the notation a bit more clear times p model of wt plus c given wt given theta this is going to be theta this is going to be written as the product from j isal to", "image_path": "img_data/video_35_chunk_45.jpg"}
{"video": "video_35", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "theta this is going to be written as the product from j isal to written as the product from j isal to minus c to j isal plus c where j less minus c to j isal plus c where j less than zero of p model of wt plus j given wt comma theta in a bit more compact wt comma theta in a bit more compact form this is the na kind of base form this is the na kind of base assumption that governs the p model and assumption that governs the p model and this actually also gives us some ability this actually also gives us some ability to kind of decouple the", "image_path": "img_data/video_35_chunk_46.jpg"}
{"video": "video_35", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "this actually also gives us some ability to kind of decouple the to kind of decouple the architecture we'll see how this architecture we'll see how this decoupling takes place and the next step is to define place and the next step is to define if you a loss function and this if you a loss function and this loss function should not be any loss function should not be any different from the maximum likelihood different from the maximum likelihood loss function i can write here that loss function i can write here that according to ml to maximum likelihood principle of course we recognize that", "image_path": "img_data/video_35_chunk_47.jpg"}
{"video": "video_35", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "to ml to maximum likelihood principle of course we recognize that principle of course we recognize that the naive base assumption given that the naive base assumption given that the fact we have probabilities over here fact we have probabilities over here then we need to be operating in the in then we need to be operating in the log domain and we'll do that but the log domain and we'll do that but according to the maximum likehood according to the maximum likehood assumption principle another assumption principle another assumption principle we need to minimize the principle we need to minimize the following kind of loss and for those who following kind of loss and for those who are not who have forgotten about this are not who have forgotten about this kind of loss i suggest that you look at kind of loss i suggest that you look at them maximum likely treatment that we them maximum likely treatment that we have seen this loss it is an", "image_path": "img_data/video_35_chunk_48.jpg"}
{"video": "video_35", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "them maximum likely treatment that we have seen this loss it is an have seen this loss it is an expectation negative of the expectation negative of the expectation which of course replaced by a sample which of course replaced by a sample mean as an approximation is a minus the mean as an approximation is a minus the expectation of u what was then back then expectation of u what was then back then it we had x's and y's over it we had x's and y's over here the x's that are actually coming here the x's that are actually coming in is the center word and this is the in is the center word and this is the x and what is the y is the", "image_path": "img_data/video_35_chunk_49.jpg"}
{"video": "video_35", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "in is the center word and this is the x and what is the y is the x and what is the y is the context word we have here the context word we have here the x we have examples that consist the x we have examples that consist of x which is wt and y's which are the of x which is wt and y's which are the wt + wt + j all of the set of words in fact an the set of words in fact an example consist of the center word and example consist of the center word and the set of the context words that are of the set of the context words that are of course distributed according to p course distributed according to p data", "image_path": "img_data/video_35_chunk_50.jpg"}
{"video": "video_35", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "course distributed according to p data hat of the log this is the log domain if you log this is the log domain if you of the log of the product sort of from j is equal to min product sort of from j is equal to min of the of our p model from j to minus c of the of our p model from j to minus c j is equal to plus c the p model w t + j given wt comma", "image_path": "img_data/video_35_chunk_51.jpg"}
{"video": "video_35", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "w t + j given wt comma theta this is our maximum lik theta this is our maximum lik principle from the first principle from the first principles kind of perspective and how principles kind of perspective and how it is written in the specific problem it is written in the specific problem here all right this is our sort of here all right this is our sort of equation the law the equation of the equation the law the equation of the loss function we need to construct loss function we need to construct an architecture that it will an architecture that it will model this we'll model this and we'll", "image_path": "img_data/video_35_chunk_52.jpg"}
{"video": "video_35", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "an architecture that it will model this we'll model this and we'll model this and we'll minimize this kind of loss function in minimize this kind of loss function in order for the architecture to be able to order for the architecture to be able to predict accurately the context words and the architecture actually looks this we have the typical projection that we have seen typical projection that we have seen in a dense layers we have as input", "image_path": "img_data/video_35_chunk_53.jpg"}
{"video": "video_35", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "typical projection that we have seen in a dense layers we have as input in a dense layers we have as input the context word sorry the center the context word sorry the center word wt definitely the as we discussed word wt definitely the as we discussed the wt has a span of cardinality v dimension it's of cardinality v dimension it's pretty sizable typical size of pretty sizable typical size of vocabulary sizes are let's say 10,000 vocabulary sizes are let's say 10,000 we've seen it kind of earlier in the we've seen it kind of earlier in the demo this is typical sizes for let's demo this is typical sizes for let's say your alexa smartphones the number of smartphones the number of words that they're able to understand", "image_path": "img_data/video_35_chunk_54.jpg"}
{"video": "video_35", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "smartphones the number of words that they're able to understand words that they're able to understand are typically in the few thousands and are typically in the few thousands and of course when you do google search and of course when you do google search and on you are looking at much larger on you are looking at much larger vocabulary sizes typically up to a vocabulary sizes typically up to a million kind of tokens are actually million kind of tokens are actually present there the center word let's present there the center word let's say is going to be let's say the car say is going to be let's say the car from the demo and this will actually from the demo and this will actually need to be reduced down to d dimensions as we be reduced down to d dimensions as we discussed the center world will live discussed the center world will live in a lower dimensional", "image_path": "img_data/video_35_chunk_55.jpg"}
{"video": "video_35", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "discussed the center world will live in a lower dimensional in a lower dimensional space and we expect i mean we space and we expect i mean we intuitively understand why has to be intuitively understand why has to be lower dimensional and the reason lower dimensional and the reason is that the semantic the meaning of is that the semantic the meaning of what let's say hotel and motel are going what let's say hotel and motel are going to be let's say or in this case kind to be let's say or in this case kind of car and driver are going to be of car and driver are going to be associated with a kind of a lower associated with a kind of a lower dimensional space of d dimensions dimensional space of d dimensions we have here our d dimensional vector we have here our d dimensional vector we will be calling this we will be calling this z and this projection as we have done", "image_path": "img_data/video_35_chunk_56.jpg"}
{"video": "video_35", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "we will be calling this z and this projection as we have done z and this projection as we have done earlier will be done with a dense earlier will be done with a dense matrix this dense matrix w capital w i matrix this dense matrix w capital w i hope there not too many w's it's a small hope there not too many w's it's a small w this a capital w that has a size of w this a capital w that has a size of cardinality v * cardinality v * d dimensions z will belong to r to the power of d belong to r to the power of d and then what we need to do now", "image_path": "img_data/video_35_chunk_57.jpg"}
{"video": "video_35", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "belong to r to the power of d and then what we need to do now and then what we need to do now is we will be taking this z is we will be taking this z dimensional kind of vector and we dimensional kind of vector and we are in inverting this kind of are in inverting this kind of projection to speak with the help of projection to speak with the help of a w prime matrix to produce a vector let's call it this vector z", "image_path": "img_data/video_35_chunk_58.jpg"}
{"video": "video_35", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "vector let's call it this vector z prime that it will be such that when prime that it will be such that when i normalize it via the soft max operator to produce the posterior probability at the output which is let's probability at the output which is let's say y hat that it will say y hat that it will be give me let's say a be give me let's say a diagram a plot that will look this diagram a plot that will look this if this is let's say is the word car if this is let's say is the word car the one hood encoded version of the word", "image_path": "img_data/video_35_chunk_59.jpg"}
{"video": "video_35", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "if this is let's say is the word car the one hood encoded version of the word the one hood encoded version of the word car then the will be a posterior car then the will be a posterior probability distribution that this probability distribution that this index will correspond to the word let's index will correspond to the word let's say driver and some other indices we've seen from the demo will be there and this of from the demo will be there and this of course will be v dimensional we course will be v dimensional we went from v dimensions to v dimensions went from v dimensions to v dimensions because our context words are in the", "image_path": "img_data/video_35_chunk_60.jpg"}
{"video": "video_35", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "went from v dimensions to v dimensions because our context words are in the because our context words are in the v dimensional kind of space and we are v dimensional kind of space and we are actually by doing we are compressing actually by doing we are compressing down the dimensions tod and creating down the dimensions tod and creating if you the right presentation if you the right presentation if the presentation is the right one the presentation is the right one then we are going to be able to then we are going to be able to predict the context word with very predict the context word with very high probability correctly with a very high probability correctly with a very high probability and of course we have not probability and of course we have not only just one codex squ but we have", "image_path": "img_data/video_35_chunk_61.jpg"}
{"video": "video_35", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "probability and of course we have not only just one codex squ but we have only just one codex squ but we have cap minus c to plus c four we cap minus c to plus c four we are going to be indexing this matrix are going to be indexing this matrix with j that we have met with j that we have met earlier that indic is the context word earlier that indic is the context word these are all js if we understand these are all js if we understand the approach for one of the approach for one of the context word then we can understand context word then we can understand for many in fact for other context words for many in fact for other context words we will still", "image_path": "img_data/video_35_chunk_62.jpg"}
{"video": "video_35", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "for many in fact for other context words we will still start we will have to jointly optimize if i may draw it jointly optimize if i may draw it this i'm not sure if this is going to be the right drawing but if you can draw it this you can see that there's going to be another matrix let's say w j +", "image_path": "img_data/video_35_chunk_63.jpg"}
{"video": "video_35", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "matrix let's say w j + j + one that will give me zj + one vector one that will give me zj + one vector and via soft max operator i'll be producing a y hat j+ one that it is producing a y hat j+ one that it is associated with a posterior associated with a posterior probability distribution in this case probability distribution in this case will be let's say engine will be let's say engine this one will", "image_path": "img_data/video_35_chunk_64.jpg"}
{"video": "video_35", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "will be let's say engine this one will correspond to engine because engine is was a very engine because engine is was a very frequent contextual word that it was frequent contextual word that it was quoted next to the world quoted next to the world car in my corpus i hope it is car in my corpus i hope it is actually clear what is actually clear what is actually happening over here as we seen happening over here as we seen earlier we in other kind of videos we earlier we in other kind of videos we can actually write down the forward", "image_path": "img_data/video_35_chunk_65.jpg"}
{"video": "video_35", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "earlier we in other kind of videos we can actually write down the forward can actually write down the forward propagation equations to just convert propagation equations to just convert the diagram into a set of equations the diagram into a set of equations this are very straightforward we this are very straightforward we produce the z which is the wt transpose w this will be d * 1 then we w this will be d * 1 then we have zj prime as the z * w prime have zj prime as the z * w prime j and the y", "image_path": "img_data/video_35_chunk_66.jpg"}
{"video": "video_35", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "have zj prime as the z * w prime j and the y hat j is a soft max of zj prime definitely we have a prime definitely we have a loss that as we discussed in the maximum loss that as we discussed in the maximum likehood section we have a likehood section we have a prediction over here this prediction over here this prediction is a classification prediction because is a classification prediction because we are trying to produce the posterior we are trying to produce the posterior probability distribution let me write probability distribution let me write this down again y j is the probability", "image_path": "img_data/video_35_chunk_67.jpg"}
{"video": "video_35", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "probability distribution let me write this down again y j is the probability this down again y j is the probability of w t + j given wt comma of w t + j given wt comma theta this is the posterior probability distribution we i posterior probability distribution we i need to form and we can going select as need to form and we can going select as the context word the word with the context word the word with the maximum posterior probability for maximum posterior probability for that specific index kind of j and that specific index kind of j and that's how we will form the law the that's how we will form the law the cross entropy is", "image_path": "img_data/video_35_chunk_68.jpg"}
{"video": "video_35", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "that's how we will form the law the cross entropy is between the predicted sort of a word and the predicted sort of a word and the ground truth word and we've seen in this ground truth word and we've seen in this kind of cla monet example how the ground kind of cla monet example how the ground truth word was produced truth word was produced this is going to be the loss and of course we the loss and of course we have a set of trainable parameters that have a set of trainable parameters that correspond to w and wj correspond to w and wj these are the wj prime these are the", "image_path": "img_data/video_35_chunk_69.jpg"}
{"video": "video_35", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "correspond to w and wj these are the wj prime these are the wj prime these are the matrices of course we have other matrices of course we have other matrices as well i may include matrices as well i may include them as well as shown over here that's basically it and since we are going to form and since we are going to form that kind of a network over here that kind of a network over here we're going to train it and we are we're going to train it and we are going to at the end of the day at the going to at the end of the day at the end of this kind of training kind of", "image_path": "img_data/video_35_chunk_70.jpg"}
{"video": "video_35", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "going to at the end of the day at the end of this kind of training kind of end of this kind of training kind of process what we're going to see is we're process what we're going to see is we're going to get the matrices out of the going to get the matrices out of the matrices that the theta stars at matrices that the theta stars at the end of the training process let the end of the training process let me write down at the end we get the theta star from the theta star of course we get the w", "image_path": "img_data/video_35_chunk_71.jpg"}
{"video": "video_35", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "end we get the theta star from the theta star of course we get the w star of course we get the w star and it's really this w star that star and it's really this w star that we are going to store we are not really we are going to store we are not really concerned about anything over here concerned about anything over here other than the w matrix and the reason other than the w matrix and the reason why we don't concern of anything why we don't concern of anything that is that we are trying to create a that is that we are trying to create a lookup table and lookup table is going lookup table and lookup table is going to have as indices the token ids to have as indices the token ids that of course are v dimensional and that of course are v dimensional and we are going to me this w matrix", "image_path": "img_data/video_35_chunk_72.jpg"}
{"video": "video_35", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "that of course are v dimensional and we are going to me this w matrix we are going to me this w matrix is going to correspond to the is going to correspond to the following dimensions w will have d following dimensions w will have d columns and we have v rows as you understand these are the dimensions of w to make the zd dimensions of w to make the zd dimensional and therefore the dimensional and therefore the row the moment we have a token id row the moment we have a token id we can actually look up we can actually look up this w matrix as a lookup table to", "image_path": "img_data/video_35_chunk_73.jpg"}
{"video": "video_35", "start": "0:37:00", "end": "0:37:30", "timestamp": "0:37:00 - 0:37:30", "text": "we can actually look up this w matrix as a lookup table to this w matrix as a lookup table to retrieve the this is basically retrieve the this is basically the token id the sort of u whatever row token id the sort of u whatever row carard let's say corresponds to carard let's say corresponds to 1,535 row of this matrix w star is 1,535 row of this matrix w star is going to correspond to the d dimensional to correspond to the d dimensional vector we are actually able to get vector we are actually able to get the representation according to this", "image_path": "img_data/video_35_chunk_74.jpg"}
{"video": "video_35", "start": "0:37:30", "end": "0:38:00", "timestamp": "0:37:30 - 0:38:00", "text": "the representation according to this the representation according to this word to the construction from this word to the construction from this matrix and these matrices are the matrix and these matrices are the ones that are typically are stored in ones that are typically are stored in hubs and they are you are able to hubs and they are you are able to retrieve those representations for retrieve those representations for any construction not only war to but any construction not only war to but other embeddings constructions by other embeddings constructions by effectively a single line of code in effectively a single line of code in a f the popular byor u enhanced with transformers or", "image_path": "img_data/video_35_chunk_75.jpg"}
{"video": "video_35", "start": "0:38:00", "end": "0:38:30", "timestamp": "0:38:00 - 0:38:30", "text": "byor u enhanced with transformers or u enhanced with transformers or tensor flow caras enhanced again with tensor flow caras enhanced again with transformers which is transformers which is transformers is really a very i will call it is really a very i will call it popular library to work in natural popular library to work in natural language processing we'll be language processing we'll be seeing some examples of that a bit later seeing some examples of that a bit later in the next now that we have our in the next now that we have our edings what comes next is we are going edings what comes next is we are going to start using some of these eddings to start using some of these eddings in our tasks the we're in our tasks the we're interested in starting with the task of", "image_path": "img_data/video_35_chunk_76.jpg"}
{"video": "video_35", "start": "0:38:30", "end": "0:39:00", "timestamp": "0:38:30 - 0:39:00", "text": "in our tasks the we're interested in starting with the task of interested in starting with the task of language modeling but i wanted to language modeling but i wanted to emphasize one kind of key observation emphasize one kind of key observation here before we close this discussion here before we close this discussion and the key observation here is that and the key observation here is that this is a this word to vector this is a this word to vector representation is a context free a bedding construction and you can actually wonder why on earth this actually wonder why on earth this context ph since we are actually you context ph since we are actually the in the network we have the", "image_path": "img_data/video_35_chunk_77.jpg"}
{"video": "video_35", "start": "0:39:00", "end": "0:39:30", "timestamp": "0:39:00 - 0:39:30", "text": "context ph since we are actually you know the in the network we have the know the in the network we have the context all over us and the reason why context all over us and the reason why it is actually called context 3 is not it is actually called context 3 is not that we are not using the contextual that we are not using the contextual words to be able to follow the words to be able to follow the first kind of principle and predict first kind of principle and predict if you with a good probability if you with a good probability the words which are surrounding the word the words which are surrounding the word of interest but it's because at the end of interest but it's because at the end of the day if i had let's say corpus of the day if i had let's say corpus of documents let's say on news and in", "image_path": "img_data/video_35_chunk_78.jpg"}
{"video": "video_35", "start": "0:39:30", "end": "0:40:00", "timestamp": "0:39:30 - 0:40:00", "text": "of the day if i had let's say corpus of documents let's say on news and in documents let's say on news and in the news we had let's say the quote of the news we had let's say the quote of the world bank in from some kind the world bank in from some kind of financial kind of news clips and of financial kind of news clips and the world bank was quoted from some kind the world bank was quoted from some kind of disaster or some kind of flooding of disaster or some kind of flooding that happened then at the end of that happened then at the end of the day the world bank will have one vector as a represent vector as a represent and not two or three or whatever", "image_path": "img_data/video_35_chunk_79.jpg"}
{"video": "video_35", "start": "0:40:00", "end": "0:40:25.233333", "timestamp": "0:40:00 - 0:40:25.233333", "text": "vector as a represent and not two or three or whatever and not two or three or whatever this is the reason why i actually this is the reason why i actually call this a construction a context call this a construction a context freeding and we will see that more freeding and we will see that more advanced constructions are possible advanced constructions are possible those are somehow merged together with those are somehow merged together with the tasks that we have to do and the tasks that we have to do and we'll see them when we deal with u other we'll see them when we deal with u other kind of architectures in another kind of architectures in another video", "image_path": "img_data/video_35_chunk_80.jpg"}
{"video": "video_36", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "have seen in earlier video the very first stage is that of tokenization and first stage is that of tokenization and the tokenization if you layer of the tokenization if you layer of processing is definitely processing is definitely consist of two discussions one is really consist of two discussions one is really what today's tokenizers are doing and what today's tokenizers are doing and the second discussion is why they're the second discussion is why they're doing what they are doing and we are doing what they are doing and we are not going to be expanding much as to not going to be expanding much as to the why because there are plenty of the why because there are plenty of tokenizers and the explaining the why behind each one of explaining the why behind each one of them will actually take too much of a", "image_path": "img_data/video_36_chunk_0.jpg"}
{"video": "video_36", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "explaining the why behind each one of them will actually take too much of a them will actually take too much of a discussion we will limit the discussion we will limit the discussion here to two main types because we can here to two main types because we can actually start understanding the actually start understanding the tradeoff that exists between the two tradeoff that exists between the two types the sort of word and subword level types the sort of word and subword level tokenization and that's what we're tokenization and that's what we're going to discuss first we will going to discuss first we will distinguish the discussion over here distinguish the discussion over here divide the discussion here into two divide the discussion here into two parts the first is title the word", "image_path": "img_data/video_36_chunk_1.jpg"}
{"video": "video_36", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "divide the discussion here into two parts the first is title the word parts the first is title the word level and the other is the subword and today we are actually pretty much here but i think actually pretty much here but i think it's instructive to just discuss the it's instructive to just discuss the simple case first which is that of the simple case first which is that of the world level tokenization and for that if world level tokenization and for that if we actually go to our site we can we actually go to our site we can actually see here an actually see here an example let's assum i have a example let's assum i have a sentence and i have let's say a new gpu", "image_path": "img_data/video_36_chunk_2.jpg"}
{"video": "video_36", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "example let's assum i have a sentence and i have let's say a new gpu sentence and i have let's say a new gpu exclamation mark and the tokenizer exclamation mark and the tokenizer which is very straightforward which is very straightforward to actually bring in this kind of to actually bring in this kind of high level apis that are provided either high level apis that are provided either by spacey or hugging phase we are by spacey or hugging phase we are able to actually print each of the able to actually print each of the tokens we have 1 two 3 four five six tokens we have 1 two 3 four five six tokens in this kind of case and then tokens in this kind of case and then basically we are done every token is basically we are done every token is going to be further processed by our going to be further processed by our kind of pipeline and as we will see", "image_path": "img_data/video_36_chunk_3.jpg"}
{"video": "video_36", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "going to be further processed by our kind of pipeline and as we will see kind of pipeline and as we will see later the next processing stage is that later the next processing stage is that of the edding every token what will of the edding every token what will at some point result into a vector at some point result into a vector however the disadvantage of however the disadvantage of this kind of method is that it suffers from the method is that it suffers from the problem of the -called unknown words problem of the -called unknown words if we have if you a tokenization if we have if you a tokenization model that we have developed just", "image_path": "img_data/video_36_chunk_4.jpg"}
{"video": "video_36", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "if we have if you a tokenization model that we have developed just model that we have developed just the one which is actually being used the one which is actually being used here and in this kind of model the here and in this kind of model the specific word was never met let's say specific word was never met let's say gpu was never met then the tokenizer gpu was never met then the tokenizer will not really know how what kind of will not really know how what kind of token to produce out of an unknown word token to produce out of an unknown word and therefore we will attack that word and therefore we will attack that word as such as in other words unknown and as such as in other words unknown and that's obviously a suboptimal you that's obviously a suboptimal thing to do because evidently thing to do because evidently you want all of the words to have some form want all of the words to have some form of meaning later and therefore this", "image_path": "img_data/video_36_chunk_5.jpg"}
{"video": "video_36", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "want all of the words to have some form of meaning later and therefore this of meaning later and therefore this unknown word if we have too many of unknown word if we have too many of them then all of these words will have them then all of these words will have exactly the same unknown meaning and exactly the same unknown meaning and that is basically the disadvantage of that is basically the disadvantage of world level tokenization if we world level tokenization if we step now into the space where we discuss step now into the space where we discuss the subword level tokenization we have a in the tokenization we have a in the other extreme the -called character other extreme the -called character level tokenization where we are solving", "image_path": "img_data/video_36_chunk_6.jpg"}
{"video": "video_36", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "other extreme the -called character level tokenization where we are solving level tokenization where we are solving the unknown word problem by encoding the unknown word problem by encoding all of the words as a char as a single all of the words as a char as a single character effectively we are going to character effectively we are going to have a pretty sizable number of tokens have a pretty sizable number of tokens that are produced out of a very simple that are produced out of a very simple sentence and that is actually causing sentence and that is actually causing the other tradeoff in a sense that the other tradeoff in a sense that the longer our sentences becomes in the longer our sentences becomes in terms of token then as we have seen in terms of token then as we have seen in an earlier video on recurring your", "image_path": "img_data/video_36_chunk_7.jpg"}
{"video": "video_36", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "terms of token then as we have seen in an earlier video on recurring your an earlier video on recurring your networks and this is also true in terms networks and this is also true in terms of transformers as well there going of transformers as well there going to be some far more challenges to recall to be some far more challenges to recall the information in especially in a the information in especially in a current kind of architecture and that is the we architecture and that is the we solv one problem but we created another solv one problem but we created another and as we've seen gradient flow is a and as we've seen gradient flow is a challenging thing to do in recurent challenging thing to do in recurent architectures and that's the reason", "image_path": "img_data/video_36_chunk_8.jpg"}
{"video": "video_36", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "challenging thing to do in recurent architectures and that's the reason architectures and that's the reason why character level tokenization why character level tokenization although it works it is probably although it works it is probably geared for small experiments and geared for small experiments and therefore it does not have the and therefore it does not have the performance of what we are will end up performance of what we are will end up doing which is the -called bite pair doing which is the -called bite pair and coding which is one of the many and coding which is one of the many subword level tokenization approaches subword level tokenization approaches which we'll discuss now to explain the which we'll discuss now to explain the bite pair and coding as it a variant of bite pair and coding as it a variant of which is been used in the soal gpt", "image_path": "img_data/video_36_chunk_9.jpg"}
{"video": "video_36", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "bite pair and coding as it a variant of which is been used in the soal gpt which is been used in the soal gpt type of language models or the type of language models or the regressive language models we have regressive language models we have here a string u and from the geto i here a string u and from the geto i should mention that this is a should mention that this is a compression technique that is the b compression technique that is the b parent coding it was back in 1994 where parent coding it was back in 1994 where it was invented and you probably also heard invented and you probably also heard other similar compression techniques other similar compression techniques such as lz coding and the and it such as lz coding and the and it goes as follows we have let's say goes as follows we have let's say this string over here and at the very", "image_path": "img_data/video_36_chunk_10.jpg"}
{"video": "video_36", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "goes as follows we have let's say this string over here and at the very this string over here and at the very first stage we are merging into a first stage we are merging into a another symbol let's say z all the another symbol let's say z all the occurrences of the symbols that occurrences of the symbols that are repeating the original symbols that are repeating the original symbols that are repeating effectively with this repeating effectively with this first replacement we are end ended up first replacement we are end ended up with the string z a b d capital z let's with the string z a b d capital z let's say a b a c i have chosen if you a", "image_path": "img_data/video_36_chunk_11.jpg"}
{"video": "video_36", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "with the string z a b d capital z let's say a b a c i have chosen if you a say a b a c i have chosen if you a symbol to replace it that it is not symbol to replace it that it is not really part of the string and really part of the string and what we actually can do now is we can what we actually can do now is we can actually go and see exactly any other actually go and see exactly any other repetition we have here yet another repetition we have here yet another repetition is ab and ab over there we repetition is ab and ab over there we will actually proceed to the second step will actually proceed to the second step this is z maybe y then we have d", "image_path": "img_data/video_36_chunk_12.jpg"}
{"video": "video_36", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "this is z maybe y then we have d z y a this is basically the a this is basically the u the end result if you the u the end result if you and we can stop over here but and we can stop over here but we could also start at a sort of second we could also start at a sort of second level start replacing these now symbols level start replacing these now symbols with something that we have not with something that we have not seen before let's say x and that's the", "image_path": "img_data/video_36_chunk_13.jpg"}
{"video": "video_36", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "with something that we have not seen before let's say x and that's the seen before let's say x and that's the recursive nature of the kind of the recursive nature of the kind of the algorithm this is x d x a algorithm this is x d x a c and this is basically where we stop c and this is basically where we stop and we have z is equal to a capital y and we have z is equal to a capital y is equal to a and capital x is equal to a and capital x is equal to z y and that's how we will encode the with y and that's how we will encode the with a bite pair encoding", "image_path": "img_data/video_36_chunk_14.jpg"}
{"video": "video_36", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "y and that's how we will encode the with a bite pair encoding a bite pair encoding and that was the original kind of and that was the original kind of algorithm and if we are to see what how algorithm and if we are to see what how this is kind of used in nlp let's this is kind of used in nlp let's take a sentence for example let's take a sentence for example let's take the sentence c cells by the shore this is the input sentence and let's see how we can just do a and let's see how we can just do a couple of iterations here of this", "image_path": "img_data/video_36_chunk_15.jpg"}
{"video": "video_36", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "and let's see how we can just do a couple of iterations here of this couple of iterations here of this kind of the bite pa and coding kind of the bite pa and coding kind of algorithm we actually form kind of algorithm we actually form just a visual presentation of just a visual presentation of what's happening we form a what's happening we form a table of the original tokens the input kind of original tokens the input kind of text we first break this into the text we first break this into the correspoding symbol each symbol correspoding symbol each symbol corresponds to a character here we corresponds to a character here we have let's say the let's put the space", "image_path": "img_data/video_36_chunk_16.jpg"}
{"video": "video_36", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "corresponds to a character here we have let's say the let's put the space have let's say the let's put the space symbol let's represent the space symbol let's represent the space with the underscore over here then with the underscore over here then we have s then we have h e l o r b y t these are all the input y t these are all the input characters and let's it's right next", "image_path": "img_data/video_36_chunk_17.jpg"}
{"video": "video_36", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "y t these are all the input characters and let's it's right next characters and let's it's right next to them the frequency that we are to them the frequency that we are meeting each of these kind of characters meeting each of these kind of characters in this text we have five we have in this text we have five we have also five here we have five s's we have also five here we have five s's we have three h's five is four l's 1 o 1 r or everything else is one the first kind of iteration", "image_path": "img_data/video_36_chunk_18.jpg"}
{"video": "video_36", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "one the first kind of iteration is to find from the all of is to find from the all of these frequences we have produced here these frequences we have produced here probably we will be looking at the f probably we will be looking at the f at the first top positions probably at the first top positions probably to find the most frequent pairs pairings to find the most frequent pairs pairings that we can find in this input text that we can find in this input text the most frequent one turns out the most frequent one turns out to be sh it is met over here and", "image_path": "img_data/video_36_chunk_19.jpg"}
{"video": "video_36", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "the most frequent one turns out to be sh it is met over here and to be sh it is met over here and over here this is the most over here and over here this is the most frequent one out of the all the frequent one out of the all the possibilities if we are to write possibilities if we are to write how this sentence kind of evolved how this sentence kind of evolved we will be merging snh h to form now we will be merging snh h to form now a new token that we will be appending we will token that we will be appending we will be appending this token in the", "image_path": "img_data/video_36_chunk_20.jpg"}
{"video": "video_36", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "token that we will be appending we will be appending this token in the be appending this token in the original kind of list of tokens that we original kind of list of tokens that we have we will i'm not going to write have we will i'm not going to write everything but what i'm going to do is everything but what i'm going to do is i'm going to change the i'm going to change the frequencies because the merging frequencies because the merging will change the sentence to what let's will change the sentence to what let's see let's replace with see let's replace with y the sh the what we decided to merge y the sh the what we decided to merge the new sentence", "image_path": "img_data/video_36_chunk_21.jpg"}
{"video": "video_36", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "y the sh the what we decided to merge the new sentence is y e cells y e lls by the y o r e and this is a the y o r e and this is a sentence but definitely the number of sentence but definitely the number of spaces is still five the s's have now been five the s's have now been used to two over here and over here s", "image_path": "img_data/video_36_chunk_22.jpg"}
{"video": "video_36", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "five the s's have now been used to two over here and over here s used to two over here and over here s is two h's are zero and all the other elements are zero and all the other elements are intact except that now we have a intact except that now we have a new token the sh with frequency new token the sh with frequency three this is the second three this is the second iteration and in the third iteration and in the third iteration what we will actually going to be doing what we will actually going to be doing is we're going to repeat this kind of", "image_path": "img_data/video_36_chunk_23.jpg"}
{"video": "video_36", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "what we will actually going to be doing is we're going to repeat this kind of is we're going to repeat this kind of process we will need to find now the process we will need to find now the most frequent pairing that most frequent pairing that we can find in this case well could be we can find in this case well could be that we decide to do i'm not 100% that we decide to do i'm not 100% sure if it is the most frequent one but sure if it is the most frequent one but just for illustration purposes let's just for illustration purposes let's merge the y and the e i can see here merge the y and the e i can see here that there are at least two we that there are at least two we will be merging it with we have", "image_path": "img_data/video_36_chunk_24.jpg"}
{"video": "video_36", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "that there are at least two we will be merging it with we have will be merging it with we have sh we have all of these sh we have all of these tokens again evidently the tokens again evidently the frequency of e will actually change frequency of e will actually change this let's say this becomes z is this let's say this becomes z is equal to sh n e in fact i should not be sh n e in fact i should not be writing as h i should be writing as y writing as h i should be writing as y capital", "image_path": "img_data/video_36_chunk_25.jpg"}
{"video": "video_36", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "writing as h i should be writing as y capital y and then the sentence will actually y and then the sentence will actually be zcore cells underscore z zcore cells underscore z lls by the y we have a sh definitely and then y we have a sh definitely and then we have sh e of frequency 2", "image_path": "img_data/video_36_chunk_26.jpg"}
{"video": "video_36", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "y we have a sh definitely and then we have sh e of frequency 2 and the definitely the number of ss are have actually gone down to ss are have actually gone down to one and on we repeat this if you one and on we repeat this if you this process and then at the end of this process and then at the end of the day we have the some of the day we have the some of these elements which are zero are not these elements which are zero are not going to be kept obviously we going to be kept obviously we eliminate this actually tokens and at eliminate this actually tokens and at the end of the day we stop the", "image_path": "img_data/video_36_chunk_27.jpg"}
{"video": "video_36", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "eliminate this actually tokens and at the end of the day we stop the end of the day we stop the process when we run out of iteration process when we run out of iteration we have here two iterations we may we have here two iterations we may actually have reach the number of actually have reach the number of iterations that we pos as a parameter in iterations that we pos as a parameter in the algorithm or we reach the desired the algorithm or we reach the desired number of tokens the maximum number of tokens the maximum number of tokens that we would be able to tokens that we would be able to have and that's basically have and that's basically how the algorith kind of stops and we how the algorith kind of stops and we produce the tokens at the end if you produce the tokens at the end if you go into your notes then you can actually see this", "image_path": "img_data/video_36_chunk_28.jpg"}
{"video": "video_36", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "go into your notes then you can actually see this then you can actually see this api how this api is sort of called api how this api is sort of called with this sentence that we have as with this sentence that we have as actually as an example and over here you actually as an example and over here you can actually see the u tokens that the can actually see the u tokens that the algorith has produced obviously the algorith is produced obviously the algorith is actually is doing more pairings actually is doing more pairings and what i illustrated over here and what i illustrated over here and therefore we don't have necessarily 100%", "image_path": "img_data/video_36_chunk_29.jpg"}
{"video": "video_36", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "and what i illustrated over here and therefore we don't have necessarily 100% therefore we don't have necessarily 100% agreement with all the tokens we have agreement with all the tokens we have identified up to this point but that identified up to this point but that here's the api and you probably notice here's the api and you probably notice that some kind of weird character this g that some kind of weird character this g symbol over here the this is symbol over here the this is effectively the character that is effectively the character that is representing the space symbol and the representing the space symbol and the kind of explanation here is here for kind of explanation here is here for you to understand how on earth this g you to understand how on earth this g symbol came to be and we don't report symbol came to be and we don't report space and that's basically it", "image_path": "img_data/video_36_chunk_30.jpg"}
{"video": "video_36", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "symbol came to be and we don't report space and that's basically it space and that's basically it with respect to bite pair and coding with respect to bite pair and coding the next step for us is to understand the next step for us is to understand how we will be doing the embedding how we will be doing the embedding en coding we now have our tokens en coding we now have our tokens and we need to our tokens now are and we need to our tokens now are represented in a table with some integer represented in a table with some integer numbers next to each one of them we numbers next to each one of them we have now our vocabulary that has been have now our vocabulary that has been determined from this exercise and determined from this exercise and these integer numbers we will see now", "image_path": "img_data/video_36_chunk_31.jpg"}
{"video": "video_36", "start": "0:16:00", "end": "0:16:12.833333", "timestamp": "0:16:00 - 0:16:12.833333", "text": "determined from this exercise and these integer numbers we will see now these integer numbers we will see now how they are converted to a how they are converted to a representation a vector representation a vector representation for further processing into our kind of for further processing into our kind of u pipeline", "image_path": "img_data/video_36_chunk_32.jpg"}
{"video": "video_37", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we are starting to u get into a very important application space into a very important application space in ai called natural language in ai called natural language processing in this kind of space we processing in this kind of space we are obviously the space is vast we are obviously the space is vast we were not going to treat everything were not going to treat everything that it is to be treated is actually that it is to be treated is actually quite exciting but we are going to quite exciting but we are going to hit some main points that we can all hit some main points that we can all understand the basics and some kind of understand the basics and some kind of foundations we will start with", "image_path": "img_data/video_37_chunk_0.jpg"}
{"video": "video_37", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "understand the basics and some kind of foundations we will start with foundations we will start with understanding about the understanding about the beddings then we will proceed to beddings then we will proceed to use the neural architectures the use the neural architectures the recurring neur networks in the form of recurring neur networks in the form of lstms that we have seen in a earlier lstms that we have seen in a earlier video and initially start with those and video and initially start with those and understand the kind of what is called understand the kind of what is called attention the attention mechanism spe attention the attention mechanism spe specifically for sequence to sequence specifically for sequence to sequence problems and then we will be", "image_path": "img_data/video_37_chunk_1.jpg"}
{"video": "video_37", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "specifically for sequence to sequence problems and then we will be problems and then we will be looking at language models and finally look at the models and finally look at the transformer architectures and these transformer architectures and these architectures are going to be motivated architectures are going to be motivated by some notebooks as well u not only by some notebooks as well u not only those transformer architecture but also those transformer architecture but also other architecture has to do with a other architecture has to do with a sequence to sequence modeling and sequence to sequence modeling and lstms in a similar setting that we have lstms in a similar setting that we have seen in another video with where we seen in another video with where we actually were looking at actually were looking at the sort of structure of the age agent", "image_path": "img_data/video_37_chunk_2.jpg"}
{"video": "video_37", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "actually were looking at the sort of structure of the age agent the sort of structure of the age agent as it is acting in the in an as it is acting in the in an environment the chatbot will take the environment the chatbot will take the form of an agent that is evidently will form of an agent that is evidently will be acting in an environment which is be acting in an environment which is obviously different than a physical obviously different than a physical robot definitely this environment robot definitely this environment is effectively the users let me is effectively the users let me just draw them this and write them just draw them this and write them as let's say this chatboard as let's say this chatboard is a b that it will be", "image_path": "img_data/video_37_chunk_3.jpg"}
{"video": "video_37", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "as let's say this chatboard is a b that it will be is a b that it will be interacting with users of a customer interacting with users of a customer service the environment is are the environment is are the users of customer service and not only that but also it will be the and this interaction will be the and this interaction will be in a piece of software that will be in a piece of software that is a chat application i mean famous chat is a chat application i mean famous chat application such as discord i guess application such as discord i guess everyone has some familiar", "image_path": "img_data/video_37_chunk_4.jpg"}
{"video": "video_37", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "application such as discord i guess everyone has some familiar everyone has some familiar with discord or slack that is with discord or slack that is basically the environment and as we have basically the environment and as we have seen previously this environment will seen previously this environment will have some form of a state the state of the environment state the state of the environment is fully observed in this case in is fully observed in this case in a in what sense that the you a in what sense that the what the users are actually typing know what the users are actually typing to towards the boat asking the boat to towards the boat asking the boat questions or are issuing kind of questions or are issuing kind of queries into this boat are", "image_path": "img_data/video_37_chunk_5.jpg"}
{"video": "video_37", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "questions or are issuing kind of queries into this boat are queries into this boat are fully observable by the bot fully observable by the bot and the agent this natural language and the agent this natural language processing agent in this case will processing agent in this case will be drawn now on the sort of left will be drawn now on the sort of left side over here of that environment side over here of that environment the equivalent to the perception the equivalent to the perception subsystem that we had in robotics over subsystem that we had in robotics over here we have a subsystem called the have a subsystem called the parsing subsystem and evidently it is receiving subsystem and evidently it is receiving some form of textual or in many", "image_path": "img_data/video_37_chunk_6.jpg"}
{"video": "video_37", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "subsystem and evidently it is receiving some form of textual or in many some form of textual or in many instances also combination of text images and even video although most of the bots that video although most of the bots that we will be dealing with in this we will be dealing with in this lecture are going to be associated lecture are going to be associated with text the parsing with text the parsing stage we will see exactly what the stage we will see exactly what the parsing stage kind of includes if i can parsing stage kind of includes if i can draw the kind of the rest of the blog", "image_path": "img_data/video_37_chunk_7.jpg"}
{"video": "video_37", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "parsing stage kind of includes if i can draw the kind of the rest of the blog draw the kind of the rest of the blog diagram then the text is actually diagram then the text is actually coming in it is pared and this coming in it is pared and this subsequent kind of stage is called the analysis or planning and not or but and planning and not or but and planning kind of stage and then finally we have kind of stage and then finally we have some form of natural language some form of natural language generation", "image_path": "img_data/video_37_chunk_8.jpg"}
{"video": "video_37", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "some form of natural language generation where we respond back to that environment that is these are basically very high that is these are basically very high level sort of diagram of that level sort of diagram of that represents the chatbot represents the chatbot application and what is happening application and what is happening at every kind of stage largely", "image_path": "img_data/video_37_chunk_9.jpg"}
{"video": "video_37", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "application and what is happening at every kind of stage largely at every kind of stage largely defines the sort of stages also of a defines the sort of stages also of a typical nlp kind of pipeline the text typical nlp kind of pipeline the text that it is inserted by the users that it is inserted by the users are the first stage is segmentation for example in our site over here we have actually given some over here we have actually given some examples of what is the segmentation examples of what is the segmentation the effective", "image_path": "img_data/video_37_chunk_10.jpg"}
{"video": "video_37", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "examples of what is the segmentation the effective let's take a sentence london is the let's take a sentence london is the capital and most popular city of england capital and most popular city of england and the united kingdom during that kind and the united kingdom during that kind of stage the u the paragraph that the user may have paragraph that the user may have inserted it can be segmented into in inserted it can be segmented into in this case a sentence the paragraph this case a sentence the paragraph could be pretty sizable and that could be pretty sizable and that segmentation stag is just gives us if segmentation stag is just gives us if you a sentence from the beginning you a sentence from the beginning of the sense until the full", "image_path": "img_data/video_37_chunk_11.jpg"}
{"video": "video_37", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "you a sentence from the beginning of the sense until the full beginning of the sense until the full stop that is the an example of stop that is the an example of segmentation the subsequent segmentation the subsequent kind of stage is called kind of stage is called tokenization in this kind of tokenization in this kind of tokenization we are converting the u tokenization we are converting the u fragmented the segmented if you fragmented the segmented if you text into tokens now there's a you text into tokens now there's a much deeper discussion to be had as know much deeper discussion to be had as to how we actually go about doing that to how we actually go about doing that and we'll provide some kind of a you and we'll provide some kind of a few ideas about how to do a", "image_path": "img_data/video_37_chunk_12.jpg"}
{"video": "video_37", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "and we'll provide some kind of a few ideas about how to do a know few ideas about how to do a tokenization let me also write down tokenization let me also write down that kind of stage tokenization and in this specific simple setting we can imagine simple setting we can imagine tokenization to be a whole world tokenization to be a whole world tokenization plus any symbols that we tokenization plus any symbols that we have the full stop over here have the full stop over here that is basically fairly easy in this that is basically fairly easy in this specific example of stage", "image_path": "img_data/video_37_chunk_13.jpg"}
{"video": "video_37", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "that is basically fairly easy in this specific example of stage specific example of stage then we have other stages that are then we have other stages that are helping us if you understand helping us if you understand what is being typed and one of them i what is being typed and one of them i wanted to mention is the wanted to mention is the -called part of speech tagging the part of speech tagging and in the part of speech tagging what we in the part of speech tagging what we actually can", "image_path": "img_data/video_37_chunk_14.jpg"}
{"video": "video_37", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "in the part of speech tagging what we actually can see is that is the following is see is that is the following is happening the every single of the happening the every single of the tokens is being classified to be a noun tokens is being classified to be a noun a verb a conjunction an adverb an verb a conjunction an adverb an adjective all of these are helping adjective all of these are helping us to understand exactly what the us to understand exactly what the sort of the sentence is sort of the sentence is all about and then we have some other about and then we have some other subsequent stages is called lization", "image_path": "img_data/video_37_chunk_15.jpg"}
{"video": "video_37", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "about and then we have some other subsequent stages is called lization subsequent stages is called lization which is some kind of a simple which is some kind of a simple normalization stages stop w normalization stages stop w identification that we can you identification that we can definitely have an idea about know definitely have an idea about where this sentences are where this sentences are ending and then we have some more ending and then we have some more advanced tasks which i wanted to advanced tasks which i wanted to refer to and then the first one is refer to and then the first one is called dependency", "image_path": "img_data/video_37_chunk_16.jpg"}
{"video": "video_37", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "refer to and then the first one is called dependency parsing and in the kind of dependency parsing and in the kind of dependency parsing we have our goal is really to parsing we have our goal is really to form a tree and this in this tree form a tree and this in this tree what we did what we do is at the root what we did what we do is at the root node we assign a single node we assign a single parent one of the words is parent one of the words is assigned to be the parent the root node assigned to be the parent the root node of the tree in this case is b and of the tree in this case is b and we also in this case we're also we also in this case we're also predicting the what type of relationship", "image_path": "img_data/video_37_chunk_17.jpg"}
{"video": "video_37", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "we also in this case we're also predicting the what type of relationship predicting the what type of relationship is the other tokens with respect to this is the other tokens with respect to this kind of root node in this case the kind of root node in this case the relationship is with london is relationship is with london is subject in the relationship with subject in the relationship with capital and is attribute and on capital and is attribute and on this type of and on this type of algorithms or predictors are starting to algorithms or predictors are starting to become a bit more involved and we become a bit more involved and we actually have to use for all the stages", "image_path": "img_data/video_37_chunk_18.jpg"}
{"video": "video_37", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "become a bit more involved and we actually have to use for all the stages actually have to use for all the stages we are going to be using certain we are going to be using certain libraries there are historically in the libraries there are historically in the natural language processing space there natural language processing space there are two libraries one of them was are two libraries one of them was called nltk and it was kind of a bit called nltk and it was kind of a bit more research oriented but over here we are oriented but over here we are going to see a demo from another going to see a demo from another library called spacey in this example library called spacey in this example you can see that the root node is the you can see that the root node is the verb having and the relationship", "image_path": "img_data/video_37_chunk_19.jpg"}
{"video": "video_37", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "verb having and the relationship having and the relationship that spacey can predict are actually that spacey can predict are actually here based on an existing here based on an existing model and i think it's notable the model and i think it's notable the compactness of the api for this kind compactness of the api for this kind of library how quickly you can actually of library how quickly you can actually see and visualize exactly all the see and visualize exactly all the dependencies that space is able to dependencies that space is able to predict and of course we have many other predict and of course we have many other libraries that we will meet gradually libraries that we will meet gradually in this and other videos we have the dependency", "image_path": "img_data/video_37_chunk_20.jpg"}
{"video": "video_37", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "in this and other videos we have the dependency parsing and we have also named entity recognition or ner and here is an example of name dat recognition that example of name dat recognition that we can actually see here we have a if we can actually see here we have a if you a in this kind of paragraph", "image_path": "img_data/video_37_chunk_21.jpg"}
{"video": "video_37", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "we can actually see here we have a if you a in this kind of paragraph you a in this kind of paragraph we the purpose is to identify and we the purpose is to identify and classify by classification what is really the classification what is really the category that corresponds to every word category that corresponds to every word at least those who are words that are at least those who are words that are corresponds as name the socal named corresponds as name the socal named entities in this case 2019 is obviously entities in this case 2019 is obviously a date it's classified as such there a date it's classified as such there are obviously the united states is a are obviously the united states is a geographical entity and is classified as geographical entity and is classified as such and nyu grman school of medicine is", "image_path": "img_data/video_37_chunk_22.jpg"}
{"video": "video_37", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "geographical entity and is classified as such and nyu grman school of medicine is such and nyu grman school of medicine is an organization and on this person is an organization and on this person is this d dav man is a person and there obviously also some person and there obviously also some mistakes as you can see u and in this mistakes as you can see u and in this case the profession of the person case the profession of the person which is a medical doctor is which is a medical doctor is misinterpreted as a ge graphical entity misinterpreted as a ge graphical entity and covid-19 is called an and covid-19 is called an organization and we can have of course other errors", "image_path": "img_data/video_37_chunk_23.jpg"}
{"video": "video_37", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "organization and we can have of course other errors and we can have of course other errors and all these errors are originate from and all these errors are originate from the statistical nature of these the statistical nature of these predictors and to close the discussion predictors and to close the discussion about what other stages we can actually about what other stages we can actually see in the in this kind of parsing see in the in this kind of parsing subsystem i think it's worthwhile subsystem i think it's worthwhile mentioning about core reference mentioning about core reference resolution because it is really one of resolution because it is really one of the more complicated tasks and people the more complicated tasks and people are actually very humans are actually very humans are actually very of establishing some form of", "image_path": "img_data/video_37_chunk_24.jpg"}
{"video": "video_37", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "are actually very humans are actually very of establishing some form of very of establishing some form of relationship between let's say it and relationship between let's say it and london but cor reference resolution london but cor reference resolution evidently that relationship has to be evidently that relationship has to be predicted by an algorithm in predicted by an algorithm in neur architecture and here we see architecture and here we see another example of that in this another example of that in this case we have a sentence my in this case we have a sentence my sister is swimming with her sister is swimming with her classmates they are not bad but let see classmates they are not bad but let see is better let's stop here the they", "image_path": "img_data/video_37_chunk_25.jpg"}
{"video": "video_37", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "classmates they are not bad but let see is better let's stop here the they is better let's stop here the they is evidently associated with is evidently associated with classmates and the hair is associated classmates and the hair is associated with the sister and on and with the sister and on the core reference on the core reference resolution is also helping us to resolution is also helping us to understand some of the language understand some of the language which is behind if you the input which is behind if you the input text that came to this kind of chatboard text that came to this kind of chatboard and with that we kind of conclude the and with that we kind of conclude the discussion about the parsing subsystem discussion about the parsing subsystem and we can actually move into", "image_path": "img_data/video_37_chunk_26.jpg"}
{"video": "video_37", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "discussion about the parsing subsystem and we can actually move into and we can actually move into discussing this analysis and planning discussing this analysis and planning and in this case what we have here if and in this case what we have here if i may just go back to here and i may just go back to here and mention quickly the core mention quickly the core reference resolution that we have seen if we resolution that we have seen if we may actually do something this to may actually do something this to indicate that this part of parsing and indicate that this part of parsing and in the case of analysis and planning we in the case of analysis and planning we should be associating with this kind of should be associating with this kind of states with that datase that we will", "image_path": "img_data/video_37_chunk_27.jpg"}
{"video": "video_37", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "should be associating with this kind of states with that datase that we will states with that datase that we will call knowledge base now we've seen the call knowledge base now we've seen the term knowledge base in another video as term knowledge base in another video as part of logical reasoning but this is a part of logical reasoning but this is a bit more conventional kind of database bit more conventional kind of database it's not constrained from the it's not constrained from the propositional logic and the various kind propositional logic and the various kind of logical sentences that are there is a of logical sentences that are there is a relational database or a non sql relational database or a non sql database as are also known or document database as are also known or document oriented database and this kind of analysis database and this kind of analysis over here we may actually see using over here we may actually see using case associated let's say with if", "image_path": "img_data/video_37_chunk_28.jpg"}
{"video": "video_37", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "over here we may actually see using case associated let's say with if case associated let's say with if i may open up some space the i may open up some space the sentiment let's say sentiment analysis we may actually need to predict as to whether this customer is angry as to whether this customer is angry happier is what is this kind of happier is what is this kind of psychological state in order for us to psychological state in order for us to react kind of appropriately we may react kind of appropriately we may actually do power our use cases out", "image_path": "img_data/video_37_chunk_29.jpg"}
{"video": "video_37", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "react kind of appropriately we may actually do power our use cases out actually do power our use cases out of the system that associated with of the system that associated with semantic search and in sematic search we are effectively trying to find effectively trying to find relevant responses that the chatbot will responses that the chatbot will be able to select from and send as be able to select from and send as part of this kind of response in part of this kind of response in many simpler kind of chat bots many simpler kind of chat bots some of these responses are kind of some of these responses are kind of already kind of structured and then we", "image_path": "img_data/video_37_chunk_30.jpg"}
{"video": "video_37", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "some of these responses are kind of already kind of structured and then we already kind of structured and then we can with semantic search we can with semantic search we can actually find some additional actually find some additional information or retrieve some kind of information or retrieve some kind of additional information necessary for the additional information necessary for the subsequent block to be able to form a subsequent block to be able to form a natural language response natural language response as an example we may the c may as an example we may the c may be mentioning the u model number or be mentioning the u model number or something that can identify if you something that can identify if you a product or a service and with semantic search we", "image_path": "img_data/video_37_chunk_31.jpg"}
{"video": "video_37", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "a product or a service and with semantic search we or a service and with semantic search we actually can look in the database in the actually can look in the database in the knowledge base and retrieve back some knowledge base and retrieve back some more information about this kind of more information about this kind of product this could potentially acting as product this could potentially acting as a prompt for the subsequent case a prompt for the subsequent case of natural language generation the of natural language generation the subsequent block of that the energy subsequent block of that the energy block which of course over here we block which of course over here we will see that a lot of things are will see that a lot of things are happening but definitely we happening but definitely we will have to have the -called", "image_path": "img_data/video_37_chunk_32.jpg"}
{"video": "video_37", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "happening but definitely we will have to have the -called will have to have the -called language model and as of recently this is qualified with kind of a foundational is qualified with kind of a foundational models i call large language models i call large language models but it doesn't really have to be models but it doesn't really have to be large it could be a language model has large it could be a language model has that has been constructed for the very that has been constructed for the very specific narrow domain that the bot is specific narrow domain that the bot is able to respond to the customer and able to respond to the customer and we may actually have sequence to", "image_path": "img_data/video_37_chunk_33.jpg"}
{"video": "video_37", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "able to respond to the customer and we may actually have sequence to we may actually have sequence to sequence models over there or sequence models over there or associated with responding to customers associated with responding to customers potentially in multiple languages potentially in multiple languages let me mention that and on the natural language generation is kind of a rich space generation is kind of a rich space that we actually can borrow from a that we actually can borrow from a lot of the stuff that we'll be lot of the stuff that we'll be discussing in those in this", "image_path": "img_data/video_37_chunk_34.jpg"}
{"video": "video_37", "start": "0:17:30", "end": "0:17:35.600000", "timestamp": "0:17:30 - 0:17:35.600000", "text": "lot of the stuff that we'll be discussing in those in this discussing in those in this video series on natural language video series on natural language processing", "image_path": "img_data/video_37_chunk_35.jpg"}
{"video": "video_38", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "and in this part of the notebook we actually see here the summary of what we actually see here the summary of what we have just discussed there are certain have just discussed there are certain numer sorry not numerology but certain numer sorry not numerology but certain notations that the literature has kind notations that the literature has kind of adopted to express all these kind of adopted to express all these kind of variances that are involved both on the variances that are involved both on the measurement side as well also the state measurement side as well also the state estimates and update formulas for estimates and update formulas for the u both measurements in the u predic", "image_path": "img_data/video_38_chunk_0.jpg"}
{"video": "video_38", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "estimates and update formulas for the u both measurements in the u predic the u both measurements in the u predic and but this is the summary of and but this is the summary of what we have just discussed the what we have just discussed the initialization stage the prediction initialization stage the prediction stage where we use the model the stage where we use the model the kinematic model to account for to kinematic model to account for to come up with a belief hat and then the come up with a belief hat and then the update and this update is going to be update and this update is going to be u done in a calman filter using a u done in a calman filter using a formula which is very close if not formula which is very close if not identical to the formula we have seen in identical to the formula we have seen in the maximum of the", "image_path": "img_data/video_38_chunk_1.jpg"}
{"video": "video_38", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "identical to the formula we have seen in the maximum of the in the maximum of the estimate which calman again is going estimate which calman again is going to be which actually called scaling to be which actually called scaling factor over here going to be computed factor over here going to be computed and based on that calman gain the and based on that calman gain the expressions of which we will now write expressions of which we will now write down at the with a general case where down at the with a general case where we have more than one dimension we have more than one dimension we have multiple dimensions in both multiple dimensions in both measurements as well also the state measurements as well also the state which is really the typically the case which is really the typically the case even in the case of a drone here we have", "image_path": "img_data/video_38_chunk_2.jpg"}
{"video": "video_38", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "which is really the typically the case even in the case of a drone here we have even in the case of a drone here we have measurements which are more than one measurements which are more than one because we have u uncertainties in because we have u uncertainties in terms of measurement in the terms of measurement in the three-dimensional plane space and of three-dimensional plane space and of course our state is also in a course our state is also in a threedimensional space at the very threedimensional space at the very minimum we have the need to minimum we have the need to express this in a three in three express this in a three in three dimensions in this case but we write dimensions in this case but we write down all these equations in that kind of down all these equations in that kind of a general form but here you can actually a general form but here you can actually see the equations that we have used see the equations that we have used in our python code up to now and how", "image_path": "img_data/video_38_chunk_3.jpg"}
{"video": "video_38", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "see the equations that we have used in our python code up to now and how in our python code up to now and how these equations are typed in or these equations are typed in or expressed in the -called calman form expressed in the -called calman form where the matrices p and q are where the matrices p and q are actually used we'll write now a actually used we'll write now a series of equations that are going to be series of equations that are going to be look a bit dry they will be look a bit dry they will be look a bit ugly in terms of sort of bit ugly in terms of sort of absorbing them it will take some effort absorbing them it will take some effort but at the end of the day these are but at the end of the day these are the equations that we can actually type", "image_path": "img_data/video_38_chunk_4.jpg"}
{"video": "video_38", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "but at the end of the day these are the equations that we can actually type the equations that we can actually type in python and we'll do that in python and we'll do that after we write them let's write down after we write them let's write down now the general multi-dimensional calman now the general multi-dimensional calman filter method or function let filter method or function let me just write it over here as calman open parenthesis we have u definitely the usual suspect of the definitely the usual suspect of the belief of the previous u state", "image_path": "img_data/video_38_chunk_5.jpg"}
{"video": "video_38", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "definitely the usual suspect of the belief of the previous u state we'll see how this is going to be represented we have an action and we represented we have an action and we have a measurement that is actually have a measurement that is actually coming in these are the arguments just coming in these are the arguments just we had it in the earlier we had it in the earlier instance of recursive state estimation instance of recursive state estimation but now everything is gausian but now everything is gausian everything that we will see here will be everything that we will see here will be represented with the sufficient represented with the sufficient statistics of ga and distribution statistics of ga and distribution multi-dimensional ga and multi-dimensional ga and distributions or multi varied gaa", "image_path": "img_data/video_38_chunk_6.jpg"}
{"video": "video_38", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "multi-dimensional ga and distributions or multi varied gaa distributions or multi varied gaa distributions which are the mean and distributions which are the mean and the covariance matrix and in the covariance matrix and in this respect the belief this is going to be equivalent to calman the belief will be represented by the mu t minus one which is going to the mu t minus one which is going to be a vector and the calian matrix be a vector and the calian matrix capital sigma of t minus one and given capital sigma of t minus one and given an action and some kind of measurement", "image_path": "img_data/video_38_chunk_7.jpg"}
{"video": "video_38", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "capital sigma of t minus one and given an action and some kind of measurement an action and some kind of measurement which is also a vector in general which is also a vector in general for those who have kind of for those who have kind of forgotten multivar gaussians i suggest forgotten multivar gaussians i suggest that you go back to the earlier video that you go back to the earlier video where we were reviewing a probability where we were reviewing a probability and at that time we spend some time and at that time we spend some time understanding the biar gausin if you understanding the biar gausin if you understand the biar gausin you can understand the biar gausin you can understand the multivaried gausian in a understand the multivaried gausian in a kind of a general case the second", "image_path": "img_data/video_38_chunk_8.jpg"}
{"video": "video_38", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "understand the multivaried gausian in a kind of a general case the second kind of a general case the second i'm going to this is basically the i'm going to this is basically the function call the first function call the first equation over here is going to equation over here is going to be the update which i'll call new hat t because i'm expressing now one of the because i'm expressing now one of the two parameters of the belief hat the two parameters of the belief hat as we said is going to be belief hat as we said is going to be gausian multivi gausian therefore now gausian multivi gausian therefore now we are writing the first parameter of we are writing the first parameter of that gausian it's going to that gausian it's going to be 8 t i'll specify what the matrix", "image_path": "img_data/video_38_chunk_9.jpg"}
{"video": "video_38", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "that gausian it's going to be 8 t i'll specify what the matrix be 8 t i'll specify what the matrix is mt minus one i think i can well for capital symbols can well for capital symbols i'm not going to use the underbar i'm not going to use the underbar because it assumed to be matrices but because it assumed to be matrices but for everything else i will for everything else i will bt a that's the equation that a that's the equation that updates the mean let me also write down", "image_path": "img_data/video_38_chunk_10.jpg"}
{"video": "video_38", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "a that's the equation that updates the mean let me also write down updates the mean let me also write down the equation that updates the equation that updates the covariance is sigma t is equal 8 sigma t minus one a transpose that's a quadratic expression transpose that's a quadratic expression plus r t we'll see now this all t we'll see now this all generalization what we have seen nearly generalization what we have seen nearly for the single dimensional calman", "image_path": "img_data/video_38_chunk_11.jpg"}
{"video": "video_38", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "generalization what we have seen nearly for the single dimensional calman for the single dimensional calman filter the key assumption regarding filter the key assumption regarding the calman filters is that the calman filters is that the beliefs the as the belief is kind of updated as the belief is kind of updated according to what is called a linear according to what is called a linear gausian kind of model the if i gausian kind of model the if i write kind of the underlying assumption write kind of the underlying assumption behind these kind of", "image_path": "img_data/video_38_chunk_12.jpg"}
{"video": "video_38", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "write kind of the underlying assumption behind these kind of behind these kind of expressions is that the probability let me write down as a as probability let me write down as a state transition st is equal to a state transition st is equal to a st minus one plus bt a this is also by the way a this is also by the way a vector this is a vector u", "image_path": "img_data/video_38_chunk_13.jpg"}
{"video": "video_38", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "a this is also by the way a vector this is a vector u epsilon t this is the what we call a linear gausian system all right let's look at the dimensionality now of look at the dimensionality now of everything which is kind of involved everything which is kind of involved here the first thing that we need to", "image_path": "img_data/video_38_chunk_14.jpg"}
{"video": "video_38", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "everything which is kind of involved here the first thing that we need to here the first thing that we need to specify is the number of dimensions for specify is the number of dimensions for our state s of t in fact i may need some more space t in fact i may need some more space here to write everything s of here to write everything s of t is going to be a vector of s1 t s2t t is going to be a vector of s1 t s2t dot snt this will be n r i mean evidently a", "image_path": "img_data/video_38_chunk_15.jpg"}
{"video": "video_38", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "snt this will be n r i mean evidently a this will be n r i mean evidently a column vector that will actually be column vector that will actually be the state and the action we have m rows for the actions a1t amt action can actually be m dimensional and therefore as a sort dimensional and therefore as a sort of a i will call it a result of that of", "image_path": "img_data/video_38_chunk_16.jpg"}
{"video": "video_38", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "dimensional and therefore as a sort of a i will call it a result of that of a i will call it a result of that of these dimensions we want the a matrix to these dimensions we want the a matrix to be an n by n come on mn by n and the bt matrix to be n by n and the bt matrix to be n by m that when these are multiplied m that when these are multiplied together with the dimensionality of the together with the dimensionality of the error we will actually result into an n error we will actually result into an n dimensional vector the error", "image_path": "img_data/video_38_chunk_17.jpg"}
{"video": "video_38", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "error we will actually result into an n dimensional vector the error now is according to distribut according to a normal distribution with zero to a normal distribution with zero vector mean and covariance vector mean and covariance rt that captures what we said earlier rt that captures what we said earlier the state transition uncertainty or the state transition uncertainty or the back then when we did the single back then when we did the single dimensional calman f it was sigma square", "image_path": "img_data/video_38_chunk_18.jpg"}
{"video": "video_38", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "back then when we did the single dimensional calman f it was sigma square transition and the path and code was actually called process variance this is the same uncertainty i think we are kind of done i think we are kind of done with everything in this kind of with everything in this kind of equation right this is the equation that equation right this is the equation that dictates the how the ass it's an assumption about how the", "image_path": "img_data/video_38_chunk_19.jpg"}
{"video": "video_38", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "dictates the how the ass it's an assumption about how the ass it's an assumption about how the state is going to evolve over state is going to evolve over time definitely we have time definitely we have a gausian state because we are adding a gausian state because we are adding into some n dimensional vector and into some n dimensional vector and another n dimensional gausian kind another n dimensional gausian kind of vector and we are of vector and we are compliant with what we have promised compliant with what we have promised that everything is going to be gausian that everything is going to be gausian here all right in this respect", "image_path": "img_data/video_38_chunk_20.jpg"}
{"video": "video_38", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "that everything is going to be gausian here all right in this respect here all right in this respect we are done with the first equation we are done with the first equation we have now the belief hat specification we have now the belief hat specification given an a and an bt that u the given an a and an bt that u the what is really the at matrix is what is really the at matrix is doing the at matrix is the doing the at matrix is the generalization what we have seen earlier generalization what we have seen earlier to be the -called vx vector time delta to be the -called vx vector time delta t in other words we are going to be t in other words we are going to be assuming in general u sort of", "image_path": "img_data/video_38_chunk_21.jpg"}
{"video": "video_38", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "t in other words we are going to be assuming in general u sort of assuming in general u sort of linear system dynamics this is the linear system dynamics this is the assumption about linear system dynamics assumption about linear system dynamics this is i think this is the first term over here and the second term just write it over here this is the term just write it over here this is the assumption of", "image_path": "img_data/video_38_chunk_22.jpg"}
{"video": "video_38", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "assumption of linear system dynamics and this is going to be our transition model kind of assumption transition model kind of assumption now if we go now to write the second now if we go now to write the second equation if we write the second equation if we write the second equation of the calvin filter the second is the of the calvin filter the second is the specification of the calman gain specification of the calman gain capital the calman gain capital t is going to be produced by the", "image_path": "img_data/video_38_chunk_23.jpg"}
{"video": "video_38", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "the calman gain capital t is going to be produced by the t is going to be produced by the equation capital sigma t then it's going to be a matrix aarian matrix c t well yeah is going to be well yeah is going to be another matrix ct times another matrix ct times another parenthesis", "image_path": "img_data/video_38_chunk_24.jpg"}
{"video": "video_38", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "another matrix ct times another parenthesis ct sigma t ct transpose plus qt inverse that will be actually be inverse that will be actually be the calman gain is going to be computed let's to going to be computed let's look at a little bit on the", "image_path": "img_data/video_38_chunk_25.jpg"}
{"video": "video_38", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "to going to be computed let's look at a little bit on the look at a little bit on the exactly what the cman gain is telling us exactly what the cman gain is telling us something which is kind of telling us something which is kind of something we have seen a bit earlier as something we have seen a bit earlier as well let me write down here the well let me write down here the calman gain incor is specifies the degree to which", "image_path": "img_data/video_38_chunk_26.jpg"}
{"video": "video_38", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "degree to which the measurement is incorporated into the new state estimate and that new state estimate is actually coming in a sense that it's", "image_path": "img_data/video_38_chunk_27.jpg"}
{"video": "video_38", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "estimate and that new state estimate is actually coming in a sense that it's actually coming in a sense that it's going to be the subsequent kind of going to be the subsequent kind of expression we'll see the calman gain expression we'll see the calman gain in that kind of subsequent in that kind of subsequent expression and in fact it will be in expression and in fact it will be in a broad terms generalization what we a broad terms generalization what we have seen earlier in a sense that in have seen earlier in a sense that in initially the calman gain will be high initially the calman gain will be high and therefore kind of causing the and therefore kind of causing the new state to be driven primarily from new state to be driven primarily from the measurement and then subsequently it the measurement and then subsequently it will actually tail off and that is in a kind of station off and that is in a kind of station case and this is the sort of the", "image_path": "img_data/video_38_chunk_28.jpg"}
{"video": "video_38", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "off and that is in a kind of station case and this is the sort of the case and this is the sort of the third equation is third equation the third equation is going to be the specification of the going to be the specification of the belief and similar to what we have seen belief and similar to what we have seen with the belief hat where we actually with the belief hat where we actually wrote the sufficient statistics of wrote the sufficient statistics of that represent the belief c let that represent the belief c let me sort of indicate that this me sort of indicate that this way that's the belief c we and this way that's the belief c we and this is the belief c of st", "image_path": "img_data/video_38_chunk_29.jpg"}
{"video": "video_38", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "way that's the belief c we and this is the belief c of st is the belief c of st in a similar way we will need to write in a similar way we will need to write the sufficient statistics of course the sufficient statistics of course sufficient because it's gausian sufficient because it's gausian of me t the me t is mu hat of me t the me t is mu hat of t which is the belief hat plus t which is the belief hat plus kt oh the pencil is causing some trouble kt oh the pencil is causing some trouble k t the calman gain we have seen earlier k t the calman gain we have seen earlier and then we see the usual and then we see the usual difference inside it the current difference inside it the current measurement minus", "image_path": "img_data/video_38_chunk_30.jpg"}
{"video": "video_38", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "difference inside it the current measurement minus ct newad that's the innovation we have seen newad that's the innovation we have seen earlier in our discussion about earlier in our discussion about the recursive update on during the ml the recursive update on during the ml discussion all right and then we have discussion all right and then we have the equivalent sigma t which is the equivalent sigma t which is the identity minus kt ct", "image_path": "img_data/video_38_chunk_31.jpg"}
{"video": "video_38", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "minus kt ct sigma t hat and in order for us to hat and in order for us to understand this ct a bit better understand this ct a bit better and i kind of forgot to mention the and i kind of forgot to mention the measurement model because we measurement model because we specified the transition model but at specified the transition model but at the same time we have to specify over the same time we have to specify over here the equivalent measurement model here the equivalent measurement model this was a transition let me write this was a transition let me write it over here the measurement model", "image_path": "img_data/video_38_chunk_32.jpg"}
{"video": "video_38", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "this was a transition let me write it over here the measurement model it over here the measurement model because the measurement model is really because the measurement model is really the generalization what we have seen the generalization what we have seen earlier in the uni dimensional kind of earlier in the uni dimensional kind of case the measurement model is going case the measurement model is going to be zt is equal to ct to be zt is equal to ct st plus delta t in this case ct is going to be t in this case ct is going to be a matrix we'll see now the dimensions of a matrix we'll see now the dimensions of this kind of matrix that this kind of matrix that captures how the linear kind", "image_path": "img_data/video_38_chunk_33.jpg"}
{"video": "video_38", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "this kind of matrix that captures how the linear kind captures how the linear kind of evolution of linear dependency of evolution of linear dependency between the state and the kind of between the state and the kind of measurement let's see the measurement let's see the zt is in general is going to zt is in general is going to be a k dimensional vector be is going to be a k dimensional vector z1 to z k of t that's k dimensions", "image_path": "img_data/video_38_chunk_34.jpg"}
{"video": "video_38", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "be is going to be a k dimensional vector z1 to z k of t that's k dimensions and then we have delta t normal with zero mean and a covariance with zero mean and a covariance qt this is the measurement covariance qt this is the measurement covariance matrix and therefore ct has no option to matrix and therefore ct has no option to than be a k * n matrix this is the exp matrix this is the exp expectation the mean of this", "image_path": "img_data/video_38_chunk_35.jpg"}
{"video": "video_38", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "matrix this is the exp expectation the mean of this expectation the mean of this measurement dct kind of dictates the measurement dct kind of dictates the mean of the measurement and the mean of the measurement and the covariance of the measurement is qt covariance of the measurement is qt that's that is really the that's that is really the assumption regarding the -called assumption regarding the -called measurement model we have the measurement model we have the transition and the measurement these transition and the measurement these are the two let me write it down this model and measurement", "image_path": "img_data/video_38_chunk_36.jpg"}
{"video": "video_38", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "this model and measurement these are the general forms of those i think the important thing i think the important thing now is to go ahead and finish if now is to go ahead and finish if you the equation i mean the c you the equation i mean the c fter now is done and the fter now is done and the return the returns the return the returns the mt and sigma t and exactly the same way is happening", "image_path": "img_data/video_38_chunk_37.jpg"}
{"video": "video_38", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "mt and sigma t and exactly the same way is happening and exactly the same way is happening as it was happening before that's the as it was happening before that's the we're going to recurse to it and we're going to recurse to it and constantly updated and we will be constantly updated and we will be initializing this algorith with initializing this algorith with evidently a byari gausian sorry evidently a byari gausian sorry multivariate gausian the multivariate gausian the initialization of that i'm just going to initialization of that i'm just going to write it over here is the belief of s z write it over here is the belief of s z it's the usual form of", "image_path": "img_data/video_38_chunk_38.jpg"}
{"video": "video_38", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "write it over here is the belief of s z it's the usual form of multivaried goin as we have met it earlier all of these are vectors that is basically the initialization took some time to write initialization took some time to write every all the kind of equations but i", "image_path": "img_data/video_38_chunk_39.jpg"}
{"video": "video_38", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "initialization took some time to write every all the kind of equations but i every all the kind of equations but i want you also to expose you to the book want you also to expose you to the book that i mentioned earlier now this derivation of mentioned earlier now this derivation of why this equations ends up being if why this equations ends up being if you assume linear dynamics and also you assume linear dynamics and also linear measurement models linear measurement models linear gaan kind of measurement models all gaan kind of measurement models all of these kind of derivation is included of these kind of derivation is included in that textbook that i mentioned to you in that textbook that i mentioned to you it covers the 3.2.4 we are going to have a pdf file", "image_path": "img_data/video_38_chunk_40.jpg"}
{"video": "video_38", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "covers the 3.2.4 we are going to have a pdf file 3.2.4 we are going to have a pdf file linked into this trying to get all of linked into this trying to get all of you that may be interested to understand you that may be interested to understand why the equations came to be this why the equations came to be this it is spanning probably four it is spanning probably four five pages of derivation but if you want five pages of derivation but if you want to have the complete kind of view i to have the complete kind of view i think it's worthwhile having it at least think it's worthwhile having it at least as a reference maybe at some point you as a reference maybe at some point you get interesting to understand everything get interesting to understand everything behind this kind of fundamental m al behind this kind of fundamental m al algorith as i mentioned earlier the", "image_path": "img_data/video_38_chunk_41.jpg"}
{"video": "video_38", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "behind this kind of fundamental m al algorith as i mentioned earlier the algorith as i mentioned earlier the calman filters and there are of course calman filters and there are of course enhancements into those calman filters enhancements into those calman filters extended calman filters that are extended calman filters that are making some kind of deviate from making some kind of deviate from this kind of linear gausian system model this kind of linear gausian system model and many others mont carlo based methods and many others mont carlo based methods which are being used in practice in which are being used in practice in many of the robotics many of the robotics whenever we do tracking of a state whenever we do tracking of a state but also as we will see in one of but also as we will see in one of your assignments we will also have", "image_path": "img_data/video_38_chunk_42.jpg"}
{"video": "video_38", "start": "0:21:30", "end": "0:21:57.533333", "timestamp": "0:21:30 - 0:21:57.533333", "text": "but also as we will see in one of your assignments we will also have your assignments we will also have the calman filters being used to track the calman filters being used to track the centroids of the objects which are the centroids of the objects which are being transmitted to that being transmitted to that probabilistic reasoning over time block probabilistic reasoning over time block from the object detector i think from the object detector i think that exercise will actually be helpful that exercise will actually be helpful to for us to understand how the to for us to understand how the correction happens in the after the correction happens in the after the object detector the reflexive object detector the reflexive object detector", "image_path": "img_data/video_38_chunk_43.jpg"}
{"video": "video_39", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now we will see this notebook that is inside this section called inside this section called onedimensional calman filter it's a very onedimensional calman filter it's a very instructive notebook because it is instructive notebook because it is presenting the single dimensional case presenting the single dimensional case and this is always the simplest and this is always the simplest possible case is always the best to possible case is always the best to learn the basics from and in that kind learn the basics from and in that kind of section updates with gaussians is of section updates with gaussians is really the what we actually wrote a few really the what we actually wrote a few moments ago about the moments ago about the measurement update formula and as it", "image_path": "img_data/video_39_chunk_0.jpg"}
{"video": "video_39", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "moments ago about the measurement update formula and as it measurement update formula and as it turns out there are close form solutions when out there are close form solutions when you have two gaans and you multiply with you have two gaans and you multiply with them there are close form solutions them there are close form solutions to the with respect to the mean and the to the with respect to the mean and the variance of the resulting posterior variance of the resulting posterior the belief of our location xt this is the belief of our location xt this is the formula this is the close form the formula this is the close form formula for that posterior and formula for that posterior and therefore given the presence of this therefore given the presence of this formula what we can actually do we can", "image_path": "img_data/video_39_chunk_1.jpg"}
{"video": "video_39", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "therefore given the presence of this formula what we can actually do we can formula what we can actually do we can start plotting the two gaussians start plotting the two gaussians which one refers to the which one refers to the likelihood which is the likelihood which is the probability of zt given xt that probability of zt given xt that is really the distribution that is really the distribution if you of that we wrote over here if you of that we wrote over here this is the likelihood and then this is the likelihood and then this will be the prior in the notebook will be the prior in the notebook kind of setting over here and then we kind of setting over here and then we will implement this kind of function", "image_path": "img_data/video_39_chunk_2.jpg"}
{"video": "video_39", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "kind of setting over here and then we will implement this kind of function will implement this kind of function gausia multiply and this gaia multiply gausia multiply and this gaia multiply function will given the analytically function will given the analytically derived mean and the variances that derived mean and the variances that we have u typed over here we will see we have u typed over here we will see how the posterior looks how the posterior looks and that is going to be the and that is going to be the exercise now all right we are going exercise now all right we are going to have let's say the first case where to have let's say the first case where we have two gaussians that are right we have two gaussians that are right on top of each other we have two", "image_path": "img_data/video_39_chunk_3.jpg"}
{"video": "video_39", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "we have two gaussians that are right on top of each other we have two on top of each other we have two components that looks the blue components that looks the blue distribution over here and we have distribution over here and we have now a multiplication between both the now a multiplication between both the measurement and the prior just coincide measurement and the prior just coincide and that's the case where your and that's the case where your measurement effectively agrees perfectly measurement effectively agrees perfectly in terms of distribution with your in terms of distribution with your predicted state the belief c one predicted state the belief c one component is the belief c the other component is the belief c the other component is the measurement what component is the measurement what happens when they are agreeing well when happens when they are agreeing well when they are gre you have some form", "image_path": "img_data/video_39_chunk_4.jpg"}
{"video": "video_39", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "happens when they are agreeing well when they are gre you have some form they are gre you have some form intuitively you get some form of intuitively you get some form of reinforcement of your belief your reinforcement of your belief your posterior will actually be peier and posterior will actually be peier and that is the result analytically as that is the result analytically as well and that is going to be the well and that is going to be the posterior probability density posterior probability density that's one thing what happens now when that's one thing what happens now when we have various some kind of distance between various some kind of distance between the prior and the likelihood the belief c and the likelihood the belief c and the likelihood and the measurement u", "image_path": "img_data/video_39_chunk_5.jpg"}
{"video": "video_39", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "likelihood the belief c and the likelihood and the measurement u likelihood and the measurement u information in that kind of case information in that kind of case and let's assume that we have some and let's assume that we have some the difference only expands on the difference only expands on the mean and variances are identical then mean and variances are identical then the posterior will be somewhere in the posterior will be somewhere in between and right here it is between and right here it is the dist it's exactly at the dist it's exactly at the midpoint between the two peaks of the midpoint between the two peaks of the gaussians all right here is gaussians all right here is another plot that other plots", "image_path": "img_data/video_39_chunk_6.jpg"}
{"video": "video_39", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "gaussians all right here is another plot that other plots another plot that other plots actually show the behavior where we actually have the behavior where we actually have the in one case we have comp one component is the prior have comp one component is the prior and the other is the measurement and the other is the measurement distribution and that's in this distribution and that's in this case we have differences in the variance case we have differences in the variance we have let's say this is the prior we have let's say this is the prior the belief cut and this is what the belief cut and this is what the measurement is the posterior will be", "image_path": "img_data/video_39_chunk_7.jpg"}
{"video": "video_39", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "the belief cut and this is what the measurement is the posterior will be measurement is the posterior will be not in the midpoint but it will actually not in the midpoint but it will actually be closer to the more confident of be closer to the more confident of the two as you can actually see the two as you can actually see over here it's actually closer to over here it's actually closer to the measurement and obviously the measurement and obviously the reverse is true when reverse the reverse is true when we have reverse the roles of the we have reverse the roles of the prior and the likelihood we have prior and the likelihood we have with this kind of information in mind with this kind of information in mind what we actually will do now next is what we actually will do now next is we will actually see", "image_path": "img_data/video_39_chunk_8.jpg"}
{"video": "video_39", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "what we actually will do now next is we will actually see we will actually see the sec the first equation we the sec the first equation we already can be confident that we can already can be confident that we can update the right the measurement update the right the measurement update python and version of the python and version of the recursive state estimator we will be recursive state estimator we will be calling also from now on the rec state calling also from now on the rec state estimator that we will write estimator that we will write calman filter that's a single calman filter that's a single dimensional calman filter and calman was dimensional calman filter and calman was the first inventor the inventor of the of this kind of filter inventor of the of this kind of filter of this form of the filter when of this form of the filter when everything is gausian and this is going", "image_path": "img_data/video_39_chunk_9.jpg"}
{"video": "video_39", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "of this form of the filter when everything is gausian and this is going everything is gausian and this is going to be the one of the two functions to be the one of the two functions that we are going to see is this that we are going to see is this update the gaus multiply or the update the gaus multiply or the measurement update as we will call it measurement update as we will call it later let's now see the first later let's now see the first equation in the first equation we will equation in the first equation we will use definitely the state use definitely the state transition model as we that is transition model as we that is obviously the kinematic model here we obviously the kinematic model here we have u let me write this down the", "image_path": "img_data/video_39_chunk_10.jpg"}
{"video": "video_39", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "obviously the kinematic model here we have u let me write this down the have u let me write this down the prediction step just to be able to visualize it over here we have the definitely a gaen distribution that the definitely a gaen distribution that governs our belief on the previous state our belief on the previous state this is this has some kind of a this is this has some kind of a mean and some kind of a val", "image_path": "img_data/video_39_chunk_11.jpg"}
{"video": "video_39", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "this is this has some kind of a mean and some kind of a val mean and some kind of a val variance this is the statistics of the variance this is the statistics of the effectively the argument that goes into effectively the argument that goes into the base filter and this the base filter and this represents the belief of the x t minus one the belief of the x t minus one that is basically this that is basically this distribution and this let me write it distribution and this let me write it down this is the probability of xt minus one given", "image_path": "img_data/video_39_chunk_12.jpg"}
{"video": "video_39", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "down this is the probability of xt minus one given probability of xt minus one given and some kind of variance all right and then what will variance all right and then what will actually is going to happen now is that actually is going to happen now is that the drone will actually potentially the drone will actually potentially change its location and now we change its location and now we are dealing not with a hovering kind are dealing not with a hovering kind of drone but we are dealing with a drone of drone but we are dealing with a drone that is actually going to move and if that is actually going to move and if it moves then what is going to moves then what is going to happen then the mean will actually happen then the mean will actually change shift", "image_path": "img_data/video_39_chunk_13.jpg"}
{"video": "video_39", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "happen then the mean will actually change shift by the kinematic model the new location of the mean is going to the new location of the mean is going to be let's say here and this will be this point will here and this will be this point will be given by new plus let's say vx delta t plus whatever second order vx delta t plus whatever second order trans for acceleration that's not trans for acceleration that's not really important right now to have an really important right now to have an accurate kind of kinematic model we accurate kind of kinematic model we have some velocity let's assume this", "image_path": "img_data/video_39_chunk_14.jpg"}
{"video": "video_39", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "accurate kind of kinematic model we have some velocity let's assume this have some velocity let's assume this velocity is known and we have an velocity is known and we have an update delta t we get a new update delta t we get a new indication about from the measurement indication about from the measurement every delta t as we said within delta every delta t as we said within delta t the sort of drone moved by some t the sort of drone moved by some sort of some feet and then we sort of some feet and then we have some v we will see some kind of have some v we will see some kind of distribution that is going distribution that is going to in fact this distribution is going to in fact this distribution is going to be not as picky as the one as we be not as picky as the one as we had discussed", "image_path": "img_data/video_39_chunk_15.jpg"}
{"video": "video_39", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "be not as picky as the one as we had discussed earlier it's going to be some kind of earlier it's going to be some kind of a distribution that is going to be a distribution that is going to be indicating that it is sort of smeared indicating that it is sort of smeared a little bit flattened compared to a little bit flattened compared to this one because as we discussed the this one because as we discussed the fact that we are moving without any fact that we are moving without any presence of the without any presence of the without any measurement information is going to measurement information is going to result into some form of uncertainty result into some form of uncertainty increase this is going to be my", "image_path": "img_data/video_39_chunk_16.jpg"}
{"video": "video_39", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "result into some form of uncertainty increase this is going to be my increase this is going to be my belief hatut of my xt and the belief heart of my xt is xt and the belief heart of my xt is going to possess some form of going to possess some form of variance let me call this variance let me call this variance some increase in the variance this is the variance was added to the previous kind of variance and to the previous kind of variance and of course we wr we wrote actually also", "image_path": "img_data/video_39_chunk_17.jpg"}
{"video": "video_39", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "to the previous kind of variance and of course we wr we wrote actually also of course we wr we wrote actually also the mean and the calman filter will the mean and the calman filter will definitely specify how we all update definitely specify how we all update how what are going to be this variant how what are going to be this variant and obviously is going to tell us and obviously is going to tell us given a kinematic model what will be the given a kinematic model what will be the mean of that distribution in the section of the same distribution in the section of the same notebook that we have seen the one notebook that we have seen the one dimensional calman fitter the dimensional calman fitter the predictions the section prediction with predictions the section prediction with gaussians we actually see here how the gaussians we actually see here how the kinematic model is applied what is", "image_path": "img_data/video_39_chunk_18.jpg"}
{"video": "video_39", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "gaussians we actually see here how the kinematic model is applied what is kinematic model is applied what is our kinematic model is assumed here it our kinematic model is assumed here it is the previous state plus v the is the previous state plus v the velocity time delta t and how the two velocity time delta t and how the two how the belief hat turns out to be how the belief hat turns out to be both in terms of mean as well also the both in terms of mean as well also the variance we are definitely adding variance we are definitely adding this the movement into the mean of the belief of the into the mean of the belief of the previous time instance of the", "image_path": "img_data/video_39_chunk_19.jpg"}
{"video": "video_39", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "into the mean of the belief of the previous time instance of the previous time instance of the state and we're also adding the state and we're also adding the variance the predict function is variance the predict function is it's kind of pretty straightforward and it's kind of pretty straightforward and simple is going to be a gausian simple is going to be a gausian that gausian will have as mean the that gausian will have as mean the previous mean plus the mean that previous mean plus the mean that results in from the movement that is results in from the movement that is fairly deterministic given our kinematic fairly deterministic given our kinematic model obviously and we have this", "image_path": "img_data/video_39_chunk_20.jpg"}
{"video": "video_39", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "fairly deterministic given our kinematic model obviously and we have this model obviously and we have this variance the initial kind of variance the initial kind of variance plus some kind of transition or movement plus some kind of transition or movement variance that is what we will be variance that is what we will be looking now to see how we can looking now to see how we can actually generalize these equations how actually generalize these equations how we can actually bring together the we can actually bring together the two equations and write down the two equations and write down the onedimensional calman filter in python onedimensional calman filter in python and in this part of the notebook we have and in this part of the notebook we have the complete single dimensional the complete single dimensional calman filter at least as we see it calman filter at least as we see it here in the up to this point here in the up to this point we have some kind of a what is", "image_path": "img_data/video_39_chunk_21.jpg"}
{"video": "video_39", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "here in the up to this point we have some kind of a what is we have some kind of a what is called here process variance this called here process variance this process variance is associated with what process variance is associated with what we called earlier the sigma transition we called earlier the sigma transition squared our transition variance we have squared our transition variance we have a variance which associate with a variance which associate with measurements that was the sigma z squ measurements that was the sigma z squ that we've seen earlier and we have that we've seen earlier and we have an information about the position x of an information about the position x of the u of the agent over here which is the u of the agent over here which is zero mean and has some kind of a", "image_path": "img_data/video_39_chunk_22.jpg"}
{"video": "video_39", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "the u of the agent over here which is zero mean and has some kind of a zero mean and has some kind of a variance itself this is 20 to the variance itself this is 20 to the part of two and we have a velocity we part of two and we have a velocity we assuming we know the velocity this assuming we know the velocity this velocity is our vx that we called velocity is our vx that we called earlier and we have a delta t which is earlier and we have a delta t which is one in this case and we have a process one in this case and we have a process model is our transition model process model is our transition model another way to quote a model another way to quote a transition a state transition model is transition a state transition model is also called process model in the also called process model in the literature which is evidently", "image_path": "img_data/video_39_chunk_23.jpg"}
{"video": "video_39", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "also called process model in the literature which is evidently literature which is evidently gaan that has the sigma square gaan that has the sigma square transition over there and the transition over there and the velocity times the delta t velocity times the delta t this is what we will be we had this is what we will be we had over here as well we had a process over here as well we had a process model that included the transition model that included the transition plus this component over here and the plus this component over here and the mean was we added the transition mean was we added the transition mean the change and then we added mean the change and then we added a transition to the previous kind of", "image_path": "img_data/video_39_chunk_24.jpg"}
{"video": "video_39", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "mean the change and then we added a transition to the previous kind of a transition to the previous kind of belief hat and if i go back belief hat and if i go back effectively now we are having a effectively now we are having a loop where we are moving if you loop where we are moving if you this drone or u the agent this drone or u the agent this starting from some u initial starting from some u initial location that is the xzero and having location that is the xzero and having some u the velocity and some", "image_path": "img_data/video_39_chunk_25.jpg"}
{"video": "video_39", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "location that is the xzero and having some u the velocity and some u the velocity and some measurement kind of update and then measurement kind of update and then we are just going to create a list of we are just going to create a list of measurements here we have 10 measurements here we have 10 measurements where we actually move measurements where we actually move the agent and sense the and get a the agent and sense the and get a measurement vector for each measurement vector for each movement we get a measurement and movement we get a measurement and that is basically the list of that is basically the list of measurements the stream of measurements the stream of measurements are actually coming into our direction are actually coming into our direction and this will actually be now the and this will actually be now the calman filter has", "image_path": "img_data/video_39_chunk_26.jpg"}
{"video": "video_39", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "and this will actually be now the calman filter has calman filter the calman filter has the two equations the predict equation the two equations the predict equation that starts with an location that's that starts with an location that's the belief of the previous state xt the belief of the previous state xt minus one and the a process model or minus one and the a process model or the transition model which is another the transition model which is another gausian with mean vx time delta t plus gausian with mean vx time delta t plus and varian sigma transition squared and varian sigma transition squared then we have the likelihood then we have the likelihood which is of course gausian", "image_path": "img_data/video_39_chunk_27.jpg"}
{"video": "video_39", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "then we have the likelihood which is of course gausian which is of course gausian also the this is the belief cut also the this is the belief cut by the way the prior we get the by the way the prior we get the likelihood which is the measurements likelihood which is the measurements which are gausian with some mean the which are gausian with some mean the mean of the mean is the z's that are coming the z's that are coming the z is from the list of zs's that we the z is from the list of zs's that we have produced earlier have produced earlier and we also have some error", "image_path": "img_data/video_39_chunk_28.jpg"}
{"video": "video_39", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "have produced earlier and we also have some error and we also have some error variance sigma z squ and that is variance sigma z squ and that is going to be the gausian model and that going to be the gausian model and that is going to be what's called the lik is going to be what's called the lik p of z given xt p of zt given xc that we wrote xt p of zt given xc that we wrote earlier and then given the fact we have earlier and then given the fact we have prior and likelihood we are ready to do prior and likelihood we are ready to do the update and we are also ready to the update and we are also ready to then after this update we have a then after this update we have a belief for the next state for the", "image_path": "img_data/video_39_chunk_29.jpg"}
{"video": "video_39", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "then after this update we have a belief for the next state for the belief for the next state for the next time instant which is xt in this next time instant which is xt in this case all right and then this is the case all right and then this is the how the filter will f function the how the filter will f function the predict step from x with the predict step from x with the corresponding if you a variance corresponding if you a variance after the measurement is received after the measurement is received how the next state and variance how the next state and variance are determined notice that they are determined notice that they increased in the variance of the initial increased in the variance of the initial location of the drone is due to the fact", "image_path": "img_data/video_39_chunk_30.jpg"}
{"video": "video_39", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "increased in the variance of the initial location of the drone is due to the fact location of the drone is due to the fact that we effectively say hey we have that we effectively say hey we have we do not know exactly where the drone we do not know exactly where the drone is and that's why we're trying to find is and that's why we're trying to find out we are positioning a gaan which out we are positioning a gaan which is really very flat and over the is really very flat and over the whole domain of ais of all possible whole domain of ais of all possible locations and then look at the dramatic locations and then look at the dramatic decrease of that variance even in the decrease of that variance even in the first iteration of the filter given the first iteration of the filter given the fact that we have now a measurement fact that we have now a measurement which is actually coming in and it's which is actually coming in and it's telling us something telling us something and this actually goes back to what", "image_path": "img_data/video_39_chunk_31.jpg"}
{"video": "video_39", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "telling us something and this actually goes back to what and this actually goes back to what we have seen earlier about the sort we have seen earlier about the sort of calman gain and the gain in that kind of calman gain and the gain in that kind of recursive equation of the update of recursive equation of the update of the location of the statistics of the location of the statistics of the location the maximum likelihood update location the maximum likelihood update that is where the measurement that is where the measurement initially takes a kind of a prime role initially takes a kind of a prime role in the update of the state and as the in the update of the state and as the state estimates are becoming peier state estimates are becoming peier and peier than the measurement takes", "image_path": "img_data/video_39_chunk_32.jpg"}
{"video": "video_39", "start": "0:16:30", "end": "0:16:52.533333", "timestamp": "0:16:30 - 0:16:52.533333", "text": "state estimates are becoming peier and peier than the measurement takes and peier than the measurement takes increasingly reduced kind of role and increasingly reduced kind of role and finally after 10 iterations we have finally after 10 iterations we have an estimate of our location and also an estimate of our location and also the authors over here produce the actual the authors over here produce the actual location since we are doing a location since we are doing a simulation here we always know the true simulation here we always know the true location of that kind of drone and location of that kind of drone and they're pretty close to each other", "image_path": "img_data/video_39_chunk_33.jpg"}
{"video": "video_40", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "i want to start the discussion about the continuous random state as the continuous random state as well also measurements with giving well also measurements with giving you a kind of a use case over here we you a kind of a use case over here we meet our kind of a another structure meet our kind of a another structure another radar structure remember that another radar structure remember that this structure we also saw in the binary this structure we also saw in the binary kind of classification problem we kind of classification problem we have if you a radar tower with a kind of a l panel array", "image_path": "img_data/video_40_chunk_0.jpg"}
{"video": "video_40", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "have if you a radar tower with a kind of a l panel array tower with a kind of a l panel array over here antenna that it is sort of over here antenna that it is sort of whose job is really to whose job is really to estimate the location of a estimate the location of a drone let me just represent a drone with drone let me just represent a drone with this kind of symbol over here and with this kind of symbol over here and out of the obviously we live in a kind of a obviously we live in a kind of a three-dimensional in reality coordinate three-dimensional in reality coordinate system where we have let's say the x system where we have let's say the x axis over here the z- axxis coming out of this here the z- axxis coming out of this kind of pce on the y ais we are going", "image_path": "img_data/video_40_chunk_1.jpg"}
{"video": "video_40", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "here the z- axxis coming out of this kind of pce on the y ais we are going kind of pce on the y ais we are going to just limit the discussion here in to just limit the discussion here in one dimension and evidently at any one dimension and evidently at any point in time the location of the drone point in time the location of the drone is of course given by some kinematic is of course given by some kinematic equations let me write the first one equations let me write the first one let's say x0 plus some let's say x0 plus some vx the projection of the velocity vx the projection of the velocity component in the x-axis", "image_path": "img_data/video_40_chunk_2.jpg"}
{"video": "video_40", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "vx the projection of the velocity component in the x-axis component in the x-axis time delta t and i'll explain what time delta t and i'll explain what delta t is+ 1 / 2 a x delta t^ delta t is+ 1 / 2 a x delta t^ 2 the delta t is the time that it delta t is the time that it takes for the antenna array to rotate takes for the antenna array to rotate 360° the radars have this kind of 360° the radars have this kind of rotating antennas either electronically rotating antennas either electronically rotated or mechanically kind of rotated", "image_path": "img_data/video_40_chunk_3.jpg"}
{"video": "video_40", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "rotating antennas either electronically rotated or mechanically kind of rotated or mechanically kind of rotated in the older kind of versions of them in the older kind of versions of them but they are sort of trying to get 360 but they are sort of trying to get 360 degree coverage around them to degree coverage around them to detect the incoming let's say drones and detect the incoming let's say drones and this is basically the delta t every this is basically the delta t every delta t we will actually be getting a delta t we will actually be getting a measurement and let's assume now measurement and let's assume now that we also write down the y and the z that we also write down the y and the z i'm not going to write the this kind of i'm not going to write the this kind of equations a x is the", "image_path": "img_data/video_40_chunk_4.jpg"}
{"video": "video_40", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "i'm not going to write the this kind of equations a x is the equations a x is the acceleration and the acceleration and the acceleration component at the x-axis it's a kind component at the x-axis it's a kind of a very simple kinematic kind of model of a very simple kinematic kind of model that defines where the that defines where the location of the drone will be given some location of the drone will be given some velocities and some accelerations velocities and some accelerations some initial location all right some initial location all right the question that i actually we the question that i actually we are posing over here is the are posing over here is the following what is the optimal way", "image_path": "img_data/video_40_chunk_5.jpg"}
{"video": "video_40", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "are posing over here is the following what is the optimal way to estimate xt and this is the at every moment in time we have an the at every moment in time we have an xt location when we receive", "image_path": "img_data/video_40_chunk_6.jpg"}
{"video": "video_40", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "xt location when we receive sequential sequentially measurements z i will call these measurements zt that is the question zt that is the question all right if we am to assume some kind of", "image_path": "img_data/video_40_chunk_7.jpg"}
{"video": "video_40", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "all right if we am to assume some kind of if we am to assume some kind of u the measurements which are u the measurements which are effectively going to be given by an effectively going to be given by an additive model where the measurement additive model where the measurement is going to be definitely depend on the is going to be definitely depend on the location of the drone the true location of the drone the true location of the drone xt plus some location of the drone xt plus some epsilon some error u that governs if you epsilon some error u that governs if you the random variable which is the random variable which is indicates the measurement this is indicates the measurement this is effectively my measurement", "image_path": "img_data/video_40_chunk_8.jpg"}
{"video": "video_40", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "indicates the measurement this is effectively my measurement model and my epsilon is going to be epsilon is going to be normal let's say with zero mean and some normal let's say with zero mean and some kind of variance that is my error and i variance that is my error and i think if you kind of represent what is going to you kind of represent what is going to happen in terms of a kind of estim mator", "image_path": "img_data/video_40_chunk_9.jpg"}
{"video": "video_40", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "you kind of represent what is going to happen in terms of a kind of estim mator happen in terms of a kind of estim mator that we want to develop we over time we may be starting although the true starting although the true location of the drone is around xt we location of the drone is around xt we may be starting at every sort of square over here every sort of square over here represents delta t this is delta t represents delta t this is delta t over here this is 2 delta t and on over here this is 2 delta t and on this is in general t * delta t", "image_path": "img_data/video_40_chunk_10.jpg"}
{"video": "video_40", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "this is in general t * delta t maybe starting from some maybe starting from some initial kind of estimate of that initial kind of estimate of that kind of location and gradually as the kind of location and gradually as the measurements are actually coming in we measurements are actually coming in we will be converging into some around will be converging into some around the true value that is what's the true value that is what's supposed to be happening in this kind of supposed to be happening in this kind of estimation approach all right estimation approach all right we will also be assuming for the we will also be assuming for the purposes of avoiding to involve the purposes of avoiding to involve the kinematic kind of model that right now", "image_path": "img_data/video_40_chunk_11.jpg"}
{"video": "video_40", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "purposes of avoiding to involve the kinematic kind of model that right now kinematic kind of model that right now the drone is modeling is sorry is hovering and there's no wind and the xt is at this it's going to be constant it's going to be constant we are going to be looking at this kind", "image_path": "img_data/video_40_chunk_12.jpg"}
{"video": "video_40", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "it's going to be constant we are going to be looking at this kind we are going to be looking at this kind of stationary kind of model to make of stationary kind of model to make the discussion a bit simpler and also we the discussion a bit simpler and also we make the assumption let me also write make the assumption let me also write the other assumption is that the other assumption is that every delta t we get a measurement we get a measurement now if you remember that video now if you remember that video in the kind of previous video in the kind of previous video when we were looking at the u the", "image_path": "img_data/video_40_chunk_13.jpg"}
{"video": "video_40", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "in the kind of previous video when we were looking at the u the when we were looking at the u the maximum l estimation about gausian maximum l estimation about gausian random variables and i hope you random variables and i hope you recognize that this zt is a gaus and recognize that this zt is a gaus and random variable because we are adding a random variable because we are adding a constant as per the assumption over constant as per the assumption over here into a gausian kind of error signal here into a gausian kind of error signal we actually have wrote down the we actually have wrote down the maximum likelihood estimate of the maximum likelihood estimate of the gausian random variables and the gausian random variables and we had two estimat of the gausian", "image_path": "img_data/video_40_chunk_14.jpg"}
{"video": "video_40", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "the gausian random variables and we had two estimat of the gausian we had two estimat of the gausian parameter we have a probability parameter we have a probability distribution that governs now zt and distribution that governs now zt and this probability distribution has two this probability distribution has two parameters the mean and the variance and parameters the mean and the variance and right now i'm actually interesting to right now i'm actually interesting to write down the equation about the write down the equation about the maximum likelihood estimate the best maximum likelihood estimate the best according to that maximum lik criterion according to that maximum lik criterion estimate of xt is 1 /t the number of is 1 /t the number of samples that we have received at", "image_path": "img_data/video_40_chunk_15.jpg"}
{"video": "video_40", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "is 1 /t the number of samples that we have received at samples that we have received at this moment in time t this is goes this moment in time t this is goes from let's say i is equal to 1 to from let's say i is equal to 1 to d and then we have z the location effectively that we are interested in to find is really we are interested in to find is really the mean of this gausian probability the mean of this gausian probability distribution because you're adding a distribution because you're adding a zero mean component into a con that", "image_path": "img_data/video_40_chunk_16.jpg"}
{"video": "video_40", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "distribution because you're adding a zero mean component into a con that zero mean component into a con that constant goes and takes the place of the constant goes and takes the place of the mean of the z of the mean of the z of the of this variable z and this is the this variable z and this is the this is from this formula over here this is from this formula over here is from maximum likelihood from maximum likelihood estimate of the mean of a gausian", "image_path": "img_data/video_40_chunk_17.jpg"}
{"video": "video_40", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "mean of a gausian random variable and for those who are kind of forgotten the derivation who are kind of forgotten the derivation over there it's i think they over there it's i think they can review the -call marginal maximum can review the -call marginal maximum laud video to recall that kind of laud video to recall that kind of formula all right we have the u formula all right we have the u this formula we have now the this formula we have now the estimate let me rewrite this as one estimate let me rewrite this as one overt i'll break down the summation into overt i'll break down the summation into two parts the summation from i is = to 1 to", "image_path": "img_data/video_40_chunk_18.jpg"}
{"video": "video_40", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "two parts the summation from i is = to 1 to parts the summation from i is = to 1 to t minus 1 and z i + t minus 1 and z i + zt and then now i'm going to write zt and then now i'm going to write the 1 / the 1 / t summation from i = to 1 to t summation from i = to 1 to t minus1 z i + 1 / t zt and i'm going to write the first zt and i'm going to write the first one as 1 / t i'm going to m multiply with as 1 / t i'm going to m multiply with tus1 and divide with t minus1 the", "image_path": "img_data/video_40_chunk_19.jpg"}
{"video": "video_40", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "as 1 / t i'm going to m multiply with tus1 and divide with t minus1 the tus1 and divide with t minus1 the summation from i = 1 to summation from i = 1 to tus1 z plus 1 / t zt it is t minus1 / t now this whole it is t minus1 / t now this whole thing over here is exactly the ml when i have t minus one", "image_path": "img_data/video_40_chunk_20.jpg"}
{"video": "video_40", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "ml when i have t minus one samples + 1 / samples + 1 / t zt and therefore this becomes 1 - 1 / t zt and therefore this becomes 1 - 1 / t that is this xt minus1 mle of course stands for maximum l estim estimate plus 1 / t estim estimate plus 1 / t ct and the i'm actually going to write", "image_path": "img_data/video_40_chunk_21.jpg"}
{"video": "video_40", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "estim estimate plus 1 / t ct and the i'm actually going to write ct and the i'm actually going to write now the final form of now the final form of this which is xt mle is equal xt minus one mle plus i will write this capital mle plus i will write this capital kt i will write this factor capital k kt i will write this factor capital k t and actually i'm going to t and actually i'm going to write capital kt yeah this factor and write capital kt yeah this factor and then i'm going to write it as", "image_path": "img_data/video_40_chunk_22.jpg"}
{"video": "video_40", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "write capital kt yeah this factor and then i'm going to write it as then i'm going to write it as zt minus xt minus1 ml where capital t capital kt i will call it later the socalled calman gain", "image_path": "img_data/video_40_chunk_23.jpg"}
{"video": "video_40", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "kt i will call it later the socalled calman gain this obiously does not have something to do with recursive status has do with recursive status has something to do with recursive state something to do with recursive state estimation but right now we are just estimation but right now we are just looking at the maximum likelihood looking at the maximum likelihood estimate that we can actually form from estimate that we can actually form from -called streaming data streaming -called streaming data streaming measurements are actually coming in one measurements are actually coming in one after another the formula kind of after another the formula kind of says something this the current says something this the current estimate", "image_path": "img_data/video_40_chunk_24.jpg"}
{"video": "video_40", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "says something this the current estimate is the previous estimate plus a gain factor of the difference between the current measurement minus the previous", "image_path": "img_data/video_40_chunk_25.jpg"}
{"video": "video_40", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "measurement minus the previous estimate this one is sometimes called in the literature innovation the literature innovation or that indicates the new thing that we or that indicates the new thing that we have learned from the measurement that have learned from the measurement that came in or residual and this formula will actually be kind of a instructive mind you that be kind of a instructive mind you that this kt is going to be a factor", "image_path": "img_data/video_40_chunk_26.jpg"}
{"video": "video_40", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "be kind of a instructive mind you that this kt is going to be a factor this kt is going to be a factor between 0 and 1 and the and this formula is actually going to and this formula is actually going to give us the ability to update our give us the ability to update our estimate given if you the current estimate given if you the current measurement that actually came in and measurement that actually came in and the previous estimate notice also that the previous estimate notice also that in this kind of discussion we had in this kind of discussion we had this definition about the -call this definition about the -call gain in this kind of formula but later gain in this kind of formula but later will become the calman gain and this will become the calman gain and this is definitely a function of t", "image_path": "img_data/video_40_chunk_27.jpg"}
{"video": "video_40", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "will become the calman gain and this is definitely a function of t is definitely a function of t initially the u gain is one and initially the u gain is one and then as the time goes by the gain is then as the time goes by the gain is reduces and which means that initially reduces and which means that initially the gain is putting quite a lot of the gain is putting quite a lot of emphasis in the u in this kind of emphasis in the u in this kind of difference and then this difference and then this difference the -called information comes out the -called information comes out of measurements in other words is of measurements in other words is amplified initially and then kind of is amplified initially and then kind of is staling off some something some behavior", "image_path": "img_data/video_40_chunk_28.jpg"}
{"video": "video_40", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "amplified initially and then kind of is staling off some something some behavior staling off some something some behavior this we'll see explicitly in the this we'll see explicitly in the calman filter equations that are to calman filter equations that are to follow up to now we have not follow up to now we have not really discussed anything about really discussed anything about recursive state estimation the only recursive state estimation the only thing as we have done it earlier the thing as we have done it earlier the base filter and we have not talked about base filter and we have not talked about the base filter at all and or anything the base filter at all and or anything basian treatment at all because we basian treatment at all because we have not really mention anything about have not really mention anything about the prior what we will do next is the prior what we will do next is we will introduce if you this", "image_path": "img_data/video_40_chunk_29.jpg"}
{"video": "video_40", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "the prior what we will do next is we will introduce if you this we will introduce if you this prior and start writing the prior and start writing the one of the two equations and i'm going one of the two equations and i'm going to start with the second equation of the to start with the second equation of the base filter we will assume let's assume that the prior", "image_path": "img_data/video_40_chunk_30.jpg"}
{"video": "video_40", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "that the prior over the location let me call this p of xt is given by a normal distribution of let's say we use the distribution of let's say we use the notation before x0 given mu 0 and sigma 0 squ there is some prior that information we have is some prior that information we have about the location if you of the", "image_path": "img_data/video_40_chunk_31.jpg"}
{"video": "video_40", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "is some prior that information we have about the location if you of the about the location if you of the drone and if i start of the drone and if i start with the second really equation the -call measurement update measurement", "image_path": "img_data/video_40_chunk_32.jpg"}
{"video": "video_40", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "measurement update we will definitely can write it again b the belief for the location of again b the belief for the location of the drone xt is ita times the probability of zt given ita times the probability of zt given xt times the belief hat of xt and nothing changes with to what xt and nothing changes with to what we have seen earlier in the discrete", "image_path": "img_data/video_40_chunk_33.jpg"}
{"video": "video_40", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "xt and nothing changes with to what we have seen earlier in the discrete we have seen earlier in the discrete kind of case and the only thing that kind of case and the only thing that we are going to be assuming over here is we are going to be assuming over here is that the prior is gausian the that the prior is gausian the belief hat of xt is gausian and the probability that measure mement the probability that measure mement zt given", "image_path": "img_data/video_40_chunk_34.jpg"}
{"video": "video_40", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "the probability that measure mement zt given xt is also gausian and we saw how this kind of gaussianity for the measurement kind of gaussianity for the measurement kind of led to writing the maximum l of estimate led to writing the maximum l of estimate as an expression over there and as an expression over there and then how that maximum la estimate can then how that maximum la estimate can be recursively kind of updated be recursively kind of updated itself then the posterior", "image_path": "img_data/video_40_chunk_35.jpg"}
{"video": "video_40", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "be recursively kind of updated itself then the posterior itself then the posterior here these two are gazan see both of here these two are gazan see both of these and this are gazan these and this are gazan then the posterior after the measurement update is also gausian", "image_path": "img_data/video_40_chunk_36.jpg"}
{"video": "video_40", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "update is also gausian and we'll see now that's actually the kind of important that's actually the kind of important result we'll see now the curves how result we'll see now the curves how they look in a kind of a notebook kind they look in a kind of a notebook kind of setting and just understand how of setting and just understand how the difference between the how far away the difference between the how far away let's say the prior information which is let's say the prior information which is the belief hat over there and the belief hat over there and the measurement the belief hat is what measurement the belief hat is what was the state of affairs after that", "image_path": "img_data/video_40_chunk_37.jpg"}
{"video": "video_40", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "was the state of affairs after that the state of affairs after that kinematic movement we have not really kinematic movement we have not really talked about kinematic movement because talked about kinematic movement because we started with the second we started with the second kind of equation but that was basically kind of equation but that was basically what was predicted to be our state after what was predicted to be our state after that kind of kinematic movement that kind of kinematic movement and then that's gausian and this and then that's gausian and this will actually also be assumed to be will actually also be assumed to be gausian we will see now how the gausian we will see now how the difference between the two gausian difference between the two gausian probability distribution affect probability distribution affect the posterior we have some kind of", "image_path": "img_data/video_40_chunk_38.jpg"}
{"video": "video_40", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "probability distribution affect the posterior we have some kind of the posterior we have some kind of visualization because it's not entirely visualization because it's not entirely obvious and the second thing we need to obvious and the second thing we need to come up with and given the fact that come up with and given the fact that this one is also gausian then we need to this one is also gausian then we need to write down the expression about the mean write down the expression about the mean and the variance of that gausian and the variance of that gausian distribution which governs the posterior distribution which governs the posterior the belief of our state as it the belief of our state as it turns out this is analytically turns out this is analytically tractable because of the gausian the tractable because of the gausian the assumption and that what we'll do next", "image_path": "img_data/video_40_chunk_39.jpg"}
{"video": "video_40", "start": "0:20:00", "end": "0:20:00.033333", "timestamp": "0:20:00 - 0:20:00.033333", "text": "tractable because of the gausian the assumption and that what we'll do next", "image_path": "img_data/video_40_chunk_40.jpg"}
{"video": "video_41", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we are going to see now the u an example of what we call a the u an example of what we call a discrete base filter everything here discrete base filter everything here it is discrete all the states are it is discrete all the states are discrete and the measurements are discrete and the measurements are also discrete and the set of also discrete and the set of factions are also obviously discrete factions are also obviously discrete and the state and the scene is the following we have a scene is the following we have a robot the yellow robot that you see robot the yellow robot that you see over there it's equipped with a over there it's equipped with a proximity sensor", "image_path": "img_data/video_41_chunk_0.jpg"}
{"video": "video_41", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "over there it's equipped with a proximity sensor where and effectively the job here is where and effectively the job here is to estimate the state of the door to estimate the state of the door whether the door is open or closed whether the door is open or closed we have a binary state in this case we have a binary state in this case and as we will starting and we and as we will starting and we will start from a situation where we will start from a situation where we have absolutely no knowledge about the have absolutely no knowledge about the state of the door and the state of the door and the state of the door is discussed here as we the door is discussed here as we said in an earlier with a belief is said in an earlier with a belief is denoted here with this belief or of s0 0", "image_path": "img_data/video_41_chunk_1.jpg"}
{"video": "video_41", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "said in an earlier with a belief is denoted here with this belief or of s0 0 denoted here with this belief or of s0 0 is the initial state t is equal to z is the initial state t is equal to z in this case and the state as we said in this case and the state as we said could take two values open or close could take two values open or close therefore we are each of these therefore we are each of these posterior probabilities is one is 1/2 posterior probabilities is one is 1/2 that's basically the starting that's basically the starting kind of point and we will kind of point and we will be using if you the sort of a be using if you the sort of a formula for the general kind of base formula for the general kind of base filter that we have seen kind of earlier", "image_path": "img_data/video_41_chunk_2.jpg"}
{"video": "video_41", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "formula for the general kind of base filter that we have seen kind of earlier filter that we have seen kind of earlier this formula is u going to this formula is u going to be applied now first and actually be applied now first and actually this is basically what we actually wrote this is basically what we actually wrote we are going to have a for loop we are going to have a for loop the in this kind of for loop we're the in this kind of for loop we're going to have the first step over here going to have the first step over here let's see that how it is let's see that how it is done now before we apply this kind of", "image_path": "img_data/video_41_chunk_3.jpg"}
{"video": "video_41", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "now before we apply this kind of equations in the base filter we equations in the base filter we have to assume and to specify the two have to assume and to specify the two probabilistic models that we had the probabilistic models that we had the generative models we have mentioned generative models we have mentioned that we need the first is the that we need the first is the measurement model and the second is the measurement model and the second is the transition model in the measurement model we have in the measurement model we have translated from english sentences in translated from english sentences in this kind of table to the probability", "image_path": "img_data/video_41_chunk_4.jpg"}
{"video": "video_41", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "translated from english sentences in this kind of table to the probability this kind of table to the probability distribution to a probability distribution to a probability distribution that you see here the distribution that you see here the first line is that if it's open the first line is that if it's open the agent can sense can says that it is agent can sense can says that it is indeed open with a probability indeed open with a probability 60% that's my conversion to the 60% that's my conversion to the probability if it is open the agent probability if it is open the agent can sense it as close with a probability can sense it as close with a probability 40% if it is closed the agent will 40% if it is closed the agent will sense it as open with pro probility 20%", "image_path": "img_data/video_41_chunk_5.jpg"}
{"video": "video_41", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "40% if it is closed the agent will sense it as open with pro probility 20% sense it as open with pro probility 20% and if it is closed the agent can sense and if it is closed the agent can sense it as such with a probability of 80% it as such with a probability of 80% all of these we have two all of these we have two possible states that are either open or states that are either open or close and we have two possible close and we have two possible sense senses again a binary sense senses again a binary measurement and this in this case we measurement and this in this case we have four possibilities that we specify have four possibilities that we specify in this distribution now in this distribution now you're going to ask", "image_path": "img_data/video_41_chunk_6.jpg"}
{"video": "video_41", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "in this distribution now you're going to ask where are these probabilities came from where are these probabilities came from why is 0.6 and not 0.7 and on and the why is 0.6 and not 0.7 and on and the answer to that is that these are answer to that is that these are basically a simple assumption in basically a simple assumption in order for us to make a case and order for us to make a case and understand how the discrete base understand how the discrete base filter is implemented and most of the filter is implemented and most of the same kind of information will be the same kind of information will be obtained from the data sheet of obtained from the data sheet of sensor ventor and that will actually sensor ventor and that will actually be u enough for us to be able to be u enough for us to be able to quantify in that kind of case equivalent", "image_path": "img_data/video_41_chunk_7.jpg"}
{"video": "video_41", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "be u enough for us to be able to quantify in that kind of case equivalent quantify in that kind of case equivalent probability distributions based on probability distributions based on the histograms in terms of errors and the histograms in terms of errors and things that they are reporting things that they are reporting typically in those data sheets typically in those data sheets that's basically the measurement that's basically the measurement model and now let's transition now to model and now let's transition now to the transition model and in the to the transition model and in the transition model we will see now two transition model we will see now two variables which are being conditioned variables which are being conditioned and because of that we will expect to and because of that we will expect to see a much longer kind of table", "image_path": "img_data/video_41_chunk_8.jpg"}
{"video": "video_41", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "and because of that we will expect to see a much longer kind of table see a much longer kind of table we see the transition is going we see the transition is going from it requires a specification from it requires a specification of an action it is the earlier state of an action it is the earlier state an action of open an action of push an action of open an action of push and a subsequent kind of state and these and a subsequent kind of state and these are all the possibilities i am not going to go possibilities i am not going to go through line by line through the table through line by line through the table but you can actually see how we are but you can actually see how we are actually specifying this that we need actually specifying this that we need to specify all these probabilities and", "image_path": "img_data/video_41_chunk_9.jpg"}
{"video": "video_41", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "actually specifying this that we need to specify all these probabilities and to specify all these probabilities and we have of course eight length eight kind of table eight length eight kind of table because we have 2 the^ of two because we have 2 the^ of two and times 2 the^ of two is and times 2 the^ of two is coming from the presence of the two coming from the presence of the two conditioning variables and another conditioning variables and another two is because of the two possible two is because of the two possible states of the starting state of the states of the starting state of the sorry of the ending state we sorry of the ending state we have that basically an the", "image_path": "img_data/video_41_chunk_10.jpg"}
{"video": "video_41", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "sorry of the ending state we have that basically an the have that basically an the transition kind of model and we will now transition kind of model and we will now use are we ready now to write the base use are we ready now to write the base filter and apply it we will be filter and apply it we will be applying it at district time applying it at district time instances we were in time instance instances we were in time instance equal to zero where the whole process equal to zero where the whole process kind of start and now we are starting kind of start and now we are starting with specifying what will happen with specifying what will happen at time instant t is equal to one a at time instant t is equal to one a time instant t is equal to one the agent time instant t is equal to one the agent takes no action and senses the door", "image_path": "img_data/video_41_chunk_11.jpg"}
{"video": "video_41", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "time instant t is equal to one the agent takes no action and senses the door takes no action and senses the door as open we have an a and we have as open we have an a and we have a z and we are now on applying the z and we are now on applying the first step in that base filter first step in that base filter sort of python sudo python kind of sort of python sudo python kind of code that we wrote earlier the first code that we wrote earlier the first is a line is the prediction step is a line is the prediction step and here we are summing over all and here we are summing over all possible states", "image_path": "img_data/video_41_chunk_12.jpg"}
{"video": "video_41", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "and here we are summing over all possible states st minus one we are in s0 st minus one we are in s0 all over all possible states as all over all possible states as zero and this will be the u remaining zero and this will be the u remaining kind of terms associated with the some kind of terms associated with the some rule the conditioning times the rule the conditioning times the belief the posterior of the belief the posterior of the state s z remember that the posterior of state s z remember that the posterior of the state s z was actually given to us the state s z was actually given to us and it was", "image_path": "img_data/video_41_chunk_13.jpg"}
{"video": "video_41", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "the state s z was actually given to us and it was half for both of the possible half for both of the possible situations both of s z is closed and situations both of s z is closed and open we have a binary state we're open we have a binary state we're expecting to see in this summation two expecting to see in this summation two terms that's the first term and that's terms that's the first term and that's the second term and no surprises there the second term and no surprises there if we specify the transition model as if we specify the transition model as it is given for the stateus zero open it is given for the stateus zero open and if we specify the transition and if we specify the transition model for when it is closed", "image_path": "img_data/video_41_chunk_14.jpg"}
{"video": "video_41", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "and if we specify the transition model for when it is closed model for when it is closed then we are going to estimate a then we are going to estimate a belief hatut that the next state our belief hatut that the next state our state is open which is again 0.5 and this open which is again 0.5 and this should not be a surprise because should not be a surprise because the door will remain in because the door will remain in its itemps of uncertainty we are still its itemps of uncertainty we are still not sure about the state of the door not sure about the state of the door because we took no action", "image_path": "img_data/video_41_chunk_15.jpg"}
{"video": "video_41", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "not sure about the state of the door because we took no action because we took no action should intuitively result in no should intuitively result in no update of the belief that we have update of the belief that we have about the state we're still at one about the state we're still at one half the recursion the step two now the recursion the step two now comes the step two involves the comes the step two involves the measurement update and remember that the measurement update and remember that the measurement says that the door is open measurement says that the door is open and based on the earlier and based on the earlier discussion we're expecting to see some discussion we're expecting to see some reduction in the uncertain", "image_path": "img_data/video_41_chunk_16.jpg"}
{"video": "video_41", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "discussion we're expecting to see some reduction in the uncertain reduction in the uncertain let's see if that happens we let's see if that happens we believe that here is the general believe that here is the general equation about the posterior belief equation about the posterior belief about the state s1 evidently will about the state s1 evidently will still be binary either open or closed still be binary either open or closed and this will be the general equation and this will be the general equation and we applied for both options of and we applied for both options of the status one both open and closed and the status one both open and closed and in that case we have the normalizing in that case we have the normalizing factor ita we will leave it as such and factor ita we will leave it as such and here we have the this part is", "image_path": "img_data/video_41_chunk_17.jpg"}
{"video": "video_41", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "factor ita we will leave it as such and here we have the this part is here we have the this part is coming from the measurement model which coming from the measurement model which we'll look up from the lookup table we'll look up from the lookup table we have specified earlier and obviously the have specified earlier and obviously the belief cut of where we believe we were belief cut of where we believe we were given the action we have taken the given the action we have taken the inaction we have taken effectively inaction we have taken effectively earlier that is the one that we will plug in that is the one that we will plug in over here all right mind you that over here all right mind you that over here we need to also write the over here we need to also write the belief c for s1 is equal to we clos belief c for s1 is equal to we clos and this is again 0.5 that's the", "image_path": "img_data/video_41_chunk_18.jpg"}
{"video": "video_41", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "belief c for s1 is equal to we clos and this is again 0.5 that's the and this is again 0.5 that's the reason why i didn't write it over here reason why i didn't write it over here all right we are replacing the two all right we are replacing the two 0.6 from the lookup table time 0.6 from the lookup table time 0.5 is ita time 0.3 and the posterior for close is ea 0.3 and the posterior for close is ea time 0.2 * 0.5 this there is an 0.2 * 0.5 this there is an ita which is actually missing over ita which is actually missing over here and for these two post iors", "image_path": "img_data/video_41_chunk_19.jpg"}
{"video": "video_41", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "ita which is actually missing over here and for these two post iors here and for these two post iors for these two posteriors obviously for these two posteriors obviously this sorry for these two posterior this sorry for these two posterior probabilities we need to specify at probabilities we need to specify at the end of the day a posterior the end of the day a posterior probability distribution a proper probability distribution a proper distribution and this requires that the distribution and this requires that the summation of these two should be equal summation of these two should be equal to 1.0 based on that to 1.0 based on that requirement we are summing the two requirement we are summing the two the ita * 0.1 plus ita * 0.3 and make the ita * 0.1 plus ita * 0.3 and make it equal to 1.0 and from that equation", "image_path": "img_data/video_41_chunk_20.jpg"}
{"video": "video_41", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "the ita * 0.1 plus ita * 0.3 and make it equal to 1.0 and from that equation it equal to 1.0 and from that equation we are very it's very easy to estimate we are very it's very easy to estimate the ita the normalizing constant of 2.5 the ita the normalizing constant of 2.5 in this case and based on that we can in this case and based on that we can actually go back and update the actually go back and update the posteriors for open or closed we can posteriors for open or closed we can actually see that the moment we have actually see that the moment we have received a measurement that indicates received a measurement that indicates that is open that immediately that is open that immediately improved the certainty or reduce improved the certainty or reduce the uncertainty if you about the uncertainty if you about the state at time is equal to one and", "image_path": "img_data/video_41_chunk_21.jpg"}
{"video": "video_41", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "the uncertainty if you about the state at time is equal to one and state at time is equal to one and this now we have we believe that there this now we have we believe that there is more likely that the door is open is more likely that the door is open now this is the time to pause the video now this is the time to pause the video don't look at the solution and just don't look at the solution and just focus on going back to that equation focus on going back to that equation that we have wrote earlier about the that we have wrote earlier about the basian kind of filter and look at basian kind of filter and look at the do it yourself with your the do it yourself with your own kind of sort of way to come up own kind of sort of way to come up with a estimate of the posterior with a estimate of the posterior probabilities for time is equal to two", "image_path": "img_data/video_41_chunk_22.jpg"}
{"video": "video_41", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "with a estimate of the posterior probabilities for time is equal to two probabilities for time is equal to two and if you can write down what i'm going and if you can write down what i'm going to tell you now that's the input for to tell you now that's the input for your calculation the agent is going to calculation the agent is going to push the door that's the action that push the door that's the action that will take a time is equal to two and will take a time is equal to two and we'll also sense that the door is open we'll also sense that the door is open for this measurements and action for this measurements and action for this measurement and action go ahead for this measurement and action go ahead and calculate the posterior and calculate the posterior probability for the door state you probability for the door state you need to come up with the b belief of s2", "image_path": "img_data/video_41_chunk_23.jpg"}
{"video": "video_41", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "probability for the door state you need to come up with the b belief of s2 need to come up with the b belief of s2 and you obviously you need two values and you obviously you need two values because s2 is a binary you have to because s2 is a binary you have to come up with that and then come back come up with that and then come back here and check that your sort of here and check that your sort of calculations are correct and this calculations are correct and this if you do that this actually also if you do that this actually also clos the discussion about what happens clos the discussion about what happens in the discrete case and the discrete in the discrete case and the discrete case is fairly straightforward in terms case is fairly straightforward in terms of calculations evidently the discrete of calculations evidently the discrete case is the simplest possible discret", "image_path": "img_data/video_41_chunk_24.jpg"}
{"video": "video_41", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "of calculations evidently the discrete case is the simplest possible discret case is the simplest possible discret case that we can ever have where case that we can ever have where everything is kind of binary over here everything is kind of binary over here and then we have a python and then we have a python notebook now that we are going to go notebook now that we are going to go through to understand what through to understand what is happening in a bit more general case is happening in a bit more general case we have nonbinary although if you we have nonbinary although if you understood this kind of section that's understood this kind of section that's kind of plenty and but more importantly kind of plenty and but more importantly we will be dealing next with the we will be dealing next with the continuous dimension this is a continuous dimension this is a single dimensional continous space for", "image_path": "img_data/video_41_chunk_25.jpg"}
{"video": "video_41", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "continuous dimension this is a single dimensional continous space for single dimensional continous space for my state and the measurements and both my state and the measurements and both of these probabbly distributions will of these probabbly distributions will actually be goen instead of being actually be goen instead of being discret let us now look at the notebook discret let us now look at the notebook is u called the discret the notebook is u called the discret base filter section and we are going base filter section and we are going to actually see here an example where to actually see here an example where we have three doors in we have three doors in hallway and this doors at location 0 1 hallway and this doors at location 0 1 and 8 and we would to get an", "image_path": "img_data/video_41_chunk_26.jpg"}
{"video": "video_41", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "hallway and this doors at location 0 1 and 8 and we would to get an and 8 and we would to get an understanding of about where is the understanding of about where is the location of our agent and as we location of our agent and as we actually can see here the notebook is actually can see here the notebook is actually fairly long but i will actually fairly long but i will actually focus here into the most important part focus here into the most important part which is towards the which is towards the middle where we have this middle where we have this discussion about the exactly discussion about the exactly the same similar discussion we had just the same similar discussion we had just in a previous video on the robot in a previous video on the robot standing in front of a door now here the standing in front of a door now here the disc state space is not to the door", "image_path": "img_data/video_41_chunk_27.jpg"}
{"video": "video_41", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "standing in front of a door now here the disc state space is not to the door disc state space is not to the door is not open or closed but in fact we is not open or closed but in fact we have a state which is associated with have a state which is associated with location and in the location space we location and in the location space we have here 10 discrete location values have here 10 discrete location values from 0 to 9 and as you can see we from 0 to 9 and as you can see we start with some effectively start with some effectively claiming that we have a uniform properly claiming that we have a uniform properly the distribution of our belief in a very distribution of our belief in a very similar way as we started also with the similar way as we started also with the unform probability distribution in the", "image_path": "img_data/video_41_chunk_28.jpg"}
{"video": "video_41", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "similar way as we started also with the unform probability distribution in the unform probability distribution in the door example and here we have let's say door example and here we have let's say 1/10th of that kind of probability 1/10th of that kind of probability being in each of these 10 being in each of these 10 states then we are going to be states then we are going to be calculating our posterior u assuming calculating our posterior u assuming that we have no action with that we have no action with respect to the agent and we are respect to the agent and we are going to effectively going to effectively however we do have a measurement and", "image_path": "img_data/video_41_chunk_29.jpg"}
{"video": "video_41", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "going to effectively however we do have a measurement and however we do have a measurement and this measurement is shown actually also this measurement is shown actually also here as z and with a the measurement here as z and with a the measurement is equal to one here indicates that is equal to one here indicates that the agent is in front of the agent is in front of the door and we are going to door and we are going to update with the posterior the update with the posterior the belief that which we call belief that which we call belief earlier and this is what we get", "image_path": "img_data/video_41_chunk_30.jpg"}
{"video": "video_41", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "belief that which we call belief earlier and this is what we get earlier and this is what we get our prior effectively is our belief effectively is our belief hatut and effectively remained the hatut and effectively remained the same as we get just getting started and same as we get just getting started and then the this posterior here the then the this posterior here the posterior plot is our belief in a posterior plot is our belief in a similar way as we had no action but similar way as we had no action but we did had a measurement and t is equal we did had a measurement and t is equal to one case in the door example the to one case in the door example the two the notebook and the robot that we two the notebook and the robot that we have just seen are pretty", "image_path": "img_data/video_41_chunk_31.jpg"}
{"video": "video_41", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "two the notebook and the robot that we have just seen are pretty have just seen are pretty analogous with respect to what they do analogous with respect to what they do here we have the update which here we have the update which is the measurement update is the measurement update for that specific measurement and for that specific measurement and now what we will do is we are going to take a do is we are going to take a step we are going to step we are going to move we are moving from left to move we are moving from left to right starting from some kind right starting from some kind of location and in this location we", "image_path": "img_data/video_41_chunk_32.jpg"}
{"video": "video_41", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "right starting from some kind of location and in this location we kind of location and in this location we are starting from it was an are starting from it was an obviously an unknown location to us but obviously an unknown location to us but we have here the measurement which is we have here the measurement which is actually coming in now is also actually coming in now is also indicating that we are seeing a door indicating that we are seeing a door after we moved we are also after we moved we are also receiving the measurement what receiving the measurement what happens to our belief c and our belief happens to our belief c and our belief well the belief had changed this is", "image_path": "img_data/video_41_chunk_33.jpg"}
{"video": "video_41", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "well the belief had changed this is the belief had changed this is actually shown here on the left which actually shown here on the left which is called prior and we actually we is called prior and we actually we can see that the belief u has moved can see that the belief u has moved from u by one if you compare from u by one if you compare the posterior that we had the posterior that we had earlier and this posterior has become the prior in posterior has become the prior in this kind of iteration this comes as an this kind of iteration this comes as an argument into our base filter and it", "image_path": "img_data/video_41_chunk_34.jpg"}
{"video": "video_41", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "this kind of iteration this comes as an argument into our base filter and it argument into our base filter and it sort of results into this posterior sort of results into this posterior now after this movement and now after this movement and measurement update notice that this is pretty update notice that this is pretty much indicating a very high much indicating a very high possibility that we are in location one possibility that we are in location one there are effectively two three there are effectively two three doors in the system one door is in doors in the system one door is in location zero the other door is in location zero the other door is in location one and the other door is in", "image_path": "img_data/video_41_chunk_35.jpg"}
{"video": "video_41", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "location zero the other door is in location one and the other door is in location one and the other door is in location i believe the location 8 location i believe the location 8 these are the three doors these are the three doors that are present since these two doors are consecutive to since these two doors are consecutive to each other and we got two measurements each other and we got two measurements here and here that are indicated in here and here that are indicated in front of the door then there are only front of the door then there are only two consecutive doors in this kind of two consecutive doors in this kind of hallway there's no ambiguity the hallway there's no ambiguity the posterior distribution is actually posterior distribution is actually indicating that this is the case", "image_path": "img_data/video_41_chunk_36.jpg"}
{"video": "video_41", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "posterior distribution is actually indicating that this is the case indicating that this is the case and the robot is now present in location and the robot is now present in location one with high probability than one with high probability than others the system will others the system will actually continue and update this actually continue and update this we have here the belief cut given that kind of posterior belief cut given that kind of posterior we have yet another movement this is we have yet another movement this is indicated here by the predict the indicated here by the predict step that movement to the", "image_path": "img_data/video_41_chunk_37.jpg"}
{"video": "video_41", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "indicated here by the predict step that movement to the predict step that movement to the right the predict step effectively as right the predict step effectively as we discussed changed the prior the we discussed changed the prior the this is the belief hut and as you can this is the belief hut and as you can see here it introduced also some more see here it introduced also some more sign more uncertainty into the sign more uncertainty into the distribution as if you compare this distribution as if you compare this posterior that we started from in the posterior that we started from in the previous iteration to this after the previous iteration to this after the movement to the right the", "image_path": "img_data/video_41_chunk_38.jpg"}
{"video": "video_41", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "previous iteration to this after the movement to the right the movement to the right the distribution a bit more the distribution a bit more smeared indicating larger uncertainty smeared indicating larger uncertainty and then a measurement arrives and then a measurement arrives indicating that we have not a indicating that we have not a door and then for sure we are door and then for sure we are after door one and therefore the our after door one and therefore the our posterior the our measurement in other posterior the our measurement in other words agrees and reinforces the prior", "image_path": "img_data/video_41_chunk_39.jpg"}
{"video": "video_41", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "words agrees and reinforces the prior reinforces the prior information and we are actually information and we are actually correcting the prior forming the correcting the prior forming the posterior with the likelihood posterior with the likelihood effectively that's the base rule that effectively that's the base rule that we're applying over here and the we're applying over here and the whole thing the previous posterior whole thing the previous posterior becomes after the measurement becomes after the measurement after the prediction step the prior after the prediction step the prior for the measurement for the subsequent for the measurement for the subsequent measurement update and we are actually measurement update and we are actually proceeding as exactly the", "image_path": "img_data/video_41_chunk_40.jpg"}
{"video": "video_41", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "measurement update and we are actually proceeding as exactly the proceeding as exactly the same way and we are u obtaining if you same way and we are u obtaining if you a posterior probability a posterior probability distribution that actually moves distribution that actually moves together as you can see where with the together as you can see where with the location of the agent of the location of the agent of the robot the diagram over here is robot the diagram over here is sort of the previous kind of sort of the previous kind of discussion indicated indicates exactly discussion indicated indicates exactly what is happening kind of a bit more what is happening kind of a bit more visually this is really the function visually this is really the function that implements the discrete base filter", "image_path": "img_data/video_41_chunk_41.jpg"}
{"video": "video_41", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "visually this is really the function that implements the discrete base filter that implements the discrete base filter and you can see here we are starting and you can see here we are starting with a belief hatut and a belief which with a belief hatut and a belief which are empty arrays and with an initial are empty arrays and with an initial belief of one10 for each of the 10 belief of one10 for each of the 10 locations and we are enumerating with a locations and we are enumerating with a for loop to first with a predict for loop to first with a predict step to get our belief cut and append step to get our belief cut and append to the list over here and with the to the list over here and with the update step we are considering in", "image_path": "img_data/video_41_chunk_42.jpg"}
{"video": "video_41", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "to the list over here and with the update step we are considering in update step we are considering in the update step the likelihood which of the update step the likelihood which of course in brings in the measurement and course in brings in the measurement and the belief cut to form the update the belief cut to form the update step as we have also seen step as we have also seen in handwritten form of this base in handwritten form of this base filter earlier and therefore we are filter earlier and therefore we are returning both of them both of the returning both of them both of the priors and the posteriors because we priors and the posteriors because we also would to be able to plot them also would to be able to plot them and sort of create some form of an and sort of create some form of an animation as we are moving this slider", "image_path": "img_data/video_41_chunk_43.jpg"}
{"video": "video_41", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "and sort of create some form of an animation as we are moving this slider animation as we are moving this slider in this kind of notebook we will be able in this kind of notebook we will be able to see how the prior and the posterior to see how the prior and the posterior steps are being drawn i'm not steps are being drawn i'm not going to go through the animations but going to go through the animations but definitely you are able to run definitely you are able to run this notebook in your own environment notebook in your own environment or in the in collab provided of or in the in collab provided of course you install all the necessary course you install all the necessary kind of packages and we have developed kind of packages and we have developed a video on how to run the site itself in", "image_path": "img_data/video_41_chunk_44.jpg"}
{"video": "video_41", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "kind of packages and we have developed a video on how to run the site itself in a video on how to run the site itself in your own local environment because all your own local environment because all these packages have been pre-installed these packages have been pre-installed and you are able to execute all the and you are able to execute all the notebooks of this course notebooks of this course as well also this video what we will do next is that video what we will do next is that we will be looking at the continuous we will be looking at the continuous case now which is a bit more exciting case now which is a bit more exciting in the sense that it really in the sense that it really introduces is a family of basian filters", "image_path": "img_data/video_41_chunk_45.jpg"}
{"video": "video_41", "start": "0:23:00", "end": "0:23:14.766667", "timestamp": "0:23:00 - 0:23:14.766667", "text": "in the sense that it really introduces is a family of basian filters introduces is a family of basian filters called calman filters which they called calman filters which they assume gausian distributions for both assume gausian distributions for both the measurements as well also the state the measurements as well also the state we are dealing with continuous states we are dealing with continuous states and this is what we will discuss next", "image_path": "img_data/video_41_chunk_46.jpg"}
{"video": "video_42", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now we now introduce the core algorith for this section this lecture which for this section this lecture which is the base filter and i'm going to is the base filter and i'm going to write the algorith in a form of a kind write the algorith in a form of a kind of a python function the algorith of a python function the algorith is going is this the is going is this the base let's say underscore filter it has", "image_path": "img_data/video_42_chunk_0.jpg"}
{"video": "video_42", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "is going is this the base let's say underscore filter it has base let's say underscore filter it has three arguments the first argument is three arguments the first argument is the belief of where we are we in the belief of where we are we in the previous time instance st minus one and previous time instance st minus one and this is ev evidently the this is ev evidently the posterior belief or posterior probability of what we are in the probability of what we are in the previous state what was our previous state what was our previous state was and then the other previous state was and then the other one argument is this action that we take one argument is this action that we take at this point in time t and the", "image_path": "img_data/video_42_chunk_1.jpg"}
{"video": "video_42", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "one argument is this action that we take at this point in time t and the at this point in time t and the measurement zt these are the measurement zt these are the three arguments and there is a kind of a arguments and there is a kind of a for loop we expect to see a for loop we expect to see a for loop because as we said the discussion the because as we said the discussion the algorith is going to be the algorith is going to be recursive for all sts for all the recursive for all sts for all the states sts first we are estimating the belief hatut of that estimating the belief hatut of that state", "image_path": "img_data/video_42_chunk_2.jpg"}
{"video": "video_42", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "estimating the belief hatut of that state because we are going because we are going to execute an action and for that action that we action and for that action that we are executing we are effectively going to executing we are effectively going to create a posterior that we will create a posterior that we will effectively integrate the all the integrate the all the possibilities almost the possibilities almost the convolution or some rule let's let me write it down", "image_path": "img_data/video_42_chunk_3.jpg"}
{"video": "video_42", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "convolution or some rule let's let me write it down this is st given st minus one this is my transitional model if you remember transitional model if you remember from earlier and this is the belief of st earlier and this is the belief of st minus one this is the incoming minus one this is the incoming belief where i believe i was before belief where i believe i was before and i use where sometimes in this and i use where sometimes in this discussion because in the back of my discussion because in the back of my mind sometimes i have this simple mind sometimes i have this simple localization kind of sort of use", "image_path": "img_data/video_42_chunk_4.jpg"}
{"video": "video_42", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "mind sometimes i have this simple localization kind of sort of use localization kind of sort of use case but please replace the wear with case but please replace the wear with what was my previous state and what was my previous state and that is basically the first that is basically the first formula we'll actually draw it a little formula we'll actually draw it a little bit to see exactly what is going bit to see exactly what is going on a bit later and then the second going on a bit later and then the second part of the algorith is part the second part of the algorith is now that i have the belief had i will do now that i have the belief had i will do the -called measurement the -called measurement update this is also known as", "image_path": "img_data/video_42_chunk_5.jpg"}
{"video": "video_42", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "the -called measurement update this is also known as prediction step and this is the measurement update step the prediction step is kind of blind we are not step is kind of blind we are not considering any measurement and as we considering any measurement and as we are just taking the action and then the are just taking the action and then the second step involves the measurement second step involves the measurement update it's a normalizing constant update it's a normalizing constant times p", "image_path": "img_data/video_42_chunk_6.jpg"}
{"video": "video_42", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "update it's a normalizing constant times p of zt given st times the belief cut of st which we calculated over here cut of st which we calculated over here and then after we ex exceed this kind of and then after we ex exceed this kind of for loop we are returning the belief of let me write it as a return statement", "image_path": "img_data/video_42_chunk_7.jpg"}
{"video": "video_42", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "returning the belief of let me write it as a return statement the belief of std which also worth mentioning how similar because it's mentioning how similar because it's exactly what's happening this exactly what's happening this line two line is to with the two line is to with the two probability rules that we have looked at probability rules that we have looked at the probability review video this the probability review video this one is very similar to the sum rule and i'll write the sum rule in a moment and the this one is similar to", "image_path": "img_data/video_42_chunk_8.jpg"}
{"video": "video_42", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "rule and i'll write the sum rule in a moment and the this one is similar to moment and the this one is similar to the product rule and we'll see how this is also obviously the product rule is obviously the product rule is associating itself with the very associating itself with the very definition of the base rule and that's definition of the base rule and that's why we took the name of base filter and why we took the name of base filter and filter is a kind of a term that we have filter is a kind of a term that we have used extensively in electrical used extensively in electrical engineering to denote to engineering to denote to actually call functions that are going actually call functions that are going to be receiving if you a stream and", "image_path": "img_data/video_42_chunk_9.jpg"}
{"video": "video_42", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "actually call functions that are going to be receiving if you a stream and to be receiving if you a stream and are doing exactly the same job are doing exactly the same job over and over again this is over and over again this is basically what's happening over there basically what's happening over there this function will be called now this function will be called now that we have the belief s of t in the next iteration belief s of t in the next iteration this belief of s will actually go here this belief of s will actually go here another action will be taken and a another action will be taken and a measurement will be received and then measurement will be received and then the whole thing will repeat over and the whole thing will repeat over and over again that's the kind of recursive over again that's the kind of recursive step but coming back to these kind of step but coming back to these kind of two lines the one and let's me call it one and", "image_path": "img_data/video_42_chunk_10.jpg"}
{"video": "video_42", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "two lines the one and let's me call it one and the one and let's me call it one and two the line one the prediction step is related to the sum rule or the related to the sum rule or the marginalization if you rule and if marginalization if you rule and if you remember the marginalization rule you remember the marginalization rule was p of x let me call this", "image_path": "img_data/video_42_chunk_11.jpg"}
{"video": "video_42", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "you remember the marginalization rule was p of x let me call this was p of x let me call this p of x i think i used i think x and y p of x i think i used i think x and y variables when i was discussing these variables when i was discussing these two probability foundational two probability foundational probability rules i said p of x is probability rules i said p of x is the sum over all y's of p of x comma the sum over all y's of p of x comma y or now that i can use also the product y or now that i can use also the product rule over there to expand this joint is rule over there to expand this joint is p of", "image_path": "img_data/video_42_chunk_12.jpg"}
{"video": "video_42", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "p of x given y * p of y replace the y with x given y * p of y replace the y with the state no matter the index and replace state no matter the index and replace the x with the i think you can replace a x with the i think you can replace a state with a belief on t minus one and this one belief on t minus one and this one with the state st then", "image_path": "img_data/video_42_chunk_13.jpg"}
{"video": "video_42", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "belief on t minus one and this one with the state st then with the state st then effectively you have what we have in effectively you have what we have in equation one over here this in equation one over here this is the prediction step is related is the prediction step is related to the sum rule and then the other comment is the that the measurement update", "image_path": "img_data/video_42_chunk_14.jpg"}
{"video": "video_42", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "rule and then the other comment is the that the measurement update is related to the product rule and to the base rule as follows we know and to the base rule as follows we know that the u product rule was the very that the u product rule was the very much the p of x comma y is equal to p much the p of x comma y is equal to p of x given y * p of y we also know that of x given y * p of y we also know that p of y given comma x is equal to p of y given comma x is equal to p of y given x time p of", "image_path": "img_data/video_42_chunk_15.jpg"}
{"video": "video_42", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "p of y given comma x is equal to p of y given x time p of x and from these two given the fact x and from these two given the fact that p of x comma y is equal to p of y that p of x comma y is equal to p of y given an x we can actually conclude the very important kind of conclude the very important kind of relationship that gives us", "image_path": "img_data/video_42_chunk_16.jpg"}
{"video": "video_42", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "conclude the very important kind of relationship that gives us relationship that gives us the -called posterior probability p of the -called posterior probability p of y given x is equal to p of x given y times x is equal to p of x given y times the probability of y which is called the probability of y which is called likelihood times the prior / p of x and likelihood times the prior / p of x and actually we can write that as actually we can write that as ea where ita is associated with the ea where ita is associated with the normalizing a factor 1 / p of x which is normalizing a factor 1 / p of x which is sometimes very difficult to estimate", "image_path": "img_data/video_42_chunk_17.jpg"}
{"video": "video_42", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "normalizing a factor 1 / p of x which is sometimes very difficult to estimate sometimes very difficult to estimate and but in this case we will see that and but in this case we will see that it's kind of a straightforward for the it's kind of a straightforward for the case of we will do in case of the case of we will do in it has to do with a discrete version of it has to do with a discrete version of this base filter which is the first this base filter which is the first implementation we'll see in a moment implementation we'll see in a moment times a p of y sorry of x given y time p of y here you actually can see that instead of x in this case we see that instead of x in this case we have the me measurement z let me", "image_path": "img_data/video_42_chunk_18.jpg"}
{"video": "video_42", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "see that instead of x in this case we have the me measurement z let me have the me measurement z let me write it instead of z the measurement and instead of why we have the state and this is basically what we have in this u kind of what we have in this u kind of association between going from association between going from the first step and the second step to the first step and the second step to another recursion to update", "image_path": "img_data/video_42_chunk_19.jpg"}
{"video": "video_42", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "the first step and the second step to another recursion to update another recursion to update and find the next version of the and find the next version of the state in the next time instant now state in the next time instant now obviously we need to see some kind of obviously we need to see some kind of visualization of what's really happen to visualization of what's really happen to see why this kind of makes sense and see why this kind of makes sense and we'll have plenty of those and some kind we'll have plenty of those and some kind of a python notebooks in the simple of a python notebooks in the simple single dimensional case to see what is single dimensional case to see what is happening but in the broad happening but in the broad terms let's say that i am in a terms let's say that i am in a room and i'm using now the localization room and i'm using now the localization kind of use case", "image_path": "img_data/video_42_chunk_20.jpg"}
{"video": "video_42", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "room and i'm using now the localization kind of use case let's assume that i have some kind of let's assume that i have some kind of furniture which i'm kind furniture which i'm kind of aware of u it's coordinates and i of aware of u it's coordinates and i have if you a robot and let's say have if you a robot and let's say that is located right here in this in that is located right here in this cell and makes a this cell and makes a move the moment that the robot makes a the moment that the robot makes a move its location without any", "image_path": "img_data/video_42_chunk_21.jpg"}
{"video": "video_42", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "the moment that the robot makes a move its location without any move its location without any measurement information can be represented by some kind of probability represented by some kind of probability distribution which is the posterior distribution which is the posterior probability distribution that is my probability distribution that is my belief c the moment i take an action c the moment i take an action this thing is was induced this thing is was induced this transition was induced by this kind of transition was induced by this kind of action and that gave raise to the action and that gave raise to the belief c obviously i have some kind of belief c obviously i have some kind of estimate of some belief", "image_path": "img_data/video_42_chunk_22.jpg"}
{"video": "video_42", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "belief c obviously i have some kind of estimate of some belief estimate of some belief for let's say the st minus one i have this is my st minus one i have this is my belief of st a transition from st minus one st a transition from st minus one to st and as we will see the to st and as we will see the uncertainty about the new state uncertainty about the new state is growing in the absence of any is growing in the absence of any measurement and then what i do is measurement and then what i do is the measurement is actually coming the measurement is actually coming in indicating let's say some", "image_path": "img_data/video_42_chunk_23.jpg"}
{"video": "video_42", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "the measurement is actually coming in indicating let's say some in indicating let's say some measurement that kind of indicates how measurement that kind of indicates how in a proximity kind of sensor how close in a proximity kind of sensor how close is this kind of new location with is this kind of new location with respect to this object and what will respect to this object and what will actually happen in the second kind of actually happen in the second kind of step the uncertainty there will be step the uncertainty there will be some correction and maybe that correction and maybe that correction will be this will be my belief of", "image_path": "img_data/video_42_chunk_24.jpg"}
{"video": "video_42", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "will be this will be my belief of will be my belief of st and this correction as we will see st and this correction as we will see reduces the uncertainty associated reduces the uncertainty associated with the new state we are every with the new state we are every time we are moving blind as you can time we are moving blind as you can intuitively understand in an environment intuitively understand in an environment we are expanding in terms of my we are expanding in terms of my covariance of error that i see about covariance of error that i see about my state represented by this kind of my state represented by this kind of byar contour diagram of this byar let's byar contour diagram of this byar let's say gausian and then every time that say gausian and then every time that i have a measurement which i now", "image_path": "img_data/video_42_chunk_25.jpg"}
{"video": "video_42", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "say gausian and then every time that i have a measurement which i now i have a measurement which i now do the correction step the measurement do the correction step the measurement update kind of step that conv variance update kind of step that conv variance sort of reduces and the intuition sort of reduces and the intuition behind it is that now that i have behind it is that now that i have that additional kind of variable what that additional kind of variable what can i say more about that kind of can i say more about that kind of posterior and definitely that kind posterior and definitely that kind of will reduce the uncertainty about my state which the uncertainty about my state which in this case is a localization use case", "image_path": "img_data/video_42_chunk_26.jpg"}
{"video": "video_42", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "the uncertainty about my state which in this case is a localization use case in this case is a localization use case that is the whole story of that is the whole story of what we will see how this kind of filter what we will see how this kind of filter operates and we will see now that operates and we will see now that very clearly in the single dimensional very clearly in the single dimensional case what we will do now next we will case what we will do now next we will first address two cases one is the first address two cases one is the discrete case and in the discrete case and in the discrete case we will see a robot trying to case we will see a robot trying to perform that kind of base filter and perform that kind of base filter and then we will address the continuous case then we will address the continuous case and this in the continuous case we", "image_path": "img_data/video_42_chunk_27.jpg"}
{"video": "video_42", "start": "0:14:00", "end": "0:14:14.700000", "timestamp": "0:14:00 - 0:14:14.700000", "text": "then we will address the continuous case and this in the continuous case we and this in the continuous case we will be looking at gausian will be looking at gausian distributions to help us quantify the distributions to help us quantify the operation in that kind of space", "image_path": "img_data/video_42_chunk_28.jpg"}
{"video": "video_43", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in an earlier video we have seen object detection was object detection was presented as a capability of a neural presented as a capability of a neural network to present to us a stream of network to present to us a stream of json a json stream of where that json a json stream of where that contains bounding box let's say contains bounding box let's say information about objects of interest information about objects of interest and in that kind of discussion we and in that kind of discussion we also saw a video where bounding boxes also saw a video where bounding boxes were simply appearing in this specific were simply appearing in this specific frame and maybe disappearing in the frame and maybe disappearing in the next frame and reappearing in the frame next frame and reappearing in the frame after that in many mission critical", "image_path": "img_data/video_43_chunk_0.jpg"}
{"video": "video_43", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "next frame and reappearing in the frame after that in many mission critical after that in many mission critical applications it's almost impossible to applications it's almost impossible to deal with such a kind of a deal with such a kind of a flickering sort of appearance of flickering sort of appearance of the object detection output and the object detection output and in fact this appearance this and in fact this appearance this flickering that we observed in the flickering that we observed in the in those videos originates from the in those videos originates from the fact that neural networks are sort of fact that neural networks are sort of reflexive in nature they are reflexive in nature they are probabilistic devices and as such they", "image_path": "img_data/video_43_chunk_1.jpg"}
{"video": "video_43", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "reflexive in nature they are probabilistic devices and as such they probabilistic devices and as such they are sort of reflexive in nature what are sort of reflexive in nature what we will do in this kind of video is we will do in this kind of video is we will investigate approaches where we are will investigate approaches where we are going to simply try to correct going to simply try to correct these video outputs and we will be these video outputs and we will be calling this discussion probabilistic calling this discussion probabilistic reasoning over time where the time kind reasoning over time where the time kind of ais needs to be involved now to of ais needs to be involved now to correct for those and come up with a correct for those and come up with a video stream that contains object video stream that contains object detections which are very much detections which are very much continuous and without this kind to have", "image_path": "img_data/video_43_chunk_2.jpg"}
{"video": "video_43", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "detections which are very much continuous and without this kind to have continuous and without this kind to have flickering effects we should also flickering effects we should also mention that the topic is a bit more mention that the topic is a bit more significant just correcting for missing significant just correcting for missing frames and an object detection kind of frames and an object detection kind of stream i think it can expand into stream i think it can expand into covering the -cal covering in the gaps covering the -cal covering in the gaps the ability of humans to cover the ability of humans to cover the gaps with reason when they don't have gaps with reason when they don't have perception information let's say if i'm perception information let's say if i'm behind an occlusion and i was behind an occlusion and i was visible a few moments ago then every", "image_path": "img_data/video_43_chunk_3.jpg"}
{"video": "video_43", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "behind an occlusion and i was visible a few moments ago then every visible a few moments ago then every person who is kind of watching this person who is kind of watching this scene is able to reason that scene is able to reason that this other person is behind now in this other person is behind now in inclusion and the kind of inclusion and the kind of discussion over here will kind of expand discussion over here will kind of expand into this type of general topic of into this type of general topic of tracking objects in a scene i think tracking objects in a scene i think it's also a good idea to see the it's also a good idea to see the on our side we have this kind of block on our side we have this kind of block diagram that positions the topic at diagram that positions the topic at least the part of the topic we're least the part of the topic we're discussing in general probabilistic discussing in general probabilistic reasoning over time", "image_path": "img_data/video_43_chunk_4.jpg"}
{"video": "video_43", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "discussing in general probabilistic reasoning over time this is the yellow block over here which this is the yellow block over here which follows the perception it receives the follows the perception it receives the json stream and delivers another stream json stream and delivers another stream that it is kind of richer in a that it is kind of richer in a sense to the planner richer and sense to the planner richer and obviously more correct as it has obviously more correct as it has stitched together the imperfect stitched together the imperfect detections that we have we it is detections that we have we it is presented that we are presenting to it presented that we are presenting to it out of the many areas that", "image_path": "img_data/video_43_chunk_5.jpg"}
{"video": "video_43", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "presented that we are presenting to it out of the many areas that out of the many areas that we can actually categorize as we can actually categorize as probabilistic reasoners over time and probabilistic reasoners over time and capabilities over here you see in and capabilities over here you see in these kind of boxes will be these kind of boxes will be focusing on use cases that are focusing on use cases that are associated with tracking objects associated with tracking objects as we discussed as well also the topic as we discussed as well also the topic of localization and needless to say localization and needless to say that this block is present in the that this block is present in the what we will be calling later on basian", "image_path": "img_data/video_43_chunk_6.jpg"}
{"video": "video_43", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "that this block is present in the what we will be calling later on basian what we will be calling later on basian filters as in every single robot which filters as in every single robot which is out there and in fact it is not is out there and in fact it is not only robots but it's also embedded in only robots but it's also embedded in your cell phones as they have they your cell phones as they have they also need to track various signals also need to track various signals which are actually being received from which are actually being received from the cellular and satellite the cellular and satellite infrastructure let's now try to infrastructure let's now try to establish some kind of terminology establish some kind of terminology and notation on the things that we need and notation on the things that we need to study to understand a little bit", "image_path": "img_data/video_43_chunk_7.jpg"}
{"video": "video_43", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "and notation on the things that we need to study to understand a little bit to study to understand a little bit how we going to go about addressing this how we going to go about addressing this kind of a issue of correcting for kind of a issue of correcting for imperfect let's say detections or in imperfect let's say detections or in general tracking all sorts of u general tracking all sorts of u sensing information that is being sent sensing information that is being sent to us by the sensors and also the to us by the sensors and also the predictors that we have in the predictors that we have in the perception subsystem if i may perception subsystem if i may actually redraw a little bit the actually redraw a little bit the problem that we have seen in the kind of problem that we have seen in the kind of a earlier kind of discussion i have the", "image_path": "img_data/video_43_chunk_8.jpg"}
{"video": "video_43", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "problem that we have seen in the kind of a earlier kind of discussion i have the a earlier kind of discussion i have the environment over here and i do have environment over here and i do have my perception block that is actually sending to me predictions about all sorts of things predictions about all sorts of things row sensing information is coming in and row sensing information is coming in and predictions are let's say are coming out predictions are let's say are coming out and then i have this and then i have this system that i'm discussing about the system that i'm discussing about the sort of tracking", "image_path": "img_data/video_43_chunk_9.jpg"}
{"video": "video_43", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "system that i'm discussing about the sort of tracking and which will also call and which will also call probabilistic reasoning over time and probabilistic reasoning over time and then i have a planner and finally i have this kind of a actuation or controller if i may call it actuation or controller if i may call it this and this is basically this and this is basically what is been fed back into this environment been fed back into this environment to speak that actions are being taken", "image_path": "img_data/video_43_chunk_10.jpg"}
{"video": "video_43", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "been fed back into this environment to speak that actions are being taken to speak that actions are being taken are causing the change of what we'll be are causing the change of what we'll be calling the state is the what we try to predict it is kept in the we try to predict it is kept in the environment and the evidently the environment and the evidently the perception is actually giving us some perception is actually giving us some information about what is there let's information about what is there let's say in terms of location of objects say in terms of location of objects let's say in the current frame and let's say in the current frame and in all frames obviously of the video", "image_path": "img_data/video_43_chunk_11.jpg"}
{"video": "video_43", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "let's say in the current frame and in all frames obviously of the video in all frames obviously of the video stream and the tracking is going stream and the tracking is going to provide a better estimate of this to provide a better estimate of this kind of state to correct for all this kind of state to correct for all this kind of reflexive behavior that we see kind of reflexive behavior that we see inside the perception system i inside the perception system i think it's a good idea to take this term think it's a good idea to take this term first this concept of a state and kind first this concept of a state and kind of categorize it and we have plenty of categorize it and we have plenty of states in various problems we'll be states in various problems we'll be dealing with in this kind of course dealing with in this kind of course the first i will call it le level", "image_path": "img_data/video_43_chunk_12.jpg"}
{"video": "video_43", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "dealing with in this kind of course the first i will call it le level the first i will call it le level of a state is the -call atomic of a state is the -call atomic state in the atomic state we simply state in the atomic state we simply do not care what is this state do not care what is this state consist of let's say you are consist of let's say you are looking at a best path to or best looking at a best path to or best trajectory to exit a room and you trajectory to exit a room and you divide the room a floor into cells and divide the room a floor into cells and that cell is the state that cell is the state that you in right now let's say the location", "image_path": "img_data/video_43_chunk_13.jpg"}
{"video": "video_43", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "that cell is the state that you in right now let's say the location you in right now let's say the location and how what locations you will occupy and how what locations you will occupy to optimally exit the room let's say to optimally exit the room let's say in a shortest path or minimum cost in a shortest path or minimum cost then in order to solve this problem then in order to solve this problem we do not need to decompose that we do not need to decompose that state of that object or robot in any state of that object or robot in any finer way that is the sort of finer way that is the sort of let me write it down planning let me write it down planning problems sometimes use this atomic kind of", "image_path": "img_data/video_43_chunk_14.jpg"}
{"video": "video_43", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "sometimes use this atomic kind of states and in general there will states and in general there will be solved with search based be solved with search based approaches the second approaches the second level and planning is obviously another level and planning is obviously another video series of videos that we video series of videos that we will sort of see a bit later the will sort of see a bit later the second level is the -called second level is the -called factored states and we've been factored states and we've been dealing with factor states all along", "image_path": "img_data/video_43_chunk_15.jpg"}
{"video": "video_43", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "factored states and we've been dealing with factor states all along dealing with factor states all along to give you an idea of a factor state to give you an idea of a factor state that is a bit outside of the space of that is a bit outside of the space of computer vision let's take the computer vision let's take the description of a kinematic state of a of description of a kinematic state of a vehicle i'll be calling the a vehicle i'll be calling the state s let's say from now on and i'll state s let's say from now on and i'll also be decorating it with subscript t also be decorating it with subscript t to indicate that there is some kind of a to indicate that there is some kind of a time dependency on this state and this", "image_path": "img_data/video_43_chunk_16.jpg"}
{"video": "video_43", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "to indicate that there is some kind of a time dependency on this state and this time dependency on this state and this date definitely can be decomposed into date definitely can be decomposed into let's say the pose in general the pose let's say the pose in general the pose will be consist of let's say xyz be consist of let's say xyz coordinates in the cartesian kind of coordinates in the cartesian kind of coordinate system and maybe it will be system and maybe it will be associated with also three other associated with also three other variables p r and yo", "image_path": "img_data/video_43_chunk_17.jpg"}
{"video": "video_43", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "variables p r and yo and then maybe some velocity and some form of let's say acceleration these are the six socalled six degrees of freedom or dof in general we have an in fact i", "image_path": "img_data/video_43_chunk_18.jpg"}
{"video": "video_43", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "dof in general we have an in fact i have here also a figure in wikipedia have here also a figure in wikipedia that kind of explains the p is the that kind of explains the p is the pitch the r is the roll and the y is pitch the r is the roll and the y is the yo you have an object in general the yo you have an object in general which is located let's say here just which is located let's say here just my hand and it has a pitch it my hand and it has a pitch it can actually has a roll and it can also can actually has a roll and it can also has a y in terms of the has a y in terms of the these type of movements to capture all these type of movements to capture all the potential movements that consist", "image_path": "img_data/video_43_chunk_19.jpg"}
{"video": "video_43", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "these type of movements to capture all the potential movements that consist the potential movements that consist that the pose of an object consist that the pose of an object consist of and then of course you consist of and then of course you have some additional ones that's have some additional ones that's basically a simple example of basically a simple example of decomposable state and very much we decomposable state and very much we are in that kind of domain all in our are in that kind of domain all in our journey from the perception system until journey from the perception system until now and then of course at the top of now and then of course at the top of that you have the -called that you have the -called structured", "image_path": "img_data/video_43_chunk_20.jpg"}
{"video": "video_43", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "that you have the -called structured where actually i forgot to give an example here perception localization and pos estimation and on another example of let's say on another example of let's say structure state in that kind of level structure state in that kind of level we are not really interested to just we are not really interested to just capture the state of an object in a capture the state of an object in a detailed way but we are also interested", "image_path": "img_data/video_43_chunk_21.jpg"}
{"video": "video_43", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "capture the state of an object in a detailed way but we are also interested detailed way but we are also interested to create some kind of global view of to create some kind of global view of what we have right now for example what we have right now for example create some form of relationships capture relations between let's say objects and we have of course let's say objects and we have of course neural approaches to capture neural approaches to capture relationships these are all part of a relationships these are all part of a bit more advanced discussion on visual bit more advanced discussion on visual question answering such as relation question answering such as relation networks and other kind of innovations", "image_path": "img_data/video_43_chunk_22.jpg"}
{"video": "video_43", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "question answering such as relation networks and other kind of innovations networks and other kind of innovations in that kind of domain that allow a in that kind of domain that allow a differentiable machines to be able to differentiable machines to be able to capture this type of capture this type of relationships let's introduce now the relationships let's introduce now the remaining kind of notation and i should remaining kind of notation and i should actually point out that all of the actually point out that all of the treatment over here in this kind of treatment over here in this kind of video and in this video series on video and in this video series on probabilistic vision over time is on probabilistic vision over time is going to come out of a very important going to come out of a very important book from sebastian thrun and his book from sebastian thrun and his co-authors called pro bistic", "image_path": "img_data/video_43_chunk_23.jpg"}
{"video": "video_43", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "book from sebastian thrun and his co-authors called pro bistic co-authors called pro bistic robotics and this is i think a robotics and this is i think a book that i think you should be in your book that i think you should be in your bookshelf aside from the sort of bookshelf aside from the sort of books that we have actually recommended books that we have actually recommended in this kind of course because this in this kind of course because this will actually open the door to more will actually open the door to more advanced topics simultaneous advanced topics simultaneous localization and mapping extensions to localization and mapping extensions to that on with visual slam as we call it and but visual slam as we call it and but apart from that let's try and", "image_path": "img_data/video_43_chunk_24.jpg"}
{"video": "video_43", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "visual slam as we call it and but apart from that let's try and apart from that let's try and introduce the what we will be calling introduce the what we will be calling now from now on the recursive state now from now on the recursive state estimator let me write this down all rsc recursive state estimation rsc recursive state estimation effectively is going to happen over here effectively is going to happen over here in this block and the state we're in this block and the state we're referring to is a state which is referring to is a state which is kept in the environment as from", "image_path": "img_data/video_43_chunk_25.jpg"}
{"video": "video_43", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "referring to is a state which is kept in the environment as from kept in the environment as from the earlier kind of discussions this the earlier kind of discussions this state is stochastic it's time ving it state is stochastic it's time ving it is not necessarily fully observed is not necessarily fully observed from the agent is actually the whole thing over here actually the whole thing over here and the in addition to that and the in addition to that is going to be sort of is going to be sort of partially invol partially kind of partially invol partially kind of estimated from the tracking from the", "image_path": "img_data/video_43_chunk_26.jpg"}
{"video": "video_43", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "partially invol partially kind of estimated from the tracking from the estimated from the tracking from the inside this kind of a tracking or inside this kind of a tracking or recursive state estimator tracking recursive state estimator tracking and recursive state estimation is going and recursive state estimation is going to be one and the same thing to be one and the same thing the state the recursive part is the state the recursive part is because the estimation is going to because the estimation is going to actually have happen recursively in actually have happen recursively in the steps that we are now going to in the steps that we are now going to be discussing now in terms of be discussing now in terms of notation we are going to have the notation we are going to have the first apart from the state s oft i", "image_path": "img_data/video_43_chunk_27.jpg"}
{"video": "video_43", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "notation we are going to have the first apart from the state s oft i first apart from the state s oft i need a notation about the action is actually going to happen as part of this block over happen as part of this block over here but for our discussion we are here but for our discussion we are going to be abstracting away all of this going to be abstracting away all of this kind of planning let's say trajectory kind of planning let's say trajectory planning and on and the various kind planning and on and the various kind of control subsystem over here and we'll of control subsystem over here and we'll assume that yes indeed we're taking some assume that yes indeed we're taking some kind of an ction that will involve", "image_path": "img_data/video_43_chunk_28.jpg"}
{"video": "video_43", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "assume that yes indeed we're taking some kind of an ction that will involve kind of an ction that will involve potentially some let's say some movement potentially some let's say some movement of the agent in the environment we of the agent in the environment we will be denoting with a and with subt u will be denoting with a and with subt u that kind of action and then that kind of action and then and the convention is that the and the convention is that the agent first takes the action", "image_path": "img_data/video_43_chunk_29.jpg"}
{"video": "video_43", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "agent first takes the action and then senses the environment the sensing of the environment for that we will not have environment for that we will not have another notation for i will call another notation for i will call it the measurement we will call it the measurement we will call it that measurement we'll call that as zt we'll denote it this as the zt denote it this as the zt and of course the measurement as", "image_path": "img_data/video_43_chunk_30.jpg"}
{"video": "video_43", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "denote it this as the zt and of course the measurement as and of course the measurement as far as this block is concerned is the far as this block is concerned is the whatever is comes out of the whatever is comes out of the perception subsystem i we know perception subsystem i we know that we have been denoting whatever that we have been denoting whatever prediction we are making as y hat prediction we are making as y hat that is actually coming out of this kind that is actually coming out of this kind of block now for this treatment to make of block now for this treatment to make it a bit more compatible with the it a bit more compatible with the literature we changes this y hat to the literature we changes this y hat to the measurement zt but the framew also measurement zt but the framew also accommodates a situation where we don't", "image_path": "img_data/video_43_chunk_31.jpg"}
{"video": "video_43", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "measurement zt but the framew also accommodates a situation where we don't accommodates a situation where we don't have necessarily any processing over have necessarily any processing over here and we just measure with zt here and we just measure with zt whatever row sensing information that's whatever row sensing information that's the reason why you actually harmonize the reason why you actually harmonize both options both cases into the same both options both cases into the same sort of variable over here zt sort of variable over here zt another convention we will be another convention we will be adopting is that we'll be adopting a adopting is that we'll be adopting a convention that it is that say for convention that it is that say for actions is notational convention", "image_path": "img_data/video_43_chunk_32.jpg"}
{"video": "video_43", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "convention that it is that say for actions is notational convention actions is notational convention is we will be calling a is we will be calling a of t1 colum t2 the sequence of actions that happen t2 the sequence of actions that happen between instances t1 and t2 and the between instances t1 and t2 and the same thing will be let me finish same thing will be let me finish this kind of measurement zt this kind of measurement zt may accommodate actually not accommodate but represent", "image_path": "img_data/video_43_chunk_33.jpg"}
{"video": "video_43", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "accommodate actually not accommodate but represent predictions from the perception sub system sub or row measurements row sensory information", "image_path": "img_data/video_43_chunk_34.jpg"}
{"video": "video_43", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "row measurements row sensory information also generally we will be discussing a sequence of measurements from z of a sequence of measurements from z of from between t1 and t2 with this from between t1 and t2 with this colon kind of notation this col notation colon kind of notation this col notation originates also from a language originates also from a language called ma laab from math works and for called ma laab from math works and for those who are working in the robotic those who are working in the robotic space in control theory and things space in control theory and things that are very familiar with that are very familiar with that kind of tool as it is kind of", "image_path": "img_data/video_43_chunk_35.jpg"}
{"video": "video_43", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "that are very familiar with that kind of tool as it is kind of that kind of tool as it is kind of monopolizing some of the domains over monopolizing some of the domains over there now that we have there now that we have introduced if you these variables introduced if you these variables we were definitely going to be we were definitely going to be expressing probability distributions expressing probability distributions and we'll be dealing with probability and we'll be dealing with probability distributions all along in this distributions all along in this discussion and we need to define discussion and we need to define some assumptions regarding this some assumptions regarding this structure of this probability structure of this probability distributions but in general we will", "image_path": "img_data/video_43_chunk_36.jpg"}
{"video": "video_43", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "structure of this probability distributions but in general we will distributions but in general we will be discussing a probability be discussing a probability distribution that involves the state the probability involves the state the probability of being at a state s a time t that of being at a state s a time t that called is called s given the called is called s given the sequence of previous states let's say sequence of previous states let's say from the beginning of time until t minus from the beginning of time until t minus one comma the sequence of actions from one comma the sequence of actions from one to t and the sequence of one to t and the sequence of measurements from 1 to t", "image_path": "img_data/video_43_chunk_37.jpg"}
{"video": "video_43", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "one to t and the sequence of measurements from 1 to t minus1 this is the this is going to be the third important kind of concept that third important kind of concept that we need to introduce here and we will be we need to introduce here and we will be calling this generative model of calling this generative model of state evolution we will be calling this evolution we will be calling this state transition model", "image_path": "img_data/video_43_chunk_38.jpg"}
{"video": "video_43", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "evolution we will be calling this state transition model is generative we call all of course approaches where we are actually course approaches where we are actually going to be generating creating this going to be generating creating this directly this probability directly this probability distribution it's a general and of distribution it's a general and of course this reminds us the discussion we course this reminds us the discussion we had also in classification in terms of generative classification in terms of generative versus discrimin native kind of versus discrimin native kind of classifiers back when we're discussing", "image_path": "img_data/video_43_chunk_39.jpg"}
{"video": "video_43", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "versus discrimin native kind of classifiers back when we're discussing classifiers back when we're discussing this in that kind of video state this in that kind of video state transition model is a transition model is a generative model for the state evolution the important kind of piece to recognize important kind of piece to recognize here is the following i'm taking here is the following i'm taking some measurements i am evident some measurements i am evident processing them in the perception system", "image_path": "img_data/video_43_chunk_40.jpg"}
{"video": "video_43", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "some measurements i am evident processing them in the perception system processing them in the perception system making some kind of predictions where making some kind of predictions where i'm correcting and tracking whatever i'm correcting and tracking whatever is coming out of the perception is coming out of the perception subsystem i am creating if you subsystem i am creating if you plans based on that information and then plans based on that information and then these plans are executed by the these plans are executed by the controller who is taking actions it is controller who is taking actions it is going through the these actions are going through the these actions are changing the state of the environment changing the state of the environment let's say an action to move and that let's say an action to move and that change state is being observed again change state is being observed again by via the row sensing measurements by via the row sensing measurements from the perception system and the whole", "image_path": "img_data/video_43_chunk_41.jpg"}
{"video": "video_43", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "by via the row sensing measurements from the perception system and the whole from the perception system and the whole thing is kind of constantly evolving and thing is kind of constantly evolving and this evolution of state is what we this evolution of state is what we are going to be capturing by the are going to be capturing by the -called state transition model how -called state transition model how probabilistically the state will probabilistically the state will actually change given previous states actually change given previous states and as you will see a bit later in and as you will see a bit later in fact right now we'll make an important fact right now we'll make an important assumption regarding the completeness assumption regarding the completeness of that state will make this what is of that state will make this what is called a arian assumption and now we", "image_path": "img_data/video_43_chunk_42.jpg"}
{"video": "video_43", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "of that state will make this what is called a arian assumption and now we called a arian assumption and now we will discuss the that kind of assumption will discuss the that kind of assumption in detail a state st is going to be called complete if it is a complete that's why it is a complete that's why the name complete is a complete", "image_path": "img_data/video_43_chunk_43.jpg"}
{"video": "video_43", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "it is a complete that's why the name complete is a complete summary of the past in other words s of t is a sufficient statistic for all past measurements and actions", "image_path": "img_data/video_43_chunk_44.jpg"}
{"video": "video_43", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "statistic for all past measurements and actions the based on that this is also known as the marvian assumption and this actually it is that assumption that actually simplifies assumption that actually simplifies quite a bit a lot of the probability quite a bit a lot of the probability this conditional probability this conditional probability distribution we just wrote which i'm distribution we just wrote which i'm going to repeat now we wrote this s", "image_path": "img_data/video_43_chunk_45.jpg"}
{"video": "video_43", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "distribution we just wrote which i'm going to repeat now we wrote this s going to repeat now we wrote this s of t probability distribution of s of t probability distribution of s of t given the previous states the actions up to this moment in time t and actions up to this moment in time t and all previous kind of all previous kind of measurements and this is what allows us measurements and this is what allows us to write it as follows this assumption p of s of t given s t minus one comma a of t and this is a much", "image_path": "img_data/video_43_chunk_46.jpg"}
{"video": "video_43", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "given s t minus one comma a of t and this is a much one comma a of t and this is a much simpler probability distribution simpler probability distribution that we will definitely are going to that we will definitely are going to have some hope that we will be have some hope that we will be able to estimate and given able to estimate and given that sort of assumption that we that sort of assumption that we actually made over here i think it's actually made over here i think it's worthwhile kind of going a little bit worthwhile kind of going a little bit over it we say that obviously the over it we say that obviously the measurements that are", "image_path": "img_data/video_43_chunk_47.jpg"}
{"video": "video_43", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "over it we say that obviously the measurements that are measurements that are not really needed at all because at the not really needed at all because at the end of the day the only thing that end of the day the only thing that matters is really the previous state s matters is really the previous state s of t minus one because the state is of t minus one because the state is complete and but the current complete and but the current action previous actions are not action previous actions are not needed but definitely out of the all needed but definitely out of the all actions the current action is needed actions the current action is needed because it is really the current action because it is really the current action that it used the st the state transition that it used the st the state transition let me write it down action let me write it down action at t", "image_path": "img_data/video_43_chunk_48.jpg"}
{"video": "video_43", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "at t induces the state transition from st minus1 to st that's why we have minus1 to st that's why we have to have and considering that kind of action in other considering that kind of action in other words then we have this transition words then we have this transition the specific action ofd and now we will the specific action ofd and now we will need to introduce another u the second need to introduce another u the second probabilistic model the we have", "image_path": "img_data/video_43_chunk_49.jpg"}
{"video": "video_43", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "need to introduce another u the second probabilistic model the we have probabilistic model the we have going to have two main probabilistic going to have two main probabilistic model what we have to introduce we model what we have to introduce we introduce a state transition what causes introduce a state transition what causes the states of course the current action the states of course the current action and the simplifying assumption that and the simplifying assumption that allows us to capture that in and allows us to capture that in and the third and the u sort of second the third and the u sort of second probabilistic model is going to be probabilistic model is going to be called the generative model of the", "image_path": "img_data/video_43_chunk_50.jpg"}
{"video": "video_43", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "generative model of the measurements we have to have a probability distribution that a model in probability distribution that a model in other words that will capture those other words that will capture those measurements again we will making measurements again we will making this assumption given the complete this assumption given the complete state of the assumption will make the state of the assumption will make the following can actually write the following can actually write the following equation zt is now the following equation zt is now the random variable and we have this variable and we have this measurement that we are getting a time t measurement that we are getting a time t will depend on u previous sequence of", "image_path": "img_data/video_43_chunk_51.jpg"}
{"video": "video_43", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "measurement that we are getting a time t will depend on u previous sequence of will depend on u previous sequence of states and actions that we have taken states and actions that we have taken and also previous measurements from and also previous measurements from one t minus one and this one based on that kind of assumption can be written as p of assumption can be written as p of zt given st and it's actually st and it's actually also important to just spend a couple of", "image_path": "img_data/video_43_chunk_52.jpg"}
{"video": "video_43", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "st and it's actually also important to just spend a couple of also important to just spend a couple of minutes trying to discuss that kind of minutes trying to discuss that kind of simplification the fact that i let's simplification the fact that i let's say i'm moving from i have moved and say i'm moving from i have moved and i actually have reached this kind of i actually have reached this kind of state let's say next to this kind of state let's say next to this kind of desk that state that i am right desk that state that i am right now is the only thing that i need to now is the only thing that i need to consider and the full condition on in consider and the full condition on in order for me to specify the probability order for me to specify the probability distribution of let's say how far away distribution of let's say how far away that lets let's say a sensor will", "image_path": "img_data/video_43_chunk_53.jpg"}
{"video": "video_43", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "distribution of let's say how far away that lets let's say a sensor will that lets let's say a sensor will bring me on how far away i am from this bring me on how far away i am from this kind of obstacle of the wall kind of obstacle of the wall or let's say the desk over here that or let's say the desk over here that is what this probability distribution is what this probability distribution is actually capturing for example we'll actually capturing for example we'll be assuming in a bit later will be assuming in a bit later will be assuming that this probability assuming that this probability distribution has could be continuous and distribution has could be continuous and it will be let's say or discrete it will be let's say or discrete and will take some kind of and will take some kind of distribution around it let's say for distribution around it let's say for continuous will be a gausian continuous will be a gausian and that is u the second kind", "image_path": "img_data/video_43_chunk_54.jpg"}
{"video": "video_43", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "continuous will be a gausian and that is u the second kind and that is u the second kind of model we have now two models one of model we have now two models one governs the state transition the other governs the state transition the other governs the measurements and in we governs the measurements and in we also need to have some we also need to have some representation is not a compulsory thing representation is not a compulsory thing to have a representation of the problem to have a representation of the problem statement in a graphical kind of form statement in a graphical kind of form but this has been really the norm but this has been really the norm in this kind of domain to in this kind of domain to represent random variabl as nodes and", "image_path": "img_data/video_43_chunk_55.jpg"}
{"video": "video_43", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "in this kind of domain to represent random variabl as nodes and represent random variabl as nodes and with as edges with arrows the with as edges with arrows the transition the evolution of those transition the evolution of those probability distributions in fact these probability distributions in fact these arrows in this case will indicate arrows in this case will indicate dependencies we will be dependencies we will be calling this representation the dynamic calling this representation the dynamic basia network it's dynamic because it's going network it's dynamic because it's going to be time varying and basian because we will be creating if you", "image_path": "img_data/video_43_chunk_56.jpg"}
{"video": "video_43", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "varying and basian because we will be creating if you because we will be creating if you a filter as we will call it call a filter as we will call it call a basian kind of filter that it will be basian kind of filter that it will be able to give us a good estimates of able to give us a good estimates of what we'll be calling now a hidden state what we'll be calling now a hidden state let me write it first in this kind of basian network first in this kind of basian network which is also known in this case as a which is also known in this case as a hidden mark of model or", "image_path": "img_data/video_43_chunk_57.jpg"}
{"video": "video_43", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "or hmm we will represent it hmm we will represent it as the states are represented with as the states are represented with this kind of circles i'm going to write just three circles i'm going to write just three state st minus one this is obviously my hidden state one this is obviously my hidden state i do not know that kind of state it is i do not know that kind of state it is that state that it is in that kind of that state that it is in that kind of environment kind of box and that environment kind of box and that state is dependent from the action because the", "image_path": "img_data/video_43_chunk_58.jpg"}
{"video": "video_43", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "is dependent from the action because the action will inu this transition from st minus one to st and then we have evidently we know the action we have evidently we know the action we will assume that we know the action that will assume that we know the action that the robot or the agent has actually the robot or the agent has actually taken and in the literature we are", "image_path": "img_data/video_43_chunk_59.jpg"}
{"video": "video_43", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "the robot or the agent has actually taken and in the literature we are taken and in the literature we are shading this whenever you see a shading this whenever you see a shaded kind of node then this kind shaded kind of node then this kind of node is not a random of node is not a random variable but it is a known value it variable but it is a known value it takes a known value in fact i don't need takes a known value in fact i don't need this dots because i only going to draw this dots because i only going to draw just three states from st minus1 to st +1 st + 1 and this is going to be my", "image_path": "img_data/video_43_chunk_60.jpg"}
{"video": "video_43", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "+1 st + 1 and this is going to be my 80 + 1 action and the other node is the measurements the measurement measurements the measurement node obviously i'm going to be assuming node obviously i'm going to be assuming here that i'm going to be getting some here that i'm going to be getting some estimate out of the measurement some estimate out of the measurement some kind of a row value from a sensor or kind of a row value from a sensor or from as a predicted value from let's from as a predicted value from let's say an object", "image_path": "img_data/video_43_chunk_61.jpg"}
{"video": "video_43", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "from as a predicted value from let's say an object detector zt and we have zt + zt and we have zt + [music] [music] one and this continues into the one and this continues into the future the arrows of course are the future the arrows of course are the dependency the measurement i'm getting dependency the measurement i'm getting evidently is dependent on the state i am evidently is dependent on the state i am and the action and the current state and the action and the current state is inducing that kind of state", "image_path": "img_data/video_43_chunk_62.jpg"}
{"video": "video_43", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "and the action and the current state is inducing that kind of state is inducing that kind of state transition we will be need transition we will be need now to define what we'll be calling some two to define what we'll be calling some two key posterior probabilities that represent the probabilities that represent the state the and this will be called the state the and this will be called the beliefs two different versions of the beliefs two different versions of the beliefs that we have about where about beliefs that we have about where about we are in terms of state note we are in terms of state note that the state is the hidden node a that the state is the hidden node a hidden", "image_path": "img_data/video_43_chunk_63.jpg"}
{"video": "video_43", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "that the state is the hidden node a hidden variable and that's why the model is called hidden markov because we don't called hidden markov because we don't have any dependency from this state have any dependency from this state other than the previous state and the other than the previous state and the current action and that is what was expressed action and that is what was expressed over here and model because evidently over here and model because evidently we are dealing with these two we are dealing with these two probabilistic models that actually probabilistic models that actually define this diagram and this is also known in the", "image_path": "img_data/video_43_chunk_64.jpg"}
{"video": "video_43", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "define this diagram and this is also known in the diagram and this is also known in the past as probabilistic graphical models past as probabilistic graphical models and we had people have been devising and we had people have been devising for a couple of decades now for a couple of decades now specific algorithms to be able to specific algorithms to be able to provide inferences do inferences given provide inferences do inferences given this kind of probabilistic dependencies this kind of probabilistic dependencies between random variables the between random variables the first posterior which i'll be calling first posterior which i'll be calling the belief hatut of st and i'll will symbolize it with", "image_path": "img_data/video_43_chunk_65.jpg"}
{"video": "video_43", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "of st and i'll will symbolize it with belief of the state st but this hatut belief of the state st but this hatut will be mean something is the will be mean something is the probability of s of t given probability of s of t given a from one to t and previous measurements", "image_path": "img_data/video_43_chunk_66.jpg"}
{"video": "video_43", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "t and previous measurements now we will simp simplify this posterior also but this belief heart will be the this belief heart will be the belief will indicate the belief of being in state s of t before that's the emphasis before taking", "image_path": "img_data/video_43_chunk_67.jpg"}
{"video": "video_43", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "that's the emphasis before taking measurements before receiving measurement zt and that's why the index ends at z zt and that's why the index ends at z t minus one this is the belief t minus one this is the belief and we'll see now the belief c and this and we'll see now the belief c and this is we'll see in a moment how is we'll see in a moment how the two are involved together how the two are involved together how they we are going to use them the", "image_path": "img_data/video_43_chunk_68.jpg"}
{"video": "video_43", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "the two are involved together how they we are going to use them the they we are going to use them the second posterior probability second posterior probability distribution that i want probability i distribution that i want probability i want to specify is the belief of want to specify is the belief of being in state st and this is the posterior p of st given a from one to t and c from one to t this time t and c from one to t this time it's exactly the same thing the", "image_path": "img_data/video_43_chunk_69.jpg"}
{"video": "video_43", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "t and c from one to t this time it's exactly the same thing the belief after a receiving measurement zt after receiving r city these are the two i will call it key these are the two i will call it key posteriors that we will need to", "image_path": "img_data/video_43_chunk_70.jpg"}
{"video": "video_43", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "these are the two i will call it key posteriors that we will need to posteriors that we will need to start finding a way recursive way of start finding a way recursive way of estimating evidently we need this and estimating evidently we need this and then what was going to happen in this then what was going to happen in this kind of recursion is that if we have kind of recursion is that if we have given if you the diagram if we have given if you the diagram if we have an estimate of the belief of an estimate of the belief of this kind of state in an estimate a this kind of state in an estimate a posterior probability of being in that posterior probability of being in that kind of state then we'll be updated kind of state then we'll be updated meaning that in the next time this", "image_path": "img_data/video_43_chunk_71.jpg"}
{"video": "video_43", "start": "0:36:00", "end": "0:36:10.066667", "timestamp": "0:36:00 - 0:36:10.066667", "text": "kind of state then we'll be updated meaning that in the next time this meaning that in the next time this will actually be used next time to will actually be used next time to come with an estimate of this posterior come with an estimate of this posterior at t +1 now we'll need to see how this at t +1 now we'll need to see how this is going to be done", "image_path": "img_data/video_43_chunk_72.jpg"}
{"video": "video_44", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now we are going to expand beyond metrics and actually start covering the metrics and actually start covering the u various kind of object detections u various kind of object detections detectors that we actually see and we detectors that we actually see and we can actually broadly categorize them can actually broadly categorize them actually shown in this picture as kind actually shown in this picture as kind of a single stage or a two stage of a single stage or a two stage detectors and there's a kind of a detectors and there's a kind of a trade-off between the two the u trade-off between the two the u single stage detectors are actually single stage detectors are actually typically simpler and they are more typically simpler and they are more appropriate for target devices", "image_path": "img_data/video_44_chunk_0.jpg"}
{"video": "video_44", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "typically simpler and they are more appropriate for target devices appropriate for target devices such as smartphones and other kind of lower smartphones and other kind of lower power devices and they are quite power devices and they are quite efficient in what they do however in efficient in what they do however in terms of performance they're not really terms of performance they're not really as performant as the two- stage as performant as the two- stage detectors and the two here we are detectors and the two here we are going to be focusing on two stage going to be focusing on two stage detectors and we are going to be detectors and we are going to be treating the -cal region based treating the -cal region based family of object detectors and that will family of object detectors and that will take us", "image_path": "img_data/video_44_chunk_1.jpg"}
{"video": "video_44", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "family of object detectors and that will take us all the way from rcnn faster cnn all the way from rcnn faster cnn faster rcnn and this is basically rcnn and this is basically faster rn is also the last kind of faster rn is also the last kind of object detector we're going to see that object detector we're going to see that it is almost considered today as a bread it is almost considered today as a bread and but for object detector even in the and but for object detector even in the year 2023 where this video is actually sh 2023 where this video is actually sh faster cnn is considered to be faster cnn is considered to be a mainstream object detector several", "image_path": "img_data/video_44_chunk_2.jpg"}
{"video": "video_44", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "faster cnn is considered to be a mainstream object detector several a mainstream object detector several years after its original appearance years after its original appearance because it is very performant it is very because it is very performant it is very intuitive to understand and also intuitive to understand and also potentially to tune and has been potentially to tune and has been deployed to realtime systems which deployed to realtime systems which are and the mission kind of are and the mission kind of critical side robotics and on as critical side robotics and on as well as video surveillance and i also well as video surveillance and i also wanted to mention it that definitely wanted to mention it that definitely we have advance advancements on this we have advance advancements on this topic with transformer based topic with transformer based architectures the visual transformance", "image_path": "img_data/video_44_chunk_3.jpg"}
{"video": "video_44", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "topic with transformer based architectures the visual transformance architectures the visual transformance and on and attention based mechanism and on and attention based mechanism many of them however are do suffer many of them however are do suffer from the latency problem and that from the latency problem and that does not allow us to deploy them in does not allow us to deploy them in realtime systems at least not yet realtime systems at least not yet we'll be focusing on rcnn based we'll be focusing on rcnn based approaches origion based approaches origion based approaches and try to understand them and then and try to understand them and then and perhaps at a subsequent iteration of and perhaps at a subsequent iteration of this we will treat transformer based this we will treat transformer based or attention based mechanism for", "image_path": "img_data/video_44_chunk_4.jpg"}
{"video": "video_44", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "this we will treat transformer based or attention based mechanism for or attention based mechanism for object detection we are going to see now the detection we are going to see now the first of the members of this family first of the members of this family that is started if you the whole that is started if you the whole family from that point onwards family from that point onwards several years ago called regional cnn several years ago called regional cnn and the regional cnn can be described in and the regional cnn can be described in a kind of a straightforward way of a kind of a straightforward way of course there are some i will call it u course there are some i will call it u algorithmic details that you should algorithmic details that you should not be very familiar with and", "image_path": "img_data/video_44_chunk_5.jpg"}
{"video": "video_44", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "algorithmic details that you should not be very familiar with and should not be very familiar with and i'll refer to those from the website i'll refer to those from the website from the course website when i'm from the course website when i'm actually reach this point but let's actually reach this point but let's assume that i have let's say a scene assume that i have let's say a scene that consists let's say of i don't that consists let's say of i don't know two objects let's say a table and a objects let's say a table and a person over there and the first thing person over there and the first thing that actually happens and that's why that actually happens and that's why i took actually the name region is i took actually the name region is that we need to come up with some region that we need to come up with some region proposals that it will allow proposals that it will allow subsequent kind of stages to further", "image_path": "img_data/video_44_chunk_6.jpg"}
{"video": "video_44", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "proposals that it will allow subsequent kind of stages to further subsequent kind of stages to further refine and make up their mind with refine and make up their mind with respect to whether they contain an respect to whether they contain an object or not but right now at this very object or not but right now at this very st very first stage i will call this st very first stage i will call this kind of first stage segmentation in this stage is we are we need some kind stage is we are we need some kind of a data structure we'll call it of a data structure we'll call it this data structure a graph the graph this data structure a graph the graph is typically evidently and is consist", "image_path": "img_data/video_44_chunk_7.jpg"}
{"video": "video_44", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "this data structure a graph the graph is typically evidently and is consist is typically evidently and is consist of vertices and edges and with of vertices and edges and with vertices we are associated with vertices we are associated with vertices a pixel vertices are pixels let's say vi that belongs to the set of vertices belongs to the set of vertices v and with edges we represent here as", "image_path": "img_data/video_44_chunk_8.jpg"}
{"video": "video_44", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "edges we represent here as we represent here as pairs of neighboring pixels and we do define also a the similarity or similarity you can think similarity or similarity you can think about in any way you want vi comma vj about in any way you want vi comma vj metric that i'll call here w some kind metric that i'll call here w some kind of weight i will call it let me", "image_path": "img_data/video_44_chunk_9.jpg"}
{"video": "video_44", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "metric that i'll call here w some kind of weight i will call it let me of weight i will call it let me write down as dis similarity between vi and vj let's assume that i define in this case the dissimilarity as let's say case the dissimilarity as let's say the light intensity of difference between vi intensity of difference between vi and vj in case that's one of the", "image_path": "img_data/video_44_chunk_10.jpg"}
{"video": "video_44", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "and vj in case that's one of the vj in case that's one of the potential dissimilarity matrix we can potential dissimilarity matrix we can actually have and during this actually have and during this segmentation kind of segmentation kind of stage we partition let me write stage we partition let me write this down a segmentation s is the partitioning of v into comp", "image_path": "img_data/video_44_chunk_11.jpg"}
{"video": "video_44", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "partitioning of v into comp components such that it's of component c that belongs to that segmentation s corresponds to a group of pixels", "image_path": "img_data/video_44_chunk_12.jpg"}
{"video": "video_44", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "corresponds to a group of pixels and that have high similarity it's actually useful now to go to our site over here and now to go to our site over here and actually see a visual example of actually see a visual example of that and let's see this kind of a that and let's see this kind of a graph based segmentation se", "image_path": "img_data/video_44_chunk_13.jpg"}
{"video": "video_44", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "that and let's see this kind of a graph based segmentation se graph based segmentation se ction what is the result of that what ction what is the result of that what we just discussed the ultimately we just discussed the ultimately we have an input image we are forming we have an input image we are forming some form of a graph to represent if you the graph to represent if you the segmentation problem and based on that segmentation problem and based on that graph and based on that kind of metric graph and based on that kind of metric that we just discussed the outcome of that we just discussed the outcome of this exercise is the and set of in this exercise is the and set of in initial", "image_path": "img_data/video_44_chunk_14.jpg"}
{"video": "video_44", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "this exercise is the and set of in initial segments that are produced and as you segments that are produced and as you can see here even slight light can see here even slight light differences between what is differences between what is definitely one semantic part of an definitely one semantic part of an object let's say the leg of the athlete object let's say the leg of the athlete over here will result into multiple over here will result into multiple segments we're not really looking segments we're not really looking here to do anything more intelligent here to do anything more intelligent than just looking at similarity or than just looking at similarity or the similarity between", "image_path": "img_data/video_44_chunk_15.jpg"}
{"video": "video_44", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "than just looking at similarity or the similarity between the similarity between pixels to be able to get these kind pixels to be able to get these kind of very rough ideas about an object and of very rough ideas about an object and in some instances even as in some instances even as you can see here in this kind of area we you can see here in this kind of area we may actually define a segment as may actually define a segment as well and that is have absolutely no well and that is have absolutely no semantic information as to what is semantic information as to what is behind it just basically based on color behind it just basically based on color and after the initial this kind of", "image_path": "img_data/video_44_chunk_16.jpg"}
{"video": "video_44", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "and after the initial this kind of initial this kind of regions are produced based on this regions are produced based on this similarity metric we will use now an similarity metric we will use now an algorithm that it will algorithm that it will is actually titled here the is actually titled here the hierarchical grouping algorithm to group hierarchical grouping algorithm to group these segments initial segments these segments initial segments together and we form at the output of together and we form at the output of this kind of grouping algorith we this kind of grouping algorith we that it is kind of iterative in nature that it is kind of iterative in nature and will start result it will it and will start result it will gradually kind of evolve in", "image_path": "img_data/video_44_chunk_17.jpg"}
{"video": "video_44", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "and will start result it will gradually kind of evolve in will gradually kind of evolve in getting from a situation we have plenty getting from a situation we have plenty of segments and to few segments of segments and to few segments and these fewer segments are and these fewer segments are ultimately going to be limited to 2,000 ultimately going to be limited to 2,000 we are going to be producing 2,000 of we are going to be producing 2,000 of those regions inside this region is those regions inside this region is basically one of those segments or basically one of those segments or more than one segment but more than one segment but the bottom line is that the algorith", "image_path": "img_data/video_44_chunk_18.jpg"}
{"video": "video_44", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "more than one segment but the bottom line is that the algorith the bottom line is that the algorith is going to result into these proposals is going to result into these proposals these are the regions that we propose these are the regions that we propose for further evaluation at a later kind for further evaluation at a later kind of stage you don't necessarily need of stage you don't necessarily need to although you're quite welcome to look to although you're quite welcome to look at the initial publication which is at the initial publication which is actually linked over here to understand actually linked over here to understand it in full detail of what is really it in full detail of what is really happening there given that those stages happening there given that those stages that i'm discussing right now are going that i'm discussing right now are going to be replaced at a later member of this to be replaced at a later member of this family that will be calling in faster r", "image_path": "img_data/video_44_chunk_19.jpg"}
{"video": "video_44", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "to be replaced at a later member of this family that will be calling in faster r family that will be calling in faster r cnn then you don't have to spend a cnn then you don't have to spend a lot of time just going through them lot of time just going through them just understand the high level picture just understand the high level picture of what we're trying to achieve and of what we're trying to achieve and focus a little bit more of your focus a little bit more of your attention at the faster cnn which attention at the faster cnn which is effectively doing exactly the same is effectively doing exactly the same work quite differently using a neura work quite differently using a neura network in fact here's the network in fact here's the proposals all these green proposals all these green boxes a proposal effectively means to", "image_path": "img_data/video_44_chunk_20.jpg"}
{"video": "video_44", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "proposals all these green boxes a proposal effectively means to boxes a proposal effectively means to the subsequent kind of stage a there is the subsequent kind of stage a there is something in there i do not know if it something in there i do not know if it is a object or a portion of an object is a object or a portion of an object but there must be some kind of an but there must be some kind of an objectness inside that kind of region objectness inside that kind of region that i want you to evaluate and the that i want you to evaluate and the subsequent kind of stage what it will subsequent kind of stage what it will actually do is will actually use actually do is will actually use convolutional neuron networks to extract convolutional neuron networks to extract features out of every proposal and you features out of every proposal and you kind of guessed it we have u kind of guessed it we have u regions that are", "image_path": "img_data/video_44_chunk_21.jpg"}
{"video": "video_44", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "kind of guessed it we have u regions that are transformed into rectangular regions of transformed into rectangular regions of 227 by 227 let's say pixels and the 227 by 227 let's say pixels and the output of this feature extractor which output of this feature extractor which could be a residual network as we have could be a residual network as we have seen in the convolutional new networks seen in the convolutional new networks this residual network or s net is able this residual network or s net is able to produce a vector out of the whole to produce a vector out of the whole region that represents what it is inside region that represents what it is inside creates representations of a length 496 creates representations of a length 496 elements this is at the high elements this is at the high level this is basically what the block", "image_path": "img_data/video_44_chunk_22.jpg"}
{"video": "video_44", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "elements this is at the high level this is basically what the block level this is basically what the block diagram is what rcnn diagram is what rcnn is actually doing proposal is going is actually doing proposal is going through the convolutional your through the convolutional your network and then the network and then the 4,096 element vector that comes out of 4,096 element vector that comes out of it is fed into a regressor and a it is fed into a regressor and a classifier and what is really the classifier and what is really the regression and the classifier is doing regression and the classifier is doing you probably have guessed it bas b on you probably have guessed it bas b on all the previous discussion about", "image_path": "img_data/video_44_chunk_23.jpg"}
{"video": "video_44", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "you probably have guessed it bas b on all the previous discussion about all the previous discussion about metrics we have to understand whether or metrics we have to understand whether or not we have the specific class that we not we have the specific class that we are interested in present there we at are interested in present there we at the time they were using a method the time they were using a method that's called support vector machines that's called support vector machines and that is basically the and that is basically the classification that results into kind classification that results into kind of k classes and in terms of regression classes and in terms of regression the regression part is the regression part is responsible producing the bounding box", "image_path": "img_data/video_44_chunk_24.jpg"}
{"video": "video_44", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "the regression part is responsible producing the bounding box responsible producing the bounding box coordinates for that specific object coordinates for that specific object that we believe that it is present in that we believe that it is present in the inside that kind of proposal or the inside that kind of proposal or objects we have the another algorith that it the another algorith that it will process this kind of bounding will process this kind of bounding boxes and it will it's called non-max boxes and it will it's called non-max suppression and this based on the suppression and this based on the metrical intersection over union a metrical intersection over union a metric we have seen also during the", "image_path": "img_data/video_44_chunk_25.jpg"}
{"video": "video_44", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "metrical intersection over union a metric we have seen also during the metric we have seen also during the metrix discussion is going to select out metrix discussion is going to select out of the many bounding boxes that are of the many bounding boxes that are being produced one and that will be the being produced one and that will be the predicted bounding box that is going to predicted bounding box that is going to be the embedded into the y hat into our be the embedded into the y hat into our prediction this is basically at a prediction this is basically at a high level what is happening the u high level what is happening the u segments to bigger segments to regions to evaluation of the to regions to evaluation of the to feature extraction to classification", "image_path": "img_data/video_44_chunk_26.jpg"}
{"video": "video_44", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "regions to evaluation of the to feature extraction to classification feature extraction to classification and regression and that was the block and regression and that was the block diagram of the rcnn algorith diagram of the rcnn algorith evidently we are dealing with a evidently we are dealing with a rcnn algorithm that it is non- rcnn algorithm that it is non- differentiable end to endend because we differentiable end to endend because we have we don't have a neural network have we don't have a neural network architecture that it is going to be architecture that it is going to be applied end to end the only neural applied end to end the only neural network that is actually present over network that is actually present over here is the feature extractor the here is the feature extractor the whole training process is whole training process is was very complicated but worse than", "image_path": "img_data/video_44_chunk_27.jpg"}
{"video": "video_44", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "whole training process is was very complicated but worse than was very complicated but worse than that we had some significant that we had some significant latency and when i say significant latency and when i say significant and completely unusable latency even for and completely unusable latency even for batch pring jobs even for offline batch pring jobs even for offline processing jobs and every frame was processing jobs and every frame was being processed by 2,000 being processed by 2,000 convolutional neural networks and convolutional neural networks and that number itself tells a lot about that number itself tells a lot about the perceived latency the latency we are", "image_path": "img_data/video_44_chunk_28.jpg"}
{"video": "video_44", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "that number itself tells a lot about the perceived latency the latency we are the perceived latency the latency we are getting is on the order of many seconds getting is on the order of many seconds and we needed few milliseconds and we needed few milliseconds and that's for that kind of reason this that's for that kind of reason this architecture over here was replaced architecture over here was replaced with an another architecture that we with an another architecture that we will see now and it is called fast will see now and it is called fast rcnn moving on now to the fast rcnn moving on now to the fast rcnn the in the faster cnn we actually see the in the faster cnn we actually see already some improvements that it will already some improvements that it will actually help us a lot in reducing", "image_path": "img_data/video_44_chunk_29.jpg"}
{"video": "video_44", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "already some improvements that it will actually help us a lot in reducing actually help us a lot in reducing the latency the first improvement the latency the first improvement that actually we see here is we don't that actually we see here is we don't have 2,000 cnn anymore we have a single have 2,000 cnn anymore we have a single cnn and this has if you an cnn and this has if you an architecture suitable for feature architecture suitable for feature extraction we are now extracting a extraction we are now extracting a feature that it is from the feature that it is from the whole image rather than a single whole image rather than a single region we maintain if you region we maintain if you the selective search that we have", "image_path": "img_data/video_44_chunk_30.jpg"}
{"video": "video_44", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "region we maintain if you the selective search that we have the selective search that we have actually seen in the previous kind actually seen in the previous kind of generation the rcnn to produce of generation the rcnn to produce these region proposals and the but these region proposals and the but now these regions are suggested based now these regions are suggested based on the features extracted from the on the features extracted from the whole fe cnn for the whole image whole fe cnn for the whole image we actually doing this selective we actually doing this selective search at a fature space search at a fature space which of course reduces the computation", "image_path": "img_data/video_44_chunk_31.jpg"}
{"video": "video_44", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "search at a fature space which of course reduces the computation which of course reduces the computation and then for each of the and then for each of the proposals what we have here that proposals what we have here that we have a region of interest and roi as we have a region of interest and roi as we call it pulling layer that will we call it pulling layer that will extract a fixed length feature vector extract a fixed length feature vector from the feature map we have a from the feature map we have a feature map this is now as we said in feature map this is now as we said in convolutional networks is the what is convolutional networks is the what is being produced to represent if you being produced to represent if you this whole image over here and", "image_path": "img_data/video_44_chunk_32.jpg"}
{"video": "video_44", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "being produced to represent if you this whole image over here and this whole image over here and we are going to suggest proposals in we are going to suggest proposals in this feature map and for each of the this feature map and for each of the proposals we have the an roi pulling proposals we have the an roi pulling layer that it will extract a feature layer that it will extract a feature vector from the feature map vector from the feature map that's what is really this kind of that's what is really this kind of pulling layer what is really is pulling layer what is really is doing the roi pulling layer converts the doing the roi pulling layer converts the features inside any region of", "image_path": "img_data/video_44_chunk_33.jpg"}
{"video": "video_44", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "features inside any region of interest into a small or proposal interest into a small or proposal into a small feature map that with has a into a small feature map that with has a very specific special extent it is for very specific special extent it is for example 7 by7 and the max pooling example 7 by7 and the max pooling layers are actually present in there layers are actually present in there takes the initial h by w small takes the initial h by w small letters h by w proposal window and it letters h by w proposal window and it is dividing into a grid each of these", "image_path": "img_data/video_44_chunk_34.jpg"}
{"video": "video_44", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "letters h by w proposal window and it is dividing into a grid each of these is dividing into a grid each of these grids is a grid of u 7 by seven sub grids is a grid of u 7 by seven sub windows and of course it is selecting windows and of course it is selecting one feature because it's a max one feature because it's a max pooling as we've seen in convolutional pooling as we've seen in convolutional networks it's selecting the maximum networks it's selecting the maximum feature out of every sub window and feature out of every sub window and that's basically what is been produced that's basically what is been produced at the output we have u at the output we have u effectively from the features of each", "image_path": "img_data/video_44_chunk_35.jpg"}
{"video": "video_44", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "at the output we have u effectively from the features of each effectively from the features of each proposal via the roi pulling layer we proposal via the roi pulling layer we have this 7x7 feature map which is have this 7x7 feature map which is being produced and this takes the being produced and this takes the form of a vector which is flattened to form of a vector which is flattened to create a vector each feature vector create a vector each feature vector then is it is fed subsequently to the then is it is fed subsequently to the fully connected layers that are fully connected layers that are going to create the information which is required to", "image_path": "img_data/video_44_chunk_36.jpg"}
{"video": "video_44", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "create the information which is required to the information which is required to feed the regression and the feed the regression and the classification branches classification branches the evidently the soft max is producing a k a length k soft max is producing a k a length k posterior probability distribution and the bound box distribution and the bound box regressor is producing a four times k", "image_path": "img_data/video_44_chunk_37.jpg"}
{"video": "video_44", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "distribution and the bound box regressor is producing a four times k regressor is producing a four times k candidate regression boxes for candidate regression boxes for each of these proposals each of these proposals that's basically the architecture we that's basically the architecture we have three we have replacement of have three we have replacement of the -called classic machine the -called classic machine learning methods support vector learning methods support vector machines and regression into machines and regression into support into fully connected based support into fully connected based regression classifiers respectively as", "image_path": "img_data/video_44_chunk_38.jpg"}
{"video": "video_44", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "support into fully connected based regression classifiers respectively as regression classifiers respectively as well also have the rep placement of u well also have the rep placement of u the 2000 convolution neur networks we do the 2000 convolution neur networks we do maintain though nondifferentiable blocks though nondifferentiable blocks the generation of the proposals it's non the generation of the proposals it's non differentiable the training is still differentiable the training is still somewhat kind of complicated but somewhat kind of complicated but definitely it has been definitely it has been improved from what we have seen earlier improved from what we have seen earlier the second and the you", "image_path": "img_data/video_44_chunk_39.jpg"}
{"video": "video_44", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "improved from what we have seen earlier the second and the you the second and the final destination is really the know final destination is really the faster rcnn and this is where we pay faster rcnn and this is where we pay quite a lot of attention because that quite a lot of attention because that is really the sort of architecture is really the sort of architecture that we will be using for executing that we will be using for executing some of the tasks and demonstrating some of the tasks and demonstrating object detection inside this rpn there object detection inside this rpn there is this kind of concept of anchors and is this kind of concept of anchors and to understand exactly why we have this to understand exactly why we have this concept over there i think we need to concept over there i think we need to understand how typically what we", "image_path": "img_data/video_44_chunk_40.jpg"}
{"video": "video_44", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "concept over there i think we need to understand how typically what we understand how typically what we call multi-stage detection sorry not multistate but multiscale detection is done multis skate detection is going to be done now for detection is going to be done now for using three typically three methods using three typically three methods the first actually i will call it the", "image_path": "img_data/video_44_chunk_41.jpg"}
{"video": "video_44", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "using three typically three methods the first actually i will call it the first actually i will call it the sort of pyramid of images in this sort of pyramid of images in this kind of option this is the pyramid of images and before we go into this kind of discussion of these options this kind of discussion of these options what we are trying to achieve what we are trying to achieve with the -cal multiscale detection is with the -cal multiscale detection is evidently we would to have exactly evidently we would to have exactly the same level of detection as we have the same level of detection as we have performance if we have small objects", "image_path": "img_data/video_44_chunk_42.jpg"}
{"video": "video_44", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "the same level of detection as we have performance if we have small objects performance if we have small objects or larger kind of objects and u for or larger kind of objects and u for example coco is also notoriously known example coco is also notoriously known to contain many object which are kind of to contain many object which are kind of a small in scale which are more a small in scale which are more difficult of course to detect than difficult of course to detect than larger kind of objects that occupy a larger kind of objects that occupy a much larger portion of the image if you much larger portion of the image if you and what we need to do here and what we need to do here is to allow to train the object is to allow to train the object detector to be able to accommodate detector to be able to accommodate objects of various scales if you can objects of various scales if you can imagine kind of trying to approach imagine kind of trying to approach the camera and maybe i can sort of", "image_path": "img_data/video_44_chunk_43.jpg"}
{"video": "video_44", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "imagine kind of trying to approach the camera and maybe i can sort of the camera and maybe i can sort of use my hand now to demonstrate this use my hand now to demonstrate this if i'm approaching my hand then this if i'm approaching my hand then this my hand is actually becoming larger and my hand is actually becoming larger and larger we' to maintain larger we' to maintain exactly the same performance for no exactly the same performance for no matter what the scale of the object is matter what the scale of the object is in this kind of first approach this in this kind of first approach this kind of pyramid of images we have kind of pyramid of images we have let's say an image that it is sort of let's say an image that it is sort of double w by h pixels and we are", "image_path": "img_data/video_44_chunk_44.jpg"}
{"video": "video_44", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "let's say an image that it is sort of double w by h pixels and we are double w by h pixels and we are forming and we are working with this forming and we are working with this 3x3 spatial dimensionality of 3x3 spatial dimensionality of convolutional filters in that kind of convolutional filters in that kind of image but at the same time we would image but at the same time we would to work with that images which work with that images which are half of the size of the original are half of the size of the original image still maintaining if you the image still maintaining if you the same", "image_path": "img_data/video_44_chunk_45.jpg"}
{"video": "video_44", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "image still maintaining if you the same 3x3 filter over there in terms of 3x3 filter over there in terms of spatial extent we are effectively u spatial extent we are effectively u growing the receptive field of growing the receptive field of this convolutional filter as this convolutional filter as related to the size of the image we related to the size of the image we are forming a pyramid of images are forming a pyramid of images of h by w h of 2 by w/2 h over", "image_path": "img_data/video_44_chunk_46.jpg"}
{"video": "video_44", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "are forming a pyramid of images of h by w h of 2 by w/2 h over images of h by w h of 2 by w/2 h over 4 by w over 4 and on and we are 4 by w over 4 and on and we are effectively u trying to create effectively u trying to create input data for u all sorts of scales input data for u all sorts of scales by adjusting the image size itself now by adjusting the image size itself now the second option we are forming option the second option we are forming the -called pyramid of filters and this pyramid of filters option we are kind of forming an", "image_path": "img_data/video_44_chunk_47.jpg"}
{"video": "video_44", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "filters and this pyramid of filters option we are kind of forming an option we are kind of forming an equivalent situation in this case we are situation in this case we are not adjusting the images but what we do not adjusting the images but what we do adjust is we adjust the size of the adjust is we adjust the size of the convolutional filters that we have for convolutional filters that we have for example this is a 3x3 this is a 9 by9 example this is a 3x3 this is a 9 by9 and on is equivalent to that and on is equivalent to that and of course there are some publications of course there are some publications prefer this option two compared to", "image_path": "img_data/video_44_chunk_48.jpg"}
{"video": "video_44", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "of course there are some publications prefer this option two compared to prefer this option two compared to option one u for in terms of option one u for in terms of ability to accommodate multiscale ability to accommodate multiscale detection but it's kind of beyond the detection but it's kind of beyond the scope to see all these kind of scope to see all these kind of details the approach that faster kind details the approach that faster kind of adopted is the -called pyramid this is the -cal option three and this is the pyramid of ankors", "image_path": "img_data/video_44_chunk_49.jpg"}
{"video": "video_44", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "pyramid this is the -cal option three and this is the pyramid of ankors pyramid of anchors sometimes these anchors are called priors and what are those anchors and what do they do i will draw now a what do they do i will draw now a larger kind of picture an image of let's say w by high by h pixels and i'm going to form", "image_path": "img_data/video_44_chunk_50.jpg"}
{"video": "video_44", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "w by high by h pixels and i'm going to form pixels and i'm going to form anchors by and this is basically anchors by and this is basically the center of i'm going to the center of a i'm going to the center of a number of anchors which are number of anchors which are effectively windows or regions if i call it this rectangular regions they have this rectangular regions they have the soal anchor boxes that are the soal anchor boxes that are multiple scales they have multiple scales they have multiple scales and aspect ratios", "image_path": "img_data/video_44_chunk_51.jpg"}
{"video": "video_44", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "multiple scales they have multiple scales and aspect ratios scales and aspect ratios they will have a each one of them they will have a each one of them is going to have a certain scale and is going to have a certain scale and aspect ratio and we have multiple of aspect ratio and we have multiple of those let me draw some those let me draw some more maybe this one u maybe this one different scale these are examples of scale these are examples of the thing that actually", "image_path": "img_data/video_44_chunk_52.jpg"}
{"video": "video_44", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "scale these are examples of the thing that actually the thing that actually happens here is that the filter as we have seen earlier in option two it was varing but now the option two it was varing but now the filter and the image is kept constant throughout the pyramid and we have and we", "image_path": "img_data/video_44_chunk_53.jpg"}
{"video": "video_44", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "we have and we define k anor boxes anor anchor boxes of varus scales and aspect ratios let's assume that we have let's say", "image_path": "img_data/video_44_chunk_54.jpg"}
{"video": "video_44", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "let's assume that we have let's say three scales and three aspect ratios then we have let's say nine anor boxes we now define a sliding window this pixel over here is the window this pixel over here is the center of this kind of sliding window center of this kind of sliding window that the for each of these", "image_path": "img_data/video_44_chunk_55.jpg"}
{"video": "video_44", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "center of this kind of sliding window that the for each of these that the for each of these locations we produce a vector of locations we produce a vector of features for we define also window that for each location we produce a vector of", "image_path": "img_data/video_44_chunk_56.jpg"}
{"video": "video_44", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "location we produce a vector of features and u effectively what we actually have here this kind of actually have here this kind of feature vector is going to be an feature vector is going to be an extension i mean we're going to be extension i mean we're going to be created from a convolutional network created from a convolutional network which is an extension to the main which is an extension to the main network if i go back to this to network if i go back to this picture over here we have", "image_path": "img_data/video_44_chunk_57.jpg"}
{"video": "video_44", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "network if i go back to this picture over here we have this picture over here we have let's say the cnn and which is the let's say the cnn and which is the main network over here and we are going main network over here and we are going actually to add layers we will be actually to add layers we will be sharing some of the layers of this of sharing some of the layers of this cnn and adding some more layers this cnn and adding some more layers on top of them in order to form the on top of them in order to form the -called region proposal network the -called region proposal network the region proposal network is not region proposal network is not completely decoupled branch from the main branch", "image_path": "img_data/video_44_chunk_58.jpg"}
{"video": "video_44", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "completely decoupled branch from the main branch decoupled branch from the main branch if you but is sharing some portion if you but is sharing some portion of those layers with the main with of those layers with the main cnn if i may draw now the u cnn if i may draw now the u sort of picture over here towards the sort of picture over here towards the bottom i have the sort of backbone have the sort of backbone cnn that receives this input image cnn that receives this input image x and it has a number of layers some of x and it has a number of layers some of them are going to be shared between the rpn", "image_path": "img_data/video_44_chunk_59.jpg"}
{"video": "video_44", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "them are going to be shared between the rpn are going to be shared between the rpn and the backbone in terms of feature vectors sorry feature maps that they sorry feature maps that they produce and at the output of this we produce and at the output of this we have the production of a feature vector and then this feature vector is fed to classifiers and to regressions", "image_path": "img_data/video_44_chunk_60.jpg"}
{"video": "video_44", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "to classifiers and to regressions classifiers and to regressions regressors that are producing for each regressors that are producing for each of the anchors the following decisions of the anchors the following decisions we have k classification results and we have k classification results and 4k because four are the number of 4k because four are the number of coordinates of each bounding box coordinates of each bounding box regression results and for this we'll have results and for this we'll have the rpn effectively this network that you rpn effectively this network that you actually see here is", "image_path": "img_data/video_44_chunk_61.jpg"}
{"video": "video_44", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "rpn effectively this network that you actually see here is responsible for generating the following output answer the following output answer the question is there an object in the jf anchor and the regressor", "image_path": "img_data/video_44_chunk_62.jpg"}
{"video": "video_44", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "anchor and the regressor to produce coordinates for each of the k anchors this is the high level picture of course there are many details picture of course there are many details many of these details are contained in many of these details are contained in the original kind of paper which i have the original kind of paper which i have over here and i invite you also to get over here and i invite you also to get the print out or get it from the", "image_path": "img_data/video_44_chunk_63.jpg"}
{"video": "video_44", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "over here and i invite you also to get the print out or get it from the print out or get it from the internet and go over some of the internet and go over some of the as kind of primarily reading exercise to as kind of primarily reading exercise to see exactly how all these details are see exactly how all these details are coming together however we i'm going to coming together however we i'm going to provide some more details what is provide some more details what is happening with respect to training happening with respect to training now but at the high level picture i hope now but at the high level picture i hope everyone is comfortable in everyone is comfortable in understanding what is happening we understanding what is happening we are implementing the -call pyramid are implementing the -call pyramid of anchors that corresponds of u nine", "image_path": "img_data/video_44_chunk_64.jpg"}
{"video": "video_44", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "are implementing the -call pyramid of anchors that corresponds of u nine of anchors that corresponds of u nine let's say anchors boxes for each of the let's say anchors boxes for each of the locations that we visit in this kind locations that we visit in this kind of sliding window scheme and for each of sliding window scheme and for each of these locations we are generating a these locations we are generating a feature vector sharing some of the feature vector sharing some of the layers of the backbone cnn and the layers of the backbone cnn and the main backbone and the rpn network are main backbone and the rpn network are not independent and this of course has some kind of and this of course has some kind of spillover effects on the training kind", "image_path": "img_data/video_44_chunk_65.jpg"}
{"video": "video_44", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "and this of course has some kind of spillover effects on the training kind spillover effects on the training kind of approach and for each of these of approach and for each of these feature vectors we define whether or feature vectors we define whether or not we have for these specific reference not we have for these specific reference boxes a an object which is being boxes a an object which is being created which is contained in there created which is contained in there and if this if there is an object and if this if there is an object and we also produce the corresponding we also produce the corresponding coordinates for that specific u or coordinates for that specific u or that we believe that it contains the that we believe that it contains the object that is basically the", "image_path": "img_data/video_44_chunk_66.jpg"}
{"video": "video_44", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "that we believe that it contains the object that is basically the object that is basically the high level kind of view and now what we high level kind of view and now what we need to see now let's look at an need to see now let's look at an example on how we are going to example on how we are going to basically train the rpn evidently we basically train the rpn evidently we need to have a training kind of data need to have a training kind of data set that is very specific to proposals set that is very specific to proposals to answer this type of questions whether to answer this type of questions whether or not let's say we have an object in or not let's say we have an object in the jf anor because the original data the jf anor because the original data set has only in boxes for the object", "image_path": "img_data/video_44_chunk_67.jpg"}
{"video": "video_44", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "the jf anor because the original data set has only in boxes for the object set has only in boxes for the object here we need information for on the here we need information for on the anchor level we need to go from a anchor level we need to go from a data set that has ground truth data set that has ground truth information at the object level to information at the object level to ground truth information at the anchor ground truth information at the anchor level this is what i'm going to level this is what i'm going to draw next i'm going to draw here a draw next i'm going to draw here a picture another image over here that will contain let's say a person over", "image_path": "img_data/video_44_chunk_68.jpg"}
{"video": "video_44", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "here that will contain let's say a person over here and evidently at the kind of coco data set if might call this level data set if might call this level we have a bounding box the ground truth and let's say for this specific truth and let's say for this specific u location we are visiting in we may u location we are visiting in we may have maybe i throw it somewhere a little have maybe i throw it somewhere a little bit on the left on that i may have an bit on the left on that i may have an anchor", "image_path": "img_data/video_44_chunk_69.jpg"}
{"video": "video_44", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "bit on the left on that i may have an anchor this i may have an anchor this and let's draw also another anchor let's draw also another anchor this these are the all the anchors that this these are the all the anchors that i have sort of defined in u in i have sort of defined in u in what i call earlier 3x3 three scales and", "image_path": "img_data/video_44_chunk_70.jpg"}
{"video": "video_44", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "i have sort of defined in u in what i call earlier 3x3 three scales and what i call earlier 3x3 three scales and three aspect ratios given the three aspect ratios given the values the iu thresholds let's say 0.5 values the iu thresholds let's say 0.5 and 0.3 values and notation here is kind 0.3 values and notation here is kind of familiar from the earlier discussion of familiar from the earlier discussion on metrics for each of the anchors on metrics for each of the anchors what we go and what we do is we what we go and what we do is we use this kind of thres course to use this kind of thres course to classify these anchors as positive or classify these anchors as positive or negative and to annotate i negative and to annotate i should say the anchors as positive or", "image_path": "img_data/video_44_chunk_71.jpg"}
{"video": "video_44", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "negative and to annotate i should say the anchors as positive or should say the anchors as positive or nega or negative for any anchor whose intersection of a union with the ground truth bounding box union with the ground truth bounding box is greater than tp then this is going to be a tp then this is going to be a positive anchor and an anchor for iou less than dn is going to be a negative", "image_path": "img_data/video_44_chunk_72.jpg"}
{"video": "video_44", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "anchor and an anchor for iou less than dn is going to be a negative than dn is going to be a negative anchor and anor which lie in between anchor and anor which lie in between let's say tn less than or equal to let's say tn less than or equal to iou less than or equal to tp these are iou less than or equal to tp these are going to be called neutral this is how we are actually taking a data set which is actually taking a data set which is originally specified at a obite kind originally specified at a obite kind of level and we convert it to a data set", "image_path": "img_data/video_44_chunk_73.jpg"}
{"video": "video_44", "start": "0:37:00", "end": "0:37:30", "timestamp": "0:37:00 - 0:37:30", "text": "originally specified at a obite kind of level and we convert it to a data set of level and we convert it to a data set which is at the anchor which is at the anchor level during training we'll be using level during training we'll be using positive and negative only we'll be positive and negative only we'll be ignoring the soal neutral kind of ignoring the soal neutral kind of anchors and then we will need to define anchors and then we will need to define now our kind of a loss function which is now our kind of a loss function which is now a composition of the loss function that composition of the loss function that associated with classification and associated with classification and regression and typically what we have regression and typically what we have here is we have the this type of here is we have the this type of losses we have let's say y comma y", "image_path": "img_data/video_44_chunk_74.jpg"}
{"video": "video_44", "start": "0:37:30", "end": "0:38:00", "timestamp": "0:37:30 - 0:38:00", "text": "here is we have the this type of losses we have let's say y comma y losses we have let's say y comma y hat this is going to be associated hat this is going to be associated with some kind of a form of a with some kind of a form of a summation of the cross summation of the cross entropy associated with the anchor i this y i are the ones that we obtain", "image_path": "img_data/video_44_chunk_75.jpg"}
{"video": "video_44", "start": "0:38:00", "end": "0:38:30", "timestamp": "0:38:00 - 0:38:30", "text": "i this y i are the ones that we obtain at the anchor level the labels we at the anchor level the labels we obtain with this iou overlapping method obtain with this iou overlapping method just now and why hat are going to be just now and why hat are going to be the predictions of the classifier that the predictions of the classifier that we have drawn earlier for on at for each we have drawn earlier for on at for each anchor and this is going to be your anchor and this is going to be your in fact i think i called the anchor in fact i think i called the anchor earlier with a small letter k let me earlier with a small letter k let me just change this to maintain kind of just change this to maintain kind of consistency all right and then this is u", "image_path": "img_data/video_44_chunk_76.jpg"}
{"video": "video_44", "start": "0:38:30", "end": "0:39:00", "timestamp": "0:38:30 - 0:39:00", "text": "just change this to maintain kind of consistency all right and then this is u consistency all right and then this is u this is going to be some summation this is going to be some summation where we add to the classification where we add to the classification result some other summation which result some other summation which associated with the regression and associated with the regression and the regression is a loss which is called the a loss which is called the huber loss for aggression huber loss for aggression and over", "image_path": "img_data/video_44_chunk_77.jpg"}
{"video": "video_44", "start": "0:39:00", "end": "0:39:30", "timestamp": "0:39:00 - 0:39:30", "text": "huber loss for aggression and over here we can we multiply with the here we can we multiply with the ground prooof label which is either one ground prooof label which is either one or zero to allow only positive or zero to allow only positive anchors to be penalizing the total loss anchors to be penalizing the total loss otherwise doesn't really make any sense otherwise doesn't really make any sense to penalize the total loss for anchors to penalize the total loss for anchors which are negative therefore we don't which are negative therefore we don't have any kind of bounding box let me have any kind of bounding box let me write this down only positive anchors", "image_path": "img_data/video_44_chunk_78.jpg"}
{"video": "video_44", "start": "0:39:30", "end": "0:40:00", "timestamp": "0:39:30 - 0:40:00", "text": "anchors are penalizing the regression loss of course also in terms of indexing we also recognize that if we do we also recognize that if we do the training on a with typically mini the training on a with typically mini batches we have more than one of batches we have more than one of those frames and therefore the those frames and therefore the associated indices will be at a sort of", "image_path": "img_data/video_44_chunk_79.jpg"}
{"video": "video_44", "start": "0:40:00", "end": "0:40:30", "timestamp": "0:40:00 - 0:40:30", "text": "those frames and therefore the associated indices will be at a sort of associated indices will be at a sort of anchor mini bats kind of level but anchor mini bats kind of level but that's kind of a minor kind of detail that's kind of a minor kind of detail let me also write down what this y k let me also write down what this y k over there represents is the over there represents is the probability that anchor contains an object this is basically the and what is really this huber loss and what is really this huber loss earlier we have seen mean square error", "image_path": "img_data/video_44_chunk_80.jpg"}
{"video": "video_44", "start": "0:40:30", "end": "0:41:00", "timestamp": "0:40:30 - 0:41:00", "text": "and what is really this huber loss earlier we have seen mean square error earlier we have seen mean square error we also have the mean absolute error which is the mean absolute error which is the function the absolute value function the absolute value function remember that in the regression case remember that in the regression case we have seen the need to sum to we have seen the need to sum to make all the differences between make all the differences between predictions and ground ruths predictions and ground ruths positive and one way of doing it with positive and one way of doing it with the squaring operation and that was the squaring operation and that was basically gives raise to the mean square", "image_path": "img_data/video_44_chunk_81.jpg"}
{"video": "video_44", "start": "0:41:00", "end": "0:41:30", "timestamp": "0:41:00 - 0:41:30", "text": "the squaring operation and that was basically gives raise to the mean square basically gives raise to the mean square error over here we can actually take the error over here we can actually take the absolute value and this gives raise to absolute value and this gives raise to the mean absolute error but the m the mean absolute error but the m absolute error over here has some form absolute error over here has some form of difficulty to calculate derivatives of difficulty to calculate derivatives at this point what we do is with at this point what we do is with smooth we smooth this and we call this smooth we smooth this and we call this the -called huber loss this is the -called huber loss this is the huber loss the shape of the huber loss", "image_path": "img_data/video_44_chunk_82.jpg"}
{"video": "video_44", "start": "0:41:30", "end": "0:42:00", "timestamp": "0:41:30 - 0:42:00", "text": "huber loss the shape of the huber loss the shape of the huber loss function and this is basically we are function and this is basically we are going to be training the rpn and going to be training the rpn and we are going to then be able to suggest we are going to then be able to suggest objectness at the anchor kind of level objectness at the anchor kind of level and then we are going to suggest this and then we are going to suggest this as proposals those who exceed a as proposals those who exceed a certain confidence threshold we're certain confidence threshold we're going to propose it to the main network going to propose it to the main network work for the main path the main work for the main path the main branch if you to do the what", "image_path": "img_data/video_44_chunk_83.jpg"}
{"video": "video_44", "start": "0:42:00", "end": "0:42:21.033333", "timestamp": "0:42:00 - 0:42:21.033333", "text": "work for the main path the main branch if you to do the what branch if you to do the what we've been doing in fast rcnn to we've been doing in fast rcnn to classify now these proposals as to classify now these proposals as to and make the final determination both and make the final determination both at the classification stage as well at the classification stage as well also the regression stage giving us back also the regression stage giving us back the desired coordinates", "image_path": "img_data/video_44_chunk_84.jpg"}
{"video": "video_45", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in an earlier video we have seen ms coko the data set that we'll be using to the data set that we'll be using to gauge whether or not a specific gauge whether or not a specific object detector is performing well object detector is performing well and the in this kind of video what we and the in this kind of video what we are trying to do is to before we dive are trying to do is to before we dive into the specific architectures the neural architectures the neural architectures of object detection that architectures of object detection that we will deal with a bit later we need we will deal with a bit later we need to spend some time on what is actually to spend some time on what is actually called the metrics and the metrics", "image_path": "img_data/video_45_chunk_0.jpg"}
{"video": "video_45", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "to spend some time on what is actually called the metrics and the metrics called the metrics and the metrics are actually quite important that kind are actually quite important that kind of discussion because it kind of discussion because it kind of transverses all this kind of transverses all this kind of architectures and we have potentially architectures and we have potentially tens of architectures in object tens of architectures in object detection we can deal with and out of detection we can deal with and out of the many we'll be dealing with a very the many we'll be dealing with a very specific family i think it's specific family i think it's worthwhile to spend some time trying worthwhile to spend some time trying to understand what are those metrics are to understand what are those metrics are and i wanted to start that kind of and i wanted to start that kind of discussion by just writing down discussion by just writing down the key sort of events that we the key sort of events that we have seen also in the", "image_path": "img_data/video_45_chunk_1.jpg"}
{"video": "video_45", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "the key sort of events that we have seen also in the have seen also in the binary classification discussion in that classification discussion in that kind of video i think i can kind of video i think i can distinguish here in object detection distinguish here in object detection three types of well three types of well three types of events the first event is the events the first event is the -called true positive i'm just -called true positive i'm just going to draw here an image i'm going to draw here an image i'm going to use the person class as the use the person class as the class of interest here and the data", "image_path": "img_data/video_45_chunk_2.jpg"}
{"video": "video_45", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "use the person class as the class of interest here and the data class of interest here and the data set always will contain i'll indicate set always will contain i'll indicate this let's say a bounding box as far as this let's say a bounding box as far as this object is concerned and this object is concerned and this will be with red is going to be the will be with red is going to be the ground truth and my object detector is ground truth and my object detector is going to position another bounding going to position another bounding box around that kind of class and box around that kind of class and when we have we will be calling we'll be have we will be calling we'll be calling this", "image_path": "img_data/video_45_chunk_3.jpg"}
{"video": "video_45", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "have we will be calling we'll be calling this with u the bgt the bounding with u the bgt the bounding box of the ground truth and we will box of the ground truth and we will be sort of that bounding box will be sort of that bounding box will be accompanied with a class let's call accompanied with a class let's call it this class person and if my detector also calls the same", "image_path": "img_data/video_45_chunk_4.jpg"}
{"video": "video_45", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "person and if my detector also calls the same agrees with the ground truth then i'm agrees with the ground truth then i'm going to have a true positive event i going to have a true positive event i will further qualify i will use the will further qualify i will use the index ey for this true positive events index ey for this true positive events i'll further qualify what is really i'll further qualify what is really happening with respect to how close happening with respect to how close this predicted bounding box is and the this predicted bounding box is and the ground earth b bounding box is a bit ground earth b bounding box is a bit later and but the second type of later and but the second type of event i will have in object detection is event i will have in object detection is as you can imagine the soal called false", "image_path": "img_data/video_45_chunk_5.jpg"}
{"video": "video_45", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "event i will have in object detection is as you can imagine the soal called false as you can imagine the soal called false positive event where i have positive event where i have a let's say a car the training my data set specifies training my data set specifies that as such but unfortunately my that as such but unfortunately my object detector classifies that as a detector classifies that as a person despite the presence of the person despite the presence of the bone the box around it the class id is", "image_path": "img_data/video_45_chunk_6.jpg"}
{"video": "video_45", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "person despite the presence of the bone the box around it the class id is bone the box around it the class id is different this will actually be different this will actually be called a false positive and i'll use the called a false positive and i'll use the index j for those false positives the index j for those false positives the third is the situation where i have present in the situation where i have present in the image a person and as you can imagine this will be and as you can imagine this will be called person in the training data set however", "image_path": "img_data/video_45_chunk_7.jpg"}
{"video": "video_45", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "called person in the training data set however person in the training data set however i have absolutely no detection event and i have absolutely no detection event and this will become false negative this will become false negative because evidently i have i'm because evidently i have i'm predicting negative and i'm wrong predicting negative and i'm wrong here i'm predicting positive but i'm here i'm predicting positive but i'm predicing positive for a different class predicing positive for a different class i'm actually wrong that's the false i'm actually wrong that's the false positive that's basically the positive that's basically the three events and there's only there's three events and there's only there's one category which is actually missing one category which is actually missing from what we have seen earlier here", "image_path": "img_data/video_45_chunk_8.jpg"}
{"video": "video_45", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "one category which is actually missing from what we have seen earlier here from what we have seen earlier here well definitely we do not have the cate well definitely we do not have the cate category of true negatives and the category of true negatives and the reason why we don't have true negatives reason why we don't have true negatives is in object detection we are not is in object detection we are not interesting to not to classify or to interesting to not to classify or to detect the not presentent presence of detect the not presentent presence of an object we're interesting only for an object we're interesting only for detecting the pro objects which are detecting the pro objects which are present in the image and that's why present in the image and that's why we don't have any true we don't have any true negative the i think it's negative the i think it's not worthwhile discussing given the not worthwhile discussing given the presence of only these three type of", "image_path": "img_data/video_45_chunk_9.jpg"}
{"video": "video_45", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "not worthwhile discussing given the presence of only these three type of presence of only these three type of events a couple of thresholds which events a couple of thresholds which are going to be becoming important in are going to be becoming important in quantification of this benchmark metrics quantification of this benchmark metrics the first threshold is coming as i the first threshold is coming as i mentioned a bit earlier from the proximity of the earlier from the proximity of the detected object bounding box and that of detected object bounding box and that of the ground ruth let's imagine that i the ground ruth let's imagine that i have here the class person and i have here the class person and i have a tight bounding box around it and", "image_path": "img_data/video_45_chunk_10.jpg"}
{"video": "video_45", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "have here the class person and i have a tight bounding box around it and a tight bounding box around it and this is the ground truth bounding box this is the ground truth bounding box and my kind of predictor is placing a and my kind of predictor is placing a bounding box this and it's correct the class is correct i'm going to be defining here let's call this b predict b pred that's i'm going to predict b pred that's i'm going to be defining and the -cal be defining and the -cal intersection over union what is this intersection over union what is this intersection over union this", "image_path": "img_data/video_45_chunk_11.jpg"}
{"video": "video_45", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "intersection over union what is this intersection over union this intersection over union this iou which stands for intersection over union it is a ratio of the intersection of the area of the intersection of the area of the intersection of the bounding box of that intersection of the bounding box of that i'm predicting and the bounding box which is the ground truth divided by the", "image_path": "img_data/video_45_chunk_12.jpg"}
{"video": "video_45", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "predicting and the bounding box which is the ground truth divided by the ground truth divided by the corresponding area of b bread union bgt if you want to have a kind of a bgt if you want to have a kind of a visualization of that this was the visualization of that this was the ground truth this is ground truth this is my prediction and this is the what my prediction and this is the what will be the intersection then evidently the intersection then evidently the denominator i have the", "image_path": "img_data/video_45_chunk_13.jpg"}
{"video": "video_45", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "intersection then evidently the denominator i have the denominator i have the ground truth the prediction and i'm going to have the union as the in this denominator the it's this denominator the it's evidently that when the two boxes are evidently that when the two boxes are right on top of each other i am going to right on top of each other i am going to have the highest iou which is going to have the highest iou which is going to be one", "image_path": "img_data/video_45_chunk_14.jpg"}
{"video": "video_45", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "have the highest iou which is going to be one and when the two bound boxes are further away from each other then i'm going away from each other then i'm going to have the smallest i which is actually to have the smallest i which is actually zero and this is i'm going to be zero and this is i'm going to be using now the iou this concept using now the iou this concept over here to define the threshold over here to define the threshold the -called iou threshold the iou threshold is going to", "image_path": "img_data/video_45_chunk_15.jpg"}
{"video": "video_45", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "threshold the iou threshold is going to be defined as t denoted as t and i am be defined as t denoted as t and i am going to specify in my metric that going to specify in my metric that when the iou is greater than t i am going to classify this as the t i am going to classify this as the true positive and when iou is less than t then this will be classified as false", "image_path": "img_data/video_45_chunk_16.jpg"}
{"video": "video_45", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "positive and when iou is less than t then this will be classified as false t then this will be classified as false positive the relationship between the positive the relationship between the bounding box despite the fact that i bounding box despite the fact that i will be getting the class will be getting the class right is also going to lead to in right is also going to lead to in false positives it's not only the false positives it's not only the fact that they got the class wrong but fact that they got the class wrong but also the relationship between the two also the relationship between the two bounding boxes that will also define bounding boxes that will also define which events are going to be classified which events are going to be classified as long typically we", "image_path": "img_data/video_45_chunk_17.jpg"}
{"video": "video_45", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "which events are going to be classified as long typically we as long typically we have the threshold belongs to typical have the threshold belongs to typical values for threshold is let's say 0.5 values for threshold is let's say 0.5 and 0.75 for that's of course going to be specified by the benchmark in to be specified by the benchmark in the corresponding data set and we the corresponding data set and we will basically need to do the evaluation will basically need to do the evaluation based on those kind of threshold values based on those kind of threshold values and report them and then we have if you and report them and then we have if you a common around with respect to how", "image_path": "img_data/video_45_chunk_18.jpg"}
{"video": "video_45", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "and report them and then we have if you a common around with respect to how a common around with respect to how tight my bounding box is within the tight my bounding box is within the how good that predictor is within how good that predictor is within placing the bounding box very close placing the bounding box very close to that ground truth but now that we have seen the iou truth but now that we have seen the iou threshold and of course recalling that threshold and of course recalling that we have another threshold that is we have another threshold that is actually associated with how with the actually associated with how with the -call confidence threshold i the -call confidence threshold i think it's worthwhile drawing a blog think it's worthwhile drawing a blog diag will actually put this together and diag will actually put this together and this block diagram will actually govern", "image_path": "img_data/video_45_chunk_19.jpg"}
{"video": "video_45", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "diag will actually put this together and this block diagram will actually govern this block diagram will actually govern all the subsequent discussion about all the subsequent discussion about metrics and here i have a block metrics and here i have a block diagram that it is receiving a diagram that it is receiving a white hat and this why hat evidently contains all this why hat evidently contains all the information required for me to sort the information required for me to sort of count accurately these kind of events of count accurately these kind of events necessary for my benchmarking the necessary for my benchmarking the first thing that happens is that first thing that happens is that i'm it passes through the -called iou", "image_path": "img_data/video_45_chunk_20.jpg"}
{"video": "video_45", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "first thing that happens is that i'm it passes through the -called iou i'm it passes through the -called iou threshold block and the parameter for threshold block and the parameter for this u threshold is the value t and this u threshold is the value t and as we discussed if we have if as we discussed if we have if the iou that is defined from the b prediction the bounding box the prediction the bounding box the predicted bounding box that's contained predicted bounding box that's contained inside the yat is greater than t", "image_path": "img_data/video_45_chunk_21.jpg"}
{"video": "video_45", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "predicted bounding box that's contained inside the yat is greater than t inside the yat is greater than t then we classify this as a true then we classify this as a true positive event let's let me call this a true event let's let me call this a true positive event and this true positive event and this true positive event is going to be give raise to the event is going to be give raise to the counter that i will be calling end counter that i will be calling end through positive and this let me through positive and this let me call this incrementing of this call this incrementing of this counter with this arrow this ntp", "image_path": "img_data/video_45_chunk_22.jpg"}
{"video": "video_45", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "call this incrementing of this counter with this arrow this ntp counter with this arrow this ntp and i will call this ntp a function of t and i will call this ntp a function of t because evidently it is and in the i because evidently it is and in the i case where we have the unfortunate case where we have the unfortunate situation where the iou is less than t we have the false iou is less than t we have the false positive event that will give raise to n positive event that will give raise to n false positives which is also a function false positives which is also a function of t n true positives and of t n true positives and false positive are the counter events false positive are the counter events which are actually going to be", "image_path": "img_data/video_45_chunk_23.jpg"}
{"video": "video_45", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "false positive are the counter events which are actually going to be which are actually going to be in the second block that is the -call confidence threshold comparison block that it will be calling this threshold will be calling this threshold comparison", "image_path": "img_data/video_45_chunk_24.jpg"}
{"video": "video_45", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "will be calling this threshold comparison tow we be calling the confidence tow we be calling the confidence threshold to the comparison is actually happening to the comparison is actually happening with this value over here and the other with this value over here and the other events which are not affected by any events which are not affected by any prediction because we definitely our prediction because we definitely our detector unfortunately did not predict detector unfortunately did not predict anything are the false negatives", "image_path": "img_data/video_45_chunk_25.jpg"}
{"video": "video_45", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "the false negatives k give rais to the false negatives k give rais to the count false negatives and this count false negatives and this definitely not a function of t but is it definitely not a function of t but is it will be a function of to only and here i have the false pos these are going to be the false pos these are going to be affected both from t and also to and affected both from t and also to and here i have the new the n true", "image_path": "img_data/video_45_chunk_26.jpg"}
{"video": "video_45", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "affected both from t and also to and here i have the new the n true here i have the new the n true positives that are going to be affected positives that are going to be affected by both iou and confidence thresholds by both iou and confidence thresholds and now i have this kind of a blog and now i have this kind of a blog diagram that it will actually help me diagram that it will actually help me to u understand what is really going on to u understand what is really going on with respect to the with respect to the benchmark precision and recall benchmark precision and recall values that the data set benchmark is values that the data set benchmark is asking me to calculate now that we have the diagram calculate now that we have the diagram let's write down what we", "image_path": "img_data/video_45_chunk_27.jpg"}
{"video": "video_45", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "calculate now that we have the diagram let's write down what we let's write down what we understood also from the treatment of understood also from the treatment of classification binary classification binary classification in another video classification in another video let me write the kind of the headline let me write the kind of the headline here what constitutes a good detector here what constitutes a good detector can pick up all available", "image_path": "img_data/video_45_chunk_28.jpg"}
{"video": "video_45", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "up all available ground truths which means that the false negatives should be zero means that negatives should be zero means that we don't have any events that we don't have any events that we don't have any bounding box over there while avoiding to detect irrel relevant", "image_path": "img_data/video_45_chunk_29.jpg"}
{"video": "video_45", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "avoiding to detect irrel relevant objects which means that the false positives should be zero and in fact to positives should be zero and in fact to be more accurate the counters of n false be more accurate the counters of n false negatives and n false positives should negatives and n false positives should be zero that is basically the be zero that is basically the ultimate detector if we can actually ultimate detector if we can actually design it and i think it's worthwhile design it and i think it's worthwhile kind of trying to see here what is kind of trying to see here what is really happening with respect to this really happening with respect to this kind of thresholds and if i ask the", "image_path": "img_data/video_45_chunk_30.jpg"}
{"video": "video_45", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "really happening with respect to this kind of thresholds and if i ask the kind of thresholds and if i ask the question as to what will actually be the question as to what will actually be the trend of using this threshold tow to plot this threshold tow to plot this kind of the true positive the counts of kind of the true positive the counts of true positives as a function of this true positives as a function of this confidence threshold that then we can confidence threshold that then we can actually call all understand that the actually call all understand that the number of i mean the number of i mean the relationship should be something", "image_path": "img_data/video_45_chunk_31.jpg"}
{"video": "video_45", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "number of i mean the relationship should be something relationship should be something this some kind of a downwards kind of this some kind of a downwards kind of trend of the confidence the stricter trend of the confidence the stricter we become with respect to what we become with respect to what we classify as a true positive or not classify as a true positive or not then it then the then obviously the number of true positives obviously the number of true positives the counts will actually become lower the counts will actually become lower and lower that's kind of easy to and lower that's kind of easy to and intuitive to understand what happens intuitive to understand what happens now with respect to false positives now with respect to false positives with respect to the tow now let's try", "image_path": "img_data/video_45_chunk_32.jpg"}
{"video": "video_45", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "now with respect to false positives with respect to the tow now let's try with respect to the tow now let's try to think about it and we'll still see to think about it and we'll still see this kind of a downwards kind of this kind of a downwards kind of relationship decreasing relationship decreasing relationship with the confidence relationship with the confidence tow one way to understand that tow one way to understand that relationship is the claim relationship is the claim that the stricter we become that the stricter we become in terms of confidence threshold in terms of confidence threshold then all positive detections are going then all positive detections are going to be affected both true positive but to be affected both true positive but also false positive the strier of the", "image_path": "img_data/video_45_chunk_33.jpg"}
{"video": "video_45", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "to be affected both true positive but also false positive the strier of the also false positive the strier of the threshold all positives are being threshold all positives are being sort of decreasing and if i may sort of decreasing and if i may now plot the n fn with respect to now plot the n fn with respect to then we'll see some kind of a to then we'll see some kind of a reversal over here and the reversal over here and the reason actually we see this kind of reason actually we see this kind of reversal is that when we do increase the that when we do increase the threshold now since we have less", "image_path": "img_data/video_45_chunk_34.jpg"}
{"video": "video_45", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "that when we do increase the threshold now since we have less threshold now since we have less positive detections we will have positive detections we will have conversely a larger number of conversely a larger number of negative detections from that kind of negative detections from that kind of perspective this is what we our perspective this is what we our intuition actually is telling us with intuition actually is telling us with respect to the relationship between respect to the relationship between those counts and the threshold tow and those counts and the threshold tow and this relationship will become important this relationship will become important now that we would to go back into now that we would to go back into metrics that we have seen earlier the metrics that we have seen earlier the -called precision and recall metrix", "image_path": "img_data/video_45_chunk_35.jpg"}
{"video": "video_45", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "metrics that we have seen earlier the -called precision and recall metrix -called precision and recall metrix and kind of write them down in this and kind of write them down in this kind of a new light of being affected kind of a new light of being affected both by the iou threshold as well both by the iou threshold as well also the confidence threshold tow i also the confidence threshold tow i can actually write here the precision as the ratio of all the events that have ratio of all the events that have been classified as true positives the true classified as true positives the true positive of i which is itself affected by t and", "image_path": "img_data/video_45_chunk_36.jpg"}
{"video": "video_45", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "positive of i which is itself affected by t and of i which is itself affected by t and to and summation i'm going to i'm not going to summation i'm going to i'm not going to write this thing because it's write this thing because it's identical plus from j is equal to one now i am going to define here the a new quantity call new counter here the a new quantity call new counter called number of detections in general called number of detections in general minus ntp instead of calling in", "image_path": "img_data/video_45_chunk_37.jpg"}
{"video": "video_45", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "called number of detections in general minus ntp instead of calling in minus ntp instead of calling in the false positives i will be defining the false positives i will be defining here as number of detections as the number of true positives plus the number of false positives plus the number of false positives the number of false positives the number of false positives becomes the number of positives becomes the number of detections by the number of true detections by the number of true positives over here and here is positives over here and here is evidently my false positive event j evidently my false positive event j which is itself a function of t comma t", "image_path": "img_data/video_45_chunk_38.jpg"}
{"video": "video_45", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "evidently my false positive event j which is itself a function of t comma t which is itself a function of t comma t that's the formula for the that's the formula for the precision and if i kind of write next to precision and if i kind of write next to it the formula for the recall which it the formula for the recall which itself going to be affected by this and itself going to be affected by this and i'm going to write another ratio this i'm going to write another ratio this other ratio is going to other ratio is going to be the same numerator as i have seen earlier and the same over here this is repeated over here", "image_path": "img_data/video_45_chunk_39.jpg"}
{"video": "video_45", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "earlier and the same over here this is repeated over here this is repeated over here plus the number of from k is equal to 1 to n fn the of from k is equal to 1 to n fn the number of forse negative events of the number of forse negative events of the event false negative of k which event false negative of k which is a function this false negative event is a function this false negative event is a function of only the to is a function of only the to all right i will be", "image_path": "img_data/video_45_chunk_40.jpg"}
{"video": "video_45", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "is a function of only the to all right i will be rewriting this slightly i'll be writing rewriting this slightly i'll be writing this slightly to be the same numerator is same numerator the same numerator is from i is equal to 1 to ntp of tp of from i is equal to 1 to ntp of tp of i of t comma but in the denominator over comma but in the denominator over here i'll define another quantity here i'll define another quantity that is the number of ground", "image_path": "img_data/video_45_chunk_41.jpg"}
{"video": "video_45", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "here i'll define another quantity that is the number of ground that is the number of ground truths let me write it down explicitly truths let me write it down explicitly or number of ground truth truths for the classes of interest for the class of interest and this will actually be the number of true be the number of true positives plus number of false negative positives plus number of false negative atives remember that in the false", "image_path": "img_data/video_45_chunk_42.jpg"}
{"video": "video_45", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "positives plus number of false negative atives remember that in the false atives remember that in the false negative event i do have a bounding box negative event i do have a bounding box not from my detector but i have a not from my detector but i have a bounding box from my training and of bounding box from my training and of data set i have a ground truth data set i have a ground truth bounding box i have here ground bounding box i have here ground truth bounding boxes that are associated truth bounding boxes that are associated with the class that i'm trying to detect with the class that i'm trying to detect both in the case of a true positive both in the case of a true positive event plus a case of a false negative event plus a case of a false negative event i can rewrite this since i have event i can rewrite this since i have true positives and false negatives over true positives and false negatives over here i can write here as the", "image_path": "img_data/video_45_chunk_43.jpg"}
{"video": "video_45", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "true positives and false negatives over here i can write here as the here i can write here as the n gt the number of ground truths and the n gt the number of ground truths and the number of ground truths is going to be a number of ground truths is going to be a function of t and i'll explain that function of t and i'll explain that now and this is because despite the fact now and this is because despite the fact that number the number of false that number the number of false negatives is u an sort of increasing negatives is u an sort of increasing function of the tow the number of true function of the tow the number of true positives is a decreasing function of", "image_path": "img_data/video_45_chunk_44.jpg"}
{"video": "video_45", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "function of the tow the number of true positives is a decreasing function of positives is a decreasing function of the to their summation is not going to the to their summation is not going to be affected by to is going to be a be affected by to is going to be a constant and is only affected from the constant and is only affected from the dependency of the true positives with dependency of the true positives with respect to t that's respect to t that's the reason why we have only a the reason why we have only a function of t in then denominator here function of t in then denominator here something will become kind of important something will become kind of important to understand the shape that to understand the shape that this precision versus recall curve", "image_path": "img_data/video_45_chunk_45.jpg"}
{"video": "video_45", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "to understand the shape that this precision versus recall curve that this precision versus recall curve is taken you understand that kind of is taken you understand that kind of relationship of now recall versus the relationship of now recall versus the confidence threshold tow then we can confidence threshold tow then we can expect a relationship that will actually expect a relationship that will actually be a decreasing function of to and the be a decreasing function of to and the reason is that the denominator over here reason is that the denominator over here is not affected by too and the only is not affected by too and the only thing that's affected is the numerator thing that's affected is the numerator which is decreasing function of to which is decreasing function of to therefore if we are to plot the", "image_path": "img_data/video_45_chunk_46.jpg"}
{"video": "video_45", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "which is decreasing function of to therefore if we are to plot the therefore if we are to plot the recall curve then we expect to see recall curve then we expect to see something this just the recall something this just the recall versus to and it will be a decreasing function of to and evidently the largest possible of to and evidently the largest possible recall i can actually have is the recall i can actually have is the largest possible true positive rate largest possible true positive rate which is one 1.0 all right however if we are to 1.0 all right however if we are to do the same for the precision the", "image_path": "img_data/video_45_chunk_47.jpg"}
{"video": "video_45", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "1.0 all right however if we are to do the same for the precision the do the same for the precision unfortunately is affected precision unfortunately is affected by to both the numerator and the by to both the numerator and the denominator over here we cannot denominator over here we cannot really claim that we can a prior really claim that we can a prior kind of provide some form of kind of provide some form of relationship but definitely we can relationship but definitely we can actually do the following kind of actually do the following kind of thinking in terms of precision thinking in terms of precision now we see that the definitely the now we see that the definitely the precision is going to be the largest precision is going to be the largest precision is going to be one as well precision is going to be one as well and in terms of", "image_path": "img_data/video_45_chunk_48.jpg"}
{"video": "video_45", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "precision is going to be one as well and in terms of well and in terms of precision the graph is going to precision the graph is going to be fairly i will call it be fairly i will call it exhibiting some form of kind of a exhibiting some form of kind of a zigzag behavior and the what zigzag behavior and the what we can because of that effect of both we can because of that effect of both the numerator and the denominator we the numerator and the denominator we expect to see something some noisy expect to see something some noisy behavior with that it is going to be behavior with that it is going to be something that and if we are to", "image_path": "img_data/video_45_chunk_49.jpg"}
{"video": "video_45", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "behavior with that it is going to be something that and if we are to something that and if we are to plot the pre the precision versus the pre the precision versus recoil as a curve let me just plot it recoil as a curve let me just plot it over here which is precision on the y axis and recall on the x- axis both of them are going to be evidently one the", "image_path": "img_data/video_45_chunk_50.jpg"}
{"video": "video_45", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "recall on the x- axis both of them are going to be evidently one the are going to be evidently one the maximum will be one the ultimate kind maximum will be one the ultimate kind of detector as we discussed is some of detector as we discussed is some someone that it will present to us zero someone that it will present to us zero force negatives but at the same time force negatives but at the same time zero force positive the optimal kind of detector is going the optimal kind of detector is going to be right here evidently this detector is here evidently this detector is unimplementable and unrealizable because unimplementable and unrealizable because of all the things we've seen in binary", "image_path": "img_data/video_45_chunk_51.jpg"}
{"video": "video_45", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "unimplementable and unrealizable because of all the things we've seen in binary of all the things we've seen in binary classification the heavy overlap between classification the heavy overlap between classes and the care that classes and the care that we expect to see is that we are going to we expect to see is that we are going to start because of the zigzag behavior start because of the zigzag behavior we are going to get a curve something that this curve is plotted by varing the curve is plotted by varing the confidence thresold to this to", "image_path": "img_data/video_45_chunk_52.jpg"}
{"video": "video_45", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "curve is plotted by varing the confidence thresold to this to confidence thresold to this to over here corresponds this point over here corresponds this point over here corresponds to is equal to zero and this to over there corresponds to 100% or one and by varing if you the one and by varing if you the confidence threshold effectively you confidence threshold effectively you are varing the numerator and are varing the numerator and denominator appropriately of this of", "image_path": "img_data/video_45_chunk_53.jpg"}
{"video": "video_45", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "are varing the numerator and denominator appropriately of this of denominator appropriately of this of all of these two metrics all of these two metrics precision and recall and we'll be precision and recall and we'll be getting a curve this and what we getting a curve this and what we define as average precision and the term define as average precision and the term average is coming from the fact that we average is coming from the fact that we are now having this area under the are now having this area under the curve being defined by an a lot of curve being defined by an a lot of different values of the threshold tower different values of the threshold tower we call this the area under the care we call this the area under the care the average precision the ap the", "image_path": "img_data/video_45_chunk_54.jpg"}
{"video": "video_45", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "we call this the area under the care the average precision the ap the average precision the ap the average precision is this kind of area average precision is this kind of area and we are going to do some ming and we are going to do some ming of that kind of area to make it a bit of that kind of area to make it a bit more monotonic the we are for most of monotonic the we are for most of the benchmarks out there what we the benchmarks out there what we actually going to be defining is we are actually going to be defining is we are going to be defining some kind of going to be defining some kind of interpol interpolation method and if interpol interpolation method and if i may draw the end result out of", "image_path": "img_data/video_45_chunk_55.jpg"}
{"video": "video_45", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "interpol interpolation method and if i may draw the end result out of this is going to be some something this it's going to be the in this it's going to be the in most benchmarks what we are interested most benchmarks what we are interested in to define the average precision is in to define the average precision is the area under the -called interpolated monotonically -called interpolated monotonically kind of decreasing curve which is the", "image_path": "img_data/video_45_chunk_56.jpg"}
{"video": "video_45", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "-called interpolated monotonically kind of decreasing curve which is the kind of decreasing curve which is the red line over here sh some more light red line over here sh some more light into what we just discussed i think it's into what we just discussed i think it's worth remembering a couple of things worth remembering a couple of things whatever we discussed up to now is whatever we discussed up to now is associated with a binary classification associated with a binary classification problem and this b classification problem and this b classification problem is between the positive case problem is between the positive case where i have the presence of an object where i have the presence of an object of the positive class let's say a of the positive class let's say a person and everything else which is person and everything else which is basically all possible shapes of all", "image_path": "img_data/video_45_chunk_57.jpg"}
{"video": "video_45", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "person and everything else which is basically all possible shapes of all basically all possible shapes of all possible objects and that's what we call possible objects and that's what we call background we have a binary background we have a binary classification problem and to solve classification problem and to solve for each of the classes that are for each of the classes that are involved in my data set let's say 80 involved in my data set let's say 80 classes for the ms ccoo object dat set classes for the ms ccoo object dat set i think it's worth remembering what i think it's worth remembering what we had discussed about binary we had discussed about binary classification and no matter how classification and no matter how complicated the sort of complicated the sort of representations we're actually building representations we're actually building typically will involve some for of typically will involve some for of stacking of cnn kind of layers and maybe", "image_path": "img_data/video_45_chunk_58.jpg"}
{"video": "video_45", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "typically will involve some for of stacking of cnn kind of layers and maybe stacking of cnn kind of layers and maybe some form of fully connected layers at some form of fully connected layers at the top we have of the of this kind of the top we have of the of this kind of network we have this kind of a z network we have this kind of a z value to represent the logic that represent the logic that combines all the possible representation combines all the possible representation across let's say combinations of cnn across let's say combinations of cnn and potentially fully connected and potentially fully connected layers and then at the top of the layer", "image_path": "img_data/video_45_chunk_59.jpg"}
{"video": "video_45", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "and potentially fully connected layers and then at the top of the layer layers and then at the top of the layer at the top of the network had this at the top of the network had this kind of sigmoid and this sigmoid was kind of sigmoid and this sigmoid was actually giving us at the output a y actually giving us at the output a y hat the y hat was the hat the y hat was the following if we actually plot what following if we actually plot what was actually happening we was actually happening we were over here we had the were we were over here we had the z and over here we had the y hat z and over here we had the y hat or sigma of z and if i plot the", "image_path": "img_data/video_45_chunk_60.jpg"}
{"video": "video_45", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "z and over here we had the y hat or sigma of z and if i plot the or sigma of z and if i plot the sigmoidal kind of unit i'll plot something that with unit i'll plot something that with this basically this is this basically this is 0.5 and i have this is zero and i have let's say a z over here value that i got in my value that i got in my network based on all the", "image_path": "img_data/video_45_chunk_61.jpg"}
{"video": "video_45", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "value that i got in my network based on all the network based on all the training kind of process that is actually going on process that is actually going on this sigma of sigma is actually going to this sigma of sigma is actually going to define for me a y hat this is going define for me a y hat this is going to be my y hat which i'm going to be my y hat which i'm going to be reporting and this is basically what is reporting and this is basically what is going at the input of this block going at the input of this block diagram we i just drew on the topic diagram we i just drew on the topic of metrics or all right at the same of metrics or all right at the same time we are defining a threshold", "image_path": "img_data/video_45_chunk_62.jpg"}
{"video": "video_45", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "of metrics or all right at the same time we are defining a threshold time we are defining a threshold tow let's assume that this threshold tow let's assume that this threshold tow is right here this is basically the is right here this is basically the tow that i have quoted earlier in tow that i have quoted earlier in this kind of discussion on this in this kind of discussion on this kind of precision versus recall care for kind of precision versus recall care for example and based on that we will example and based on that we will be processing both the true be processing both the true positive and the false positive kind of", "image_path": "img_data/video_45_chunk_63.jpg"}
{"video": "video_45", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "be processing both the true positive and the false positive kind of positive and the false positive kind of events if my y hat exceeds the threshold to if my y hat exceeds the threshold to then i will and depending on then i will and depending on whether the label agrees with the ground truth label agrees with the ground truth i'll call this a true i'll call this a true positive if i have another detection positive if i have another detection and that also is the case the why and that also is the case the why had exceeds the tow but unfortunately", "image_path": "img_data/video_45_chunk_64.jpg"}
{"video": "video_45", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "and that also is the case the why had exceeds the tow but unfortunately had exceeds the tow but unfortunately the label is not really agreeing with the label is not really agreeing with me then i will be still categorizing me then i will be still categorizing this as positive of course i was wrong this as positive of course i was wrong and i will be incrementing a different and i will be incrementing a different encounter for the cases where the i'm still for the cases where the i'm still producing a output which does not producing a output which does not really exceed however the tow then this really exceed however the tow then this will be considered a negative event and will be considered a negative event and this is the reason why we actually and this is the reason why we actually saw these false negatives counts as a", "image_path": "img_data/video_45_chunk_65.jpg"}
{"video": "video_45", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "and this is the reason why we actually saw these false negatives counts as a saw these false negatives counts as a function of to kind of increasing the function of to kind of increasing the larger number of the larger tow i have larger number of the larger tow i have the more difficulties to categorize the more difficulties to categorize positive events either true positive or positive events either true positive or false positive and therefore will be false positive and therefore will be categorized as false negative now categorized as false negative now that we've seen the average precision i that we've seen the average precision i think it's worthwhile quoting the think it's worthwhile quoting the -call -call mean average precision or also denoted mean average precision or also denoted as map which simply is", "image_path": "img_data/video_45_chunk_66.jpg"}
{"video": "video_45", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "as map which simply is the map is equal to 1 / capital k the map is equal to 1 / capital k the number of classes the number of classes summation from small letter k is equal summation from small letter k is equal to 1 to capital k of the average precision k that the k of the average precision k that the average precision as we discussed average precision as we discussed corresponds to a single binary corresponds to a single binary classification case where the class k classification case where the class k is involved versus background and but i", "image_path": "img_data/video_45_chunk_67.jpg"}
{"video": "video_45", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "classification case where the class k is involved versus background and but i is involved versus background and but i also wanted to quote that in some data also wanted to quote that in some data sets more specifically coco we tend to sets more specifically coco we tend to also average not only across classes but also average not only across classes but also across various iou thresholds also across various iou thresholds we need to be kind of careful to we need to be kind of careful to avoid kind of comparisons between kind avoid kind of comparisons between kind of data sets which some of them are of data sets which some of them are quoting mean average precision and some quoting mean average precision and some others they with this kind of averaging and some", "image_path": "img_data/video_45_chunk_68.jpg"}
{"video": "video_45", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "others they with this kind of averaging and some with this kind of averaging and some other data sets which they involve some other data sets which they involve some kind of different type of averaging kind of different type of averaging and this is also quite important when and this is also quite important when you try to pick up an object detector you try to pick up an object detector from let's say a hub where there from let's say a hub where there are variety of object detectors are variety of object detectors are present there and each one will report present there and each one will report kind of a different kind of values and kind of a different kind of values and you should also be careful with you should also be careful with respect to using this mean average respect to using this mean average precision as a metric because in some precision as a metric because in some instances you're only interested about instances you're only interested about let's say and a small subset of the u", "image_path": "img_data/video_45_chunk_69.jpg"}
{"video": "video_45", "start": "0:35:00", "end": "0:35:25.566667", "timestamp": "0:35:00 - 0:35:25.566667", "text": "instances you're only interested about let's say and a small subset of the u let's say and a small subset of the u data set classes let's say you data set classes let's say you interested to detect pedestrians only interested to detect pedestrians only versus anything else and therefore it's versus anything else and therefore it's worthwhile going beyond the behind worthwhile going beyond the behind the min average precision and look at the min average precision and look at the average precision of the average precision of the classes that you're interested in to classes that you're interested in to select a detector that is able to select a detector that is able to perform according to your perform according to your desired metrics", "image_path": "img_data/video_45_chunk_70.jpg"}
{"video": "video_46", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "this is an example of a single frame output this is evidently we have a more advanced evidently we have a more advanced task than what we have seen earlier as task than what we have seen earlier as image classification when we looked at the classification when we looked at the convolution networks there was a video convolution networks there was a video over there what we have sort of over there what we have sort of treated the cats and dogs kind of image treated the cats and dogs kind of image classification problem and over here we classification problem and over here we see what we were dealing with here see what we were dealing with here we are going to not only classify the", "image_path": "img_data/video_46_chunk_0.jpg"}
{"video": "video_46", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "see what we were dealing with here we are going to not only classify the we are going to not only classify the class that it is shown and as well class that it is shown and as well also of course produce the confidence also of course produce the confidence interval some confidence about the interval some confidence about the prediction but also position a prediction but also position a bounding box around that kind of class bounding box around that kind of class that bound a box should be as tight as that bound a box should be as tight as possible as it can be and that possible as it can be and that is basically the task which is also is basically the task which is also known as localization because we are localization because we are effectively locating the object in the", "image_path": "img_data/video_46_chunk_1.jpg"}
{"video": "video_46", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "localization because we are effectively locating the object in the effectively locating the object in the frame and i think it's instructive also frame and i think it's instructive also to see from another kind of perspective to see from another kind of perspective video in this video over here we see video in this video over here we see exactly the same thing but one frame exactly the same thing but one frame is processed after another and we can is processed after another and we can actually notice a couple of things actually notice a couple of things first of all what we notice is that we first of all what we notice is that we have a some kind of flickering going have a some kind of flickering going on and this is u the flickering that on and this is u the flickering that we actually observe is because we do not", "image_path": "img_data/video_46_chunk_2.jpg"}
{"video": "video_46", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "on and this is u the flickering that we actually observe is because we do not we actually observe is because we do not implement in this setting yet implement in this setting yet something we will see a bit later called something we will see a bit later called tracking and also comes in the name tracking and also comes in the name of probabilistic reasoning over time and of probabilistic reasoning over time and the second thing actually we notice is the second thing actually we notice is that in one frame potentially the that in one frame potentially the object will appear and it will be object will appear and it will be localized and everything is fine and in localized and everything is fine and in then second frame in another in a then second frame in another in a subsequent frame it will disappear and subsequent frame it will disappear and both of these will actually be both of these will actually be addressed by the that kind of", "image_path": "img_data/video_46_chunk_3.jpg"}
{"video": "video_46", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "both of these will actually be addressed by the that kind of addressed by the that kind of tracking mechanism we'll be introducing tracking mechanism we'll be introducing but this goes to show the u sort of but this goes to show the u sort of reflexive nature of this type of reflexive nature of this type of neural networks that we will be dealing neural networks that we will be dealing with they have the this kind of a with they have the this kind of a property where in one just our property where in one just our reflexes in one instance they doing reflexes in one instance they doing their job right and in the subsequent their job right and in the subsequent instance some random sort of instance some random sort of fluctuation of light potentially may fluctuation of light potentially may happen and then they just don't detect", "image_path": "img_data/video_46_chunk_4.jpg"}
{"video": "video_46", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "fluctuation of light potentially may happen and then they just don't detect happen and then they just don't detect the object in a subsequent frame all the object in a subsequent frame all right that's basically a good way right that's basically a good way to actually to show the to actually to show the object detection task but i want the object detection task but i want to proceed a little bit further now and to proceed a little bit further now and actually discuss some other actually discuss some other additional task that we actually see additional task that we actually see under the headline of scene under the headline of scene understanding the second task understanding the second task which is more complicated than just which is more complicated than just placing a bounding box is the -called placing a bounding box is the -called semantic segmentation and in the semantic segmentation and in the semantic segmentation task we the", "image_path": "img_data/video_46_chunk_5.jpg"}
{"video": "video_46", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "semantic segmentation and in the semantic segmentation task we the semantic segmentation task we the bounding box will take the form of a bounding box will take the form of a polygon and our job over here is to polygon and our job over here is to trace the shape of the object with trace the shape of the object with that kind of polygon and now you may ask that kind of polygon and now you may ask why we need actually to do that and why we need actually to do that and in many applications the bounding box in many applications the bounding box simply will not cut it simply will not cut it imagine you are in a kind of a robot imagine you are in a kind of a robot which is actually moving a car", "image_path": "img_data/video_46_chunk_6.jpg"}
{"video": "video_46", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "imagine you are in a kind of a robot which is actually moving a car which is actually moving a car and in this scene over here in and in this scene over here in the middle you one of the main tasks in the middle you one of the main tasks in self-driving vehicles is to understand self-driving vehicles is to understand where is the empty space where the robot where is the empty space where the robot actually can move in and occupy actually can move in and occupy it in the next let's say frame and as it in the next let's say frame and as you can imagine if you place a bounding you can imagine if you place a bounding box around that kind of a scene that box around that kind of a scene that you are seeing u this kind of road that you are seeing u this kind of road that has to be sort of enclosed into it", "image_path": "img_data/video_46_chunk_7.jpg"}
{"video": "video_46", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "you are seeing u this kind of road that has to be sort of enclosed into it has to be sort of enclosed into it will contain also pedestrians other will contain also pedestrians other objects such as parked cars objects such as parked cars the sort of other kind of urban the sort of other kind of urban furniture and on the we need to go furniture and on the we need to go into a much lower resolution in terms of into a much lower resolution in terms of segmenting if you the scene segmenting if you the scene very accurately and we will be very accurately and we will be effectively as i said define this kind effectively as i said define this kind of a polygon despite the fact that of a polygon despite the fact that this is kind of shown over here as kind this is kind of shown over here as kind of a smooth trace in fact it consists", "image_path": "img_data/video_46_chunk_8.jpg"}
{"video": "video_46", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "this is kind of shown over here as kind of a smooth trace in fact it consists of a smooth trace in fact it consists of dense polygons that are that we are going polygons that are that we are going to u sort of occupy that sort of to u sort of occupy that sort of trace that kind of object in other trace that kind of object in other applications obviously in a medical applications obviously in a medical applications you can see here we applications you can see here we definitely need to avoid definitely need to avoid having the bounding box there instead having the bounding box there instead using this kind of a semantic using this kind of a semantic segmentation to very accurately trace segmentation to very accurately trace let's say malignant kind of cell from", "image_path": "img_data/video_46_chunk_9.jpg"}
{"video": "video_46", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "segmentation to very accurately trace let's say malignant kind of cell from let's say malignant kind of cell from some other matter in our some other matter in our imaging and many applications imaging and many applications require that kind of semantic require that kind of semantic segmentation and but however semantic segmentation and but however semantic segmentation given the fact that it is a segmentation given the fact that it is a bit more complicated it is a task that bit more complicated it is a task that we are going to be dealing a bit we are going to be dealing a bit later after we understand a bit later after we understand a bit object detection another task that we object detection another task that we are going to deal with is this are going to deal with is this instance segmentation and instance segmentation and instance segment ation is a further", "image_path": "img_data/video_46_chunk_10.jpg"}
{"video": "video_46", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "instance segmentation and instance segment ation is a further segment ation is a further enhancement to semantic segmentation enhancement to semantic segmentation now in semantic segmentation will be now in semantic segmentation will be able trace all the objects of the same able trace all the objects of the same class let's say the chairs over here class let's say the chairs over here with this kind of polygons and color with this kind of polygons and color them appropriately as belonging to the them appropriately as belonging to the class chair but in instant segmentation class chair but in instant segmentation we are interested to color differently we are interested to color differently than other instances of the chair if than other instances of the chair if this is magenta the other one will be this is magenta the other one will be red and on we are that's why we", "image_path": "img_data/video_46_chunk_11.jpg"}
{"video": "video_46", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "this is magenta the other one will be red and on we are that's why we red and on we are that's why we call it instant segmentation and we have call it instant segmentation and we have some other more enhanced sort of some other more enhanced sort of tasks that we can discuss a bit later tasks that we can discuss a bit later after we discuss the -cal logical after we discuss the -cal logical reasoning during the logical reasoning during the logical reasoning section but what we can discuss section but what we can discuss now is the data set we need evidently now is the data set we need evidently training data this means we need data training data this means we need data that contain if you an image and on that contain if you an image and on top of that kind of image we have", "image_path": "img_data/video_46_chunk_12.jpg"}
{"video": "video_46", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "that contain if you an image and on top of that kind of image we have top of that kind of image we have traced objects such as the chair over traced objects such as the chair over here or the laptop and on to be able here or the laptop and on to be able to train our algorithms to implement to train our algorithms to implement let's say the semantics segementation let's say the semantics segementation task one of the main data sets that task one of the main data sets that appeared several years ago and still appeared several years ago and still remains to this day and one of the remains to this day and one of the main data sets that we have we are main data sets that we have we are using to benchmark this type of using to benchmark this type of algorithms is the -called microsoft algorithms is the -called microsoft coco data set it's a data set that", "image_path": "img_data/video_46_chunk_13.jpg"}
{"video": "video_46", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "algorithms is the -called microsoft coco data set it's a data set that coco data set it's a data set that contains 80 object 80 classes that belong 80 object 80 classes that belong to category of object and classes to category of object and classes that are belonging to the 91 that are belonging to the 91 classes belong to the to of stuff you classes belong to the to of stuff grass wall and on and know grass wall and on and some of these mainly we will be some of these mainly we will be dealing with these 80 object classes as dealing with these 80 object classes as examples for implementation and i as examples for implementation and i think it's worthwhile going and spending", "image_path": "img_data/video_46_chunk_14.jpg"}
{"video": "video_46", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "as examples for implementation and i think it's worthwhile going and spending think it's worthwhile going and spending a couple of minutes on this kind of site a couple of minutes on this kind of site the link of which you can find on your the link of which you can find on your course notes just to experiment a little course notes just to experiment a little bit with and actually understand bit with and actually understand what it can offer to us all right we what it can offer to us all right we have let's say out of the 300,000 have let's say out of the 300,000 images that it contains here we images that it contains here we are able to filter images that are able to filter images that are containing the class car and person containing the class car and person and you can see that there are and you can see that there are around 88,000 images of that this is", "image_path": "img_data/video_46_chunk_15.jpg"}
{"video": "video_46", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "and you can see that there are around 88,000 images of that this is around 88,000 images of that this is for semantic segmentation but for semantic segmentation but evidently we are out of semantic evidently we are out of semantic segmentation we are able out of these segmentation we are able out of these polygon traces we're able to get polygon traces we're able to get bounding boxes fairly easily with a bounding boxes fairly easily with a simple geometric kind of algorithm to simple geometric kind of algorithm to produce the bounding box out of that produce the bounding box out of that kind of that encloses tightly this kind of that encloses tightly this kind of polygon either we have polygons or of polygon either we have polygons or bounding boxes for object detection bounding boxes for object detection we can actually do that the same we can actually do that the same thing all right this is the", "image_path": "img_data/video_46_chunk_16.jpg"}
{"video": "video_46", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "we can actually do that the same thing all right this is the thing all right this is the example of u images that we can example of u images that we can see and we can also see that we have descriptions of that image descriptions of that image and the these are descriptions that and the these are descriptions that actual humans annotators have actual humans annotators have produced trying to discuss to describe produced trying to discuss to describe that kind of image and this will that kind of image and this will actually be very useful will a bit", "image_path": "img_data/video_46_chunk_17.jpg"}
{"video": "video_46", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "that kind of image and this will actually be very useful will a bit actually be very useful will a bit later after we have gone through the later after we have gone through the natural language processing discussion natural language processing discussion and then at some point we will start and then at some point we will start synthesizing and bringing together synthesizing and bringing together object detection sorry computer object detection sorry computer vision and natural language vision and natural language processing this is i will processing this is i will call it the classes boxes that can call it the classes boxes that can filter the specific classes present in filter the specific classes present in this image now if you go to tasks", "image_path": "img_data/video_46_chunk_18.jpg"}
{"video": "video_46", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "this in this image now if you go to tasks image now if you go to tasks on the other hand you can actually see on the other hand you can actually see some more other some other tasks that some more other some other tasks that it can actually make use of this data it can actually make use of this data set the one task i think i want to stay set the one task i think i want to stay a little bit further on is the -call a little bit further on is the -call keyo detection task and the keyo detection task and the their extraction of key points in their extraction of key points in many instances is it's a key requirement many instances is it's a key requirement for example when you are designing for example when you are designing algorithms for let's say self-driving", "image_path": "img_data/video_46_chunk_19.jpg"}
{"video": "video_46", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "for example when you are designing algorithms for let's say self-driving algorithms for let's say self-driving cars these robots moving into let's say cars these robots moving into let's say the road and suddenly they come across a the road and suddenly they come across a pedestrian it's quite different to pedestrian it's quite different to as far as decision making is as far as decision making is concerned if the pedestrian is to just concerned if the pedestrian is to just standing waiting for the car to standing waiting for the car to pass to cross the zebra crossing or pass to cross the zebra crossing or to move into to have a pose that it to move into to have a pose that it will indicate some form of movement will indicate some form of movement and predicting if you the u", "image_path": "img_data/video_46_chunk_20.jpg"}
{"video": "video_46", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "will indicate some form of movement and predicting if you the u and predicting if you the u sort of some of these key points of the sort of some of these key points of the classes of certain classes let's say classes of certain classes let's say person the person class is kind of quite person the person class is kind of quite important to understand exactly the important to understand exactly the mobility state of that object and mobility state of that object and decide accordingly there are certain and decide accordingly there are certain other tasks also that i it's worth other tasks also that i it's worth mentioning most specifically the mentioning most specifically the panoptic segmentation task and this panoptic segmentation task and this definitely make uses of all of the definitely make uses of all of the classes that we have available both", "image_path": "img_data/video_46_chunk_21.jpg"}
{"video": "video_46", "start": "0:11:00", "end": "0:11:29", "timestamp": "0:11:00 - 0:11:29", "text": "definitely make uses of all of the classes that we have available both classes that we have available both the object and the staff classes of the object and the staff classes of in that kind of data set and in panoptic in that kind of data set and in panoptic segmentation we have to color every segmentation we have to color every single pixel we're not really single pixel we're not really coloring only the pixels that are within coloring only the pixels that are within the polygon that segmented the class of the polygon that segmented the class of interest but we are coloring also all of interest but we are coloring also all of the other pixels in the image and in the other pixels in the image and in many industries is also important as many industries is also important as you've seen earlier again using the you've seen earlier again using the self-driving car example", "image_path": "img_data/video_46_chunk_22.jpg"}
{"video": "video_47", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we will start on a topic called sin understanding and topic called sin understanding and evidently sin understanding is still the evidently sin understanding is still the understanding part is still a research understanding part is still a research topic but nevertheless over here topic but nevertheless over here we will start with a much simpler task we will start with a much simpler task the task that has to do with placing the task that has to do with placing boxes around objects the -called boxes around objects the -called object detection task and before we object detection task and before we start on this kind of task i wanted to start on this kind of task i wanted to present a little bit of a block diagram present a little bit of a block diagram of a system a video analytic system", "image_path": "img_data/video_47_chunk_0.jpg"}
{"video": "video_47", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "present a little bit of a block diagram of a system a video analytic system of a system a video analytic system that i'm going to just start drawing that i'm going to just start drawing that we understand all the elements that we understand all the elements of what we're trying to build and as we of what we're trying to build and as we will realize that everything we have will realize that everything we have done up to now many of task of done up to now many of task of classification and as well as regression classification and as well as regression are kind of integrated into that kind are kind of integrated into that kind of system as we will see a bit later of system as we will see a bit later i want to start the u let's consider i want to start the u let's consider a system that consist of cameras", "image_path": "img_data/video_47_chunk_1.jpg"}
{"video": "video_47", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "i want to start the u let's consider a system that consist of cameras a system that consist of cameras a network of cameras these cameras a network of cameras these cameras are evidently could be potentially many evidently could be potentially many let's say application of video let's say application of video surveillance system for physical surveillance system for physical security in important spaces security in important spaces strategic spaces airports strategic spaces airports typical number of cameras in a us typical number of cameras in a us airport is around 2 and a half thousand airport is around 2 and a half thousand of those cameras integrated into a of those cameras integrated into a network and this is the", "image_path": "img_data/video_47_chunk_2.jpg"}
{"video": "video_47", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "of those cameras integrated into a network and this is the network and this is the cameras are transmitting frames cameras are transmitting frames over a protocol video streams over a protocol video streams over a protocol called let's say realtime protocol called let's say realtime streaming protocol the details of that streaming protocol the details of that is kind of irrelevant right now and a is kind of irrelevant right now and a typical frame rate is around 30 frames typical frame rate is around 30 frames per second and the payload that they second and the payload that they are transmitting they're transmitting are transmitting they're transmitting compressed video the payload is also compressed video the payload is also known as a protocol h 264 or", "image_path": "img_data/video_47_chunk_3.jpg"}
{"video": "video_47", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "compressed video the payload is also known as a protocol h 264 or known as a protocol h 264 or 265 and this is basically the 265 and this is basically the protocols that are underline the protocols that are underline the compression mechanisms to minimize compression mechanisms to minimize the rate and minimization of the rate and minimization of this kind of rate is kind of important this kind of rate is kind of important because as we'll see later in any because as we'll see later in any real time kind of application you are real time kind of application you are always fighting against latency and you always fighting against latency and you don't want your latency to exceed a don't want your latency to exceed a certain target as we have seen in an certain target as we have seen in an earlier kind of video just to give you", "image_path": "img_data/video_47_chunk_4.jpg"}
{"video": "video_47", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "certain target as we have seen in an earlier kind of video just to give you earlier kind of video just to give you an order of magnitude over here what we an order of magnitude over here what we are dealing with a system that are dealing with a system that we are the whole system should be able we are the whole system should be able to process if we have let's say 2 and a to process if we have let's say 2 and a half thousand of those guys over here half thousand of those guys over here with 30 frames per second then we're with 30 frames per second then we're looking at 75,000 frames per looking at 75,000 frames per second that's a kind of a second that's a kind of a tremendous rate that we can actually tremendous rate that we can actually need to process with our need to process with our pipelines this kind of", "image_path": "img_data/video_47_chunk_5.jpg"}
{"video": "video_47", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "need to process with our pipelines this kind of pipelines this kind of streams of course this is also an streams of course this is also an important kind of consideration kind of important kind of consideration kind of a different perspective it's almost a different perspective it's almost impossible to do that in a public cloud impossible to do that in a public cloud aws which is and therefore all many cloud providers therefore all many cloud providers actually are placing local actually are placing local compute storage and networking the compute storage and networking the -called edge clouds into those -called edge clouds into those locations and such as airports and locations and such as airports and other strategic kind of locations", "image_path": "img_data/video_47_chunk_6.jpg"}
{"video": "video_47", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "locations and such as airports and other strategic kind of locations other strategic kind of locations where large network of cameras need to where large network of cameras need to be supported for that specific be supported for that specific reason it's impossible to transfer reason it's impossible to transfer this information over the internet this information over the internet the first thing that the first thing that this stream is under goes is this stream is under goes is this kind of a capture where we take the stream we capture where we take the stream we decompress it and we are actually decompress it and we are actually extracting the frames and all the extracting the frames and all the subsequent kind of", "image_path": "img_data/video_47_chunk_7.jpg"}
{"video": "video_47", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "extracting the frames and all the subsequent kind of processing is going to be done on a processing is going to be done on a frame by frame basis at least in this frame by frame basis at least in this simplistic sort of approach that simplistic sort of approach that we will have here we will be designing the what here we will be designing the what is called this video analytics pipelines and we call them of course pipelines because themselves consist of", "image_path": "img_data/video_47_chunk_8.jpg"}
{"video": "video_47", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "pipelines and we call them of course pipelines because themselves consist of pipelines because themselves consist of multiple stages we'll see that in a multiple stages we'll see that in a moment and those pipelines are producing moment and those pipelines are producing at the output json streams at the output json streams everything is kind of real time frames everything is kind of real time frames are coming in json streams are coming in json streams are coming out json is a format that stands for out json is a format that stands for javascript syntax object notation it's javascript syntax object notation it's actually a very popular format to actually a very popular format to transmit all the information that you transmit all the information that you have extracted out of those out of have extracted out of those frames and what kind of those frames and what kind of information we're talking about here we", "image_path": "img_data/video_47_chunk_9.jpg"}
{"video": "video_47", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "those frames and what kind of information we're talking about here we information we're talking about here we will see now but more importantly we will see now but more importantly we have u these json streams are usually u these json streams are usually typically are transmitted over another typically are transmitted over another streaming technology that is able to streaming technology that is able to implement some kind of public subscribe implement some kind of public subscribe mechanism where they publish this mechanism where they publish this information and whoever wants to information and whoever wants to receive that information they subscribe receive that information they subscribe to it and then that's how you build to it and then that's how you build sort of scalable systems and", "image_path": "img_data/video_47_chunk_10.jpg"}
{"video": "video_47", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "to it and then that's how you build sort of scalable systems and sort of scalable systems and that's beyond the scope of that's beyond the scope of this kind of discussion what happens this kind of discussion what happens later but what is really in scope is to later but what is really in scope is to understand what is really understand what is really happening over here what kind of happening over here what kind of information we are interested in information we are interested in let's assume that we have at the let's assume that we have at the input over here we have let's say input over here we have let's say the image of a person this frame is actually coming in and what is", "image_path": "img_data/video_47_chunk_11.jpg"}
{"video": "video_47", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "person this frame is actually coming in and what is actually coming in and what is going to be produced at the going to be produced at the output is a couple of things are going to a couple of things are going to produce the first is in many produce the first is in many applications we are interested to applications we are interested to superpose the frame the superpose the frame the metadata information that we are metadata information that we are let's say define a bounding box around let's say define a bounding box around that kind of person that is that kind of person that is bounding box is the typically", "image_path": "img_data/video_47_chunk_12.jpg"}
{"video": "video_47", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "that kind of person that is bounding box is the typically bounding box is the typically represented by an -called to top coord top left an -called to top coord top left coordinate x comma y u and we're talking coordinate x comma y u and we're talking about these coordinates are typically about these coordinates are typically expressed at the frame coordinate system expressed at the frame coordinate system a frame coordinate system potentially a frame coordinate system potentially place the 0 comma 0 right here that place the 0 comma 0 right here that is a frame of potentially h pixels is a frame of potentially h pixels by w pixels as we discussed earlier the by w pixels as we discussed earlier the resolution is not very high", "image_path": "img_data/video_47_chunk_13.jpg"}
{"video": "video_47", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "by w pixels as we discussed earlier the resolution is not very high resolution is not very high and we're talking about and we're talking about something of the order of the something of the order of the input frame could be let's say 640 by input frame could be let's say 640 by 640 and that the corresponding 640 and that the corresponding height and width over here could be the height and width over here could be the same and the other dimensions over same and the other dimensions over here that the bounding box u is been here that the bounding box u is been specified is with -called hbb i will specified is with -called hbb i will call this the high dimension of the", "image_path": "img_data/video_47_chunk_14.jpg"}
{"video": "video_47", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "specified is with -called hbb i will call this the high dimension of the call this the high dimension of the bounding box and of course the bounding box and of course the corresponding wbb the corresponding wbb the corresponding withd dimension of that kind of withd dimension of that kind of bounding box our job at the simplest bounding box our job at the simplest possible kind of pipeline is to just possible kind of pipeline is to just position those bounding boxes around position those bounding boxes around objects of interest that the classes i objects of interest that the classes i would to be able to do and that would to be able to do and that task is going to be called object task is going to be called object detection and we'll see also some other detection and we'll see also some other subsequent tasks in a moment as to what subsequent tasks in a moment as to what we are interested in to do we are interested in to do our y hat if i may call it", "image_path": "img_data/video_47_chunk_15.jpg"}
{"video": "video_47", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "we are interested in to do our y hat if i may call it our y hat if i may call it that which is basically what it is going that which is basically what it is going to be included inside this kind of json to be included inside this kind of json one frame after another stream is one frame after another stream is going to be many things could be the going to be many things could be the class id the confidence value and with the some something that will call", "image_path": "img_data/video_47_chunk_16.jpg"}
{"video": "video_47", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "value and with the some something that will call something that we will call later something that we will call later associated with some other threshold associated with some other threshold will callater tow and the bounding will callater tow and the bounding box information which itself is the consist information which itself is the consist of the coordinates x comma y that i of the coordinates x comma y that i called then plus the height and called then plus the height and width direction dimensions and this will be just for the dimensions and this will be just for the specific frame and we have many of", "image_path": "img_data/video_47_chunk_17.jpg"}
{"video": "video_47", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "dimensions and this will be just for the specific frame and we have many of specific frame and we have many of them one after another the them one after another the that's basically the task and what we that's basically the task and what we need to be very careful is to m need to be very careful is to m maintain some form of assumption maintain some form of assumption about latencies this will be important about latencies this will be important in what kind of algorithm we will be in what kind of algorithm we will be selecting to implement over here selecting to implement over here typically for object detection we're typically for object detection we're talking about few milliseconds and of course there are milliseconds and of course there are plenty of optimizations that in reality plenty of optimizations that in reality we are actually making to achieve this", "image_path": "img_data/video_47_chunk_18.jpg"}
{"video": "video_47", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "plenty of optimizations that in reality we are actually making to achieve this we are actually making to achieve this target the discussion about target the discussion about implementation latency will be outside implementation latency will be outside of the scope but i just want to mention of the scope but i just want to mention here the -called latency budget is for object detection just few milliseconds", "image_path": "img_data/video_47_chunk_19.jpg"}
{"video": "video_47", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "detection just few milliseconds that's u something that we will potentially see a bit later as we potentially see a bit later as we review some of the code which is review some of the code which is implementing this kind of object implementing this kind of object detection logic now what we detection logic now what we will do is we will start explain a will do is we will start explain a little bit more about object detection little bit more about object detection and see some kind of a video that is and see some kind of a video that is going to demonstrate that for us and going to demonstrate that for us and also discuss the other tasks which are also discuss the other tasks which are more advanced than object detection that more advanced than object detection that are a bit outside of the scope of this", "image_path": "img_data/video_47_chunk_20.jpg"}
{"video": "video_47", "start": "0:10:30", "end": "0:10:40.233333", "timestamp": "0:10:30 - 0:10:40.233333", "text": "more advanced than object detection that are a bit outside of the scope of this are a bit outside of the scope of this introductory kind of course but it's introductory kind of course but it's actually good to know as we'll also actually good to know as we'll also discuss some of the data sets that we'll discuss some of the data sets that we'll be using to implement those tasks be using to implement those tasks", "image_path": "img_data/video_47_chunk_21.jpg"}
{"video": "video_48", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in an earli video we have seen the convolutional networks and the basic convolutional networks and the basic operation in this video what we operation in this video what we actually introducing here is residual actually introducing here is residual networks which is to this day networks which is to this day several years after the introduction several years after the introduction remain one of the main used remain one of the main used architectures for feature extraction architectures for feature extraction and not only as a basic component of and not only as a basic component of many more advanced cnn architectures many more advanced cnn architectures and that are doing more", "image_path": "img_data/video_48_chunk_0.jpg"}
{"video": "video_48", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "many more advanced cnn architectures and that are doing more and that are doing more complicated tasks such as object complicated tasks such as object detection semantic segmentation and detection semantic segmentation and others that we will see in another in others that we will see in another video the history if another video the history if you of their introduction you of their introduction started around 2015 where people know started around 2015 where people realized that it's not really realized that it's not really possible to extend the -called", "image_path": "img_data/video_48_chunk_1.jpg"}
{"video": "video_48", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "realized that it's not really possible to extend the -called possible to extend the -called architectures of the time let's say the architectures of the time let's say the vgg architecture we've seen the v vgg architecture we've seen the v architecture on a different u video architecture on a different u video earlier in this actually earlier in this actually topic over here where we have se we topic over here where we have se we have seen the sort of architecture of have seen the sort of architecture of the v16 network and what was actually the v16 network and what was actually happening then and now that we happening then and now that we understand a couple of things about back understand a couple of things about back propagation the gradient", "image_path": "img_data/video_48_chunk_2.jpg"}
{"video": "video_48", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "understand a couple of things about back propagation the gradient propagation the gradient had a lot of problems to had a lot of problems to flow all the way to the input of the flow all the way to the input of the network and the u sort of network and the u sort of bottlenecks that were actually generated bottlenecks that were actually generated created a significant problems in the created a significant problems in the training of these architectures training of these architectures around 2015 a researcher at 2015 a researcher at microsoft found a solution microsoft found a solution on how to enable that gradient to on how to enable that gradient to flow freely in a much deeper", "image_path": "img_data/video_48_chunk_3.jpg"}
{"video": "video_48", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "flow freely in a much deeper freely in a much deeper architectures such as the ones that architectures such as the ones that we will see in a moment and the we will see in a moment and the this gave their the name residual this gave their the name residual because it in that architecture we because it in that architecture we implement what we call a residual unit implement what we call a residual unit and from now on we'll refer to and from now on we'll refer to those networks as rest nets all right those networks as rest nets all right in order for us to understand what is in order for us to understand what is going on with rest nets or what i'm going on with rest nets or what i'm going to do now is i'm going to draw going to do now is i'm going to draw a very small rest architecture just", "image_path": "img_data/video_48_chunk_4.jpg"}
{"video": "video_48", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "going to do now is i'm going to draw a very small rest architecture just a very small rest architecture just consisting of three units u and if i may draw the units u and if i may draw the architecture will that's my unit architecture will that's my unit which i'll abstract with the letter which i'll abstract with the letter f1 the input to the i'll use a bit f1 the input to the i'll use a bit different terminology from what i was different terminology from what i was used kind of earlier i'll be calling used kind of earlier i'll be calling set of x i'll be calling it y0 i could set of x i'll be calling it y0 i could have used it also x0 but in my not over", "image_path": "img_data/video_48_chunk_5.jpg"}
{"video": "video_48", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "set of x i'll be calling it y0 i could have used it also x0 but in my not over have used it also x0 but in my not over here i have the this as y z here i have the this as y z the y z goes into a block that will the y z goes into a block that will consist of one or more convolutional consist of one or more convolutional kind of layers and in the residual architecture layers and in the residual architecture and this is why we call it residuals and this is why we call it residuals we take the input and add it with a input and add it with a unit gain into to the output to the", "image_path": "img_data/video_48_chunk_6.jpg"}
{"video": "video_48", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "input and add it with a unit gain into to the output to the unit gain into to the output to form what we call now output to form what we call now a y1 the y1 is being added a y1 the y1 is being added into again with exactly the same approach and this is the output y2 and the y2 is similarly", "image_path": "img_data/video_48_chunk_7.jpg"}
{"video": "video_48", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "output y2 and the y2 is similarly added to form the final y3 output this is the kind of a basic rest net kind of a basic rest net kind of architecture in u and if you compare architecture in u and if you compare what we have seen earlier with the cnns what we have seen earlier with the cnns we had convolutional layers max pool we had convolutional layers max pool layers and nonlinear evidently over layers and nonlinear evidently over here but we never had this kind of skip", "image_path": "img_data/video_48_chunk_8.jpg"}
{"video": "video_48", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "layers and nonlinear evidently over here but we never had this kind of skip here but we never had this kind of skip over connection as we call it over connection as we call it i want to just go ahead and write now i want to just go ahead and write now the expression of the output of each of expression of the output of each of these blocks with respect to the input i hope you to the input i hope you agree that this is exactly what each agree that this is exactly what each of these blocks is", "image_path": "img_data/video_48_chunk_9.jpg"}
{"video": "video_48", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "agree that this is exactly what each of these blocks is implement we have the fi of y implement we have the fi of y ius one plus the y ius one in each of ius one plus the y ius one in each of these if i may write down these if i may write down these equations for let's say the equations for let's say the y3 is equal to f3 of y2 + y3 is equal to f3 of y2 + y2 the y2 itself is fs2 of y1 + y1", "image_path": "img_data/video_48_chunk_10.jpg"}
{"video": "video_48", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "y2 the y2 itself is fs2 of y1 + y1 itself is fs2 of y1 + y1 and the y1 is fub1 of y0 + y0 these are and the y1 is fub1 of y0 + y0 these are the three equations for each of the three equations for each of the three blocks that i have here and what i three blocks that i have here and what i want to do now is i want to start want to do now is i want to start replacing the y2 and y1 into the replacing the y2 and y1 into the equation y3 because i want to write down equation y3 because i want to write down the equation the form of the y3 as a the equation the form of the y3 as a function of only the y0 and the two", "image_path": "img_data/video_48_chunk_11.jpg"}
{"video": "video_48", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "the equation the form of the y3 as a function of only the y0 and the two function of only the y0 and the two functions that are involved in the functions that are involved in the blocks f1 and f2 all right blocks f1 and f2 all right what i'm going to do now is i'm going to what i'm going to do now is i'm going to write it as f3 of y2 i'm sorry f3 of fs2 of y1 + y1 + fs2 of y 1 +", "image_path": "img_data/video_48_chunk_12.jpg"}
{"video": "video_48", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "fs2 of y1 + y1 + fs2 of y 1 + y1 + fs2 of y 1 + y1 i just replace it y2 with it y1 i just replace it y2 with it equal and now i can replace now let me write it over here because i just need a long line to replace just need a long line to replace it to make the final replacement it to make the final replacement it is f3 of f2 now i'm going to replace the f", "image_path": "img_data/video_48_chunk_13.jpg"}
{"video": "video_48", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "is f3 of f2 now i'm going to replace the f f3 of f2 now i'm going to replace the f y1 with its equal to obtain the y1 with its equal to obtain the final expression and this is now the second square bracket it is really this bracket over here", "image_path": "img_data/video_48_chunk_14.jpg"}
{"video": "video_48", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "bracket it is really this bracket over here plus i have fs2 of fs1 of y0 + fs2 of fs1 of y0 + y0 plus fub1 of y0 + y z this is my final expression z this is my final expression with respect to the output of y3 and why i did that i to the output of y3 and why i did that i want to take this kind of architecture", "image_path": "img_data/video_48_chunk_15.jpg"}
{"video": "video_48", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "to the output of y3 and why i did that i want to take this kind of architecture want to take this kind of architecture and write its equivalent that we just and write its equivalent that we just based on this equation and that kind of based on this equation and that kind of re- plotting or redraw drawing of this re- plotting or redraw drawing of this kind of architecture will help me kind of architecture will help me kind of understand a couple of things about of understand a couple of things about the advantages of rest nets and why they the advantages of rest nets and why they solve the problem of gradient flow solve the problem of gradient flow throughout the network i am going to start network i am going to start i'm dividing this into two parts i'm dividing this into two parts i am going to", "image_path": "img_data/video_48_chunk_16.jpg"}
{"video": "video_48", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "i'm dividing this into two parts i am going to first draw the long part over here this f3 the long part over here this f3 expression here at the bottom expression here at the bottom i am going to take this accepts as input i am going to take this accepts as input y0 that's the only input in this y0 that's the only input in this diagram y0 is going into the diagram y0 is going into the function f1", "image_path": "img_data/video_48_chunk_17.jpg"}
{"video": "video_48", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "f1 we are adding now the function we are adding now the function the y z into it that's the y z into it that's basically this term all right then we take the fs2 term all right then we take the fs2 of this term the output of this is of this term the output of this is being fed to f2 and what we do is we add for f2 and what we do is we add for this fs2 we add this term over here f1 y", "image_path": "img_data/video_48_chunk_18.jpg"}
{"video": "video_48", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "f2 and what we do is we add for this fs2 we add this term over here f1 y this fs2 we add this term over here f1 y 0 + y0 we add to it another block involving the function f1 this is basically this inner term over here and then finally we are term over here and then finally we are taking the f3 of that this is", "image_path": "img_data/video_48_chunk_19.jpg"}
{"video": "video_48", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "term over here and then finally we are taking the f3 of that this is taking the f3 of that this is basically my f3 if i am a circle if i may circle this will be let's say a this a will be available over a this a will be available over here we have now finished with here we have now finished with the plotting of the first term and now the plotting of the first term and now let's look at the second let's look at the second term is simply fs2 of", "image_path": "img_data/video_48_chunk_20.jpg"}
{"video": "video_48", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "let's look at the second term is simply fs2 of term the second term is simply fs2 of y1 y 0 + y0 i am going to go again here this is in fact just one line i'm going to just take again fub1 of y0 + y0 + y0 and i'm simply going to take the f2 of", "image_path": "img_data/video_48_chunk_21.jpg"}
{"video": "video_48", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "y0 + y0 and i'm simply going to take the f2 of that and this thing over here is the point b however this is basically b let me throw it over here to b now what we have is we simply add another of these", "image_path": "img_data/video_48_chunk_22.jpg"}
{"video": "video_48", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "here to b now what we have is we simply add another of these guys we add to be this and i would to circle that be this and i would to circle that over because we see here three things over because we see here three things being involved a b and c this is c and then b and c are added to a", "image_path": "img_data/video_48_chunk_23.jpg"}
{"video": "video_48", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "c and then b and c are added to a and this will actually be my y3 and now that we have this kind of y3 and now that we have this kind of diagram we actually can make some really diagram we actually can make some really nice conclusions out of it as you can nice conclusions out of it as you can see the gradient in the backward pass see the gradient in the backward pass the point number one i want to the point number one i want to mention is about the gradient flow you can see the gradient flow in the", "image_path": "img_data/video_48_chunk_24.jpg"}
{"video": "video_48", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "you can see the gradient flow in the backward pass during back propagation backward pass during back propagation now has a diverse set of paths and to now has a diverse set of paths and to actually flow all the way to the input actually flow all the way to the input let's say it has this path that simply let's say it has this path that simply just follows all the way to the input y just follows all the way to the input y z the other path that goes z the other path that goes through this f1 to go to the input this through this f1 to go to the input this path through f2 and f the concatenation path through f2 and f the concatenation f2 and fub1 or via this skip", "image_path": "img_data/video_48_chunk_25.jpg"}
{"video": "video_48", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "path through f2 and f the concatenation f2 and fub1 or via this skip f2 and fub1 or via this skip connection to go to f0 and on and connection to go to f0 and on what we see here is we have a on what we see here is we have a diverse set of paths through each goes through of s of gates i will", "image_path": "img_data/video_48_chunk_26.jpg"}
{"video": "video_48", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "goes through of s of gates i will through of s of gates i will call it because this is what we have call it because this is what we have used as a term from back used as a term from back propagation of varing depth for varing depth that's depth for varing depth that's kind of important earlier what we had kind of important earlier what we had without those kind of skip connections without those kind of skip connections we had simply f1 con with two3 in the we had simply f1 con with two3 in the -cal let's say v architecture here -cal let's say v architecture here we had a back propagation that it was", "image_path": "img_data/video_48_chunk_27.jpg"}
{"video": "video_48", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "-cal let's say v architecture here we had a back propagation that it was we had a back propagation that it was involving just a single trajectory a involving just a single trajectory a single path through all of these single path through all of these gates and at some point the gradient was gates and at some point the gradient was actually dying and of course a dying actually dying and of course a dying gradient means that specific gradient means that specific functionality of my network they are functionality of my network they are being updated the parameters are being updated the parameters are being updated very slowly or just being updated very slowly or just simply seize to be updated that's not simply seize to be updated that's not really i mean this is one of the key really i mean this is one of the key observations that led to some kind of observations that led to some kind of leveling of the performance of these", "image_path": "img_data/video_48_chunk_28.jpg"}
{"video": "video_48", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "observations that led to some kind of leveling of the performance of these leveling of the performance of these earlier kind of architectures as the earlier kind of architectures as the number of layers were being added up in number of layers were being added up in this kind of architecture we are this kind of architecture we are effectively implement what is actually effectively implement what is actually known as highway networks and those known as highway networks and those highway highways that we creating for highway highways that we creating for the gradient empirically has shown the gradient empirically has shown that we are actually can go much that we are actually can go much deeper we'll see now some depths deeper we'll see now some depths typical depths that we experience in typical depths that we experience in we have in a resent architect in a we have in a resent architect in a moment the", "image_path": "img_data/video_48_chunk_29.jpg"}
{"video": "video_48", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "we have in a resent architect in a moment the second aspect of that is a bit more second aspect of that is a bit more nuanced and it has to do with what we nuanced and it has to do with what we call an ensample learning in this kind of ensemble learning architecture what we see is we learning architecture what we see is we see u the concatenation the see u the concatenation the combination of three predictors here combination of three predictors here three main prediction architectures each three main prediction architectures each of those predictors has a", "image_path": "img_data/video_48_chunk_30.jpg"}
{"video": "video_48", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "three main prediction architectures each of those predictors has a of those predictors has a varying kind of functionality we varying kind of functionality we see a fairly involved predictor which we see a fairly involved predictor which we call a we see another predictor which call a we see another predictor which call b and another predictor that we call b and another predictor that we call c and what we see at the output are call c and what we see at the output are the kind of combination of those the kind of combination of those simply i mean if you are familiar simply i mean if you are familiar with ample kind of methods which i'll with ample kind of methods which i'll provide some kind of background in a provide some kind of background in a moment we are adding the individual", "image_path": "img_data/video_48_chunk_31.jpg"}
{"video": "video_48", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "provide some kind of background in a moment we are adding the individual moment we are adding the individual prediction results given the input y z at the to results given the input y z at the to obtain our final prediction output to obtain our final prediction output the -called y3 hat and the -called y3 hat and that's ample methods have proven in ample methods have proven in the field to be a very powerful the field to be a very powerful approach in solving complex kind of tasks and in fact complex kind of tasks and in fact some methods are being used both for", "image_path": "img_data/video_48_chunk_32.jpg"}
{"video": "video_48", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "complex kind of tasks and in fact some methods are being used both for some methods are being used both for structure and unstructured data and in structure and unstructured data and in the structure kind of data field you the structure kind of data field you have methods such as gradient boosting have methods such as gradient boosting and on providing some real and on providing some real state-of-the-art results today a few state-of-the-art results today a few words about ample methods is arguably a words about ample methods is arguably a parenthesis but i think it's a kind parenthesis but i think it's a kind of worthwhile sort of discussing a worthwhile sort of discussing a little bit about ensample methods", "image_path": "img_data/video_48_chunk_33.jpg"}
{"video": "video_48", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "worthwhile sort of discussing a little bit about ensample methods there are various asample methods but i think a common denominator for many of i think a common denominator for many of them is that the prediction why hat them is that the prediction why hat that we get from the -called ensample that we get from the -called ensample also known as committee methods where we for form a committee a group of experts or weak learners as we group of experts or weak learners as we call them is let me call it why", "image_path": "img_data/video_48_chunk_34.jpg"}
{"video": "video_48", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "group of experts or weak learners as we call them is let me call it why call them is let me call it why hat committee is simply the average hat committee is simply the average let me call it 1 / capital k of the let me call it 1 / capital k of the summation from small letter k is equal summation from small letter k is equal to 1 to capital k where capital k is the to 1 to capital k where capital k is the number of predictors that we have here number of predictors that we have here we had three in the rest net we had three in the rest net architecture of y hat subk the architecture of y hat subk the premise of emble method is that we don't premise of emble method is that we don't necessarily to have the single server necessarily to have the single server bullet that will solve the very bullet that will solve the very complicated kind of task of us that", "image_path": "img_data/video_48_chunk_35.jpg"}
{"video": "video_48", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "bullet that will solve the very complicated kind of task of us that complicated kind of task of us that we have in front of us but a number we have in front of us but a number of what we call the -called weak predictors that it will u not perform individually very well but on aggregate individually very well but on aggregate they will actually perform much they will actually perform much better that and that is really the better that and that is really the premise of that and one premise of that and one parallel architecture with have in parallel architecture with have in the earlier method that we call the", "image_path": "img_data/video_48_chunk_36.jpg"}
{"video": "video_48", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "parallel architecture with have in the earlier method that we call the earlier method that we call the rest net is kind of resembles that kind rest net is kind of resembles that kind of architecture because we have some of architecture because we have some kind of a summation combination of these kind of a summation combination of these weak predictions the -called abs and weak predictions the -called abs and c's that i have explained earlier c's that i have explained earlier the in sample methods in general the in sample methods in general we have we can consider performance wise we have we can consider performance wise to consist of in somewhere in between to consist of in somewhere in between two bounds the lower bound the -cal two bounds the lower bound the -cal lower performance", "image_path": "img_data/video_48_chunk_37.jpg"}
{"video": "video_48", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "two bounds the lower bound the -cal lower performance bound is obtained evidently when bound is obtained evidently when you're you have very correlated you're you have very correlated predictions and if you are not able to predictions and if you are not able to randomize the operation of each of these randomize the operation of each of these predictors somehow we are going to predictors somehow we are going to exhibit this kind of lower bound where exhibit this kind of lower bound where either you form a committee or not you either you form a committee or not you get exactly the same performance it's get exactly the same performance it's just the analogy or the equivalent just the analogy or the equivalent analogy i would to sort of share analogy i would to sort of share sh is you have a committee of let's", "image_path": "img_data/video_48_chunk_38.jpg"}
{"video": "video_48", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "analogy i would to sort of share sh is you have a committee of let's sh is you have a committee of let's say a human committee of human say a human committee of human experts but each expert went to exactly experts but each expert went to exactly the same school studied exactly the same school studied exactly the same field had exactly the same university field had exactly the same university professors and they are actually now professors and they are actually now called to solve the problem and guess called to solve the problem and guess what each one of them is actually what each one of them is actually offering exactly the same view well offering exactly the same view well that's basically where the point where that's basically where the point where you experiencing a lower performance you experiencing a lower performance bound and the upper performance bound is bound and the upper performance bound is a bit more nuanced a bit more complicated to kind of understand but", "image_path": "img_data/video_48_chunk_39.jpg"}
{"video": "video_48", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "a bit more nuanced a bit more complicated to kind of understand but complicated to kind of understand but the best performance what we can actually performance what we can actually get i think it's better understood get i think it's better understood with an analogy you don't expect with an analogy you don't expect every committee member to not make every committee member to not make mistakes they will make mistakes but mistakes they will make mistakes but what you want to do is to have a what you want to do is to have a committee that they don't make the same committee that they don't make the same mistake at the same time the mistake at the same time the -called -called uncorrelated errors are involved uncorrelated errors are involved in sort of show showing some kind of", "image_path": "img_data/video_48_chunk_40.jpg"}
{"video": "video_48", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "uncorrelated errors are involved in sort of show showing some kind of in sort of show showing some kind of a performance bound that is kind of a performance bound that is kind of outside of the scope of this course but outside of the scope of this course but i think it's worthwhile providing i think it's worthwhile providing some kind of guidance as to where and some kind of guidance as to where and how we will be able to achieve that how we will be able to achieve that upper bound the main three upper bound the main three ways that we can achieve this kind of ways that we can achieve this kind of upper bound or try to achieve the best upper bound or try to achieve the best possible performance out of ample possible performance out of ample methods the first is the data component methods the first is the data component can we provide in some way different data to", "image_path": "img_data/video_48_chunk_41.jpg"}
{"video": "video_48", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "can we provide in some way different data to provide in some way different data to different of to the various kind of weak different of to the various kind of weak predictors that we have here that we predictors that we have here that we do not cause exactly the same do not cause exactly the same conclusion for each one of them the conclusion for each one of them the second is to somehow randomize their second is to somehow randomize their operation randomization is the second kind of", "image_path": "img_data/video_48_chunk_42.jpg"}
{"video": "video_48", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "randomization is the second kind of approach there and i can actually can approach there and i can actually can offer an example of randomization maybe we can actually randomization maybe we can actually offer a different set of offer a different set of hyperparameters sort of picked by hyperparameters sort of picked by some kind of distribution in this architecture distribution in this architecture see over here in this course u in see over here in this course u in some other approaches where we have some other approaches where we have let's say decision trees involved in let's say decision trees involved in these predictors again for structure these predictors again for structure data i'm referring to then we can data i'm referring to then we can randomize their operation by picking", "image_path": "img_data/video_48_chunk_43.jpg"}
{"video": "video_48", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "data i'm referring to then we can randomize their operation by picking randomize their operation by picking different features that they split different features that they split their sort of trees and there are their sort of trees and there are many approaches that are i many approaches that are i guess too many to quote over here but guess too many to quote over here but the third approach is a bit more the third approach is a bit more relevant in this specific rest not relevant in this specific rest not architecture is to simply use different predictors or weak learners", "image_path": "img_data/video_48_chunk_44.jpg"}
{"video": "video_48", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "predictors or weak learners the weak learners that we called over here are involved in the rest net here are involved in the rest net architecture and here we have a architecture and here we have a predictor of some complexity we have predictor of some complexity we have here another predictor larger complexity and yet predictor larger complexity and yet another predictor or even larger another predictor or even larger complexity we have effectively complexity we have effectively implementing the third implementing the third approach where we have approach where we have those different week learners each one", "image_path": "img_data/video_48_chunk_45.jpg"}
{"video": "video_48", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "approach where we have those different week learners each one those different week learners each one is offering a view and then finally is offering a view and then finally the network is deciding based on the network is deciding based on the composition of those views that composition of those views that has been shown to sort of provide has been shown to sort of provide performance advantages and that's performance advantages and that's what we kind of had this discussion what we kind of had this discussion about and sample methods and the third about and sample methods and the third kind of advantage i wanted to quote kind of advantage i wanted to quote here for rest n is there scalability the", "image_path": "img_data/video_48_chunk_46.jpg"}
{"video": "video_48", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "the scalability should be understood from scalability should be understood from the point of view of the point of view of complexity we are effectively able to complexity we are effectively able to have three six n or whatever number of have three six n or whatever number of residual blocks each one of them will residual blocks each one of them will actually be exactly the same as actually be exactly the same as any other block over here and any other block over here and therefore we are able to therefore we are able to accommodate architectures that are have various architectures that are have various number of these blocks let's say", "image_path": "img_data/video_48_chunk_47.jpg"}
{"video": "video_48", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "architectures that are have various number of these blocks let's say various number of these blocks let's say we see resets with 18 layers 34 layers we see resets with 18 layers 34 layers 50 layers 150 102 layers 50 layers 150 102 layers even 150 layers these are the numbers even 150 layers these are the numbers that we have defined already that we have defined already existing architectures and this is kind existing architectures and this is kind of important when you have perception of important when you have perception systems that need to comply to some systems that need to comply to some realtime latency requirement evidently realtime latency requirement evidently the larger the number of layers you have", "image_path": "img_data/video_48_chunk_48.jpg"}
{"video": "video_48", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "realtime latency requirement evidently the larger the number of layers you have the larger the number of layers you have the longer the latencies that you are the longer the latencies that you are going to experience taking an image going to experience taking an image through this kind of pipeline if we through this kind of pipeline if we have let's say a latency of let's say have let's say a latency of let's say 80 mcs we can and therefore we mcs we can and therefore we are not able to accommodate 102 are not able to accommodate 102 layers where definitely going to be layers where definitely going to be accommodating let's say 50 layers and accommodating let's say 50 layers and the exactly the same technology exactly the same technology exactly the same thinking and behavior of", "image_path": "img_data/video_48_chunk_49.jpg"}
{"video": "video_48", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "the exactly the same technology exactly the same thinking and behavior of the same thinking and behavior of rest nets will be in either of the rest nets will be in either of the numbers quoted here in terms of number numbers quoted here in terms of number of layers all of these three of layers all of these three advantages are coming together to advantages are coming together to provide a fairly robust architecture has provide a fairly robust architecture has actually proven in the field in both actually proven in the field in both real time and unreal time applications real time and unreal time applications and able to extract features and able to extract features provide if you representations on provide if you representations on visual imagery that we have the", "image_path": "img_data/video_48_chunk_50.jpg"}
{"video": "video_48", "start": "0:25:30", "end": "0:25:32.966667", "timestamp": "0:25:30 - 0:25:32.966667", "text": "provide if you representations on visual imagery that we have the visual imagery that we have the imageries that we are feeding into the imageries that we are feeding into them", "image_path": "img_data/video_48_chunk_51.jpg"}
{"video": "video_49", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in an earlier video we saw the structure of the convolutional neuron and how many of the convolutional neuron and how many of these neurons in the form of filters of these neurons in the form of filters are coming together to form are coming together to form a layer and how the multiple layers are a layer and how the multiple layers are coming again together stacked to coming again together stacked to implement this binary classification implement this binary classification task in this video that we have task in this video that we have looked at on cats versus dogs now we looked at on cats versus dogs now we will be using exactly the same model", "image_path": "img_data/video_49_chunk_0.jpg"}
{"video": "video_49", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "looked at on cats versus dogs now we will be using exactly the same model will be using exactly the same model that we have built for that kind of task that we have built for that kind of task and in this specific case what we were and in this specific case what we were interested now to see is to validate interested now to see is to validate what we have said earlier about the some what we have said earlier about the some kind of a structure or pattern that we kind of a structure or pattern that we see in the features that the see in the features that the convolution un networks kind of learn convolution un networks kind of learn in this notebook borrowed from the book in this notebook borrowed from the book deep learning with python what we deep learning with python what we actually can see a couple of things actually can see a couple of things the first is the -called u the first is the -called u the intermediate", "image_path": "img_data/video_49_chunk_1.jpg"}
{"video": "video_49", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "the first is the -called u the intermediate convolution network outputs these are convolution network outputs these are effectively what are each layer kind of effectively what are each layer kind of presents to the layer above it and i presents to the layer above it and i think it's worthwhile kind of going think it's worthwhile kind of going through that first and as we said through that first and as we said this is the sort of exactly the same this is the sort of exactly the same architecture we have seen earlier for architecture we have seen earlier for that specific data set and this is the that specific data set and this is the input images the 100 approximately input images the 100 approximately 150 by 150 pixels and natural rec col", "image_path": "img_data/video_49_chunk_2.jpg"}
{"video": "video_49", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "input images the 100 approximately 150 by 150 pixels and natural rec col 150 by 150 pixels and natural rec col images and this is the images and this is the first layer what it really learns and as first layer what it really learns and as you can see the output of the output you can see the output of the output kind of feature map it kind of presents kind of feature map it kind of presents an almost identical figure to the sort of identical figure to the sort of input picture input image except that input picture input image except that this image over here emphasizes this image over here emphasizes the edge", "image_path": "img_data/video_49_chunk_3.jpg"}
{"video": "video_49", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "this image over here emphasizes the edge these are the as we said the these are the as we said the primitive kind of shapes that the first primitive kind of shapes that the first initial layers of the conet are actually initial layers of the conet are actually learning and we can actually go and look learning and we can actually go and look at each and every layer and i think the over here i layer and i think the over here i think this im figure over here think this im figure over here shows what is really happening the shows what is really happening the initial layers are learning i will initial layers are learning i will call it a visual content the same kind call it a visual content the same kind of visual content as our eyes kind of", "image_path": "img_data/video_49_chunk_4.jpg"}
{"video": "video_49", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "call it a visual content the same kind of visual content as our eyes kind of visual content as our eyes kind of see in the image let's say you can see in the image let's say you can very clearly see the shape and form very clearly see the shape and form of the cut over here but as we of the cut over here but as we actually going further deeper into the actually going further deeper into the network then the representations that network then the representations that are actually being created are becoming are actually being created are becoming more and more abstract to the point more and more abstract to the point where this is the fifth layer as you can where this is the fifth layer as you can see from that point onwards we still see from that point onwards we still see some of the feature maps that are", "image_path": "img_data/video_49_chunk_5.jpg"}
{"video": "video_49", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "see from that point onwards we still see some of the feature maps that are some of the feature maps that are being created remember the feature maps being created remember the feature maps that are being created are volumes that are being created are volumes what actually we see here are the what actually we see here are the flattened version of those volumes we flattened version of those volumes we plot here the special plot here the special dimensions that are coming at the dimensions that are coming at the output on each image but we are output on each image but we are effectively flattening all of the effectively flattening all of the filters that we have u used all of filters that we have u used all of the depth of the feature maps", "image_path": "img_data/video_49_chunk_6.jpg"}
{"video": "video_49", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "filters that we have u used all of the depth of the feature maps the all of the depth of the feature maps that we have used to create that we have used to create this volume here you have the this volume here you have the sixth layer the seventh layer as you can sixth layer the seventh layer as you can see here we from the seventh layer see here we from the seventh layer onwards we are not really able to see onwards we are not really able to see any of the sort of visual any of the sort of visual characteristics of a cut this characteristics of a cut this become a fairly abstract kind of become a fairly abstract kind of representation here you can actually representation here you can actually also see very clearly the impact of max", "image_path": "img_data/video_49_chunk_7.jpg"}
{"video": "video_49", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "representation here you can actually also see very clearly the impact of max also see very clearly the impact of max pooling and how we can actually start pooling and how we can actually start with a representation and what is the with a representation and what is the max pooling operator with a cal of 2 2x max pooling operator with a cal of 2 2x two is actually doing is actually two is actually doing is actually picking at the more essential kind of picking at the more essential kind of in features that are presented to it in features that are presented to it if you compare this image and this image if you compare this image and that's effectively at this and that's effectively at this point we have the representations that point we have the representations that are going to be needed after the", "image_path": "img_data/video_49_chunk_8.jpg"}
{"video": "video_49", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "point we have the representations that are going to be needed after the are going to be needed after the stochastic rated descent kind of stochastic rated descent kind of converges and provided we don't have any converges and provided we don't have any over fitting and on these over fitting and on these representations are the ones that we are representations are the ones that we are going to be flattening to and then feed them into flattening to and then feed them into the fully connected layers that the fully connected layers that constitute our head and if everything constitute our head and if everything goes this head will actually see goes this head will actually see and work on those representations to and work on those representations to actually do the binary actually do the binary classification", "image_path": "img_data/video_49_chunk_9.jpg"}
{"video": "video_49", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "actually do the binary classification this is what we have seen we can this is what we have seen we can actually see in the feature maps and actually see in the feature maps and i think it's also worthwhile i think it's also worthwhile understanding what we actually see now understanding what we actually see now in as far as the filters what really in as far as the filters what really the filters the contents of those the filters the contents of those filters are to visualize those filters are to visualize those filters what we actually do is we define a what we actually do is we define a specific loss function the details are specific loss function the details are kind of outside of this course of the kind of outside of this course of the scope of this course but the at a high scope of this course but the at a high level what we do is we try to find", "image_path": "img_data/video_49_chunk_10.jpg"}
{"video": "video_49", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "scope of this course but the at a high level what we do is we try to find level what we do is we try to find input images that maximize the input images that maximize the activations that those filters produce activations that those filters produce and therefore in the process of doing and therefore in the process of doing we are able to out of this we are able to out of this optimization process to retrieve those optimization process to retrieve those filter values here we see just the filter values here we see just the first 64 filters out of the many more first 64 filters out of the many more that we have used i think we used all that we have used i think we used all the way up to 256 filters but we here we the way up to 256 filters but we here we see the first 64 filters in a 8 by8 kind see the first 64 filters in a 8 by8 kind of pattern and as you can actually", "image_path": "img_data/video_49_chunk_11.jpg"}
{"video": "video_49", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "see the first 64 filters in a 8 by8 kind of pattern and as you can actually of pattern and as you can actually see the u this filter over here is see the u this filter over here is for the various kind of layers for the various kind of layers these are effectively we have the layers these are effectively we have the layers going from the beginning of this fil going from the beginning of this fil of the beginning of the network of the beginning of the network all the way to the u towards the just all the way to the u towards the just before the head of this is before the head of this is the last layer just before the head and the last layer just before the head and as you can actually see here", "image_path": "img_data/video_49_chunk_12.jpg"}
{"video": "video_49", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "the last layer just before the head and as you can actually see here as you can actually see here in every layer we learn in every layer we learn effectively a collection of these effectively a collection of these filters that it will filters that it will decompose the input image the input decompose the input image the input feature maps are being the input feature maps are being decomposed if you go back to what we decomposed if you go back to what we have discussed earlier about the have discussed earlier about the operation of the convolutional operation of the convolutional kind of neuron imagine that you kind of neuron imagine that you have now not only just one filter but have now not only just one filter but you have let's say 64 of them", "image_path": "img_data/video_49_chunk_13.jpg"}
{"video": "video_49", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "have now not only just one filter but you have let's say 64 of them you have let's say 64 of them the each one of those filters is one the each one of those filters is one component out of the let's say 64 that component out of the let's say 64 that the input feature map will be dec the input feature map will be dec composed you can read about this as composed you can read about this as those are the components of that those are the components of that decomposition this in the last decomposition this in the last kind of layer over here we have 64 kind of layer over here we have 64 components for those who have some components for those who have some background on principal component background on principal component analysis there's some something to it", "image_path": "img_data/video_49_chunk_14.jpg"}
{"video": "video_49", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "background on principal component analysis there's some something to it analysis there's some something to it along those lines but the along those lines but the composition over here is composition over here is definitely exactly the same as any definitely exactly the same as any other decomposition it's just that these other decomposition it's just that these components as the layers are becoming components as the layers are becoming deeper and deeper are these deeper and deeper are these components are look quite different and this as you can quite different and this as you can see here the filters are simpler in see here the filters are simpler in the first layers and become the first layers and become more and more", "image_path": "img_data/video_49_chunk_15.jpg"}
{"video": "video_49", "start": "0:08:00", "end": "0:08:21.133333", "timestamp": "0:08:00 - 0:08:21.133333", "text": "the first layers and become more and more complicated in the subsequent kind of complicated in the subsequent kind of layers to match the sort of layers to match the sort of nonv visual intuitively visual nonv visual intuitively visual complexities that we have seen in the complexities that we have seen in the output activation maps or feature maps output activation maps or feature maps that we have seen earlier", "image_path": "img_data/video_49_chunk_16.jpg"}
{"video": "video_50", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this example the notebook is quite instructive because it refers to a instructive because it refers to a small data set and i think working small data set and i think working with small data sets are actually handy with small data sets are actually handy in the beginning when you are trying to in the beginning when you are trying to understand what is going on here we have understand what is going on here we have the classic case of dogs versus cats we the classic case of dogs versus cats we have also the simplest possible task in have also the simplest possible task in machine learning which is classification machine learning which is classification image classification in this case and we image classification in this case and we are going to be using convolutional", "image_path": "img_data/video_50_chunk_0.jpg"}
{"video": "video_50", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "image classification in this case and we are going to be using convolutional are going to be using convolutional layers in order to detect no sorry to layers in order to detect no sorry to detect to classify the presence of a dog or classify the presence of a dog or a cat on in an image or cats in a cat on in an image or cats in this case all right the data this case all right the data set is available in kagle the original set is available in kagle the original data set contained 25,000 images but we data set contained 25,000 images but we have cut down to 1,000 images per have cut down to 1,000 images per class and we have a split the data", "image_path": "img_data/video_50_chunk_1.jpg"}
{"video": "video_50", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "have cut down to 1,000 images per class and we have a split the data class and we have a split the data set into train and validation and set into train and validation and test data set all right we are test data set all right we are going to obviously use train and going to obviously use train and validation to create if you our validation to create if you our model and of course we are going to model and of course we are going to exercise some kind of prediction api exercise some kind of prediction api using our test data set after a model is using our test data set after a model is created the architecture we're going created the architecture we're going to be using here is an architecture that to be using here is an architecture that we have kind of developed specifically we have kind of developed specifically for this example", "image_path": "img_data/video_50_chunk_2.jpg"}
{"video": "video_50", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "we have kind of developed specifically for this example is consist evidently of convolutional is consist evidently of convolutional and interleaf with max pulling and interleaf with max pulling layers and probably you recognize layers and probably you recognize the api here in this case is a kind of a the api here in this case is a kind of a caras api similar architectures can caras api similar architectures can be develop for py the first layer be develop for py the first layer over here is a convolutional layer over here is a convolutional layer the there is input images of the there is input images of 150 by 150 pixels this is what the imag", "image_path": "img_data/video_50_chunk_3.jpg"}
{"video": "video_50", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "the there is input images of 150 by 150 pixels this is what the imag 150 by 150 pixels this is what the imag that we have transformed now are that we have transformed now are and each image is a naturally colored and each image is a naturally colored image of three channels red green and image of three channels red green and blue we have u 3x3 kernels and we blue we have u 3x3 kernels and we have the 32 here indicates the number of have the 32 here indicates the number of filters or convolutional neurons filters or convolutional neurons and we are going to be using a rectified and we are going to be using a rectified linear unit they exactly the same linear unit they exactly the same nonlinearity that we have used in the nonlinearity that we have used in the fully connected layers then we are passing the output", "image_path": "img_data/video_50_chunk_4.jpg"}
{"video": "video_50", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "fully connected layers then we are passing the output layers then we are passing the output feature map produced here and by the way feature map produced here and by the way this is where you can actually see the this is where you can actually see the usage of that kind of formula the usage of that kind of formula which i was pointing out regarding the which i was pointing out regarding the output feature map dimensions in an output feature map dimensions in an earlier video the max pooling earlier video the max pooling layer in this case is 2x two and it will layer in this case is 2x two and it will further shrink the output feat m further shrink the output feat m produced by the first layer selecting produced by the first layer selecting the most important features out of it the most important features out of it passing it over to a convolutional", "image_path": "img_data/video_50_chunk_5.jpg"}
{"video": "video_50", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "the most important features out of it passing it over to a convolutional passing it over to a convolutional layer with 64 filters here you see layer with 64 filters here you see now the pattern of increasing the number now the pattern of increasing the number of filters as the network becomes deeper of filters as the network becomes deeper and at some point after and deeper and at some point after one two three four layers four one two three four layers four convolutional layers we are going to convolutional layers we are going to have the head and i think it's have the head and i think it's worthwhile going back into this vgg worthwhile going back into this vgg kind of architecture and look exactly kind of architecture and look exactly where that head was in that architecture", "image_path": "img_data/video_50_chunk_6.jpg"}
{"video": "video_50", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "kind of architecture and look exactly where that head was in that architecture where that head was in that architecture and couple it with this code and couple it with this code here is the point where the head here is the point where the head starts and the head in this case is a starts and the head in this case is a concatenation of fully connected layers concatenation of fully connected layers why we have this kind of concatenation why we have this kind of concatenation and want do just a single layer is and want do just a single layer is gradually even within the head gradually even within the head we need to gradually reach this point of we need to gradually reach this point of desired number of classes we have a desired number of classes we have a classification use case here this is classification use case here this is a thousand classes that are need to be", "image_path": "img_data/video_50_chunk_7.jpg"}
{"video": "video_50", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "classification use case here this is a thousand classes that are need to be a thousand classes that are need to be present in at the top of the at the end present in at the top of the at the end of the of this u network and this is of the of this u network and this is basically the dimensionality of our basically the dimensionality of our posterior probability distribution posterior probability distribution we're going to have the a y hat if you we're going to have the a y hat if you that consist of a thousand numbers that consist of a thousand numbers a thousand are also the are the a thousand are also the are the number of classes in the image net data number of classes in the image net data set this dimensions correspond set this dimensions correspond to the image net classifier data", "image_path": "img_data/video_50_chunk_8.jpg"}
{"video": "video_50", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "set this dimensions correspond to the image net classifier data to the image net classifier data set and that's basically our head set and that's basically our head there is also seen over here in this there is also seen over here in this code with this portion of the model code with this portion of the model we have whatever we have produced in we have whatever we have produced in terms of convolutions over here and then terms of convolutions over here and then we flatten the network we flatten oh we flatten the network we flatten oh sorry flatten the output feature map sorry flatten the output feature map there by flattening the output f map we there by flattening the output f map we are creating effectively a volume are creating effectively a volume we're taking a volume at the input and we're taking a volume at the input and we're flattening into a vector", "image_path": "img_data/video_50_chunk_9.jpg"}
{"video": "video_50", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "we're taking a volume at the input and we're flattening into a vector we're flattening into a vector vor and this vector then is passed as vor and this vector then is passed as input to two dense layers the first dense to two dense layers the first dense layer is has 512 neurons it takes layer is has 512 neurons it takes whatever dimensionality and we'll see whatever dimensionality and we'll see now the dimensions in a moment the now the dimensions in a moment the flatten layer provided and reduces that flatten layer provided and reduces that just any fully connected layer we just any fully connected layer we have seen in a corresponding video in a have seen in a corresponding video in a different video earlier into 512 different video earlier into 512 dimensions", "image_path": "img_data/video_50_chunk_10.jpg"}
{"video": "video_50", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "different video earlier into 512 dimensions and we use the rectified linear unit for and we use the rectified linear unit for that and then with the subsequent layer that and then with the subsequent layer takes 512 dimensions and reduces it takes 512 dimensions and reduces it further into gas into a single further into gas into a single dimension because as we have seen in the dimension because as we have seen in the binary classification we have a binary classification we have a binary classification use case here either classification use case here either we're going to have a cats or dogs we're going to have a cats or dogs we have just a scaler that we need have just a scaler that we need because that is the probability of the because that is the probability of the positive glass whatever that positive glass whatever that positive classes probably the dogs here and", "image_path": "img_data/video_50_chunk_11.jpg"}
{"video": "video_50", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "positive glass whatever that positive classes probably the dogs here and classes probably the dogs here and we are of course going to be using we are of course going to be using sigmoid because only at the output of sigmoid because only at the output of the sigmoid we are actually getting this the sigmoid we are actually getting this form of the posterior probability as form of the posterior probability as we had discussed in the fully we had discussed in the fully connected layers and in that connected layers and in that lecture all right this is lecture all right this is basically our architecture very basically our architecture very simple architecture the convolution simple architecture the convolution portions the flatten and the fully portions the flatten and the fully connected or dense portion to provide connected or dense portion to provide the binary classification result at the", "image_path": "img_data/video_50_chunk_12.jpg"}
{"video": "video_50", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "connected or dense portion to provide the binary classification result at the binary classification result at the output and here is the u details of output and here is the u details of our cnn we can see the of our cnn we can see the input images that are actually we input images that are actually we coming in the first we have 32 coming in the first we have 32 filters as we discussed in terms of filters as we discussed in terms of number of parameters 896 18,000", "image_path": "img_data/video_50_chunk_13.jpg"}
{"video": "video_50", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "parameters 896 18,000 73,000 147,000 all of these are parameters that you see being quoted parameters that you see being quoted here in the next to the convolutional here in the next to the convolutional layers but the most striking thing layers but the most striking thing over here is this look at the number of over here is this look at the number of parameters which are involved in the parameters which are involved in the fully connected in one fully connected in one fully connected or dense layer 3.2 million parameters or dense layer 3.2 million parameters out of the total 3 and a2 million out of the total 3 and a2 million parameters that we have 3.2 million are", "image_path": "img_data/video_50_chunk_14.jpg"}
{"video": "video_50", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "out of the total 3 and a2 million parameters that we have 3.2 million are parameters that we have 3.2 million are associated with a fully connected layer associated with a fully connected layer and here is the kind of striking and here is the kind of striking example of why it would make sense to example of why it would make sense to actually use cnn for image actually use cnn for image classification if we didn't have the cnn classification if we didn't have the cnn and the associated advantage of that cnn and the associated advantage of that cnn provide which was actually also shown in provide which was actually also shown in this kind of snapshot architecture as this kind of snapshot architecture as you can see only the loc", "image_path": "img_data/video_50_chunk_15.jpg"}
{"video": "video_50", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "this kind of snapshot architecture as you can see only the loc pixels the one which are local to the special dimensions of local to the special dimensions of the filter are -called firing in the filter are -called firing in order to produce that kind of scaler order to produce that kind of scaler as compared to a fully connected as compared to a fully connected architecture where everything that we architecture where everything that we have here is going to be connected to have here is going to be connected to the layer to form if you the layer to form if you the output scaler z the convolutions the output scaler z the convolutions are operation is actually helping us", "image_path": "img_data/video_50_chunk_16.jpg"}
{"video": "video_50", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "the output scaler z the convolutions are operation is actually helping us are operation is actually helping us to significantly reduce the number of to significantly reduce the number of parameters at the end of the day parameters at the end of the day we have u the scalar that indicates we have u the scalar that indicates the posterior probability of the posterior probability of the positive class as we discussed and then positive class as we discussed and then the architecture is seems to be the architecture is seems to be valid we are going to evidently going valid we are going to evidently going to use binary cross entropy just to use binary cross entropy just what we have done earlier in that what we have done earlier in that other video where we looked at dense", "image_path": "img_data/video_50_chunk_17.jpg"}
{"video": "video_50", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "what we have done earlier in that other video where we looked at dense other video where we looked at dense layers only for binary classification or layers only for binary classification or multiclass classification and we are going to classification and we are going to have here well here the author selected have here well here the author selected the rms prop which is one of the cousins of stochastic is one of the cousins of stochastic gr descent we haven't really got any gr descent we haven't really got any discussion specifically on enhancements discussion specifically on enhancements of stochastic gr descent but if you do of stochastic gr descent but if you do replace it with sgd i think you will be replace it with sgd i think you will be getting very similar performance with getting very similar performance with the corresponding learning parameter and", "image_path": "img_data/video_50_chunk_18.jpg"}
{"video": "video_50", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "getting very similar performance with the corresponding learning parameter and the corresponding learning parameter and then of course the metric is our then of course the metric is our accuracy and one of the things that we accuracy and one of the things that we would to point out in u in this would to point out in u in this kind of convolution and networks is kind of convolution and networks is that we will need to do to be careful that we will need to do to be careful when we first take a data set and we when we first take a data set and we try to process the images as we have try to process the images as we have seen the images are typically given to seen the images are typically given to us as with pixels corresponds to us as with pixels corresponds to integer numbers we have to integer numbers we have to definitely normalize them we have to definitely normalize them we have to b them we have to do a lot of this kind", "image_path": "img_data/video_50_chunk_19.jpg"}
{"video": "video_50", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "definitely normalize them we have to b them we have to do a lot of this kind b them we have to do a lot of this kind of transformations in order for us to of transformations in order for us to produce the right inputs produce the right inputs for the for our network after for the for our network after a training process that involves a training process that involves multiple epochs as we would expect we multiple epochs as we would expect we have a model and we can actually plot have a model and we can actually plot the training and validation loss as well also the corresponding kind loss as well also the corresponding kind of accuracy and look at the corresp of accuracy and look at the corresp responding loss over here plot as the", "image_path": "img_data/video_50_chunk_20.jpg"}
{"video": "video_50", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "of accuracy and look at the corresp responding loss over here plot as the responding loss over here plot as the number of epochs and remember what we number of epochs and remember what we have said in at the another video have said in at the another video regarding the condition of regarding the condition of overfitting and at that time the overfitting and at that time the discussion was an example of a linear discussion was an example of a linear model on the regression task over here model on the regression task over here we have a classification task but the we have a classification task but the sort of problem of over fitting is the sort of problem of over fitting is present in across tasks in machine present in across tasks in machine learning we see some quite learning we see some quite significant difference between", "image_path": "img_data/video_50_chunk_21.jpg"}
{"video": "video_50", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "learning we see some quite significant difference between significant difference between training and validation as the accuracy training and validation as the accuracy is improving and that is really what we improving and that is really what we have said earlier as an a good indicator have said earlier as an a good indicator of overfitting it seems that of overfitting it seems that the network that we have designed the network that we have designed over here overfits the data set we are over here overfits the data set we are given and it shouldn't be a complete given and it shouldn't be a complete surprise to us given the fact that we surprise to us given the fact that we are throwing a significant number of are throwing a significant number of parameters in", "image_path": "img_data/video_50_chunk_22.jpg"}
{"video": "video_50", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "are throwing a significant number of parameters in a network in a data set which only in a network in a data set which only has a th000 labels per class and has a th000 labels per class and we can actually engage any of the we can actually engage any of the techniques that we have seen in techniques that we have seen in overfitting to address overfitting to address overfitting such as weight decay any of the such as weight decay any of the regularization techniques that we have regularization techniques that we have seen also in neuron networks to seen also in neuron networks to address it but in computer vision we address it but in computer vision we have something else that could actually have something else that could actually help us and this is actually called the help us and this is actually called the documentation i think it's worthwhile documentation i think it's worthwhile going through", "image_path": "img_data/video_50_chunk_23.jpg"}
{"video": "video_50", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "documentation i think it's worthwhile going through the data augmentation because it is the data augmentation because it is really a fairly straightforward and really a fairly straightforward and widely used approach to avoid the widely used approach to avoid the situation such as this where we have situation such as this where we have overfeeding in that augmentation what overfeeding in that augmentation what we actually do we are taking the input we actually do we are taking the input images and given the fact that we have images and given the fact that we have the knowledge of the class we try to the knowledge of the class we try to transform these input images in creating transform these input images in creating more data that's the an artificial more data that's the an artificial way of increasing the number of labels way of increasing the number of labels we have in our data set we have in our data set we have various kind of transformations we may", "image_path": "img_data/video_50_chunk_24.jpg"}
{"video": "video_50", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "we have in our data set we have various kind of transformations we may various kind of transformations we may be shifting rotating images we may be shifting rotating images we may sharing the image we have we are sharing the image we have we are zooming in zooming out and zooming in zooming out and flipping and on we are definitely flipping and on we are definitely going to be creating some nasty going to be creating some nasty cats or dogs but definitely this cats or dogs but definitely this helps our network to not overfit and helps our network to not overfit and if you are to just keep the exactly the if you are to just keep the exactly the same network chitecture as we have seen", "image_path": "img_data/video_50_chunk_25.jpg"}
{"video": "video_50", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "if you are to just keep the exactly the same network chitecture as we have seen same network chitecture as we have seen earlier not touch at all the model earlier not touch at all the model but definitely train the model with this but definitely train the model with this additional kind of data set then additional kind of data set then look what happened we have a look what happened we have a training and validation loss which are training and validation loss which are very close to each other we actually very close to each other we actually have solved the overfeeding problem have solved the overfeeding problem and our accuracy is both in terms of and our accuracy is both in terms of training and validation are also very training and validation are also very close and close to some something close and close to some something 85% i think this is a good", "image_path": "img_data/video_50_chunk_26.jpg"}
{"video": "video_50", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "close and close to some something 85% i think this is a good 85% i think this is a good example to showcase the u cnn models example to showcase the u cnn models as a as working for the simple task as a as working for the simple task of image classification and what we of image classification and what we actually also would to understand actually also would to understand now next is what we have said earlier now next is what we have said earlier about hey what how can we have some kind about hey what how can we have some kind of visualization into the internals of visualization into the internals of the cnn to understand what is the cnn to understand what is actually learning and this is what actually learning and this is what we will be discussing next", "image_path": "img_data/video_50_chunk_27.jpg"}
{"video": "video_50", "start": "0:14:00", "end": "0:14:01.966667", "timestamp": "0:14:00 - 0:14:01.966667", "text": "actually learning and this is what we will be discussing next", "image_path": "img_data/video_50_chunk_28.jpg"}
{"video": "video_51", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "this is basically what is happening in a an example over here where we in a an example over here where we have an input image and we are actually sliding image and we are actually sliding a kernel a 3x3 kernel and we are a kernel a 3x3 kernel and we are getting an output feature map getting an output feature map the input feature map here is just the input feature map here is just has one we will be calling sometimes has one we will be calling sometimes this depth channel", "image_path": "img_data/video_51_chunk_0.jpg"}
{"video": "video_51", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "has one we will be calling sometimes this depth channel and the output feature map has again and the output feature map has again one channel over here and what we have here is we have here and what we have here is we have a couple of things that we need to a couple of things that we need to introduce as terms in convolutional introduce as terms in convolutional operations that we actually doing and operations that we actually doing and inside the convolutional new networks inside the convolutional new networks the first is the form of the first is the form of the concept of padding and typically we concept of padding and typically we are padding the u input feature", "image_path": "img_data/video_51_chunk_1.jpg"}
{"video": "video_51", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "concept of padding and typically we are padding the u input feature are padding the u input feature maps in order to do two things maps in order to do two things we achieving two things probably you we achieving two things probably you have noticed that in an earlier kind have noticed that in an earlier kind of discussion or we had the operation of discussion or we had the operation of cross correlation operation in this of cross correlation operation in this kind of image over here the in this kind of image over here the output feature map was always smaller in output feature map was always smaller in terms of speciaal dimensions compared to terms of speciaal dimensions compared to the input feature map and it is the input feature map and it is evidently because the only way", "image_path": "img_data/video_51_chunk_2.jpg"}
{"video": "video_51", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "the input feature map and it is evidently because the only way evidently because the only way that this output feature mk can be that this output feature mk can be exactly the same size as the input is exactly the same size as the input is when the kernel is 1 by one when the kernel is 1 by one when the kel has a special extent of 1 by one kel has a special extent of 1 by one then we have exactly that situation then we have exactly that situation but in most cases where the can won't but in most cases where the can won't be one by one we will expect this output be one by one we will expect this output feature maps to shrink in terms of feature maps to shrink in terms of spatial content and we do not want them spatial content and we do not want them to shrink too much because sooner or to shrink too much because sooner or later we will be running out of", "image_path": "img_data/video_51_chunk_3.jpg"}
{"video": "video_51", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "to shrink too much because sooner or later we will be running out of later we will be running out of spatial dimensions in our outputs and spatial dimensions in our outputs and therefore we cannot really go deep to therefore we cannot really go deep to construct deep architectures in these construct deep architectures in these networks what we expect to do what we networks what we expect to do what we are have done is we have with padding we are have done is we have with padding we are trying to manage this special extent are trying to manage this special extent reduction on one hand as you can see reduction on one hand as you can see if we had this padding over here then if we had this padding over here then the output feature map is going the output feature map is going to be much larger than otherwise", "image_path": "img_data/video_51_chunk_4.jpg"}
{"video": "video_51", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "the output feature map is going to be much larger than otherwise to be much larger than otherwise if you can imagine that without a if you can imagine that without a padding then this sort of padding then this sort of output feature map would actually be output feature map would actually be i can't really sort of tell you i can't really sort of tell you exactly the dimensions but if you do the exactly the dimensions but if you do the if you see the sort of if you do it if you see the sort of if you do it visually then you can actually see it's visually then you can actually see it's going to be probably something a going to be probably something a 3x3 output now the sort of 3x3 output now the sort of another advantage of padding", "image_path": "img_data/video_51_chunk_5.jpg"}
{"video": "video_51", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "3x3 output now the sort of another advantage of padding another advantage of padding is that we allow the kel to actually is that we allow the kel to actually move into locations which would not be move into locations which would not be able to move otherwise a kernel as we able to move otherwise a kernel as we discussed a bit earlier contains some discussed a bit earlier contains some values and we would all of the values and we would all of the pixels including the edge pixels of the pixels including the edge pixels of the input feature map to be able to be input feature map to be able to be correlated with all of the other pixels correlated with all of the other pixels of the all of the pixels of the all of the pixels of the kel and therefore padding allows us", "image_path": "img_data/video_51_chunk_6.jpg"}
{"video": "video_51", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "of the all of the pixels of the kel and therefore padding allows us kel and therefore padding allows us to do otherwise you can to do otherwise you can imagine this red kernel over here will imagine this red kernel over here will actually be only be able to correlate actually be only be able to correlate with those three pixels of the input with those three pixels of the input feature map now this pixel over here feature map now this pixel over here can be correlated with both this pixel can be correlated with both this pixel of the kernel and that pixel of the and of the kernel and that pixel of the and this pixel of the sort of this pixel and this pixel of the sort of kel that we have the", "image_path": "img_data/video_51_chunk_7.jpg"}
{"video": "video_51", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "this pixel and this pixel of the sort of kel that we have the kel that we have the ability to sort of get more ability to sort of get more information especially towards the edges information especially towards the edges of that input feature map with of that input feature map with padding another parameter that we padding another parameter that we should also sort of understand is should also sort of understand is this kind of stride is the this kind of stride is the just the stride that you as you just the stride that you as you walk it this here actually refers to walk it this here actually refers to the number of", "image_path": "img_data/video_51_chunk_8.jpg"}
{"video": "video_51", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "walk it this here actually refers to the number of pixels that you are skipping over pixels that you are skipping over the in order for you to be able to the in order for you to be able to do the next correlation here you see do the next correlation here you see two locations of that kernel in that two locations of that kernel in that location and the blue location the red location and the blue location the red location the blue location if your location the blue location if your stride was one then the blue k have stride was one then the blue k have been right here and while with a stride of two here and while with a stride of two then we don't get one correlation", "image_path": "img_data/video_51_chunk_9.jpg"}
{"video": "video_51", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "here and while with a stride of two then we don't get one correlation then we don't get one correlation operation for every pixel of the input operation for every pixel of the input feature map and this is obviously is feature map and this is obviously is helping us to manage the complexity helping us to manage the complexity of these filters in fact goes slightly of these filters in fact goes slightly to the opposite direction of what we to the opposite direction of what we have said earlier in a sense that in have said earlier in a sense that in some instances we prefer to get for some instances we prefer to get for some of the layers of the some of the layers of the convolutional neuron the stride convolutional neuron the stride parameter to be larger than one parameter to be larger than one typically the stride parameter of", "image_path": "img_data/video_51_chunk_10.jpg"}
{"video": "video_51", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "parameter to be larger than one typically the stride parameter of typically the stride parameter of height and width are going to be the height and width are going to be the same that's what you see over here same that's what you see over here bottom line is that all of these bottom line is that all of these parameters and far more that are to parameters and far more that are to follow are hyper parameters and we follow are hyper parameters and we are going to be optimizing them are going to be optimizing them for using hyperparameter optimization in for using hyperparameter optimization in order for us to define the complete order for us to define the complete architecture of a cnn here you see some animations that cnn here you see some animations that kind of reinforce what we have just kind of reinforce what we have just quoted ed without padding the", "image_path": "img_data/video_51_chunk_11.jpg"}
{"video": "video_51", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "kind of reinforce what we have just quoted ed without padding the quoted ed without padding the kel the output feature map is going to kel the output feature map is going to be u potentially significantly reduced be u potentially significantly reduced in terms of spatial extent something in terms of spatial extent something will make any subsequent cor will make any subsequent cor correlation with kels not correlation with kels not very useful with padding this is we very useful with padding this is we avoid that and here we actually have avoid that and here we actually have padding combinations of padding and padding combinations of padding and stride i suggest that you study this stride i suggest that you study this kind of animations to just get the gist", "image_path": "img_data/video_51_chunk_12.jpg"}
{"video": "video_51", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "stride i suggest that you study this kind of animations to just get the gist kind of animations to just get the gist as to what padding and stride are as to what padding and stride are actually offering to us but now the actually offering to us but now the time has come to look at the time has come to look at the operation of the convolutional neuron operation of the convolutional neuron network and in fact the describe if network and in fact the describe if you the single convolutional kind you the single convolutional kind of layer in detail we will of layer in detail we will start drawing a snapshot of a cnn layer start drawing a snapshot of a cnn layer operation that will actually help us to operation that will actually help us to understand the general case", "image_path": "img_data/video_51_chunk_13.jpg"}
{"video": "video_51", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "operation that will actually help us to understand the general case understand the general case where we have input u and output where we have input u and output feature maps coming into the cnn feature maps coming into the cnn layer but however these input and layer but however these input and output feature maps possess different output feature maps possess different depths and this is another parameter depths and this is another parameter that we have to understand that we have to understand that the we are responsible for that the we are responsible for designing these layers with that designing these layers with that the depth of what we will produce is the depth of what we will produce is our responsibility to design our responsibility to design let's write now draw if you a", "image_path": "img_data/video_51_chunk_14.jpg"}
{"video": "video_51", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "our responsibility to design let's write now draw if you a let's write now draw if you a picture of that cnn layer in a picture of that cnn layer in operation let me call it the operation let me call it the snapshot we will see just a single snapshot we will see just a single snapshot of that layer and this will also help us layer and this will also help us understand the u what is the understand the u what is the convolutional neuron we already have convolutional neuron we already have seen the sort of sigmoidal kind of seen the sort of sigmoidal kind of neuron now we will see in the neuron now we will see in the fully connected dense layer fully connected dense layer architectures now we'll see the architectures now we'll see the convolutional neuron in front of us", "image_path": "img_data/video_51_chunk_15.jpg"}
{"video": "video_51", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "architectures now we'll see the convolutional neuron in front of us convolutional neuron in front of us the snapshot let me call it snapshot of the snapshot let me call it snapshot of a cnn layer operation all right let's draw now the general case as we discussed that we the general case as we discussed that we have an input volume", "image_path": "img_data/video_51_chunk_16.jpg"}
{"video": "video_51", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "the general case as we discussed that we have an input volume this input volume is associated with h the volume is associated with h the output feature map of an earlier layer output feature map of an earlier layer let's call that layer l minus one this let's call that layer l minus one this is basically the feature map that was is basically the feature map that was generated by the previous layer in generated by the previous layer in general and we'll have a depth of general and we'll have a depth of capital m l minus one it will have some kind of width", "image_path": "img_data/video_51_chunk_17.jpg"}
{"video": "video_51", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "capital m l minus one it will have some kind of width one it will have some kind of width let me make sure that you can let me make sure that you can actually see here this is wl minus see here this is wl minus one and the height over here would and the height over here would actually be this is a depth and actually be this is a depth and the height over here will actually be the height over here will actually be h l minus one all right that's h l minus one all right that's basically the dimensions of my incoming basically the dimensions of my incoming volume and this incoming volume", "image_path": "img_data/video_51_chunk_18.jpg"}
{"video": "video_51", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "basically the dimensions of my incoming volume and this incoming volume and this incoming volume has some kind of resolution in terms of has some kind of resolution in terms of number of height and width pixels let me number of height and width pixels let me just draw them quickly because we would just draw them quickly because we would to now draw the u what will be the to now draw the u what will be the output of out of this operation which is output of out of this operation which is the output feature map now the output feature map now the output is going to be generated at output is going to be generated at this specific moment in time i have in this specific moment in time i have in general a filter that", "image_path": "img_data/video_51_chunk_19.jpg"}
{"video": "video_51", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "this specific moment in time i have in general a filter that general a filter that has 3x3 special extent it is located has 3x3 special extent it is located let's say here at this moment in time let's say here at this moment in time because that's why you call it a because that's why you call it a snapshot and it has some depth i snapshot and it has some depth i want to discuss a little bit the depth want to discuss a little bit the depth what makes sense for this depth of the what makes sense for this depth of the filter to be but it when it is filter to be but it when it is located over here for sure i'm located over here for sure i'm expecting to have some output feature expecting to have some output feature map this output feature map will be", "image_path": "img_data/video_51_chunk_20.jpg"}
{"video": "video_51", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "expecting to have some output feature map this output feature map will be map this output feature map will be probably smaller in terms of spal extent that's why i'm kind of drawing it that's why i'm kind of drawing it this it has some kind of a number this it has some kind of a number of pixels and we have some kind of a depth and this depth is definitely depth and this depth is definitely something that i need to control something that i need to control because it's one of my main design", "image_path": "img_data/video_51_chunk_21.jpg"}
{"video": "video_51", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "something that i need to control because it's one of my main design because it's one of my main design parameters i'll be calling this depth parameters i'll be calling this depth ml and evidently we have a ml and evidently we have a different hl and wl dimensions and this is basically my dimensions and this is basically my volumes input and output volumes input and output volumes in general going to have input volumes in general going to have input and output fors the question i and output fors the question i actually have right now is to understand actually have right now is to understand a little bit about the depth of the", "image_path": "img_data/video_51_chunk_22.jpg"}
{"video": "video_51", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "actually have right now is to understand a little bit about the depth of the a little bit about the depth of the filter and we have three options either filter and we have three options either the depth of the filter will actually be the depth of the filter will actually be deeper than the input feature map deeper than the input feature map shallower than the input feature map or shallower than the input feature map or exactly the same depth as the input exactly the same depth as the input feature map let's try to do some kind feature map let's try to do some kind of reasoning over here does it make any of reasoning over here does it make any sense for the filter to be deeper than sense for the filter to be deeper than the input feature map and the input feature map and if you think about it the answer is no", "image_path": "img_data/video_51_chunk_23.jpg"}
{"video": "video_51", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "the input feature map and if you think about it the answer is no if you think about it the answer is no it does not really make a lot of sense it does not really make a lot of sense because at the end of the day we are because at the end of the day we are going to be correlating the contents of going to be correlating the contents of that filter with the contents of the that filter with the contents of the input feature map and if the filter is input feature map and if the filter is actually deeper then we are not going actually deeper then we are not going to be picking up anything from the input to be picking up anything from the input feature map because we are going to feature map because we are going to why have it deeper there's no point of doing if it is there's no point of doing if it is shallower than the input feature map shallower than the input feature map also it does not really make a lot of also it does not really make a lot of sense because we are going to leave", "image_path": "img_data/video_51_chunk_24.jpg"}
{"video": "video_51", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "also it does not really make a lot of sense because we are going to leave sense because we are going to leave content that the input feature map u content that the input feature map u contains for us on the table contains for us on the table the only reasonable assumption is the only reasonable assumption is this filter to be exactly the same in terms of the be exactly the same in terms of the input feature map depth right in terms input feature map depth right in terms of this terms of depth of the input f of this terms of depth of the input f map it's just basically draw it as map it's just basically draw it as such and it in fact it is really this such and it in fact it is really this filter that is going to be", "image_path": "img_data/video_51_chunk_25.jpg"}
{"video": "video_51", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "such and it in fact it is really this filter that is going to be filter that is going to be a going to be the one that we are a going to be the one that we are going to be using to do this kind of a going to be using to do this kind of a three-dimensional kind of a correlation three-dimensional kind of a correlation over here now to understand the contents over here now to understand the contents of that correlation is kind of important of that correlation is kind of important and what is actually even more important and what is actually even more important to understand what it will generate as to understand what it will generate as we will see shortly what it will not we will see shortly what it will not generate it will not generate the whole generate it will not generate the whole volume over here but it will actually volume over here but it will actually generate only one slice out of that", "image_path": "img_data/video_51_chunk_26.jpg"}
{"video": "video_51", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "volume over here but it will actually generate only one slice out of that generate only one slice out of that output volume to understand that output volume to understand that kind of important point let's do kind of important point let's do the following let me take the following let me take the sort of for that specific snapshot sort of for that specific snapshot that i'm actually right now i'm that i'm actually right now i'm generating the specific let me drew that there some", "image_path": "img_data/video_51_chunk_27.jpg"}
{"video": "video_51", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "specific let me drew that there some specific result which is a scaler therefore it's a result of a single therefore it's a result of a single pixel from this column which is pixel from this column which is located at the coordinate i comma located at the coordinate i comma j specially wise and i hope you j specially wise and i hope you remember what we have seen earlier in remember what we have seen earlier in the sort of example architecture the sort of example architecture sorry in the cnn architecture diagram sorry in the cnn architecture diagram we are let just show you u that kind of", "image_path": "img_data/video_51_chunk_28.jpg"}
{"video": "video_51", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "sorry in the cnn architecture diagram we are let just show you u that kind of we are let just show you u that kind of diagram again for that specific snapshot diagram again for that specific snapshot let's say the blue la snapshot i'm let's say the blue la snapshot i'm actually generating this scalar result actually generating this scalar result and using just one kernel a filter and using just one kernel a filter of depth one in this case as it will of depth one in this case as it will actually as it actually turns out actually as it actually turns out that fil that filter at that specific snapshot it filter at that specific snapshot it will do a three-dimensional correlation will do a three-dimensional correlation and it will still generate a single scaler for me", "image_path": "img_data/video_51_chunk_29.jpg"}
{"video": "video_51", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "and it will still generate a single scaler for me still generate a single scaler for me and that single scalar will be at a and that single scalar will be at a specific depth and the special specific depth and the special coordinates of that scalar is i comma j coordinates of that scalar is i comma j that the one i just drew now we will that the one i just drew now we will call that depth with an index in a call that depth with an index in a moment but what i want to do here is moment but what i want to do here is to just draw the complete to just draw the complete column of pixels at i comma j", "image_path": "img_data/video_51_chunk_30.jpg"}
{"video": "video_51", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "column of pixels at i comma j of pixels at i comma j location let me just rotate them this location let me just rotate them this column 90° and write it over here it will be 90° and write it over here it will be evidently this dimension will be ml the evidently this dimension will be ml the depth dimension and this is the depth dimension and this is the because we are correspond to the earth because we are correspond to the earth layer and let me just do exactly the layer and let me just do exactly the same thing with the filter i'm same thing with the filter i'm actually taking the filter and actually taking the filter and decompose it over here to the 3x3", "image_path": "img_data/video_51_chunk_31.jpg"}
{"video": "video_51", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "actually taking the filter and decompose it over here to the 3x3 decompose it over here to the 3x3 kels that it contains and these are going to be my contains and these are going to be my 3x3 kernels and this will be of dimension ml minus1 just took the filter rotated", "image_path": "img_data/video_51_chunk_32.jpg"}
{"video": "video_51", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "minus1 just took the filter rotated 90° and just decompos into its kels this 90° and just decompos into its kels this is the l minus one layer and since i'm going to be layer and since i'm going to be generating a scaler let's assume that generating a scaler let's assume that i'm generating right now at that i'm generating right now at that specific snap sort the this is the i specific snap sort the this is the i comma j coordinate this is the column comma j coordinate this is the column that corresponds to the e layer and the that corresponds to the e layer and the i comma j coordinate let's assume that i'm coordinate let's assume that i'm generating this scalar over here this", "image_path": "img_data/video_51_chunk_33.jpg"}
{"video": "video_51", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "coordinate let's assume that i'm generating this scalar over here this generating this scalar over here this scalar is going to be represented by the scalar is going to be represented by the letter z and we'll have evidently i comma j as a special coordinates and we have a depth coordinates and we have a depth coordinate which i will designate with coordinate which i will designate with the letter kl and evidently 1 is less than or equal kl and evidently 1 is less than or equal to kl is less than or equal to kl is less than or equal to ml and this will actually be the ml and this will actually be the values that the kl index which is the", "image_path": "img_data/video_51_chunk_34.jpg"}
{"video": "video_51", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "ml and this will actually be the values that the kl index which is the values that the kl index which is the depth index can take and i will actually be using also a take and i will actually be using also a corresponding index to address each corresponding index to address each one of those kels which are going to one of those kels which are going to be used for the determining that be used for the determining that kind of scalar z that scalar z is kind of scalar z that scalar z is going to be produced by using all of the kels of be produced by using all of the kels of the", "image_path": "img_data/video_51_chunk_35.jpg"}
{"video": "video_51", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "be produced by using all of the kels of the filter and i am going to also need to filter and i am going to also need to define two other define to define two other indexes the first index is going to be u indexes the first index is going to be u and the other index going to be v and the other index going to be v and this indices will actually be used to as this indices will actually be used to as spatial coordinates of the kernel spatial coordinates of the kernel my equation is the following given i comma j comma kl given in other words the", "image_path": "img_data/video_51_chunk_36.jpg"}
{"video": "video_51", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "comma j comma kl given in other words the j comma kl given in other words the coordinates of the scalar which i want to the scalar which i want to generate my scalar z i comma j comma kl generate my scalar z i comma j comma kl are going to be given by three are going to be given by three summations the first two summations i summations the first two summations i have seen already in the plain two- have seen already in the plain two- dimensional correlation operation the dimensional correlation operation the one that we just did in an in a earlier one that we just did in an in a earlier this is", "image_path": "img_data/video_51_chunk_37.jpg"}
{"video": "video_51", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "one that we just did in an in a earlier this is u summation over u and v definitely u summation over u and v definitely i'm expecting the special i'm expecting the special content of that kind of filter to be content of that kind of filter to be correlated and therefore dot correlated and therefore dot product to take the dot product with product to take the dot product with the contents of the input image the contents of the input image this is the two summations over here this is the two summations over here but also i'm expecting to now do a but also i'm expecting to now do a three-dimensional correlation operation three-dimensional correlation operation that's a third summation over an index that's a third summation over an index i'll be calling k l minus1 and this", "image_path": "img_data/video_51_chunk_38.jpg"}
{"video": "video_51", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "that's a third summation over an index i'll be calling k l minus1 and this i'll be calling k l minus1 and this index addresses the specific kernel which addresses the specific kernel which i'm going to be using kl minus one is i'm going to be using kl minus one is definitely the less than or equal to one definitely the less than or equal to one and less than or equal to ml minus one and less than or equal to ml minus one in a similar way as we have in a similar way as we have seen earlier what is this kind of a seen earlier what is this kind of a correlation it will be correlation it will be x of i + u j + v comma", "image_path": "img_data/video_51_chunk_39.jpg"}
{"video": "video_51", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "x of i + u j + v comma kl-1 time w where w now are the contents kl-1 time w where w now are the contents of the kernel that now has u comma kernel that now has u comma v comma kl minus1 all right we have in fact the w is not the cond of the kernel the w is not the cond of the kernel the cond of the kernel yes we can call them", "image_path": "img_data/video_51_chunk_40.jpg"}
{"video": "video_51", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "the w is not the cond of the kernel the cond of the kernel yes we can call them cond of the kernel yes we can call them w but w i would associate w with this w but w i would associate w with this line over here that for specifying this line i have the that for specifying this line i have the u comma v coordinates spal coordinates u comma v coordinates spal coordinates of the kernel that specific kernel of the kernel that specific kernel however is provided by this index however is provided by this index is identified by this index this is identified by this index this specific kel is by this index and the specific kel is by this index and the scalar it is going to be generating is", "image_path": "img_data/video_51_chunk_41.jpg"}
{"video": "video_51", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "specific kel is by this index and the scalar it is going to be generating is scalar it is going to be generating is the located at the kl depth that's the located at the kl depth that's why i need this w of q comma v comma why i need this w of q comma v comma kl minus one this will kl comma kl minus one this will actually be the weights that are going be a weights that are going be a four dimensional tensor is being used four dimensional tensor is being used here for specifying those here for specifying those parameters that we are", "image_path": "img_data/video_51_chunk_42.jpg"}
{"video": "video_51", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "here for specifying those parameters that we are parameters that we are store in those filters store in those filters and in fact we only have one filter and in fact we only have one filter right now a four dimensional tensor right now a four dimensional tensor to identify the parameters that we have to identify the parameters that we have used in this specific dot product over used in this specific dot product over here this is a three-dimensional dot here this is a three-dimensional dot product and as you can imagine as i'm product and as you can imagine as i'm sliding the filter to another sliding the filter to another location in the next snapshot the location in the next snapshot the only thing actually is changing is the only thing actually is changing is the space", "image_path": "img_data/video_51_chunk_43.jpg"}
{"video": "video_51", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "only thing actually is changing is the space coordinate that is being produced in coordinate that is being produced in this scaler the only thing by this scaler the only thing by moving the filter around i'm changing moving the filter around i'm changing the i comma j of what i'm producing the i comma j of what i'm producing therefore what i'm actually going to be therefore what i'm actually going to be producing is a slice a specific slice out of this slice a specific slice out of this sort of output feature map the sort of output feature map the specific slice i'm just drawing over specific slice i'm just drawing over here just one of the ml slices here just one of the ml slices generates the complete", "image_path": "img_data/video_51_chunk_44.jpg"}
{"video": "video_51", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "here just one of the ml slices generates the complete ml slices generates the complete volume this slice is the one that i volume this slice is the one that i am going to be generating this am going to be generating this complete slice effectively a complete slice effectively a matrix and from one filter i'll matrix and from one filter i'll be generating a single matrix and be generating a single matrix and therefore and this is the important therefore and this is the important conclusion from we need multiple", "image_path": "img_data/video_51_chunk_45.jpg"}
{"video": "video_51", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "need multiple we need multiple filters to generate the output feature map volume for a volume for the whole thing for the output feature map we need thing for the output feature map we need to be creating multiple fatures in fact to be creating multiple fatures in fact this thing over here this thing over here is really the connectivity diagram of", "image_path": "img_data/video_51_chunk_46.jpg"}
{"video": "video_51", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "this thing over here is really the connectivity diagram of is really the connectivity diagram of the convolutional neuron what we call a the convolutional neuron what we call a convolutional a single filter is and the convolutional a single filter is and the operation actually we see over here is operation actually we see over here is the operation of the convolutional the operation of the convolutional neuron and this is all of these neuron and this is all of these parameters that we have used over here parameters that we have used over here the contents if you the filter are the contents if you the filter are the -called trainable parameters", "image_path": "img_data/video_51_chunk_47.jpg"}
{"video": "video_51", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "the contents if you the filter are the -called trainable parameters and we will now see an animation of this thing in our core of this thing in our core site if i go to my core site and site if i go to my core site and actually scroll down a little bit then actually scroll down a little bit then you can see now the three-dimensional first of all three-dimensional first of all before we see the animation we can before we see the animation we can actually see it's exactly the diagram i actually see it's exactly the diagram i just drew a bit a different i have an just drew a bit a different i have an input volume which is the blue over input volume which is the blue over here with has a depth d here with has a depth d in definitely as we mentioned the", "image_path": "img_data/video_51_chunk_48.jpg"}
{"video": "video_51", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "here with has a depth d in definitely as we mentioned the in definitely as we mentioned the input sorry the filter that we're input sorry the filter that we're going to be using has the same depth d going to be using has the same depth d in as the input volume doesn't make in as the input volume doesn't make sense otherwise and then in terms of the otherwise and then in terms of the output volume a single filter is going output volume a single filter is going to be generating one slice out of the d to be generating one slice out of the d out slices on one slices let's say out slices on one slices let's say this specific matrix over here where my this specific matrix over here where my mouse pointer is that is going to be mouse pointer is that is going to be what is going to be produced by a single what is going to be produced by a single filter and this dotted line here", "image_path": "img_data/video_51_chunk_49.jpg"}
{"video": "video_51", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "what is going to be produced by a single filter and this dotted line here filter and this dotted line here indicates that if you want to generate indicates that if you want to generate the complete output volume with a the complete output volume with a depth d out you need d out of these depth d out you need d out of these filters you need d out of these orange filters you need d out of these orange cubes in order for you to be able to cubes in order for you to be able to generate a complete green output generate a complete green output feature map i think it's worth feature map i think it's worth spending some time in this animation in spending some time in this animation you can actually see this animation you can actually see exactly what i just discussed here", "image_path": "img_data/video_51_chunk_50.jpg"}
{"video": "video_51", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "this animation you can actually see exactly what i just discussed here exactly what i just discussed here in this example i have an input feature in this example i have an input feature map of depth three and i have a output feature map three and i have a output feature map of depth two let's assume that is of depth two let's assume that is the sort of a design parameter which the sort of a design parameter which i want to implement therefore if i want to implement therefore if i have an output feat map of two i need have an output feat map of two i need two filters and these are the two filters and these are the two filters this is the filter w0 and this filters this is the filter w0 and this is the filter w1 and evidently the each of w1 and evidently the each of these filters has of dep depth three", "image_path": "img_data/video_51_chunk_51.jpg"}
{"video": "video_51", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "w1 and evidently the each of these filters has of dep depth three these filters has of dep depth three because three is also the depth of the because three is also the depth of the input feature map and at every specific input feature map and at every specific snapshot let's assume this is the snapshot let's assume this is the snapshot that i just drew on piece of snapshot that i just drew on piece of paper this filter is located at this paper this filter is located at this specific location in my input feature specific location in my input feature map and it is responsible for creating map and it is responsible for creating this scalar z which is nine in this case this scalar z which is nine in this case if i may and as far", "image_path": "img_data/video_51_chunk_52.jpg"}
{"video": "video_51", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "this scalar z which is nine in this case if i may and as far as the output feature map is concerned as the output feature map is concerned this filter is only able to plot if you to only able to plot if you to determine the this specific slice of the determine the this specific slice of the output feature map if i want to continue then we map if i want to continue then we will see that the second filter is the will see that the second filter is the one which is involved in the creation of one which is involved in the creation of the second slice of the output", "image_path": "img_data/video_51_chunk_53.jpg"}
{"video": "video_51", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "one which is involved in the creation of the second slice of the output of the second slice of the output feature this is really the essence of a feature this is really the essence of a three-dimensional convolution i suggest three-dimensional convolution i suggest that you spend some time on this that you spend some time on this animation trying to understand what is animation trying to understand what is going on and you can toggle the what is going on and you can toggle the movement just to be able to replicate movement just to be able to replicate the output scaler from the input the output scaler from the input values which have been provided over here a bit on this presentation of the snapshot operation of a layer the snapshot operation of a layer the site over here has is squatting some", "image_path": "img_data/video_51_chunk_54.jpg"}
{"video": "video_51", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "snapshot operation of a layer the site over here has is squatting some site over here has is squatting some kind of important formulas regarding the kind of important formulas regarding the size the special dimensions of the size the special dimensions of the output feature map i think it's output feature map i think it's important to note them down and important to note them down and it is the floor of the height of it is the floor of the height of the input feure map plus two * the input feure map plus two * the padding size minus the kernel size padding size minus the kernel size divided by the strides divided by the strides and plus one this is the", "image_path": "img_data/video_51_chunk_55.jpg"}
{"video": "video_51", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "divided by the strides and plus one this is the and plus one this is the formula that will that you can formula that will that you can actually use to understand exactly what actually use to understand exactly what will be your output feature maps are in will be your output feature maps are in terms of spatial dimensions and of terms of spatial dimensions and of course this will be the input feature course this will be the input feature map sizes for the layer that map sizes for the layer that follows what will actually be follows what will actually be those layers i think it's those layers i think it's quite important to get into the u it's quite important to get into the u discussion now about other architectural discussion now about other architectural features before we go into some kind", "image_path": "img_data/video_51_chunk_56.jpg"}
{"video": "video_51", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "discussion now about other architectural features before we go into some kind features before we go into some kind of a discussion about the advantages of a discussion about the advantages of convolution layers as compared to fully convolution layers as compared to fully connected layers which i think is best connected layers which i think is best demonstrated using an example before demonstrated using an example before we go into that example let's look at we go into that example let's look at another operation that we'll be another operation that we'll be calling the max pulling layer or in calling the max pulling layer or in general pulling layer which is general pulling layer which is actually described here and it's best actually described here and it's best demonstrated with this kind of image demonstrated with this kind of image and this case what we see we have", "image_path": "img_data/video_51_chunk_57.jpg"}
{"video": "video_51", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "demonstrated with this kind of image and this case what we see we have and this case what we see we have an input feature map that has a an input feature map that has a depth of one in this case and we do depth of one in this case and we do still have the concept of if you of still have the concept of if you of a kernel that we slide around just a kernel that we slide around just in the convolutional layer but in the convolutional layer but in this case instead of a nonlinear this case instead of a nonlinear function a reu that we have function a reu that we have actually also seen in the fully actually also seen in the fully connected layers that we are still connected layers that we are still going to see in the evolutional layer", "image_path": "img_data/video_51_chunk_58.jpg"}
{"video": "video_51", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "connected layers that we are still going to see in the evolutional layer going to see in the evolutional layer as we will see in that example we as we will see in that example we will have another function let's call will have another function let's call that function in this specific case it's that function in this specific case it's shown as the max function where the shown as the max function where the idea behind this is that we are the idea behind this is that we are going to not form a correlation going to not form a correlation result over here a dot product result over here a dot product but we're going to select the maximum but we're going to select the maximum element of what we see in the input element of what we see in the input feature map typically we apply the that feature map typically we apply the that function at for each of the channels function at for each of the channels of the input feature map but in some", "image_path": "img_data/video_51_chunk_59.jpg"}
{"video": "video_51", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "function at for each of the channels of the input feature map but in some of the input feature map but in some instances we may apply it also across instances we may apply it also across the depth dimension what we are the depth dimension what we are achieving is evidently we are achieving is evidently we are achieving some reduction in the spatial some reduction in the spatial dimensions of the output feature map dimensions of the output feature map and that kind of intuitively and that kind of intuitively understood as trying to select the most understood as trying to select the most important features of the input feature important features of the input feature map and transfer out into the layer map and transfer out into the layer above for further process say above for further process say that is the max pooling layer in this", "image_path": "img_data/video_51_chunk_60.jpg"}
{"video": "video_51", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "above for further process say that is the max pooling layer in this that is the max pooling layer in this case which is typically interl with case which is typically interl with convolutional layers as we will see in convolutional layers as we will see in some example architectures closing i some example architectures closing i want to emphasize the another u kind of want to emphasize the another u kind of specific parameterization of the specific parameterization of the convolutional layer we call here the one convolutional layer we call here the one by one convolutional layer and it is by one convolutional layer and it is definitely a sort of a layer that it definitely a sort of a layer that it is being met in various kind of is being met in various kind of architect lectures and maybe it does", "image_path": "img_data/video_51_chunk_61.jpg"}
{"video": "video_51", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "is being met in various kind of architect lectures and maybe it does architect lectures and maybe it does not really make a lot of sense to you not really make a lot of sense to you the moment you see this kind of the moment you see this kind of animation over here why in earth we're animation over here why in earth we're going to do one by one convolutions going to do one by one convolutions since we as we discussed we're trying to since we as we discussed we're trying to detect features and typically the kernel detect features and typically the kernel sizes have larger dimensions than one by sizes have larger dimensions than one by one but i think the explanation one but i think the explanation potentially could be more intuitively potentially could be more intuitively understood if we see the understood if we see the three-dimensional version of this three-dimensional version of this one by one convolution we have a see", "image_path": "img_data/video_51_chunk_62.jpg"}
{"video": "video_51", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "three-dimensional version of this one by one convolution we have a see one by one convolution we have a see here the orange u filter that here the orange u filter that evidently the k size is one by one evidently the k size is one by one and we have as we discussed earlier and we have as we discussed earlier depth d that matches the depth of the depth d that matches the depth of the input feature map and as we also input feature map and as we also discussed earlier the this discussed earlier the this filter operation will move around we filter operation will move around we're sliding around this filter and we're sliding around this filter and we're creating one slice we're creating one slice for this filter in this one by", "image_path": "img_data/video_51_chunk_63.jpg"}
{"video": "video_51", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "we're creating one slice for this filter in this one by for this filter in this one by one convolution we have just one slice one convolution we have just one slice and as you can see what we are achieving and as you can see what we are achieving here we are forming a scalar by here we are forming a scalar by combining the depth compressing combining the depth compressing the whole depth dimension we actually the whole depth dimension we actually have we are seeing typically this have we are seeing typically this type of layers let's say towards the type of layers let's say towards the end of an network the top of the end of an network the top of the network where we just before the head network where we just before the head where we want to just summarize", "image_path": "img_data/video_51_chunk_64.jpg"}
{"video": "video_51", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "network where we just before the head where we want to just summarize where we want to just summarize everything we have done and we have everything we have done and we have learned in terms of u convolutions learned in terms of u convolutions operations and across all depth operations and across all depth dimensions that we have decided to dimensions that we have decided to do and then we just need to compress do and then we just need to compress that information to a matrix and that information to a matrix and potentially that kind of slice is potentially that kind of slice is going to be flattened in order to be going to be flattened in order to be passed over into the passed over into the head which may consist of fully head which may consist of fully connected layers as we'll see that in a connected layers as we'll see that in a moment that's one application of", "image_path": "img_data/video_51_chunk_65.jpg"}
{"video": "video_51", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "connected layers as we'll see that in a moment that's one application of moment that's one application of the one by one convol convolution the one by one convol convolution operation all right it kind operation all right it kind of looks an multi-layer petron or a of looks an multi-layer petron or a dense layer as it is combining these dense layer as it is combining these depth dimensions into that scaler all depth dimensions into that scaler all right let's now see some example right let's now see some example architectures these example architectures these example architectures could potentially be architectures could potentially be the toy network that we see here the toy network that we see here where we have as we discussed the", "image_path": "img_data/video_51_chunk_66.jpg"}
{"video": "video_51", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "the toy network that we see here where we have as we discussed the where we have as we discussed the convolutional layer followed by convolutional layer followed by nonlinearity and max pulling layers and nonlinearity and max pulling layers and finally at the end we expect to see a finally at the end we expect to see a fully connected layer that is going fully connected layer that is going to play the role of the to play the role of the head of the network where we have head of the network where we have let's say in this case five classes let's say in this case five classes that we would to do a that we would to do a classification on but instead of classification on but instead of looking at this toy network work and i looking at this toy network work and i think it's a bit more instructive to think it's a bit more instructive to look at i will call it canonical", "image_path": "img_data/video_51_chunk_67.jpg"}
{"video": "video_51", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "think it's a bit more instructive to look at i will call it canonical look at i will call it canonical architecture called vgg from the architecture called vgg from the initials of the authors of that initials of the authors of that kind of architecture this architecture kind of architecture this architecture is i call an architecture that i is i call an architecture that i suggest students to start from every suggest students to start from every time they want to look at these time they want to look at these convolutional networks because they convolutional networks because they do represent some kind of a initial good do represent some kind of a initial good architecture that we can sort of make architecture that we can sort of make some conclusions in regarding the some conclusions in regarding the dimensionality and the patterns that we", "image_path": "img_data/video_51_chunk_68.jpg"}
{"video": "video_51", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "some conclusions in regarding the dimensionality and the patterns that we dimensionality and the patterns that we expect to see in a typical cnn expect to see in a typical cnn architecture instead of looking if you architecture instead of looking if you in the most modern versions of in the most modern versions of cnn i think it's worthwhile looking at cnn i think it's worthwhile looking at this to understand a couple of things this to understand a couple of things the first thing that we'd to the first thing that we'd to capture over here is this image and capture over here is this image and understand what is really understand what is really happening the image is the happening the image is the in this figure we see a cnn layer in this figure we see a cnn layer a cnn network that consist of a cnn network that consist of multiple layers and one striking thing", "image_path": "img_data/video_51_chunk_69.jpg"}
{"video": "video_51", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "a cnn network that consist of multiple layers and one striking thing multiple layers and one striking thing from the get-go that you can see is that from the get-go that you can see is that the cnn is the dimensionality of the cnn is in terms of dimensionality of the cnn is in terms of spatial dimensions is evidently spatial dimensions is evidently shrinking as we are going deeper we shrinking as we are going deeper we see the convolutional layers followed see the convolutional layers followed by max pulling layers u for and then towards the end we layers u for and then towards the end we see the fully connected network see the fully connected network which is the head in terms of spatial which is the head in terms of spatial dimensions we are actually decreasing", "image_path": "img_data/video_51_chunk_70.jpg"}
{"video": "video_51", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "which is the head in terms of spatial dimensions we are actually decreasing dimensions we are actually decreasing the spatial dimensions because evidently the spatial dimensions because evidently we are using kels which are larger than we are using kels which are larger than one by one and but on at the same time what we and but on at the same time what we are also are seeing is we see an are also are seeing is we see an increase in the depth dimension in increase in the depth dimension in terms of numbers over here 224x 224 terms of numbers over here 224x 224 pixels are the spatial dimensions of the pixels are the spatial dimensions of the input images and then we have a 64 to be the depth", "image_path": "img_data/video_51_chunk_71.jpg"}
{"video": "video_51", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "images and then we have a 64 to be the depth and then we have a 64 to be the depth dimension of the or equivalent the dimension of the or equivalent the number of neurons in the u that we have number of neurons in the u that we have in or the number of filters that we have in or the number of filters that we have in that layer this is also our in that layer this is also our responsibility are responsibility our responsibility are twofold one is to with a padding and twofold one is to with a padding and stride parameters to massage these kind stride parameters to massage these kind of special dimensions we need and at the of special dimensions we need and at the same time also select the number of same time also select the number of filters how many any convolutional filters how many any convolutional neurons are we going to engage in that", "image_path": "img_data/video_51_chunk_72.jpg"}
{"video": "video_51", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "filters how many any convolutional neurons are we going to engage in that neurons are we going to engage in that layer as you can see we go from 64 layer as you can see we go from 64 128 256 512 that is really the end 128 256 512 that is really the end game with respect to number of filters game with respect to number of filters the intuition behind the increase in the intuition behind the increase in the number of filters as the network becomes number of filters as the network becomes deeper and deeper is the becomes deeper and deeper is the following the network is learning more following the network is learning more and more complicated features as we are and more complicated features as we are going deeper the first layers are the going deeper the first layers are the are learning representations are learning representations which are simple shapes i will call", "image_path": "img_data/video_51_chunk_73.jpg"}
{"video": "video_51", "start": "0:37:00", "end": "0:37:30", "timestamp": "0:37:00 - 0:37:30", "text": "are learning representations which are simple shapes i will call which are simple shapes i will call it similar things that you would expect it similar things that you would expect to for you to understand when you to for you to understand when you look at if you at a kind of a look at if you at a kind of a primitive shape a circle a primitive shape a circle a triangle or whatever have you and the triangle or whatever have you and the subsequent kind of layers are actually subsequent kind of layers are actually learning more and more complicated learning more and more complicated representations we'll see in a moment representations we'll see in a moment some examples of exactly what these some examples of exactly what these layers are learning and by suitable layers are learning and by suitable visualizations as you are trying to", "image_path": "img_data/video_51_chunk_74.jpg"}
{"video": "video_51", "start": "0:37:30", "end": "0:38:00", "timestamp": "0:37:30 - 0:38:00", "text": "layers are learning and by suitable visualizations as you are trying to visualizations as you are trying to create combinations of these create combinations of these simpler representations you probably simpler representations you probably need all to be doing more of those need all to be doing more of those combinations as you go deeper because combinations as you go deeper because you are trying to understand whether or you are trying to understand whether or not there's one combination that not there's one combination that actually magically generating the right actually magically generating the right set of representations in subsequent set of representations in subsequent deeper layers such that your head can deeper layers such that your head can actually do the job that's the first actually do the job that's the first intuition regarding the increase in the", "image_path": "img_data/video_51_chunk_75.jpg"}
{"video": "video_51", "start": "0:38:00", "end": "0:38:30", "timestamp": "0:38:00 - 0:38:30", "text": "intuition regarding the increase in the regarding the increase in the depth of the filters depth of the filters the second is that you can afford to i the second is that you can afford to i mean you can afford having that kind mean you can afford having that kind of increase in the depth of the filter of increase in the depth of the filter and without really paying too much and without really paying too much complexity performance sorry complexity performance sorry complexity in the terms of number complexity in the terms of number of operations because your special of operations because your special dimensions of the feature maps which dimensions of the feature maps which are produced from a earlier are produced from a earlier operations are shrinking you increase", "image_path": "img_data/video_51_chunk_76.jpg"}
{"video": "video_51", "start": "0:38:30", "end": "0:39:00", "timestamp": "0:38:30 - 0:39:00", "text": "are produced from a earlier operations are shrinking you increase operations are shrinking you increase the number of filter parameters and the number of filter parameters and still you're not really paying still you're not really paying any complexity this sort of any complexity this sort of penalty because of that these penalty because of that these are the two things that we need we are the two things that we need we can actually mention about this kind of can actually mention about this kind of architecture that looks again a architecture that looks again a pyramid but this pyramid is a kind of pyramid but this pyramid is a kind of works in a different way as works in a different way as compared to what we have seen in fully compared to what we have seen in fully connected architect pictures and now connected architect pictures and now i think it's worthwhile spending some", "image_path": "img_data/video_51_chunk_77.jpg"}
{"video": "video_51", "start": "0:39:00", "end": "0:39:30", "timestamp": "0:39:00 - 0:39:30", "text": "connected architect pictures and now i think it's worthwhile spending some i think it's worthwhile spending some time on u on an example and this time on u on an example and this example is a python notebook this example is actually shown notebook this example is actually shown over here you can actually click on this over here you can actually click on this and open it in poab for execution and open it in poab for execution however the notebook in your case over however the notebook in your case over here will actually be working as it is here will actually be working as it is in the next video we'll go through in the next video we'll go through this example and then see exactly what's", "image_path": "img_data/video_51_chunk_78.jpg"}
{"video": "video_51", "start": "0:39:30", "end": "0:39:36.233333", "timestamp": "0:39:30 - 0:39:36.233333", "text": "in the next video we'll go through this example and then see exactly what's this example and then see exactly what's going on in terms of and the api going on in terms of and the api and the implementation of a cnn", "image_path": "img_data/video_51_chunk_79.jpg"}
{"video": "video_52", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "to start understanding what is really the fundamental operation happen the fundamental operation happen inside those convolutional neuron inside those convolutional neuron networks let me just start with a networks let me just start with a simple problem and this problem goes as simple problem and this problem goes as follows i have somewhere in this kind of follows i have somewhere in this kind of time axis a signal this is the duration of signal this is the duration of this signal and the shape of the signal this signal and the shape of the signal is not really important but what is not really important but what is really important is the assumption that really important is the assumption that i do know the shape of that signal and i do know the shape of that signal and let's assume that i have let's say call let's assume that i have let's say call this signal x oft and ask", "image_path": "img_data/video_52_chunk_0.jpg"}
{"video": "video_52", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "let's assume that i have let's say call this signal x oft and ask this signal x oft and ask you the question as to can we come up you the question as to can we come up with a method that we can sort of with a method that we can sort of provide the location of that provide the location of that signal where about in this buffer signal where about in this buffer let's say u of the signal is located let's say u of the signal is located a simple kind of approach that a simple kind of approach that we can think of is the following we can think of is the following since we know the shape of this kind of since we know the shape of this kind of signal i can actually start at the signal i can actually start at the beginning of this buffer at this", "image_path": "img_data/video_52_chunk_1.jpg"}
{"video": "video_52", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "signal i can actually start at the beginning of this buffer at this beginning of this buffer at this location let say zero and for every location let say zero and for every position if you the hypothesis the that the buffer hypothesis the that the buffer is the signal is located at this is the signal is located at this specific point and start making an specific point and start making an operation that will effectively means operation that will effectively means taking the dot product between the taking the dot product between the contents of that buffer and that contents of that buffer and that signal and this means that", "image_path": "img_data/video_52_chunk_2.jpg"}
{"video": "video_52", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "contents of that buffer and that signal and this means that signal and this means that multiplying every element of this buffer multiplying every element of this buffer with the amplitude let's say the with the amplitude let's say the amplitude was one of the signal and amplitude was one of the signal and summing over all of the elements and summing over all of the elements and if we do that then it's a whole bunch of if we do that then it's a whole bunch of zeros time one then we will be getting a zeros time one then we will be getting a result that it will be zero result that it will be zero evidently that is we the zero evidently that is we the zero and we'll be getting in fact all zeros and we'll be getting in fact all zeros results up to the point where we start", "image_path": "img_data/video_52_chunk_3.jpg"}
{"video": "video_52", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "and we'll be getting in fact all zeros results up to the point where we start results up to the point where we start having my hypo hypothetical kind having my hypo hypothetical kind of a signal be right at the of a signal be right at the location of that true location of that true location of that kind of signal after this that kind of signal after this point as you can understand we going to point as you can understand we going to have some overlap between this signal have some overlap between this signal and this we be starting getting some and this we be starting getting some nonzero results out of this of operation", "image_path": "img_data/video_52_chunk_4.jpg"}
{"video": "video_52", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "and this we be starting getting some nonzero results out of this of operation nonzero results out of this of operation and right where exactly the signal is and right where exactly the signal is located right under it then we will located right under it then we will start getting a pick and then evidently we'll start going back to zero and on this is basically and on this is basically an approach if we take the ar marx of an approach if we take the ar marx of what this kind of operation this will be what this kind of operation this will be let's say the tow the location", "image_path": "img_data/video_52_chunk_5.jpg"}
{"video": "video_52", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "what this kind of operation this will be let's say the tow the location let's say the tow the location that sort of indicates the sort of that sort of indicates the sort of that we have predicted as far as this that we have predicted as far as this kind of signal is concerned what we just did is an concerned what we just did is an example of what we call a onedimensional example of what we call a onedimensional correlation operation cross correlation operation cross correlation operation because we are trying to operation because we are trying to correlate a hypothetical signal correlate a hypothetical signal located in this hypoth over here against located in this hypoth over here against some other signal the x oft i will some other signal the x oft i will actually call this simplistically", "image_path": "img_data/video_52_chunk_6.jpg"}
{"video": "video_52", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "some other signal the x oft i will actually call this simplistically actually call this simplistically let's say y of t i'm actually let's say y of t i'm actually correlating x of t and y of t and this correlating x of t and y of t and this kind of correlation operation is able to kind of correlation operation is able to retrieve for me in this simple one retrieve for me in this simple one single dimensional kind of space the single dimensional kind of space the location of x oft and this is exactly location of x oft and this is exactly what is been done in a kind of a what is been done in a kind of a two-dimensional and three-dimensional two-dimensional and three-dimensional kind of space inside the convolution", "image_path": "img_data/video_52_chunk_7.jpg"}
{"video": "video_52", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "two-dimensional and three-dimensional kind of space inside the convolution kind of space inside the convolution kind of neuron network in fact now what kind of neuron network in fact now what we will do is we will expand we will do is we will expand into the another example of where we into the another example of where we have now this taxi image and we try have now this taxi image and we try to understand how we will be able to understand how we will be able to detect the presence of that kind of taxi detect the presence of that kind of taxi in that image but before we go there i in that image but before we go there i wanted to actually share with you wanted to actually share with you some details in our site where we have some details in our site where we have if you an americal example conf if you an americal example conf confusingly the true sort of", "image_path": "img_data/video_52_chunk_8.jpg"}
{"video": "video_52", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "if you an americal example conf confusingly the true sort of confusingly the true sort of operation of what is actually called operation of what is actually called convolution is implemented inside the convolution is implemented inside the pytor tensor fls and on all these pytor tensor fls and on all these kind of machine learning frameworks as kind of machine learning frameworks as correlation operations and with the correlation operations and with the exception that we actually flip the exception that we actually flip the in the time domain that in the time domain that signal y oft we are the signal y oft we are the two operations as it actually clearly two operations as it actually clearly shown in your s l later on", "image_path": "img_data/video_52_chunk_9.jpg"}
{"video": "video_52", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "two operations as it actually clearly shown in your s l later on shown in your s l later on the two operations of convolution and the two operations of convolution and cor crosscorrelation result cor crosscorrelation result into identical results and for the into identical results and for the purposes of our purposes from a pure purposes of our purposes from a pure implementation efficiency c implementation efficiency c perspective we always prefer to refer perspective we always prefer to refer to implement them as cross to implement them as cross correlation despite the fact that we are correlation despite the fact that we are refer to them as convolution refer to them as convolution operations let me go into the operations let me go into the following kind of discussion now where following kind of discussion now where we have we are going back to that", "image_path": "img_data/video_52_chunk_10.jpg"}
{"video": "video_52", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "following kind of discussion now where we have we are going back to that we have we are going back to that kind of a taxi image and let me just kind of a taxi image and let me just write over here if i kind of try to write over here if i kind of try to squeeze it this is our squeeze it this is our image that we have seen earlier with image that we have seen earlier with this yellow cab in the this yellow cab in the middle and here i have right in middle and here i have right in the middle a kind of a the middle a kind of a taxi", "image_path": "img_data/video_52_chunk_11.jpg"}
{"video": "video_52", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "the middle a kind of a taxi all right and i ask you exactly the same question how we can potentially detect this kind of potentially detect this kind of taxi in this kind of image all taxi in this kind of image all right and maybe the i mean if right and maybe the i mean if we have known the exact shape of this we have known the exact shape of this object we have assumed object we have assumed earlier then the operation actually", "image_path": "img_data/video_52_chunk_12.jpg"}
{"video": "video_52", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "object we have assumed earlier then the operation actually earlier then the operation actually would be straightforward we'll take this straightforward we'll take this template again we'll call it y template again we'll call it y now and we will start sliding this now and we will start sliding this kind of template across this kind of template across this kind of image to be able to at some point where image to be able to at some point where the template is exactly present right the template is exactly present right on top of that u object in the on top of that u object in the image x we will then declare that's image x we will then declare that's basically the location of that of", "image_path": "img_data/video_52_chunk_13.jpg"}
{"video": "video_52", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "image x we will then declare that's basically the location of that of basically the location of that object the problem however is that object the problem however is that in this kind of approach we have that in this kind of approach we have first assume that we will know exactly first assume that we will know exactly the shape of that object which is not the shape of that object which is not realistic because the object is realistic because the object is definitely going to vary quite a definitely going to vary quite a lot let's say the object may be rotated lot let's say the object may be rotated translated in sort of or translated in sort of or being sort of because of the", "image_path": "img_data/video_52_chunk_14.jpg"}
{"video": "video_52", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "translated in sort of or being sort of because of the being sort of because of the lighting condition maybe not sort of lighting condition maybe not sort of very evident it's exact shape it's very evident it's exact shape it's shadowed for example or uded by other shadowed for example or uded by other objects and the other assumption objects and the other assumption is that in terms of computational is that in terms of computational efficiency the larger our kind of efficiency the larger our kind of prototype is in terms of number of prototype is in terms of number of pixels the more expensive this kind of pixels the more expensive this kind of operation becomes in order to solve operation becomes in order to solve these two problems what we will do these two problems what we will do what we would suggest is we will not", "image_path": "img_data/video_52_chunk_15.jpg"}
{"video": "video_52", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "these two problems what we will do what we would suggest is we will not what we would suggest is we will not abandon this kind of sliding operation abandon this kind of sliding operation that we will called cross correlation that we will called cross correlation a bit earlier but if in fact what we a bit earlier but if in fact what we will actually do now is we'll use a much will actually do now is we'll use a much smaller prototype and we are now ph the smaller prototype and we are now ph the problem as to what is really the problem as to what is really the contents of that sort of prototype contents of that sort of prototype that we will be calling from now on a that we will be calling from now on a kernel let me just draw the kernel let me just draw the kernel over here and maybe i can start over here and maybe i can start answering the question as to what should answering the question as to what should be the contents of this kernel that", "image_path": "img_data/video_52_chunk_16.jpg"}
{"video": "video_52", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "answering the question as to what should be the contents of this kernel that be the contents of this kernel that will be able to detect this cab will be able to detect this cab over here all right this image is over here all right this image is extremely simple in fact it has zeros extremely simple in fact it has zeros in these pixels and all the other in these pixels and all the other pixels which we have not drawn anything pixels which we have not drawn anything are 255 corresponds to pure white are 255 corresponds to pure white and zero corresponds to black since i and zero corresponds to black since i have a black and white image i can have a black and white image i can suggest that maybe if i can", "image_path": "img_data/video_52_chunk_17.jpg"}
{"video": "video_52", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "have a black and white image i can suggest that maybe if i can suggest that maybe if i can u have a kel who's pixels this is a kernel which is the pixels this is a kernel which is the size of that is not necessarily size of that is not necessarily accidental it's 3x3 in this accidental it's 3x3 in this case and if i locate over here in case and if i locate over here in this pixels i made them let's say zeros this pixels i made them let's say zeros and over here i made them 255 then you and over here i made them 255 then you can all understand that as i'm sliding can all understand that as i'm sliding around this kind of kernel i will", "image_path": "img_data/video_52_chunk_18.jpg"}
{"video": "video_52", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "can all understand that as i'm sliding around this kind of kernel i will around this kind of kernel i will definitely go and at some point hit this definitely go and at some point hit this location over here with this kernel and at that moment in that location then i will be getting in that location then i will be getting some definitely significant peak if i definitely significant peak if i may call it that out of my may call it that out of my crosscorrelation operation we'll see crosscorrelation operation we'll see the details the mathematical details of the details the mathematical details of that cross correlation operation shortly", "image_path": "img_data/video_52_chunk_19.jpg"}
{"video": "video_52", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "the details the mathematical details of that cross correlation operation shortly that cross correlation operation shortly but that's basically what is going but that's basically what is going to happen then and exactly the same to happen then and exactly the same thing will actually happen in this thing will actually happen in this location and potentially in this type of location and potentially in this type of locations and on but i don't think locations and on but i don't think that we have the complete story yet that we have the complete story yet because i can suggest an additional because i can suggest an additional kind of kernel again it will be 3x3 and this kernel if it is 3x3 and this kernel if it is sort of", "image_path": "img_data/video_52_chunk_20.jpg"}
{"video": "video_52", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "3x3 and this kernel if it is sort of programmed to contain this type of programmed to contain this type of information where now the black information where now the black pixels are this and the whites are pixels are this and the whites are over there they're kind of a diagonally over there they're kind of a diagonally then we will be able to pick up then we will be able to pick up probably this feature over here where probably this feature over here where the wind screen is and on and the wind screen is and on we can actually suggest other kernels as well where suggest other kernels as well where we now have the other", "image_path": "img_data/video_52_chunk_21.jpg"}
{"video": "video_52", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "suggest other kernels as well where we now have the other we now have the other diagonal these wheels can be detected diagonal these wheels can be detected by having some something else some by having some something else some other kind of kernel where maybe these guys will be need to be sort of colored this need to be sort of colored this colored in quotes or program this colored in quotes or program this with 255 and zeros and as you can with 255 and zeros and as you can understand all of these kernels if", "image_path": "img_data/video_52_chunk_22.jpg"}
{"video": "video_52", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "with 255 and zeros and as you can understand all of these kernels if understand all of these kernels if they are positioned at some point in they are positioned at some point in this kind of image and i'll be in this kind of image and i'll be calling now from now on calling now from now on this aggregation of kernels as a filter for me will be this kind of a three-dimensional kind of a three-dimensional structure that it will structure that it will have 3x3 let's say spatial coordinates", "image_path": "img_data/video_52_chunk_23.jpg"}
{"video": "video_52", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "structure that it will have 3x3 let's say spatial coordinates have 3x3 let's say spatial coordinates this is the 3x3 spatial let me call it extent and it would have some kind of a depth d and this depth let's write depth d and this depth let's write depth is of course the number of kels depth is of course the number of kels that i employ in this filter that i employ in this filter over here that is basically a over here that is basically a structure that i will see will be a", "image_path": "img_data/video_52_chunk_24.jpg"}
{"video": "video_52", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "over here that is basically a structure that i will see will be a structure that i will see will be a fairly i call it useful for me to be fairly i call it useful for me to be able to detect now pixels sorry able to detect now pixels sorry features from images and now features from images and now let's see the mathematical description of what we mathematical description of what we have just described let me go back to the described let me go back to the site and in that site we have the site and in that site we have the -called two-dimensional the extension -called two-dimensional the extension of the -called two- dimensional of the -called two- dimensional crosscorrelation operation and it's ex crosscorrelation operation and it's ex actually shown here in this kind of", "image_path": "img_data/video_52_chunk_25.jpg"}
{"video": "video_52", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "crosscorrelation operation and it's ex actually shown here in this kind of actually shown here in this kind of image this in this picture we have image this in this picture we have an input image of 3x4 pixels and we have a kernel of 2x2 in pixels and we have a kernel of 2x2 in this case and the kel is going to be case and the kel is going to be positioned on all possible locations positioned on all possible locations in this image in this input image in this input image in this specific case it is position at in this specific case it is position at this moment in time this location", "image_path": "img_data/video_52_chunk_26.jpg"}
{"video": "video_52", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "in this specific case it is position at this moment in time this location this moment in time this location over here and we are going to be forming over here and we are going to be forming the dot product between the kernel and the dot product between the kernel and the input image and evidently the dot the input image and evidently the dot product is a * w + b * x + e * y + f * z product is a * w + b * x + e * y + f * z and that will actually be a scalar and that will actually be a scalar that it will give us that it will give us the feature value extracted feature the feature value extracted feature value out of that operation and we value out of that operation and we will do exactly the same thing over will do exactly the same thing over and over again we're going to slide", "image_path": "img_data/video_52_chunk_27.jpg"}
{"video": "video_52", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "will do exactly the same thing over and over again we're going to slide and over again we're going to slide the kernel into another location bcfg the kernel into another location bcfg we'll do the dot product cdgh another we'll do the dot product cdgh another dot product with it and to cut a long dot product with it and to cut a long story short we are going to be getting a story short we are going to be getting a 2x3 output image and from now on we'll 2x3 output image and from now on we'll be calling this output images input be calling this output images input and output images feature maps because and output images feature maps because they are mapping the features that we they are mapping the features that we have extracted with the usage of", "image_path": "img_data/video_52_chunk_28.jpg"}
{"video": "video_52", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "they are mapping the features that we have extracted with the usage of have extracted with the usage of kernels here we see just a single kernel kernels here we see just a single kernel but in general we're going to have but in general we're going to have multiple kernels be involved in the multiple kernels be involved in the shape and form of filters in these operations and filters in these operations and basically this is what we will be basically this is what we will be doing from now on and we will be doing from now on and we will be sort of need to discuss next some sort of need to discuss next some architectural elements of architectural elements of this that are sort of this that are sort of encompassing this operation over here encompassing this operation over here but also enhance it with further", "image_path": "img_data/video_52_chunk_29.jpg"}
{"video": "video_52", "start": "0:15:00", "end": "0:15:08.800000", "timestamp": "0:15:00 - 0:15:08.800000", "text": "encompassing this operation over here but also enhance it with further but also enhance it with further correlation type of structures inside correlation type of structures inside convolution your networks", "image_path": "img_data/video_52_chunk_30.jpg"}
{"video": "video_53", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "the closing on the discussion of fully connected network architectures we connected network architectures we would to discuss a bit about would to discuss a bit about these kind of regularization approaches these kind of regularization approaches that we have in our disposal evidently that we have in our disposal evidently one of the regularization approaches one of the regularization approaches what we have seen during the discussion what we have seen during the discussion of linear regression and this was called of linear regression and this was called weight decay and it was covered back weight decay and it was covered back then with the -called then with the -called l2 calculating the non square of the", "image_path": "img_data/video_53_chunk_0.jpg"}
{"video": "video_53", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "then with the -called l2 calculating the non square of the l2 calculating the non square of the weight vector and adding it as a penalty weight vector and adding it as a penalty factor via a hyperparameter lambda in factor via a hyperparameter lambda in the mean square error loss and we can the mean square error loss and we can actually go back to our network over actually go back to our network over here that we have seen the multiclass here that we have seen the multiclass classification network and see exactly classification network and see exactly how this will apply over here how this will apply over here going back to this example we have going back to this example we have all the trainable parameters all the trainable parameters such as the matrix w1 the matrix w2 and", "image_path": "img_data/video_53_chunk_1.jpg"}
{"video": "video_53", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "all the trainable parameters such as the matrix w1 the matrix w2 and such as the matrix w1 the matrix w2 and on they can actually be on they can actually be extracted out of this kind of network extracted out of this kind of network and they can be led into a parallel kind and they can be led into a parallel kind of branch that is actually also shown of branch that is actually also shown over here in your site to calculate over here in your site to calculate the terms that are need to be summed the terms that are need to be summed up and act as a penalty factor for up and act as a penalty factor for overfeeding for the to for overfeeding for the to avoid overfeeding to drive the avoid overfeeding to drive the parameters in a way that they are cannot parameters in a way that they are cannot be overfitting can be avoided if i be overfitting can be avoided if i can grow a little bit this kind of", "image_path": "img_data/video_53_chunk_2.jpg"}
{"video": "video_53", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "be overfitting can be avoided if i can grow a little bit this kind of can grow a little bit this kind of situation that we have here with way dec situation that we have here with way dec as applied now to neural networks what i as applied now to neural networks what i would draw is i would draw the i the would draw is i would draw the i the network i'm not going to draw in it network i'm not going to draw in it everything we have drawn earlier we everything we have drawn earlier we have this x we have a network which is have this x we have a network which is consist if you of multiple consist if you of multiple layers let's call this layer layers let's call this layer w1 in this another layer w2 involving", "image_path": "img_data/video_53_chunk_3.jpg"}
{"video": "video_53", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "layers let's call this layer w1 in this another layer w2 involving w1 in this another layer w2 involving ev evidently dense matrices as we have ev evidently dense matrices as we have seen earlier and then here we have a seen earlier and then here we have a y hat we have a categorical cross y hat we have a categorical cross entropy loss function with a y and entropy loss function with a y and this is my scalar l and let me call it l and let me call it lce and over here then what this lce and over here then what this regularizer is going to be doing for", "image_path": "img_data/video_53_chunk_4.jpg"}
{"video": "video_53", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "lce and over here then what this regularizer is going to be doing for regularizer is going to be doing for exactly the same reasons that we have exactly the same reasons that we have seen earlier is going to seen earlier is going to calculate the element wise i comma j squared element of all the matrix w1 and is going to sum over all i and j", "image_path": "img_data/video_53_chunk_5.jpg"}
{"video": "video_53", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "w1 and is going to sum over all i and j to produce a scalar penalty that it is going to multiply some to the other if i may draw something this maybe over here just to be able to see the", "image_path": "img_data/video_53_chunk_6.jpg"}
{"video": "video_53", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "here just to be able to see the exactly the same thing for the matrix 2 exactly the same thing for the matrix 2 we calculating the squared values of the matrices of the squared values of the matrices of each element of the matrix is element each element of the matrix is element wise calculation we are summing all the calculation we are summing all the squared elements over now i comma j squared elements over now i comma j again and we are adding again and we are adding this and of course we need a this and of course we need a hyperparameter lambda", "image_path": "img_data/video_53_chunk_7.jpg"}
{"video": "video_53", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "this and of course we need a hyperparameter lambda hyperparameter lambda that we will weigh the penalty that we will weigh the penalty factor against the unregularized cross entropy loss the unregularized cross entropy loss that we are going to sum to our l total this is l to sum to our l total this is l reg and of course l is going to reg and of course l is going to be l crossentropy plus be l crossentropy plus l regularizing", "image_path": "img_data/video_53_chunk_8.jpg"}
{"video": "video_53", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "be l crossentropy plus l regularizing this is effectively the this is effectively the exactly the same equation as we the exactly the same equation as we have seen earlier and exactly the have seen earlier and exactly the same method we should expect to see same method we should expect to see exactly the same kind of behavior and exactly the same kind of behavior and rational as to why we are actually doing rational as to why we are actually doing that we are not going to be cover that we are not going to be cover that in any more detail what i'm going that in any more detail what i'm going to however cover is another approach to however cover is another approach that is also available for us and that is also available for us and this approach is actually called dropout", "image_path": "img_data/video_53_chunk_9.jpg"}
{"video": "video_53", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "that is also available for us and this approach is actually called dropout this approach is actually called dropout in the dropout approach is actually a in the dropout approach is actually a very popular approach we are very popular approach we are effectively switching off selected effectively switching off selected neurons in our original neurons in our original account of network we have the account of network we have the original fully connected let's say original fully connected let's say network architecture and as you can network architecture and as you can actually hear probabilistically we are actually hear probabilistically we are switching off some of the neurons switching off some of the neurons effectively we are forming a effectively we are forming a multiplicity of neural networks multiplicity of neural networks during", "image_path": "img_data/video_53_chunk_10.jpg"}
{"video": "video_53", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "multiplicity of neural networks during training and what we are effectively training and what we are effectively trying to do that is to u sort of trying to do that is to u sort of aage over many behaviors of aage over many behaviors of neurons of these kind of networks neurons of these kind of networks that we are sampling and it has that we are sampling and it has been shown that this kind of dropout been shown that this kind of dropout technique is and it's actually called technique is and it's actually called dropout because we are dropping certain dropout because we are dropping certain neurons in favor of the neurons in favor of the others and has been shown that this", "image_path": "img_data/video_53_chunk_11.jpg"}
{"video": "video_53", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "neurons in favor of the others and has been shown that this others and has been shown that this actually has quite good regularization actually has quite good regularization kind of behaviors effectively the way to kind of behaviors effectively the way to intuitively understand it is that we are intuitively understand it is that we are kind of switching off as a portion of kind of switching off as a portion of our total number of parameters by our total number of parameters by dropping off these neurons and we do it dropping off these neurons and we do it in a however in a probabilistic way not in a however in a probabilistic way not exactly these neurons are switched off exactly these neurons are switched off at during the whole training process but at during the whole training process but there is a probability that we but there is a probability that we are using probability distribution are using probability distribution we're actually using to turn them off", "image_path": "img_data/video_53_chunk_12.jpg"}
{"video": "video_53", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "are using probability distribution we're actually using to turn them off we're actually using to turn them off that is basically the dropout that is basically the dropout and it's actually better to experience and it's actually better to experience it in actual implementation that we'll it in actual implementation that we'll have a chance to do that in a in another have a chance to do that in a in another exercise in this course and exercise in this course and then of course we have the final then of course we have the final approach that we are going to be approach that we are going to be covering tonight is called early covering tonight is called early stopping and in that approach we are stopping and in that approach we are switching the we are stopping the switching the we are stopping the training process the moment we see training process the moment we see the validation loss sort of", "image_path": "img_data/video_53_chunk_13.jpg"}
{"video": "video_53", "start": "0:07:00", "end": "0:07:27.800000", "timestamp": "0:07:00 - 0:07:27.800000", "text": "training process the moment we see the validation loss sort of the validation loss sort of increasing as compared to the training increasing as compared to the training loss the moment we actually see that loss the moment we actually see that the network is being led to some form of the network is being led to some form of overfeeding as we have seen in an overfeeding as we have seen in an earlier kind of video it is the earlier kind of video it is the difference between validation and difference between validation and training loss that we should be watching training loss that we should be watching for overfitting then we stop the for overfitting then we stop the training process these are the training process these are the two additional regularization two additional regularization approaches that we have in neural approaches that we have in neural networks", "image_path": "img_data/video_53_chunk_14.jpg"}
{"video": "video_54", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "obviously the previous example we have seen an earlier video about the seen an earlier video about the introducing back propagation using a introducing back propagation using a very simple function with just two very simple function with just two parameters as just basically testing the parameters as just basically testing the waters of back propagation waters of back propagation obviously we need to go into a bit obviously we need to go into a bit more detailed discussion about how back more detailed discussion about how back propagation is applied in neural propagation is applied in neural networks and this section kind of covers networks and this section kind of covers some of the basics out of this section this in", "image_path": "img_data/video_54_chunk_0.jpg"}
{"video": "video_54", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "some of the basics out of this section this in basics out of this section this in this kind of introductory i think this kind of introductory i think it's worthwhile going through an it's worthwhile going through an introductory example that involves just introductory example that involves just some kind of a very simple kind of some kind of a very simple kind of neural kind of structure in fact i'll go neural kind of structure in fact i'll go through a structure that it will through a structure that it will actually involve just a single neuron actually involve just a single neuron i'll just call it the single neuron back prop and of course we can actually go prop and of course we can actually go through over more and more comp examples", "image_path": "img_data/video_54_chunk_1.jpg"}
{"video": "video_54", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "prop and of course we can actually go through over more and more comp examples through over more and more comp examples but i think this is kind of sufficient but i think this is kind of sufficient to just apply back propagation what we to just apply back propagation what we have just learned on that video to a have just learned on that video to a bit more neural kind of networking bit more neural kind of networking setting the single neuron is setting the single neuron is obviously modeled by dot product this obviously modeled by dot product this dot product is actually shown over here the is actually shown over here the followed by some kind of nonlinearity followed by some kind of nonlinearity let's assume that there's nonlinearity let's assume that there's nonlinearity is the familiar to aoid to produce", "image_path": "img_data/video_54_chunk_2.jpg"}
{"video": "video_54", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "let's assume that there's nonlinearity is the familiar to aoid to produce is the familiar to aoid to produce some kind of prediction at the output some kind of prediction at the output evidently this is a binary classifier evidently this is a binary classifier as we have seen earlier and the question now we earlier and the question now we can ask is how what about this can ask is how what about this gradient the gradient of y heart with gradient the gradient of y heart with respect to the vector parameter now vector parameter now w and this is evidently i mean if i'm w and this is evidently i mean if i'm just giv you a simple example here just giv you a simple example here where", "image_path": "img_data/video_54_chunk_3.jpg"}
{"video": "video_54", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "just giv you a simple example here where w will be 2 - 3 minus 3 this will be the w0 w1 and w2 elements 3 this will be the w0 w1 and w2 elements of this w vector and of this w vector and x is going to be x0 x1 x is going to be x0 x1 x2 equal to -1 - 2 and 1 this will be the values of", "image_path": "img_data/video_54_chunk_4.jpg"}
{"video": "video_54", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "-1 - 2 and 1 this will be the values of the x vector then we can actually the x vector then we can actually ask the question what will be the ask the question what will be the gradient of y hat with respect to w and gradient of y hat with respect to w and then evidently this is the partial then evidently this is the partial derivative of y hat with respect to w0 with respect to w1 and finally with respect to w2 we would to calculate respect to w2 we would to calculate this kind of vector and this vector", "image_path": "img_data/video_54_chunk_5.jpg"}
{"video": "video_54", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "respect to w2 we would to calculate this kind of vector and this vector this kind of vector and this vector is evidently three a dimensionality of three as we three a dimensionality of three as we have three parameters in our have three parameters in our system here three trainable parameters system here three trainable parameters the computational graph the computational graph here is fairly trivial we are not here is fairly trivial we are not going to draw it again it is actually going to draw it again it is actually given showing in front of us the only given showing in front of us the only thing we need to sort of understand thing we need to sort of understand is how if you have all the tensor is how if you have all the tensor names ob evidently this one tensor names ob evidently this one tensor here which is we can denote it as z", "image_path": "img_data/video_54_chunk_6.jpg"}
{"video": "video_54", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "names ob evidently this one tensor here which is we can denote it as z here which is we can denote it as z and we are going to understand that we are going to understand that we are going to back propagate from y hat all the way to propagate from y hat all the way to this tensor we're not really this tensor we're not really interested it's quite important to interested it's quite important to recognize in back vation problems with recognize in back vation problems with respect to what gradients you need to respect to what gradients you need to calculate no one ask us anything calculate no one ask us anything about x and therefore we are not going about x and therefore we are not going to actually go there the only thing we to actually go there the only thing we need to do is to go here in the using the forward and", "image_path": "img_data/video_54_chunk_7.jpg"}
{"video": "video_54", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "need to do is to go here in the using the forward and here in the using the forward and the backward kind of passes the backward kind of passes the forward pass is actually straight forward and the first equation here again using the principle of leist again using the principle of leist dependencies we are starting from the dependencies we are starting from the input of the network going to the output input of the network going to the output the first equation is z is equal the first equation is z is equal to w transpose x that's equation number to w transpose x that's equation number one and the", "image_path": "img_data/video_54_chunk_8.jpg"}
{"video": "video_54", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "to w transpose x that's equation number one and the second equation is y hat the equation two is y hat the equation two is y hat is equal sigma of z that's equation hat is equal sigma of z that's equation two all right now that we have two all right now that we have these two equations we can we have these two equations we can actually in fact calculate all the actually in fact calculate all the values that we have in our system and values that we have in our system and", "image_path": "img_data/video_54_chunk_9.jpg"}
{"video": "video_54", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "values that we have in our system and it's actually very easy to see here that this is 2 - 3 - that this is 2 - 3 - 3 time the vector -1 - 2 1 which is 3 time the vector -1 - 2 1 which is evidently 1.0 and this one ends up being the 1.0 and this one ends up being the application of the sigmoid that is now application of the sigmoid that is now treated as a elementary function over treated as a elementary function over here that the sigmoidal function and here that the sigmoidal function and this will if you do the ation is going", "image_path": "img_data/video_54_chunk_10.jpg"}
{"video": "video_54", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "here that the sigmoidal function and this will if you do the ation is going this will if you do the ation is going to be end up being 0.73 the for pass is done we 0.73 the for pass is done we have for each of the tensors that we are have for each of the tensors that we are interested in the corresponding values interested in the corresponding values and the now the backward pass starts the back our pass starts evidently from equation evidently from equation two and this is our first gate that we", "image_path": "img_data/video_54_chunk_11.jpg"}
{"video": "video_54", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "evidently from equation two and this is our first gate that we two and this is our first gate that we equation two corresponds to has the equation two corresponds to has the first downstream gradient with respect first downstream gradient with respect to this kind of grade which is also the to this kind of grade which is also the first gradient that we are going to first gradient that we are going to calculate we will just what we calculate we will just what we did earlier we denote with a small did earlier we denote with a small letter d in front of y hat this is a letter d in front of y hat this is a partial derivative of y hat with respect partial derivative of y hat with respect to y hat and this is 1.0 and the sigmoidal", "image_path": "img_data/video_54_chunk_12.jpg"}
{"video": "video_54", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "1.0 and the sigmoidal unit we have seen it earlier in the unit we have seen it earlier in the previous kind of video we previous kind of video we will call it as such as if you will call it as such as if you remember that kind of template the remember that kind of template the template kind of equally applies here template kind of equally applies here according to this template we have a according to this template we have a gate which receives it's the in gate which receives it's the in the for pass we have let's say a vector the for pass we have let's say a vector a tensor z and x and y were the a tensor z and x and y were the tensor it accepts a downstream", "image_path": "img_data/video_54_chunk_13.jpg"}
{"video": "video_54", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "a tensor z and x and y were the tensor it accepts a downstream tensor it accepts a downstream gradient d z and we need to calculate gradient d z and we need to calculate the dx and the dy and the template that the dx and the dy and the template that we have seen earlier it is can we have seen earlier it is can written as follows the written as follows the dx the downstream gradient dx is the dx the downstream gradient dx is the upstream gradient dz that someone upstream gradient dz that someone calculates for us times the local calculates for us times the local with which is the partial derivative with which is the partial derivative of the gate function dz", "image_path": "img_data/video_54_chunk_14.jpg"}
{"video": "video_54", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "with which is the partial derivative of the gate function dz of the gate function dz with respect to x and the same thing can with respect to x and the same thing can be for the y this is that's basically the two kind of sort of ports the two kind of sort of ports the two downstream gradients that are going two downstream gradients that are going to be calculated the first over here to be calculated the first over here we have only one port with respect to we have only one port with respect to this gate the dz is over here dz this gate the dz is over here dz is the upstream which is d y h with", "image_path": "img_data/video_54_chunk_15.jpg"}
{"video": "video_54", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "this gate the dz is over here dz is the upstream which is d y h with is the upstream which is d y h with respect to times the sigmoid the respect to times the sigmoid is sigma of z with respect to sigmoid is sigma of z with respect to z that sigmoid that partial that sigmoid that partial derivative we have seen earlier for derivative we have seen earlier for those who have missed it please go back those who have missed it please go back to that elementary back propagation to that elementary back propagation video and you should see that this gate video and you should see that this gate is one this part derivative is 1or sigma", "image_path": "img_data/video_54_chunk_16.jpg"}
{"video": "video_54", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "video and you should see that this gate is one this part derivative is 1or sigma is one this part derivative is 1or sigma of z * sigma of z and this if you do the of z * sigma of z and this if you do the calculations is around calculations is around 0.2 that is my u back propagation with 0.2 that is my u back propagation with respect to this gate now i have a respect to this gate now i have a calculate the downstream gradient which calculate the downstream gradient which evidently is going to be needed for the evidently is going to be needed for the back propagation of equation one the back propagation of equation one the back propagation of equation one with back propagation of equation one with respect to w we have now dw as the respect to w we have now dw as the tensor", "image_path": "img_data/video_54_chunk_17.jpg"}
{"video": "video_54", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "respect to w we have now dw as the tensor dw is going to be the dw is going to be the downstream sorry the upstream the dz downstream sorry the upstream the dz times the partial derivative of the times the partial derivative of the dot product of w transpose w dot product of w transpose x with respect to transpose x with respect to w all right this is going to be well w all right this is going to be well this is reused from before this is reused from before 0.2 and i am going to 0.2 and i am going to now have something we have never seen", "image_path": "img_data/video_54_chunk_18.jpg"}
{"video": "video_54", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "0.2 and i am going to now have something we have never seen now have something we have never seen before i have a partial derivative of a before i have a partial derivative of a scalar with respect to a vector that scalar with respect to a vector that partial derivative is effectively a partial derivative is effectively a gradient itself and it is partial gradient itself and it is partial derivative of i will now expand the derivative of i will now expand the gradient of w0 x0 plus w the dot product gradient of w0 x0 plus w the dot product that is plus w1 x1 + w2 x2 with respect to w0 that's the second", "image_path": "img_data/video_54_chunk_19.jpg"}
{"video": "video_54", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "x2 with respect to w0 that's the second element is as you guess it exactly the element is as you guess it exactly the same dot product i'm not going to write same dot product i'm not going to write it again with respect to w1 and the it again with respect to w1 and the third one will be exactly the same dot third one will be exactly the same dot product with respect to w2 that is my product with respect to w2 that is my this partial derivative over here and this partial derivative over here and as you can see i have a something which is a see i have a something which is a partial derivative of a sum this can be partial derivative of a sum this can be broken down into individual par broken down into individual par derivatives and only one term out of", "image_path": "img_data/video_54_chunk_20.jpg"}
{"video": "video_54", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "broken down into individual par derivatives and only one term out of derivatives and only one term out of this summation over here of partial this summation over here of partial derivatives is going to be of concern derivatives is going to be of concern to me it's going to be non zero if i to me it's going to be non zero if i may expand it is 0.2 it will be partial derivative of w0 0.2 it will be partial derivative of w0 x0 with respect to w0 plus the second x0 with respect to w0 plus the second term will be 0 plus 0 because there's term will be 0 plus 0 because there's absolutely no w0 over here that", "image_path": "img_data/video_54_chunk_21.jpg"}
{"video": "video_54", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "term will be 0 plus 0 because there's absolutely no w0 over here that absolutely no w0 over here that will be all zeros similarly the second element over here of this second element over here of this vector it will be partial derivative of vector it will be partial derivative of w1 x1 with respect to w1 there will be w1 x1 with respect to w1 there will be 0+ that plus 0 and then finally will be 0+ that plus 0 and then finally will be 0 + 0 plus the par derivative of w 2 x2 0 + 0 plus the par derivative of w 2 x2 with respect to w2 in case these are basically my", "image_path": "img_data/video_54_chunk_22.jpg"}
{"video": "video_54", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "with respect to w2 in case these are basically my w2 in case these are basically my this will be my the three elements of this will be my the three elements of that of this parti derivative this that of this parti derivative this one is going to be equal to 0.2 well the one is going to be equal to 0.2 well the partial derivative of w0 x0 with respect partial derivative of w0 x0 with respect to w0 if you can go back to that lookup w0 if you can go back to that lookup table that we have recommended in the table that we have recommended in the core site also to look up elementary core site also to look up elementary partial derivatives this will be simply partial derivatives this will be simply x0 it will be x0 the second one will be", "image_path": "img_data/video_54_chunk_23.jpg"}
{"video": "video_54", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "x0 it will be x0 the second one will be the second one will be x1 and the third one will be simply x2 x1 and the third one will be simply x2 but x0 x1 and x2 are actually given it's x0 x1 and x2 are actually given it's very easy to calculate the u final dw very easy to calculate the u final dw which is - 0.2 - 0.4 and which is - 0.2 - 0.4 and 0.2 and that actually concludes our back ration actually concludes our back ration exercise and in this case we exercise and in this case we have been given if you a very", "image_path": "img_data/video_54_chunk_24.jpg"}
{"video": "video_54", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "exercise and in this case we have been given if you a very have been given if you a very simple is a single neuron that is doing simple is a single neuron that is doing binary classification back probated it binary classification back probated it and we are going to from and we are going to from scratch implementation plug in all these scratch implementation plug in all these kind of partial derivatives in that code and we can derivatives in that code and we can continue in a deep neural network now we continue in a deep neural network now we will be facing as we have seen earlier will be facing as we have seen earlier for example in this network that we have for example in this network that we have seen earlier on doing seen earlier on doing multilayer classification evidently", "image_path": "img_data/video_54_chunk_25.jpg"}
{"video": "video_54", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "seen earlier on doing multilayer classification evidently multilayer classification evidently there will be one or more kind of there will be one or more kind of fully connected kind of layers and fully connected kind of layers and people have gone into sort of as the people have gone into sort of as the your side kind of indicates into the your side kind of indicates into the and tabulated all of the gates now and tabulated all of the gates now these gates are no longer u gates which these gates are no longer u gates which are been dealing with scalar quantities are been dealing with scalar quantities an addition multiplier and on an addition multiplier and on but they are involving now matrices and but they are involving now matrices and vectors let's say matrix and vector vectors let's say matrix and vector multiplication is a gate that we have", "image_path": "img_data/video_54_chunk_26.jpg"}
{"video": "video_54", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "vectors let's say matrix and vector multiplication is a gate that we have multiplication is a gate that we have seen earlier on fully connected and layer on fully connected and layer architecture over here we have seen architecture over here we have seen we are seeing a fully connected layer we are seeing a fully connected layer which is just doing a matrix by vector which is just doing a matrix by vector multiplication producing another tensor multiplication producing another tensor z and someone is asking us to z and someone is asking us to implement to calculate the partial implement to calculate the partial derivative of that downstream derivative of that downstream gradient know this downstream gradient", "image_path": "img_data/video_54_chunk_27.jpg"}
{"video": "video_54", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "derivative of that downstream gradient know this downstream gradient know this downstream gradient with respect to the matrix w and people with respect to the matrix w and people have actually tabulated these results have actually tabulated these results as well because they're very easy and as well because they're very easy and to symbolically tabulate them and here to symbolically tabulate them and here is the response the results the u is the response the results the u downstream gradient with respect to w is downstream gradient with respect to w is the upstream gradient transpose that the upstream gradient transpose that someone is giving you times the someone is giving you times the transpose of that vector x and we transpose of that vector x and we can actually use these tables these can actually use these tables have been tabulated in these", "image_path": "img_data/video_54_chunk_28.jpg"}
{"video": "video_54", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "can actually use these tables have been tabulated in these tables have been tabulated in these two references in this pdf file two references in this pdf file over here and this pdf file if i open it over here and this pdf file if i open it you can actually see the all this you can actually see the all this kind of formulas and the source what is formulas and the source what is behind this kind of table you can behind this kind of table you can actually see the gate of that i just explained", "image_path": "img_data/video_54_chunk_29.jpg"}
{"video": "video_54", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "actually see the gate of that i just explained the gate of that i just explained is a matrix times a column vector with is a matrix times a column vector with respect to a matrix that is the fully respect to a matrix that is the fully connected layer architecture evidently connected layer architecture evidently we have a nonlinearity theu nonlinearity we have a nonlinearity theu nonlinearity that follows that but with respect to that follows that but with respect to this kind of gate over here we are this kind of gate over here we are explaining where the final formula that explaining where the final formula that we have seen in our noes comes from we have seen in our noes comes from the you feel free to use these tables the you feel free to use these tables in any problem of back propagation in any problem of back propagation you will be facing although for this", "image_path": "img_data/video_54_chunk_30.jpg"}
{"video": "video_54", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "in any problem of back propagation you will be facing although for this you will be facing although for this introductory count of course we will not introductory count of course we will not be insisting on having if you a deeper having if you a deeper understanding or practice with understanding or practice with respect to this kind of more complicated respect to this kind of more complicated kind of networks in fact the it's kind of networks in fact the it's sufficient to deal with what the sufficient to deal with what the frameworks are actually using let's say frameworks are actually using let's say pytorch or tensor flow with the pytorch or tensor flow with the socaled autograd and auto def methods socaled autograd and auto def methods and have an example see an example of and have an example see an example of how they do it kind of numerically", "image_path": "img_data/video_54_chunk_31.jpg"}
{"video": "video_54", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "and have an example see an example of how they do it kind of numerically how they do it kind of numerically without really going into all this kind without really going into all this kind of complicated equations but at least of complicated equations but at least now you understand what is behind these now you understand what is behind these kind of libraries there is kind of libraries there is another page over here called backr another page over here called backr bation exercises where you can actually bation exercises where you can actually see the exercise we have gone through as see the exercise we have gone through as well also some more complicated cases well also some more complicated cases of having if you nonlinearities of having if you nonlinearities in the present in this kind of in the present in this kind of network again these are all exercises you", "image_path": "img_data/video_54_chunk_32.jpg"}
{"video": "video_54", "start": "0:16:30", "end": "0:16:42.133333", "timestamp": "0:16:30 - 0:16:42.133333", "text": "network again these are all exercises you again these are all exercises you actually go very easily using the actually go very easily using the existing presentation and anything more existing presentation and anything more complicated is kind of outside of the complicated is kind of outside of the scope of this course", "image_path": "img_data/video_54_chunk_33.jpg"}
{"video": "video_55", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "we introduce back propagation which is this procedure for gradi estimation is this procedure for gradi estimation but first look at the sort of what we but first look at the sort of what we call theta over here plenty of call theta over here plenty of parameters some of these matrices are parameters some of these matrices are pretty sizable as well we have u pretty sizable as well we have u w1 we have b1 we have w2 and b2 these are all our b1 we have w2 and b2 these are all our trainable parameters in this specific trainable parameters in this specific case and if you do the calculations we", "image_path": "img_data/video_55_chunk_0.jpg"}
{"video": "video_55", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "trainable parameters in this specific case and if you do the calculations we case and if you do the calculations we have a matrix the matrix w1 takes 784 have a matrix the matrix w1 takes 784 dimensions and the input and converts dimensions and the input and converts them into 128 it is really the them into 128 it is really the number of parameters we have here is number of parameters we have here is 784 by 128 that is a number of parameters over 128 that is a number of parameters over here we have this is just 12", "image_path": "img_data/video_55_chunk_1.jpg"}
{"video": "video_55", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "128 that is a number of parameters over here we have this is just 12 here we have this is just 12 parameters for the bias because it's parameters for the bias because it's just a vector to vector just a vector to vector addition and then we have another dense addition and then we have another dense matrix which is 128 * matrix which is 128 * 10 and plus 10 the number of parameters 10 and plus 10 the number of parameters with the second kind of bias with the second kind of bias term but this number is quite term but this number is quite significant it's in the many significant it's in the many thousands of parameters at the end of thousands of parameters at the end of the day we are going to be implementing the day we are going to be implementing sastic gr descent and the gr requires as", "image_path": "img_data/video_55_chunk_2.jpg"}
{"video": "video_55", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "the day we are going to be implementing sastic gr descent and the gr requires as sastic gr descent and the gr requires as we have seen before the calculation of we have seen before the calculation of the gradient this gradient operator doing gradient this gradient operator doing this calculation for thousands or this calculation for thousands or millions or even billions of parameters millions or even billions of parameters is going to require to us to invent a is going to require to us to invent a procedure that is actually called back procedure that is actually called back propagation and it was known propagation and it was known several decades ago is just that several decades ago is just that recently and when i say recently in the recently and when i say recently in the last 10 12 years it has been", "image_path": "img_data/video_55_chunk_3.jpg"}
{"video": "video_55", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "recently and when i say recently in the last 10 12 years it has been last 10 12 years it has been accelerated by the computational accelerated by the computational accelerators graphical processing accelerators graphical processing units and made it very efficient units and made it very efficient this really allowed us to actually this really allowed us to actually go deeper therefore introduce more and go deeper therefore introduce more and more parameters and scale our more parameters and scale our networks before without even networks before without even going into the details of a fairly going into the details of a fairly complex sort of back ration complex sort of back ration procedures in the neural networks procedures in the neural networks what i will do is i will introduce back what i will do is i will introduce back propagation using a very simple u", "image_path": "img_data/video_55_chunk_4.jpg"}
{"video": "video_55", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "what i will do is i will introduce back propagation using a very simple u propagation using a very simple u just function just with two just function just with two parameters i think two parameters are parameters i think two parameters are plenty to first of all define the plenty to first of all define the gradient let me just write down the gradient let me just write down the function that i'm going to be function that i'm going to be using this is going to be the f ofx using this is going to be the f ofx comm y this function is x + sigma of y this function is x + sigma of y divided by the sigma of divided by the sigma of x+ x + y", "image_path": "img_data/video_55_chunk_5.jpg"}
{"video": "video_55", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "divided by the sigma of x+ x + y squared that's my function and this function is has function and this function is has two parameters and produces a scalar two parameters and produces a scalar output and the sigmoid is definitely output and the sigmoid is definitely something we have seen before let me something we have seen before let me just remind everyone that the sigma of x just remind everyone that the sigma of x is 1 + e the- x and what is really the is 1 + e the- x and what is really the question that's the inputs that i have and the question that", "image_path": "img_data/video_55_chunk_6.jpg"}
{"video": "video_55", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "question that's the inputs that i have and the question that inputs that i have and the question that i am asking is the following this is the i am asking is the following this is the question over here what is the question over here what is the gradient of the function f with respect gradient of the function f with respect to the two parameters here x and y to the two parameters here x and y notice that i'm not really using w's notice that i'm not really using w's as parameters names or thetas in this as parameters names or thetas in this case and the reason is that i you case and the reason is that i in many stes we will'll be using know in many stes we will'll be using some lookup tables to calculate some lookup tables to calculate derivatives throughout this kind of derivatives throughout this kind of procedure and many of these lookup have", "image_path": "img_data/video_55_chunk_7.jpg"}
{"video": "video_55", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "derivatives throughout this kind of procedure and many of these lookup have procedure and many of these lookup have been authored with x and y in the been authored with x and y in the first instance we will be just calling first instance we will be just calling the parameters x and the parameters x and y all right what is the gradient y all right what is the gradient for those who need some kind of a for those who need some kind of a refresher in terms of calculus we refresher in terms of calculus we have provided some videos on the site as have provided some videos on the site as well also some pointers to can academy well also some pointers to can academy to be able to refresh a little bit to be able to refresh a little bit the u sort of calculus kind of that you the u sort of calculus kind of that you need to know some simple der but the", "image_path": "img_data/video_55_chunk_8.jpg"}
{"video": "video_55", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "the u sort of calculus kind of that you need to know some simple der but the need to know some simple der but the gradient is simply as we have said gradient is simply as we have said before an extension of the partial before an extension of the partial derivative to more than one dimension derivative to more than one dimension we have two elements here we have two elements here we have two parameters we have two elements in this parameters we have two elements in this gradient vector the first is the parti gradient vector the first is the parti derivative of f with respect to x and derivative of f with respect to x and the second as you expect is a paral the second as you expect is a paral derivative of f with respect to y this derivative of f with respect to y this is the gradient of f of respect to x and is the gradient of f of respect to x and y the back propagation operation y the back propagation operation now starts and it will", "image_path": "img_data/video_55_chunk_9.jpg"}
{"video": "video_55", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "y the back propagation operation now starts and it will now starts and it will start with us developing something start with us developing something that we actually call the that we actually call the computational graph and the computational graph and the computational graph is not really computational graph is not really something which is very complicated to something which is very complicated to understand in fact i'll use just a kind understand in fact i'll use just a kind of a positive note to open a of a positive note to open a parentheses and if i give you parentheses and if i give you something that is called that's a g is something that is called that's a g is equal to x + y this expression over equal to x + y this expression over here i will be creating a", "image_path": "img_data/video_55_chunk_10.jpg"}
{"video": "video_55", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "equal to x + y this expression over here i will be creating a here i will be creating a computational graph which in general is computational graph which in general is going to consist of nodes and going to consist of nodes and edges the over the edges tensors will edges the over the edges tensors will flow the x and y will be over here flow the x and y will be over here and g is going to be produced over there and g is going to be produced over there and what is a tensor is a and what is a tensor is a generalization of a matrix for to generalization of a matrix for to more than two dimensions we have a we more than two dimensions we have a volume three dimensions we are", "image_path": "img_data/video_55_chunk_11.jpg"}
{"video": "video_55", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "more than two dimensions we have a volume three dimensions we are have a volume three dimensions we are calling this a three dimensional tensor calling this a three dimensional tensor a matrix is called a two- dimensional a matrix is called a two- dimensional tensor a vector is called a single tensor a vector is called a single dimensional tensor and that is the dimensional tensor and that is the over the nodes of this graph will be over the nodes of this graph will be the operations here we have just a operations here we have just a simple very simple operation called an simple very simple operation called an addition and the u the node is the sort addition and the u the node is the sort of undertaking that kind of operation of undertaking that kind of operation it's adding the two tensors produce get it's adding the two tensors produce get another that's a very simple", "image_path": "img_data/video_55_chunk_12.jpg"}
{"video": "video_55", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "it's adding the two tensors produce get another that's a very simple another that's a very simple computational graph our computational graph our computational graph will be slightly more complicated graph will be slightly more complicated here but be it all right here but be it all right we are i'm going to start from the bottom over here start from the bottom over here i have at the bottom my two inputs that are coming bottom my two inputs that are coming in and i am going to start drawing if fact i'll sort of meet", "image_path": "img_data/video_55_chunk_13.jpg"}
{"video": "video_55", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "drawing if fact i'll sort of meet you in a few seconds after i have you in a few seconds after i have finished drawing the finished drawing the diagram i finished drawing the diagram i finished drawing the diagram but i didn't write everything down we but i didn't write everything down we actually can see here how the diagram actually can see here how the diagram kind of maps into this expression kind of maps into this expression here of the f function at the bottom of here of the f function at the bottom of the diagram we have the x and y at the diagram we have the x and y at the top of the diagram we are going this top of the diagram we are going this graph is going to calculate for us graph is going to calculate for us the value of the function f for whatever", "image_path": "img_data/video_55_chunk_14.jpg"}
{"video": "video_55", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "graph is going to calculate for us the value of the function f for whatever the value of the function f for whatever inputs we are feeding in here you can inputs we are feeding in here you can see these x + y are being squared we see these x + y are being squared we are adding to this term the sigma of are adding to this term the sigma of x therefore we produce a x therefore we produce a denominator and then we have the denominator and then we have the numerator which is produced by x plus numerator which is produced by x plus the sigma of y and then we are inverting the sigma of y and then we are inverting the denominator multiplying the two the denominator multiplying the two terms to form the function now what we terms to form the function now what we will need to do by convention we are", "image_path": "img_data/video_55_chunk_15.jpg"}
{"video": "video_55", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "terms to form the function now what we will need to do by convention we are will need to do by convention we are going to write down some tensor values and to write down some tensor values and i'm going to be writing these tensor i'm going to be writing these tensor values as if they are kind of a values as if they are kind of a python variables i am going to python variables i am going to have to call this t that comes at the have to call this t that comes at the input at the output of this other here input at the output of this other here xp y for x + xp y for x + y i'm going to call this guy here xp y i'm going to call this guy here xp y sqr i am going to then call this guy", "image_path": "img_data/video_55_chunk_16.jpg"}
{"video": "video_55", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "sqr i am going to then call this guy i am going to then call this guy sigx this guy sigy sig y this is definitely the new as for numerator and this is the let's call numerator and this is the let's call this the numer at denominator and we", "image_path": "img_data/video_55_chunk_17.jpg"}
{"video": "video_55", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "the numer at denominator and we have over here the in denom or the inverse of the denominator these are all the names of the tensor these are all the names of the tensor that we will keep values for us as if that we will keep values for us as if we're writing some kind of python code we're writing some kind of python code and the back propagation procedure is and the back propagation procedure is going to proceed now it's going to start going to proceed now it's going to start now it will involve two stages or two now it will involve two stages or two passes the first pass is called the", "image_path": "img_data/video_55_chunk_18.jpg"}
{"video": "video_55", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "now it will involve two stages or two passes the first pass is called the forward pass and the pa is going to start is going to be written in as a start is going to be written in as a sequence of equations the first sequence of equations the first equation i mean there's some kind equation i mean there's some kind of rulle of where you start of rulle of where you start always from the least dependency that always from the least dependency that you have in your diagram over here", "image_path": "img_data/video_55_chunk_19.jpg"}
{"video": "video_55", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "always from the least dependency that you have in your diagram over here you have in your diagram over here and by that i mean that we can not start and by that i mean that we can not start from an operation that it is further from an operation that it is further deeper into the graph over here deeper into the graph over here because evidently we depend on because evidently we depend on earlier on calculations or earlier on calculations or values we do not have yet we have values we do not have yet we have three options here where to start we three options here where to start we definitely can start with this definitely can start with this what we'll call be calling from now on a what we'll call be calling from now on a gate and this gate is has only depend", "image_path": "img_data/video_55_chunk_20.jpg"}
{"video": "video_55", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "what we'll call be calling from now on a gate and this gate is has only depend gate and this gate is has only depend on x and y therefore it's given x and y on x and y therefore it's given x and y as an input we have no other there no as an input we have no other there no problem for with that we can actually problem for with that we can actually the same way calculate sigma of x the same way calculate sigma of x because it depends on x in the same because it depends on x in the same speed we can calculate sigma of y now in speed we can calculate sigma of y now in my notes over here i have started with my notes over here i have started with sig y calculate sig y but either of sig y calculate sig y but either of the three options you select you will the three options you select you will arrive exactly the same result it arrive exactly the same result it does not really matter provided you does not really matter provided you follow this rule doesn't matter where", "image_path": "img_data/video_55_chunk_21.jpg"}
{"video": "video_55", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "does not really matter provided you follow this rule doesn't matter where follow this rule doesn't matter where you start i'm writing the sig y the you start i'm writing the sig y the tensor is going to be 1/ 1 + cus y and tensor is going to be 1/ 1 + cus y and that's my equation one then i can immediately form the one then i can immediately form the n as the x + sig y note that i'm not n as the x + sig y note that i'm not writing x + 1 / 1 to the e theus y i am writing x + 1 / 1 to the e theus y i am actually using earlier computed results", "image_path": "img_data/video_55_chunk_22.jpg"}
{"video": "video_55", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "writing x + 1 / 1 to the e theus y i am actually using earlier computed results then i'm going to have c x is = to 1/ 1 + e- x that's equation 3 we have xp y is = to x + y that's equation 4 xp y sqr is equal to xp y * x y that's", "image_path": "img_data/video_55_chunk_23.jpg"}
{"video": "video_55", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "4 xp y sqr is equal to xp y * x y that's sqr is equal to xp y * x y that's equation five we have denom is equal to c x plus x y s2 r that's equation six we have in the nome as one over the nome as one over the nome that's equation seven", "image_path": "img_data/video_55_chunk_24.jpg"}
{"video": "video_55", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "nome as one over the nome that's equation seven nome that's equation seven and finally we have f is equal and finally we have f is equal to numerator which we obtain over here numerator which we obtain over here times the in the inverse of the denominator which is held inverse of the denominator which is held in the tensor if the and that's in the tensor if the and that's basically we have finish the for pass basically we have finish the for pass it's equation eight in the for pass what the for eight in the for pass what the for pass did is we it gradually calculated", "image_path": "img_data/video_55_chunk_25.jpg"}
{"video": "video_55", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "eight in the for pass what the for pass did is we it gradually calculated pass did is we it gradually calculated all the values which is needed to all the values which is needed to calculate ultimately the function f calculate ultimately the function f it is stage wise calculation of the it is stage wise calculation of the value of the function if we had inputs x value of the function if we had inputs x and y which are let's say real numbers and y which are let's say real numbers then immediately we have a real number then immediately we have a real number over here in at the end of this equation over here in at the end of this equation eight all right now the kind of a fun eight all right now the kind of a fun starts with the second pass that will be starts with the second pass that will be calling the backwards pass", "image_path": "img_data/video_55_chunk_26.jpg"}
{"video": "video_55", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "starts with the second pass that will be calling the backwards pass and the forward pass is definitely if i can of use a pass is definitely if i can of use a ruler over here the forward pass goes ruler over here the forward pass goes from the input from the bottom of the diag was to input from the bottom of the diag was to the output to the top that is the to the output to the top that is basically my forward pass", "image_path": "img_data/video_55_chunk_27.jpg"}
{"video": "video_55", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "the to the output to the top that is basically my forward pass the backward pass will actually go the other way right this is my back and in this backward pass we will be calculating again stage wise but will be calculating again stage wise but now we'll be calculating gradients we'll be calculating gradients we'll be calculating derivative calculating derivative let's see how this is going to", "image_path": "img_data/video_55_chunk_28.jpg"}
{"video": "video_55", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "calculating derivative let's see how this is going to let's see how this is going to be done i'm going to be using a be done i'm going to be using a template this template i'm going to just template this template i'm going to just draw over here i'm going to be using draw over here i'm going to be using this template over and over again for this template over and over again for every single gate that i have every single gate that i have meeting going from the top of the meeting going from the top of the diagram i meet one gate i'll be using diagram i meet one gate i'll be using this template to culate the this template to culate the associated kind of gradients of that i associated kind of gradients of that i need to calculate i'm going to just", "image_path": "img_data/video_55_chunk_29.jpg"}
{"video": "video_55", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "associated kind of gradients of that i need to calculate i'm going to just need to calculate i'm going to just do that i'm going to write do that i'm going to write generically the name of the gate which i generically the name of the gate which i am going to meet each time all right am going to meet each time all right i have some kind of in fact let i have some kind of in fact let me throw it how the gate looks in the me throw it how the gate looks in the forward class let's say that i have a z over here let's say that i have a z over here and i have two input and i have two input ports x", "image_path": "img_data/video_55_chunk_30.jpg"}
{"video": "video_55", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "and i have two input ports x y and i am going to have to calculate i'm going to be receiving some calculate i'm going to be receiving some upstream gradient i will be by convention calling gradient i will be by convention calling all of my gradients dz all the all of my gradients dz all the tensors that i will be having over tensors that i will be having over here and drawing over here i'm going to here and drawing over here i'm going to by convention prepare the tensor name of by convention prepare the tensor name of the forward pass i'll prepend it with the forward pass i'll prepend it with the letter d for derivative and i'm", "image_path": "img_data/video_55_chunk_31.jpg"}
{"video": "video_55", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "the forward pass i'll prepend it with the letter d for derivative and i'm the letter d for derivative and i'm going to have dx over here and d y over here all right the this type of here all right the this type of gradients i'll be calling them upstream gradient and this is for abs derivative in this case because the derivative in this case because the function", "image_path": "img_data/video_55_chunk_32.jpg"}
{"video": "video_55", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "derivative in this case because the function simple but upst means that they're coming from upst means that they're coming from above and someone else is going to above and someone else is going to calulate this uping gradient for me calulate this uping gradient for me it's not my possibility to just do it's not my possibility to just do anything with it is going to arrive anything with it is going to arrive from above from a previous kind of from above from a previous kind of calculation that's the whole point of calculation that's the whole point of back propagation my responsibility is to back propagation my responsibility is to calculate the downstream gradient calculate the downstream gradient these are stands for down these are stands for down have two we have two ports we have two", "image_path": "img_data/video_55_chunk_33.jpg"}
{"video": "video_55", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "these are stands for down have two we have two ports we have two downstream gradients two down grate the and here is the template the dx which is of course based on a rule in dx which is of course based on a rule in t called the chain rle the dx is the t called the chain rle the dx is the downstream radient at the port x is the downstream radient at the port x is the abs gradient that someone sent", "image_path": "img_data/video_55_chunk_34.jpg"}
{"video": "video_55", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "the downstream radient at the port x is the abs gradient that someone sent the abs gradient that someone sent me dz times a gradient which me dz times a gradient which is called the local gradient this is called the local gradient this is the local and the same thing applies with dy local and the same thing applies with dy isal to d c of g respect to y these are all the things y these are all the things that we need to specify in that", "image_path": "img_data/video_55_chunk_35.jpg"}
{"video": "video_55", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "y these are all the things that we need to specify in that we need to specify in that template and this is template and this is effectively known as downstream gradient effectively known as downstream gradient is the upstream times local that's how is the upstream times local that's how you will remember it and that's how you will remember it and that's how we'll be applying it right now for we'll be applying it right now for starting at the top of the diagram starting at the top of the diagram therefore because back pass starts from therefore because back pass starts from the top of the diagram to the bottom we the top of the diagram to the bottom we are writing our first tensor df and this are writing our first tensor df and this first tensor it's because which first tensor it's because which is the first tensor that we not have is the first tensor that we not have anything any gate to calculate", "image_path": "img_data/video_55_chunk_36.jpg"}
{"video": "video_55", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "is the first tensor that we not have anything any gate to calculate anything any gate to calculate as a downstream gradient it's very as a downstream gradient it's very trivial to calculated that df is trivial to calculated that df is effectively the partial derivative of f effectively the partial derivative of f with respect to f and we know that this with respect to f and we know that this is one and how we know it we have next one and how we know it we have next to us a table of partial derivatives and to us a table of partial derivatives and now i think it's a good time to stop and now i think it's a good time to stop and go back to the course site and see exact go back to the course site and see exact where this table is", "image_path": "img_data/video_55_chunk_37.jpg"}
{"video": "video_55", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "go back to the course site and see exact where this table is located this is the page that we are located this is the page that we are actually dealing with today and as actually dealing with today and as you can see here there is a note you can see here there is a note first of all there are some links over first of all there are some links over here that i think we should pay some here that i think we should pay some attention to first is the a can academy attention to first is the a can academy lessons on partial derivatives and lessons on partial derivatives and gradients this is all for background gradients this is all for background information and over here we also information and over here we also have some derivative calculations have some derivative calculations specifically for tensor something will", "image_path": "img_data/video_55_chunk_38.jpg"}
{"video": "video_55", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "have some derivative calculations specifically for tensor something will specifically for tensor something will actually be a bit useful a bit later but actually be a bit useful a bit later but for now this is the table that we are for now this is the table that we are needing to consult this is the needing to consult this is the kind of give us some primitive sort kind of give us some primitive sort of derivatives from elementary kind of derivatives from elementary kind of functions you can see here when we functions you can see here when we have something which is 1 /x what have something which is 1 /x what this corresponds to instead of f and this corresponds to instead of f and g you can actually have in general g you can actually have in general any tensor of and then this is kind of a", "image_path": "img_data/video_55_chunk_39.jpg"}
{"video": "video_55", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "any tensor of and then this is kind of a of and then this is kind of a general kind of rules but this general kind of rules but this were a bit more specialized to powers were a bit more specialized to powers and polinomial here is your and polinomial here is your first that this is the entry first that this is the entry that we have used just now the that we have used just now the derivative of f with respect to itself derivative of f with respect to itself and that was the one and then as we go and that was the one and then as we go and meet additional kind of gates we'll and meet additional kind of gates we'll be using other entries out of this be using other entries out of this table now let's go back to that", "image_path": "img_data/video_55_chunk_40.jpg"}
{"video": "video_55", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "be using other entries out of this table now let's go back to that table now let's go back to that sort of site sor the drawing sort of site sor the drawing board and actually continue with the board and actually continue with the second derivative sorry the second derivative sorry the second kind of gate that we will see second kind of gate that we will see this is the in fact the this is the in fact the this is the first gate we actually see going from first gate we actually see going from the top of the diagram this is the top of the diagram this is the multiplier and as we have just multiplier and as we have just explained we will go backward we will explained we will go backward we will be back", "image_path": "img_data/video_55_chunk_41.jpg"}
{"video": "video_55", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "explained we will go backward we will be back propagating equation 8 7 6 5 4 we will propagating equation 8 7 6 5 4 we will not be deviating out of that sequence we not be deviating out of that sequence we know exactly how to go backwards we will know exactly how to go backwards we will have to do it in exactly the reverse have to do it in exactly the reverse order the back propagation of gate order the back propagation of gate 8 what is the template is actually 8 what is the template is actually telling us to do the template is telling us to do the template is telling us to do the following telling us to do the following we have now a df that we know the we have now a df that we know the upstream gradient is actually coming in upstream gradient is actually coming in we know it's value and we have two", "image_path": "img_data/video_55_chunk_42.jpg"}
{"video": "video_55", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "upstream gradient is actually coming in we know it's value and we have two we know it's value and we have two downstream gradients the first thing downstream gradients the first thing we will do is write down the tensor we will do is write down the tensor names that we will be the sensor name is a bit large be the sensor name is a bit large long in denom all right let's calculate the d first the is the upstream gradient times", "image_path": "img_data/video_55_chunk_43.jpg"}
{"video": "video_55", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "the is the upstream gradient times the local upstream is the local upstream is df and the local is the only calculation we have to local is the only calculation we have to do here it is the gate itself the do here it is the gate itself the gate is n times in the nome with respect to the p port", "image_path": "img_data/video_55_chunk_44.jpg"}
{"video": "video_55", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "in the nome with respect to the p port nome with respect to the p port the name of the port that we are doing the name of the port that we are doing in the forer pass which is in the forer pass which is n that's df is actually known c one0 times now this one is fairly straightforward we will use now a straightforward we will use now a poit note and the u lookup table that we have seen lookup table that we have seen earlier we have n times", "image_path": "img_data/video_55_chunk_45.jpg"}
{"video": "video_55", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "earlier we have n times in den n with respect to n i'm going in den n with respect to n i'm going to just replace the variables to make it to just replace the variables to make it a bit more compatible with the table a bit more compatible with the table that we just went through the n is my that we just went through the n is my x and my in denome is something which is x and my in denome is something which is unrelated to x let's call it c with unrelated to x let's call it c with respect to x and in the lookup table if i may x and in the lookup table if i may go and see i'm definitely going to go and see i'm definitely going to see here an", "image_path": "img_data/video_55_chunk_46.jpg"}
{"video": "video_55", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "go and see i'm definitely going to see here an entry this is the entry let me make entry this is the entry let me make it a bit bigger not sure why it's not making it bigger but this is the entry that we were going to it's entry that we were going to it's going to be applying here let's go going to be applying here let's go back to the site the sorry the u the back to the site the sorry the u the board it is effectively c", "image_path": "img_data/video_55_chunk_47.jpg"}
{"video": "video_55", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "back to the site the sorry the u the board it is effectively c board it is effectively c all right but see was in denome which is that's basically we have now our first downstream gradient have now our first downstream gradient it is 1 * in denom that's basically all right now we have the second now we have the second port that we need to backward by gate port that we need to backward by gate we write exactly the same way the upstream again", "image_path": "img_data/video_55_chunk_48.jpg"}
{"video": "video_55", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "the upstream again is df times we have exactly the same expression but now is with respect to in denome and making exactly analogous arguments this is simply n all right that's a n tensor there n all right that's a n tensor there now we have back gate our first gate now we have back gate our first gate we where we going next we are going", "image_path": "img_data/video_55_chunk_49.jpg"}
{"video": "video_55", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "now we have back gate our first gate we where we going next we are going we where we going next we are going next to bar propagating equation 7 and next to bar propagating equation 7 and first we identify where this gate is first we identify where this gate is this one and it has where this gate is this one and it has only one port first we are going only one port first we are going to write the tensor did den n is my tensor that's my seven i have the den nome is the upstream gradient", "image_path": "img_data/video_55_chunk_50.jpg"}
{"video": "video_55", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "seven i have the den nome is the upstream gradient the den nome is the upstream gradient well the upstream gradient is this guy well the upstream gradient is this guy over here this actually coming from over here this actually coming from above the denom times the local well the local is the partial derivative of 1 the partial derivative of 1 over denom which is evidently the equation of the g 8 7 with respect to the name of the port", "image_path": "img_data/video_55_chunk_51.jpg"}
{"video": "video_55", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "the g 8 7 with respect to the name of the port 7 with respect to the name of the port in the forer pass that we know with is n and for that we are going to go back n and for that we are going to go back to our table and our table is over to our table and our table is over here and definitely we see something here and definitely we see something which is applying to us it is one over it is this", "image_path": "img_data/video_55_chunk_52.jpg"}
{"video": "video_55", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "is applying to us it is one over it is this it is one over it is this entry over here x to the power of n this entry over here x to the power of n where n is -1 it is -1 x to theus 2 -1 x theus 2 we -1 x to theus 2 -1 x theus 2 we will basically write this down it is no minus in front and this is denom", "image_path": "img_data/video_55_chunk_53.jpg"}
{"video": "video_55", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "no minus in front and this is denom squared now we have our above propagation of this gate as well to propagation of this gate as well to cut a long story short we have cut a long story short we have calculated this downstream calculated this downstream gradient this downstream gradient this one and this downstream grage will actually similarly go through additional actually similarly go through additional kind of", "image_path": "img_data/video_55_chunk_54.jpg"}
{"video": "video_55", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "actually similarly go through additional kind of gates and we'll reach this gate over here the sigma of x now the sigma of x we should have we now the sigma of x we should have we don't have it in a lookup table but what don't have it in a lookup table but what i we have done is we have definitely i we have done is we have definitely showed that in the kind of if i can find my mouse i would", "image_path": "img_data/video_55_chunk_55.jpg"}
{"video": "video_55", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "showed that in the kind of if i can find my mouse i would kind of if i can find my mouse i would be yeah right we have definitely derived yeah right we have definitely derived it over here this is the only gate that it over here this is the only gate that it's not in that kind of lookup table i it's not in that kind of lookup table i was telling you it is sigma of x * 1 was telling you it is sigma of x * 1 - sigma of x we know how to back - sigma of x we know how to back propagate also this gate as well as propagate also this gate as well as this gate with this expression and over here is actually expression and over here is actually these junction points are kind of quite", "image_path": "img_data/video_55_chunk_56.jpg"}
{"video": "video_55", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "expression and over here is actually these junction points are kind of quite these junction points are kind of quite important and there is a special important and there is a special treatment on in your course site treatment on in your course site about them they are called u copy gates about them they are called u copy gates jaction gates or whatever jaction gates or whatever you want to call it doesn't really you want to call it doesn't really matter what is really important is to matter what is really important is to consider what is happening on the back consider what is happening on the back propagation these are effectively gates propagation these are effectively gates should be treated as gates you should be treated as gates you should always remember this kind of a rule", "image_path": "img_data/video_55_chunk_57.jpg"}
{"video": "video_55", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "should be treated as gates you should always remember this kind of a rule always remember this kind of a rule of thumb whatever gradients are arriving of thumb whatever gradients are arriving in those jackon gates they are treated in those jackon gates they are treated water flows and therefore a flow of water flows and therefore a flow of water is coming here another flow is water is coming here another flow is coming over there guess what's going to coming over there guess what's going to happen they will merge and what will happen they will merge and what will be produced is a summation of the be produced is a summation of the two this has to do with another two this has to do with another sort of chain rule in calculus sort of chain rule in calculus that and your notes actually", "image_path": "img_data/video_55_chunk_58.jpg"}
{"video": "video_55", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "sort of chain rule in calculus that and your notes actually that and your notes actually explain it exactly why it's happening explain it exactly why it's happening but i don't not want to go into details but i don't not want to go into details of explanation because i want to just of explanation because i want to just get a big picture of my propagation in get a big picture of my propagation in this lecture all right we have this lecture all right we have another downstream gradient over another downstream gradient over there finally we will be summing twice the x i'll show will be summing twice the x i'll show you the solution in a you the solution in a moment suing to get to the bottom moment suing to get to the bottom over here of the diagram similarly", "image_path": "img_data/video_55_chunk_59.jpg"}
{"video": "video_55", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "moment suing to get to the bottom over here of the diagram similarly over here of the diagram similarly over here and then finally we will over here and then finally we will have the two partial derivatives that we have two partial derivatives that we have started our problem the question started our problem the question from at the very beginning what i from at the very beginning what i think it's a good idea for you to do is think it's a good idea for you to do is to back propagate six 5 to back propagate six 5 432 or maybe back propagate just 432 or maybe back propagate just six and five and the two six and five and the two gates over there at least you get a", "image_path": "img_data/video_55_chunk_60.jpg"}
{"video": "video_55", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "six and five and the two gates over there at least you get a gates over there at least you get a gist out of this using this template gist out of this using this template and the solutions are available in solutions the solutions are available in your site we have the your site in your site we have the forward pass the one that we just did forward pass the one that we just did the template and the solution of the template and the solution of the backward pass and this is what i was the backward pass and this is what i was telling you just now about the plastic telling you just now about the plastic equal the plus equal we have two plus", "image_path": "img_data/video_55_chunk_61.jpg"}
{"video": "video_55", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "telling you just now about the plastic equal the plus equal we have two plus equal the plus equal we have two plus equals one is actually over here and the equals one is actually over here and the other is actually over there and these other is actually over there and these two plus equals for the tensor x two plus equals for the tensor x correspond to the two junctions that we correspond to the two junctions that we have seen one and two over here while have seen one and two over here while the there's only one plus equal sign for the there's only one plus equal sign for dx for dy and this obviously corresponds dx for dy and this obviously corresponds to the this junction over there for the", "image_path": "img_data/video_55_chunk_62.jpg"}
{"video": "video_55", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "to the this junction over there for the this junction over there for the d1 that's basically what d1 that's basically what i think it's a good analogy to what i think it's a good analogy to treat kind of back propagation is as treat kind of back propagation is as a kind of water flow we start with a kind of water flow we start with a unit of flow at the top of the mountain unit of flow at the top of the mountain and each gate is acting a valve and each gate is acting a valve and is actually changing the amount is actually changing the amount of flow that is actually passing through of flow that is actually passing through its sort of a", "image_path": "img_data/video_55_chunk_63.jpg"}
{"video": "video_55", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "of flow that is actually passing through its sort of a downstream ports and that of downstream ports and that of course that amounts are actually course that amounts are actually given by the template closing i think it's worthwhile revisiting where the factor where is revisiting where the factor where is the computational advantage of back the computational advantage of back propagation actually coming from the propagation actually coming from the first as you can actually recognize first as you can actually recognize here is that we have broken the global here is that we have broken the global problem the problem of connecting problem the problem of connecting effectively f to the parameters x and y effectively f to the parameters x and y of that function into a stage-wise", "image_path": "img_data/video_55_chunk_64.jpg"}
{"video": "video_55", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "effectively f to the parameters x and y of that function into a stage-wise of that function into a stage-wise computation in other words of into local computation in other words of into local many local problems and that is many local problems and that is actually giving us quite significant actually giving us quite significant commutation advantage because we can commutation advantage because we can in many instances we in many instances we are the local computation such as are the local computation such as the calculation of you of the local the calculation of you of the local gradient could actually happen in gradient could actually happen in parallel for multiple gates that are parallel for multiple gates that are not necessarily having any not necessarily having any relationship to each other that's relationship to each other that's computational advantage number one the", "image_path": "img_data/video_55_chunk_65.jpg"}
{"video": "video_55", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "relationship to each other that's computational advantage number one the computational advantage number one the second computational advantage is that second computational advantage is that everywhere we have been everywhere we have been reusing the u earlier results reusing the u earlier results for example we have calculated a for example we have calculated a downstream gradient that downstream gradient that downstream gradient is of course reduced in other gradient is of course reduced in other subsequent kind of calculations finally the fact that each calculations finally the fact that each gate is elementary an addition sigmoidal gate is elementary an addition sigmoidal unit and inversion multiplication and unit and inversion multiplication and on can allow us to tabulate all these on can allow us to tabulate all these kind of derivatives and consult them", "image_path": "img_data/video_55_chunk_66.jpg"}
{"video": "video_55", "start": "0:33:30", "end": "0:33:41.466667", "timestamp": "0:33:30 - 0:33:41.466667", "text": "on can allow us to tabulate all these kind of derivatives and consult them kind of derivatives and consult them in a symbolic way without really in a symbolic way without really having spend any time doing a having spend any time doing a significantly expensive calculations", "image_path": "img_data/video_55_chunk_67.jpg"}
{"video": "video_56", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "we should imagine these networks that we call fully connected network we call fully connected network architectures to resemble some form of a pyramidal resemble some form of a pyramidal shape i call it this we have here at the this we have here at the bottom of this pyramid is the input bottom of this pyramid is the input x this pencil is all right we have x this pencil is all right we have this x which is fitting the input", "image_path": "img_data/video_56_chunk_0.jpg"}
{"video": "video_56", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "x this pencil is all right we have this x which is fitting the input this x which is fitting the input and we'll call this the -called input and we'll call this the -called input layer and we will have layer and we will have stacks of additional layers one stacks of additional layers one on top of the other in general we'll on top of the other in general we'll have layer l+ one l +2 all the way to the last l+ one l +2 all the way to the last layer that we call as we said the layer that we call as we said the head and this input layer now i'm not", "image_path": "img_data/video_56_chunk_1.jpg"}
{"video": "video_56", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "layer that we call as we said the head and this input layer now i'm not head and this input layer now i'm not going to i'm going to just use the these going to i'm going to just use the these circles to represent the neurons this circles to represent the neurons this input is going to be input is going to be fed to all the neurons and each neuron is going to be connected to all of the be connected to all of the neurons of the layer above it and on neurons of the layer above it and on i'm not going to draw all and on i'm not going to draw all the lines because it will be a fairly the lines because it will be a fairly messy kind of di diagram and", "image_path": "img_data/video_56_chunk_2.jpg"}
{"video": "video_56", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "the lines because it will be a fairly messy kind of di diagram and messy kind of di diagram and maybe i drew too many layers let me call this now the head it's now easy to see what's happening and evidently over here we're going to have the white hat that's now going to have the white hat that's now my head", "image_path": "img_data/video_56_chunk_3.jpg"}
{"video": "video_56", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "going to have the white hat that's now my head and this whole thing is going to be the body all right that's that is if you the canonical kind of architecture we the canonical kind of architecture we will see for fully connected kind of will see for fully connected kind of networks now each of obviously we cannot networks now each of obviously we cannot really go on with drawing as we have really go on with drawing as we have been doing up to now we have to been doing up to now we have to define if you a mathematical ab", "image_path": "img_data/video_56_chunk_4.jpg"}
{"video": "video_56", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "been doing up to now we have to define if you a mathematical ab define if you a mathematical ab ruction of each layer i will just ruction of each layer i will just going to draw how we will represent one going to draw how we will represent one layer that we'll be calling the dense layer that we'll be calling the dense layer or fully connected layer but is layer or fully connected layer but is typically also known as dense because typically also known as dense because there's a dense connectivity there's a dense connectivity diagram and a dense matrix which is diagram and a dense matrix which is going to be representing this going to be representing this connectivity let me just draw connectivity let me just draw the layer i'm going to use this kind the layer i'm going to use this kind of a trapezoidal", "image_path": "img_data/video_56_chunk_5.jpg"}
{"video": "video_56", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "the layer i'm going to use this kind of a trapezoidal symbol to indicate this matrix now by vector matrix now by vector that's let's say the l that's let's say the l layer matrix by vector now since i put layer matrix by vector now since i put x there it's better to just put an index x there it's better to just put an index of one there because that's basically of one there because that's basically the first layer followed by this kind of a bias", "image_path": "img_data/video_56_chunk_6.jpg"}
{"video": "video_56", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "layer followed by this kind of a bias and followed by a new nonlinear function that we call the ru now it makes sense this kind of block ru now it makes sense this kind of block diagram we see in front of us forget diagram we see in front of us forget a little bit about this kind of a new a little bit about this kind of a new nonlinear function which we call nonlinear function which we call rectified linear unit but just rectified linear unit but just kind of stay here in this kind of stay here in this kind of portion we actually see that we take portion we actually see that we take this x vector and this x vector let's this x vector and this x vector let's call this to be has dimensionality of", "image_path": "img_data/video_56_chunk_7.jpg"}
{"video": "video_56", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "this x vector and this x vector let's call this to be has dimensionality of call this to be has dimensionality of n x let's write this down x is n x let's write this down x is belongs to r to the^ of nx in other belongs to r to the^ of nx in other words x the number of dimensions of x words x the number of dimensions of x is nx * 1 and typically by a matrix by vector multiplication we are able to do some multiplication we are able to do some form of dimensionality reduction as you form of dimensionality reduction as you can see the canonical shape of this kind can see the canonical shape of this kind of network is to per perform subsequent", "image_path": "img_data/video_56_chunk_8.jpg"}
{"video": "video_56", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "can see the canonical shape of this kind of network is to per perform subsequent of network is to per perform subsequent projections from one layer to another projections from one layer to another down to a potentially a single scale down to a potentially a single scale scalar this scalar unit is the scalar this scalar unit is the interpreted evidently as the probability interpreted evidently as the probability of the positive class given the input x of the positive class given the input x as we have seen in the case of as we have seen in the case of classification or could be the u classification or could be the u the real number in the case of the real number in the case of regression but that's really the regression but that's really the limit we can start with some really", "image_path": "img_data/video_56_chunk_9.jpg"}
{"video": "video_56", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "regression but that's really the limit we can start with some really limit we can start with some really large dimensional vector x and highly large dimensional vector x and highly dimensional vector x and actually dimensional vector x and actually conclude into this into a scaler how conclude into this into a scaler how we will actually do that well it all we will actually do that well it all we need to do is to dimension properly the need to do is to dimension properly the w matrix each layer is doing its own w matrix each layer is doing its own kind of dimensionality reduction kind of dimensionality reduction right here at this point our w matrix right here at this point our w matrix is going to be such that it will need is going to be such that it will need to we start from some desired z that", "image_path": "img_data/video_56_chunk_10.jpg"}
{"video": "video_56", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "is going to be such that it will need to we start from some desired z that to we start from some desired z that we want to form the z is given by w x we want to form the z is given by w x plus b wx + plus b wx + b that is my operation up to this b that is my operation up to this point in time if we are to point in time if we are to form some nz * 1 vector where nz is going to be much vector where nz is going to be much smaller than the n x all we need", "image_path": "img_data/video_56_chunk_11.jpg"}
{"video": "video_56", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "vector where nz is going to be much smaller than the n x all we need smaller than the n x all we need to do is to make sure that we dimension to do is to make sure that we dimension the w let's say nz time nx that's the w let's say nz time nx that's the dimensionals of w because it's a right hand sign w because it's a right hand sign multiplication some authors also treat multiplication some authors also treat it as a left hand side it's one and the it as a left hand side it's one and the same thing plus the bias which is nz * thing plus the bias which is nz * 1 this multiplication now makes sense 1 this multiplication now makes sense that it will produce the nz", "image_path": "img_data/video_56_chunk_12.jpg"}
{"video": "video_56", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "1 this multiplication now makes sense that it will produce the nz that it will produce the nz dimensions at the output and we dimensions at the output and we can just make sure that we have nz less can just make sure that we have nz less than nx again it's up to us to define than nx again it's up to us to define exactly what type of dimensions we need exactly what type of dimensions we need each layer to produce and to reduce each layer to produce and to reduce effectively now in terms of the other effectively now in terms of the other thing that we have actually written over thing that we have actually written over here empirically has been shown that and here empirically has been shown that and this is something that we will this is something that we will understand after we treat back understand after we treat back propagation empirically has been shown propagation empirically has been shown that a new nonlinear", "image_path": "img_data/video_56_chunk_13.jpg"}
{"video": "video_56", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "propagation empirically has been shown that a new nonlinear that a new nonlinear unit that i am going to plot over here this nonlinear unit that we call the reu is called rectified linear unit the reu is called rectified linear unit it is linear unit for all positive value it is linear unit for all positive value that's this angle is that's this angle is 45° 45° and then we have 45° 45° and then we have u the input z and the output this axis u the input z and the output this axis is ru of", "image_path": "img_data/video_56_chunk_14.jpg"}
{"video": "video_56", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "u the input z and the output this axis is ru of z and the all negative logits z are mapped to zero and negative logits z are mapped to zero and that's what makes it nonlinear if you that's what makes it nonlinear if you have minus one it's zero if you have one have minus one it's zero if you have one minus one million it's also zero minus one million it's also zero the reu from now on we will write the reu from now on we will write it as a an expression which it as a an expression which effectively agrees with this diagram r effectively agrees with this diagram r of z is equal to max of 0 comma z", "image_path": "img_data/video_56_chunk_15.jpg"}
{"video": "video_56", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "effectively agrees with this diagram r of z is equal to max of 0 comma z of z is equal to max of 0 comma z that is the equation when z is positive the equation when z is positive the output is z if it is negative the output will be z if it is negative the output will be zero that's how this equation is zero that's how this equation is should be read and basically we are should be read and basically we are done we have now one dense done we have now one dense layer the block diagram of a dense layer the block diagram of a dense layer and what we will do is we'll now see", "image_path": "img_data/video_56_chunk_16.jpg"}
{"video": "video_56", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "layer the block diagram of a dense layer and what we will do is we'll now see and what we will do is we'll now see an example of where we actually going to be put all these kind of multiple layers stacked these kind of multiple layers stacked together that is called the dense layer and let's now move into layer and let's now move into this example and see what this example and see what happens an example application that happens an example application that we're going to see is the -called we're going to see is the -called multiclass classification multiclass classification and for that application we'll be", "image_path": "img_data/video_56_chunk_17.jpg"}
{"video": "video_56", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "multiclass classification and for that application we'll be and for that application we'll be using a notebook that is in our site using a notebook that is in our site called the shold fashionist case called the shold fashionist case study the notebook can be launched study the notebook can be launched in google collab with a single button in google collab with a single button as you scroll this you will basically as you scroll this you will basically press this button to go to press this button to go to collab which is an environment of collab which is an environment of python runtime that is in the python runtime that is in the cloud and is offering to us also some", "image_path": "img_data/video_56_chunk_18.jpg"}
{"video": "video_56", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "python runtime that is in the cloud and is offering to us also some cloud and is offering to us also some accelerated kind of compute for all our accelerated kind of compute for all our applications here you can for that applications here you can for that specific example it's actually a good specific example it's actually a good idea to change the runtime type to idea to change the runtime type to that of gpu and you can select the cheapest kind gpu and you can select the cheapest kind of gpu it's the t t4 in this specific of gpu it's the t t4 in this specific case and if you are to you case and if you are to pay a subscription to google then know pay a subscription to google then you can select some more powerful gp but you can select some more powerful gp but for our application here that", "image_path": "img_data/video_56_chunk_19.jpg"}
{"video": "video_56", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "you can select some more powerful gp but for our application here that for our application here that gpu is sufficient you can press gpu is sufficient you can press the save button over here and what is the save button over here and what is now happening is that you can connect now happening is that you can connect to google cloud and the google cloud to google cloud and the google cloud will actually give you a computation will actually give you a computation resource as you can see here with some resource as you can see here with some memories and cpu and the t4 gpu memories and cpu and the t4 gpu to be able to run this application to be able to run this application i hope everyone is familiar with py i hope everyone is familiar with py notebooks and if not we have other", "image_path": "img_data/video_56_chunk_20.jpg"}
{"video": "video_56", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "i hope everyone is familiar with py notebooks and if not we have other py notebooks and if not we have other videos that actually can of teach you videos that actually can of teach you the basics but this application is basics but this application is actually written in a framework called actually written in a framework called tensorflow caras kasas is a kind of a tensorflow caras kasas is a kind of a higher level api than tensor flow and higher level api than tensor flow and exactly the same application you can actually find it in application you can actually find it in for pytorch if you go to the pytor for pytorch if you go to the pytor website but for now in this specific website but for now in this specific case we're going to go with caras now case we're going to go with caras now the data set that we're going to", "image_path": "img_data/video_56_chunk_21.jpg"}
{"video": "video_56", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "case we're going to go with caras now the data set that we're going to now the data set that we're going to be working with is the -called an be working with is the -called an evolution of the very famous data set evolution of the very famous data set called mnist the mnist data set was called mnist the mnist data set was generated and created for allowing generated and created for allowing to actually perform multiclass to actually perform multiclass classification on and digits from 0 classification on and digits from 0 to 9 therefore we have 10 classes and to 9 therefore we have 10 classes and what has happened is that over the years what has happened is that over the years lots of modeling of lots of modeling activity the", "image_path": "img_data/video_56_chunk_22.jpg"}
{"video": "video_56", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "lots of modeling of lots of modeling activity the lots of modeling activity the performance the classification performance the classification performance and accuracy in that kind of performance and accuracy in that kind of data set saturated to the point that data set saturated to the point that cannot really be used as a data set cannot really be used as a data set anymore at least for this specific anymore at least for this specific simple task we have here which is simple task we have here which is multiclass classification people have multiclass classification people have thought about how we can create a bit thought about how we can create a bit more challenging data set and they came more challenging data set and they came out u with the -cal fashionist which out u with the -cal fashionist which is exactly the same in terms of the", "image_path": "img_data/video_56_chunk_23.jpg"}
{"video": "video_56", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "out u with the -cal fashionist which is exactly the same in terms of the is exactly the same in terms of the imagery that we are feeding into imagery that we are feeding into the x's our x's here are 28 by 28 pixels the x's our x's here are 28 by 28 pixels grayscale images and instead of and grayscale images and instead of and digits we have the fashion digits we have the fashion items and here you scroll down a little items and here you scroll down a little bit you can first of all you can load bit you can first of all you can load the data set with just a single line of the data set with just a single line of code and here are the 10 classes code and here are the 10 classes that we will be concerned with in", "image_path": "img_data/video_56_chunk_24.jpg"}
{"video": "video_56", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "code and here are the 10 classes that we will be concerned with in that we will be concerned with in this specific example all right we have 60,000 example all right we have 60,000 training images after we split our training images after we split our data set to a train and validation data set to a train and validation or test as it's called here and the u or test as it's called here and the u we can actually run this whole we can actually run this whole notebook in one go or run every cell notebook in one go or run every cell individually as we go along if you go individually as we go along if you go to runtime since we chang it to a gpu to runtime since we chang it to a gpu and press the run all buttom then the", "image_path": "img_data/video_56_chunk_25.jpg"}
{"video": "video_56", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "to runtime since we chang it to a gpu and press the run all buttom then the and press the run all buttom then the notebook will be executed and we should notebook will be executed and we should actually start seeing some examples actually start seeing some examples some outputs from the code cells some outputs from the code cells here we see the u 28x 28 here we see the u 28x 28 pixels images that we are going to be pixels images that we are going to be processing and what we will do now is processing and what we will do now is we will scroll a little bit just to we will scroll a little bit just to see some examples and stay at this point see some examples and stay at this point that we are going to be building a model", "image_path": "img_data/video_56_chunk_26.jpg"}
{"video": "video_56", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "see some examples and stay at this point that we are going to be building a model that we are going to be building a model a predictor this predictor is going a predictor this predictor is going to be consisting of fully connected to be consisting of fully connected layers and this is our model over layers and this is our model over here and what we will do is we'll here and what we will do is we'll convert this lines of code into a convert this lines of code into a diagram in our sort of document diagram in our sort of document camera here and i'm going to stop and camera here and i'm going to stop and switch to the other scene where we have switch to the other scene where we have been will'll be writing and generating if you this", "image_path": "img_data/video_56_chunk_27.jpg"}
{"video": "video_56", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "been will'll be writing and generating if you this writing and generating if you this diagram let's see we are this diagram let's see we are going to turn the page a little bit going to turn the page a little bit because we are have a bit more elaborate because we are have a bit more elaborate diagram to draw i'm going to title diagram to draw i'm going to title this u the let me call it a multi-layer let me also bring the multi-layer let me also bring the microphone close by multilayer ler", "image_path": "img_data/video_56_chunk_28.jpg"}
{"video": "video_56", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "multi-layer let me also bring the microphone close by multilayer ler microphone close by multilayer ler classification and this is the -called fully connected sort of architecture the model if we see the code is going to be able to the code is going to be able to process this type of data however process this type of data however we are not ready yet to process images", "image_path": "img_data/video_56_chunk_29.jpg"}
{"video": "video_56", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "process this type of data however we are not ready yet to process images we are not ready yet to process images what we will do is we will flatten what we will do is we will flatten this 28x 28 images remember that we this 28x 28 images remember that we are been dealing with we have been given are been dealing with we have been given this 28 by 28 pixels images sort of representing some kind images sort of representing some kind of fashion item something that and then we are going to flatten that kind", "image_path": "img_data/video_56_chunk_30.jpg"}
{"video": "video_56", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "something that and then we are going to flatten that kind we are going to flatten that kind of image to create a vector the first of image to create a vector the first thing that happens is that we take this thing that happens is that we take this x let me call it x prime for a lack x let me call it x prime for a lack of a better letter and we will pass it of a better letter and we will pass it through the flatten function to create an x and if this x prime is 20 by 28 well if you this x prime is 20 by 28 well if you multiply 28 by 28 you have you're coming", "image_path": "img_data/video_56_chunk_31.jpg"}
{"video": "video_56", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "this x prime is 20 by 28 well if you multiply 28 by 28 you have you're coming multiply 28 by 28 you have you're coming up with a number called 768 it up with a number called 768 it is going to be 768 * 1 that's my vector over here and 768 * 1 that's my vector over here and then from that point onwards we will be then from that point onwards we will be applying a stacking of fully connected applying a stacking of fully connected or dense layers as we call them or dense layers as we call them earlier we are going to be applying this w1 * are going to be applying this w1 * x followed by the -called bias", "image_path": "img_data/video_56_chunk_32.jpg"}
{"video": "video_56", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "x followed by the -called bias one followed by aelu to generate the h1 and then we are going to see h1 and then we are going to see exactly what are the dimensions that i exactly what are the dimensions that i want us to use over here in this want us to use over here in this specific example in this specific code i specific example in this specific code i can see that they want us to use can see that they want us to use to have a dimensionality", "image_path": "img_data/video_56_chunk_33.jpg"}
{"video": "video_56", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "can see that they want us to use to have a dimensionality to have a dimensionality reduction from 768 to 128 size over here my h1 over here is going to be a 128 * 128 * 1 all right what is now going to 1 all right what is now going to happen is that we have now the first happen is that we have now the first layer kind of completed the outut of this kind of", "image_path": "img_data/video_56_chunk_34.jpg"}
{"video": "video_56", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "layer kind of completed the outut of this kind of completed the outut of this kind of layer is going to go into another layer is going to go into another dense layer but this dense layer is dense layer but this dense layer is going to be the head of the network going to be the head of the network we will take h1 and we will pass it we will take h1 and we will pass it through another transformation another through another transformation another reduction well we know exactly the reduction well we know exactly the number of dimensions that we need to hit number of dimensions that we need to hit since is our last layer in the network since is our last layer in the network is evidently going to be 10 because we", "image_path": "img_data/video_56_chunk_35.jpg"}
{"video": "video_56", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "since is our last layer in the network is evidently going to be 10 because we is evidently going to be 10 because we have 10 classes to classify we need to have 10 classes to classify we need to have a 10 dimensional vector at the have a 10 dimensional vector at the output of the network we'll output of the network we'll see exactly how we will convert this see exactly how we will convert this vector into probabilities in a moment vector into probabilities in a moment but that's basically the in terms of but that's basically the in terms of dimensions at least we know what we will dimensions at least we know what we will end up this is going to be end up this is going to be w2 h1 that's a another followed by some", "image_path": "img_data/video_56_chunk_36.jpg"}
{"video": "video_56", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "w2 h1 that's a another followed by some h1 that's a another followed by some kind of bias that's a second layer bias and then we are going to have a block that we'll be calling soft max that is going to produce for us the y heart and let me now explain what a y heart and let me now explain what a soft max is and why we didn't have a", "image_path": "img_data/video_56_chunk_37.jpg"}
{"video": "video_56", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "y heart and let me now explain what a soft max is and why we didn't have a soft max is and why we didn't have a ru first of all if we had a deeper ru first of all if we had a deeper kind of network we would actually repeat kind of network we would actually repeat more than one time this kind of layer more than one time this kind of layer evidently with a different kind of evidently with a different kind of dimensionality we will go from 128 dimensionality we will go from 128 perhaps to let's say 64 and then from 64 perhaps to let's say 64 and then from 64 down to 10 right now the author of down to 10 right now the author of the notebook suggested that one the notebook suggested that one layer is enough and i agree for the layer is enough and i agree for the specific type of complexity of the", "image_path": "img_data/video_56_chunk_38.jpg"}
{"video": "video_56", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "layer is enough and i agree for the specific type of complexity of the specific type of complexity of the target function i think it's target function i think it's we don't have another layer but we don't have another layer but in the head over here we are going to be in the head over here we are going to be facing and solving the following facing and solving the following kind of problem we are going to have kind of problem we are going to have some let me call them this vector some let me call them this vector over here z1 as we always we're calling over here z1 as we always we're calling z the soal vector called that holds all z the soal vector called that holds all the logics and z2 and by", "image_path": "img_data/video_56_chunk_39.jpg"}
{"video": "video_56", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "the logics and z2 and by logics broadly speaking we talking logics broadly speaking we talking about a sort of unnormalized probabilities that is unnormalized probabilities that is coming out of this dot product and coming out of this dot product and this is this vector z2 here evidently this is this vector z2 here evidently has dimensionality of 10 * 1 let has dimensionality of 10 * 1 let me write this down 10 * 1 what we me write this down 10 * 1 what we want to produce at the output is a want to produce at the output is a posterior probability and this posterior", "image_path": "img_data/video_56_chunk_40.jpg"}
{"video": "video_56", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "want to produce at the output is a posterior probability and this posterior probability and this posterior probability we already have met many probability we already have met many times in the binary classification case times in the binary classification case if we had a binary classification if we had a binary classification case well evidently we needed to just case well evidently we needed to just produce a scalar at the output where produce a scalar at the output where the y hat will be the probability of the y hat will be the probability of the positive class given positive class given x however over here we are not going x however over here we are not going to have a scaler we need to have 10 to have a scaler we need to have 10 probabilities we need to produce a probabilities we need to produce a probability distribution that's the probability distribution that's the first important point the", "image_path": "img_data/video_56_chunk_41.jpg"}
{"video": "video_56", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "probability distribution that's the first important point the first important point the body which is this let me call this the body let me call this the head is generating if you the some generating if you the some dimensions in fact i would call dimensions in fact i would call yeah let me just call this yeah let me just call this the head that's fine it takes", "image_path": "img_data/video_56_chunk_42.jpg"}
{"video": "video_56", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "yeah let me just call this the head that's fine it takes the head that's fine it takes 128 dimensional vector at the input 128 dimensional vector at the input the h1 and converts it into a logic the h1 and converts it into a logic vector z2 before fitting into this vector z2 before fitting into this soft max block that soft max block that soft max block the responsibility of this soft max the responsibility of this soft max block is to create this posterior block is to create this posterior probability distribution and here i have drawn the distribution and here i have drawn the probability distribution at the y at the probability distribution at the y at the output in other words the probability of output in other words the probability of y given x comma let's say theta which is y given x comma let's say theta which is theta is the set of parameters of our", "image_path": "img_data/video_56_chunk_43.jpg"}
{"video": "video_56", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "y given x comma let's say theta which is theta is the set of parameters of our theta is the set of parameters of our network and this is going to be network and this is going to be something that some probability something that some probability must function that evidently will must function that evidently will be a proper probability distribution be a proper probability distribution the summation let's say from j is the summation let's say from j is equal to zer to capital k which in this equal to zer to capital k which in this case is n9 of the p with index j case is n9 of the p with index j over here is going to be 1.0 over here is going to be 1.0 and the soft max now block in order to", "image_path": "img_data/video_56_chunk_44.jpg"}
{"video": "video_56", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "over here is going to be 1.0 and the soft max now block in order to and the soft max now block in order to introduce this kind of soft max block we introduce this kind of soft max block we need to kind of under at the end of the need to kind of under at the end of the day start from what we are trying to day start from what we are trying to achieve here we definitely want to achieve here we definitely want to report the largest possible index in report the largest possible index in that one if you the battle among that one if you the battle among all the classes that we have present all the classes that we have present here this is let's say the index k here this is let's say the index k if we are to report that this would if we are to report that this would actually perform an argmax operator actually perform an argmax operator it's", "image_path": "img_data/video_56_chunk_45.jpg"}
{"video": "video_56", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "actually perform an argmax operator it's argmax over all indices js of the u the z that it is at the input and this is this z has this sort of a j u capital k elements and sort of a j u capital k elements and this is going to be the class this is going to be the class that we are going to report if we had", "image_path": "img_data/video_56_chunk_46.jpg"}
{"video": "video_56", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "this is going to be the class that we are going to report if we had that we are going to report if we had only the requirement to do a hard only the requirement to do a hard decision we are to do a requirement decision we are to do a requirement to do a hard decision let's say the k to do a hard decision let's say the k class is the class which is actually class is the class which is actually present over there that's what have present over there that's what have reported based on this elements of reported based on this elements of the z vector we have selected if you the z vector we have selected if you the maximum vector but typically the maximum vector but typically we would do not want to do that we would do not want to do that we would not definitely report a class let's say not definitely report a class let's say that this is this specific fashion item that this is this specific fashion item but at the same time report some form of", "image_path": "img_data/video_56_chunk_47.jpg"}
{"video": "video_56", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "that this is this specific fashion item but at the same time report some form of but at the same time report some form of a confidence how confident we are about a confidence how confident we are about our prediction and that is something our prediction and that is something that we also have done in binary that we also have done in binary classification the way to think about classification the way to think about the soft max is some kind of a softer the soft max is some kind of a softer version of argmax and we are going to just write argmax and we are going to just write now the equation that this block is implementing this block is a vector input vector this block is a vector input vector output block it will be a soft max of", "image_path": "img_data/video_56_chunk_48.jpg"}
{"video": "video_56", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "this block is a vector input vector output block it will be a soft max of output block it will be a soft max of z let's say j it is the ratio of e to the power of j it is the ratio of e to the power of cj and divided by the cj and divided by the summation over all js in fact instead summation over all js in fact instead of using the j over here let me use the of using the j over here let me use the small letter k as the", "image_path": "img_data/video_56_chunk_49.jpg"}
{"video": "video_56", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "small letter k as the index small letter k and this is e to the power of cj what we do in english is to cj what we do in english is to just how to translate this equation in a just how to translate this equation in a kind of intuitive way we are going to kind of intuitive way we are going to convert all the logic into positive convert all the logic into positive numbers that's basically what the numbers that's basically what the numerator is doing and we are going to numerator is doing and we are going to normalize them that at the end of the", "image_path": "img_data/video_56_chunk_50.jpg"}
{"video": "video_56", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "numerator is doing and we are going to normalize them that at the end of the normalize them that at the end of the day the summation of what comes out of the summation of what comes out of the soft max which is basically this soft max which is basically this probability is equal to 1.0 that's probability is equal to 1.0 that's basically what the soft max is doing and basically what the soft max is doing and visually we could interpret it as visually we could interpret it as follows if we do not care about any follows if we do not care about any confidence we are effectively reporting confidence we are effectively reporting the maximum possible class the maximum possible class the maximum class index the maximum class index the index across from the maximum element index across from the maximum element here we are putting a some form of we", "image_path": "img_data/video_56_chunk_51.jpg"}
{"video": "video_56", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "index across from the maximum element here we are putting a some form of we here we are putting a some form of we are ignoring all of the other indices are ignoring all of the other indices all of the other classes and we put is it is as if classes and we put is it is as if we're putting this kind of a window we're putting this kind of a window around this class and there 100% around this class and there 100% i'm 100% confident that it is this class i'm 100% confident that it is this class that is present in my input over here that is present in my input over here in the soft max we are going to in the soft max we are going to definitely consider the heights of the definitely consider the heights of the other", "image_path": "img_data/video_56_chunk_52.jpg"}
{"video": "video_56", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "definitely consider the heights of the other logits the normalized logits and logits the normalized logits and that kind of makes sense if my over that kind of makes sense if my over here i have a 0.3 and all the others are let's 0.3 and all the others are let's say 0.2 or even 0.1 and there's one other class that is actually other class that is actually 0.29 then it actually mak sense to not 0.29 then it actually mak sense to not be 100% confident that this is the", "image_path": "img_data/video_56_chunk_53.jpg"}
{"video": "video_56", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "0.29 then it actually mak sense to not be 100% confident that this is the be 100% confident that this is the class that is actually present because class that is actually present because hey there is another index which is hey there is another index which is almost as good as what you're almost as good as what you're going to be reporting here that's going to be reporting here that's what the soft max is actually doing is what the soft max is actually doing is actually placing some form of a software actually placing some form of a software window around the u reported window around the u reported classes the reported probabilities and classes the reported probabilities and that is what at the end of the day that is what at the end of the day we are that's the reason why we are that's the reason why we are going to definitely report going to definitely report much smaller confidence than 100%", "image_path": "img_data/video_56_chunk_54.jpg"}
{"video": "video_56", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "going to definitely report much smaller confidence than 100% much smaller confidence than 100% with the soft max let's trite it down softmax is producing a softer version of arcmax we are going to of course still arcmax we are going to of course still report that class k won the battle but report that class k won the battle but we are going to also report a number we are going to also report a number next to it which is definitely much less", "image_path": "img_data/video_56_chunk_55.jpg"}
{"video": "video_56", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "we are going to also report a number next to it which is definitely much less next to it which is definitely much less than 100% when we have an output than 100% when we have an output which is all the classes are which is all the classes are very similar but only one is are very similar but only one is slightly above the others then slightly above the others then we are going to report a much lower we are going to report a much lower confidence about our prediction for confidence about our prediction for that specific class than compared to the that specific class than compared to the case where all the other classes are case where all the other classes are very suppressed and this class has very suppressed and this class has really sort of w the battle", "image_path": "img_data/video_56_chunk_56.jpg"}
{"video": "video_56", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "very suppressed and this class has really sort of w the battle really sort of w the battle inside our predictor back to our code over here we predictor back to our code over here we have translated this section of the code have translated this section of the code to a block diagram we have explained all to a block diagram we have explained all the elements of this and the elements of this and subsequently what we are actually doing subsequently what we are actually doing is we are compiling this model using an is we are compiling this model using an optimizer over here the author optimizer over here the author selected adam is an adaptive learning selected adam is an adaptive learning rate kind of scheme very similar to", "image_path": "img_data/video_56_chunk_57.jpg"}
{"video": "video_56", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "selected adam is an adaptive learning rate kind of scheme very similar to rate kind of scheme very similar to stochastic grade descent if you stochastic grade descent if you replace adam with stochastic gr descent replace adam with stochastic gr descent not really many things will not really many things will change and the u we're actually using change and the u we're actually using the non binary version of the cross the non binary version of the cross entropy loss function we have seen entropy loss function we have seen which is a bit different than the which is a bit different than the binary cross entropy but is binary cross entropy but is almost identically in the same because almost identically in the same because we are instead of having two class over we are instead of having two class over here we have and having two terms in the binary", "image_path": "img_data/video_56_chunk_58.jpg"}
{"video": "video_56", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "here we have and having two terms in the binary and having two terms in the binary crossentropy loss we have 10 classes and crossentropy loss we have 10 classes and therefore we have a summation of 10 and therefore we have a summation of 10 terms inside this categorical cross terms inside this categorical cross entropy loss function and then of entropy loss function and then of course we are using a metric which is course we are using a metric which is the accuracy metric we are going to the accuracy metric we are going to train the model with the do fit api train the model with the do fit api which is all the way from pyit learn which is all the way from pyit learn inherited and this is u being used here inherited and this is u being used here with a certain number of epochs with a certain number of epochs and having the evaluation done", "image_path": "img_data/video_56_chunk_59.jpg"}
{"video": "video_56", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "with a certain number of epochs and having the evaluation done and having the evaluation done here and printed out that's a 87% here and printed out that's a 87% accuracy what we have achieved here and accuracy what we have achieved here and we are then able to do predictions we are then able to do predictions we're going to see some outputs which we're going to see some outputs which evidently there will be some y hat evidently there will be some y hat output that we are producing we have output that we are producing we have let's say using test images we are let's say using test images we are able to use to do predictions and let's able to use to do predictions and let's do the predictions for the first image do the predictions for the first image which is index z of my test data", "image_path": "img_data/video_56_chunk_60.jpg"}
{"video": "video_56", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "do the predictions for the first image which is index z of my test data which is index z of my test data set this is evidently going to produce set this is evidently going to produce an array of 10 numbers and these numbers an array of 10 numbers and these numbers are the probabilities which i just drew are the probabilities which i just drew the in this kind of a the in this kind of a probability mass function a few moments probability mass function a few moments ago and that's basically it we are ago and that's basically it we are going to definitely use the argmax going to definitely use the argmax operator to report the class in this operator to report the class in this case it is the winning class is index 9 case it is the winning class is index 9 and as you can see here we have a and as you can see here we have a very good situation we are", "image_path": "img_data/video_56_chunk_61.jpg"}
{"video": "video_56", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "and as you can see here we have a very good situation we are very good situation we are 98.89% 98.89% confident that this is the u correct confident that this is the u correct class and this is the result of the class and this is the result of the soft max block as we discussed this soft max block as we discussed this is the i'm guessing the some plots is the i'm guessing the some plots of this posterior probability there are of this posterior probability there are some small probabilities around class 5 some small probabilities around class 5 and class 7 and definitely we have and class 7 and definitely we have the specific class which is angle boot", "image_path": "img_data/video_56_chunk_62.jpg"}
{"video": "video_56", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "and class 7 and definitely we have the specific class which is angle boot the specific class which is angle boot over here and we are also seeing over here and we are also seeing mistake that the predictor actually has mistake that the predictor actually has made for another example i made for another example i believe the example here is believe the example here is probably some other probably some other test image it doesn't really matter test image it doesn't really matter exactly which one where we have exactly which one where we have actually reported that this is a actually reported that this is a sneak sneer and in fact although", "image_path": "img_data/video_56_chunk_63.jpg"}
{"video": "video_56", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "actually reported that this is a sneak sneer and in fact although sneak sneer and in fact although it is a sneaker we reporting that this it is a sneaker we reporting that this is going this is a sandal that's is going this is a sandal that's the category five all right we are the category five all right we are just doing exactly the same for many just doing exactly the same for many images and we are concluding this images and we are concluding this notebook that's basically it we notebook that's basically it we have just went through our first have just went through our first application that is actually using fully application that is actually using fully connected layer years and we are going", "image_path": "img_data/video_56_chunk_64.jpg"}
{"video": "video_56", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "application that is actually using fully connected layer years and we are going connected layer years and we are going now to go u into discussing a little bit now to go u into discussing a little bit on how the internal internals of this on how the internal internals of this kind of training procedure are kind of training procedure are working more specifically we are going working more specifically we are going to be concerned a little bit with the to be concerned a little bit with the calculation of the gradient which is calculation of the gradient which is present in stochastic gr descent and we present in stochastic gr descent and we will look at a operation which is called will look at a operation which is called back propagation operation that it will back propagation operation that it will calculate this gradient very calculate this gradient very efficiently for us", "image_path": "img_data/video_56_chunk_65.jpg"}
{"video": "video_56", "start": "0:33:00", "end": "0:33:01", "timestamp": "0:33:00 - 0:33:01", "text": "calculate this gradient very efficiently for us", "image_path": "img_data/video_56_chunk_66.jpg"}
{"video": "video_57", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "ear video we have seen statistical learning at least in the supervised learning at least in the supervised learning approach and we have seen learning approach and we have seen linear models for aggression and linear models for aggression and generalized linear models for generalized linear models for classification in this video we are classification in this video we are going to expand now to a type of going to expand now to a type of predictors which are differentiable end predictors which are differentiable end to end we are not going to be to end we are not going to be requiring just the loss function to be requiring just the loss function to be differentiable with respect to the differentiable with respect to the parameters but we also going to be parameters but we also going to be requiring all of the components of this", "image_path": "img_data/video_57_chunk_0.jpg"}
{"video": "video_57", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "parameters but we also going to be requiring all of the components of this requiring all of the components of this prediction architecture to be prediction architecture to be differentiable with respect to differentiable with respect to parameters these predictors are part of parameters these predictors are part of what we call the connectionism what we call the connectionism philosophy where we are actually philosophy where we are actually putting together individual units that putting together individual units that we'll call sigmoidal neurons that when we'll call sigmoidal neurons that when they are connected together in a proper they are connected together in a proper way that we see exactly what proper way that we see exactly what proper means they are exhibiting some i will means they are exhibiting some i will call it universal approximation call it universal approximation properties let's get started with properties let's get started with producing just a single sig sigmoidal", "image_path": "img_data/video_57_chunk_1.jpg"}
{"video": "video_57", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "properties let's get started with producing just a single sig sigmoidal producing just a single sig sigmoidal kind of neuron and let's for that kind of neuron and let's for that let's go to the site the actual neuron that we see site the actual neuron that we see here on the side on the top figure over here on the side on the top figure over here is a fairly complicated kind of here is a fairly complicated kind of organism it has inputs that we call dent organism it has inputs that we call dent rites and outputs and in between inputs rites and outputs and in between inputs and output is actually this compressive and output is actually this compressive stage that is called myelinated action stage that is called myelinated action and signals are actually going from and signals are actually going from those dentes through this action into", "image_path": "img_data/video_57_chunk_2.jpg"}
{"video": "video_57", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "and signals are actually going from those dentes through this action into those dentes through this action into outputs and this neuron this unit is outputs and this neuron this unit is going to be connected to other units going to be connected to other units that we'll see in a moment now rosenblat that we'll see in a moment now rosenblat working in a government sponsor working in a government sponsor laboratory 60 years ago he actually laboratory 60 years ago he actually came up with an architecture that came up with an architecture that kind of resembles this organism kind of resembles this organism and we actually we have seen this type and we actually we have seen this type of architecture before let me of architecture before let me just write the kind of a d diagram on", "image_path": "img_data/video_57_chunk_3.jpg"}
{"video": "video_57", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "of architecture before let me just write the kind of a d diagram on just write the kind of a d diagram on our sort of board right now this first kind of stage of this neuron is a dot product is sigmoidal neuron is a dot product is sigmoidal neuron that is there are various kind of that is there are various kind of neurons but here we're just reading neurons but here we're just reading the sigmoidal version we have the dot the sigmoidal version we have the dot product of w transpose x these are both product of w transpose x these are both vectors and explicitly we are sort of", "image_path": "img_data/video_57_chunk_4.jpg"}
{"video": "video_57", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "vectors and explicitly we are sort of vectors and explicitly we are sort of denoting here this kind of a bias term denoting here this kind of a bias term that is kind of explicitly shown that is kind of explicitly shown as b and the last stage is a as b and the last stage is a nonlinearity which is the sigmoidal nonlinearity which is the sigmoidal nonlinearity we have also seen in the nonlinearity we have also seen in the treatment of logistic treatment of logistic regression typically this signal regression typically this signal over here is it's called activation", "image_path": "img_data/video_57_chunk_5.jpg"}
{"video": "video_57", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "regression typically this signal over here is it's called activation and this is also known as activation function and evidently just in other we have evidently just in other we have done up to now we have a y hat at the done up to now we have a y hat at the output that is the predicted target output that is the predicted target variable and also we know now that corresponds to the also we know now that corresponds to the probability of y is equal to 1 given", "image_path": "img_data/video_57_chunk_6.jpg"}
{"video": "video_57", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "also we know now that corresponds to the probability of y is equal to 1 given probability of y is equal to 1 given x that is basically the block x that is basically the block diagram of the sigmoidal kind of neuron diagram of the sigmoidal kind of neuron and i hope it's everyone is kind of and i hope it's everyone is kind of familiar with the dot products and all familiar with the dot products and all these kind of discussion we had even these kind of discussion we had even from kind of earlier kind of treatment from kind of earlier kind of treatment i'm not going to expand here what i'm not going to expand here what i think we is worthwhile actually doing i think we is worthwhile actually doing however is to try to understand how however is to try to understand how potentially a neuron with everything potentially a neuron with everything we have seen can be trained and for we have seen can be trained and for that i'm going to go into web page", "image_path": "img_data/video_57_chunk_7.jpg"}
{"video": "video_57", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "we have seen can be trained and for that i'm going to go into web page that i'm going to go into web page actually called tensorflow playground actually called tensorflow playground and in that tensorflow playground i'm and in that tensorflow playground i'm going to we're going to see now how we going to we're going to see now how we can actually take this kind of neuron can actually take this kind of neuron this block diagram and hit it with some this block diagram and hit it with some kind of a data set and see what it is kind of a data set and see what it is able to do and how it will probably able to do and how it will probably not able to do certain things for us and not able to do certain things for us and have some discussion about how we can have some discussion about how we can actually create a network of these actually create a network of these neurons when we actually go to the neurons when we actually go to the site tensor flow play ground site will", "image_path": "img_data/video_57_chunk_8.jpg"}
{"video": "video_57", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "neurons when we actually go to the site tensor flow play ground site will site tensor flow play ground site will actually the first time you go into actually the first time you go into there you'll actually see a picture such there you'll actually see a picture such as this and we would to modify this as this and we would to modify this picture to just basically bring a single picture to just basically bring a single neuron into that we will have here neuron into that we will have here just press the minus button to just press the minus button to eliminate the what is called a eliminate the what is called a layer which terat we'll see shortly and layer which terat we'll see shortly and just leave a single neuron on that just leave a single neuron on that sort of architecture have a single sort of architecture have a single neural architecture we are going to neural architecture we are going to have a sigmoidal activation", "image_path": "img_data/video_57_chunk_9.jpg"}
{"video": "video_57", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "neural architecture we are going to have a sigmoidal activation have a sigmoidal activation function and we are going to be function and we are going to be discussing the operation of this kind discussing the operation of this kind of neuron in this small data set that of neuron in this small data set that we see here it's a classification data we see here it's a classification data set with just two classes and just set with just two classes and just because we would to make things a because we would to make things a bit more challenging if you can increase bit more challenging if you can increase the noise to something 25 you the noise to something 25 you actually get a nonlinear non linear actually get a nonlinear non linear separated data set and that is separated data set and that is", "image_path": "img_data/video_57_chunk_10.jpg"}
{"video": "video_57", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "separated data set and that is basically it we're going to have a basically it we're going to have a potentially a bat size of potentially a bat size of 10 let's leave it 10 or even we can 10 let's leave it 10 or even we can make it one and we are going make it one and we are going to avoid introducing complication of to avoid introducing complication of regularization here learning rate as we regularization here learning rate as we have seen earlier should be some small have seen earlier should be some small number let's say 0.01 and we just by pressing the play 0.01 and we just by pressing the play button what you actually see you'll button what you actually see you'll actually see the output over here", "image_path": "img_data/video_57_chunk_11.jpg"}
{"video": "video_57", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "button what you actually see you'll actually see the output over here actually see the output over here this is the loss function the two this is the loss function the two loss functions the training loss and loss functions the training loss and the test loss that are very the test loss that are very quickly converging and this is u the quickly converging and this is u the decision boundary that the single decision boundary that the single neuron kind of produced very neuron kind of produced very straightforward very easy to come up straightforward very easy to come up with u if you that decision with u if you that decision boundary as you can see and of course boundary as you can see and of course everything that we have discussed before everything that we have discussed before the stochastic grade descent is in", "image_path": "img_data/video_57_chunk_12.jpg"}
{"video": "video_57", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "everything that we have discussed before the stochastic grade descent is in the stochastic grade descent is in operation here if we go back to operation here if we go back to that drawing board you will that drawing board you will definitely expect to see here a loss definitely expect to see here a loss function which is nothing else as the function which is nothing else as the binary cross entropy and that banner binary cross entropy and that banner crossentropy being fed into the stochastic grated fed into the stochastic grated descent that it will adjust the descent that it will adjust the weights w from one iteration to the next", "image_path": "img_data/video_57_chunk_13.jpg"}
{"video": "video_57", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "descent that it will adjust the weights w from one iteration to the next weights w from one iteration to the next and that is all straightforward we and that is all straightforward we have done that before as well and have done that before as well and now let's go back to that now let's go back to that site and let's see what is going to site and let's see what is going to happen if we change a little bit the happen if we change a little bit the data set we are facing and we have data set we are facing and we have another two dimensional data set i another two dimensional data set i forgot earlier to mention that here we forgot earlier to mention that here we have effectively two features two have effectively two features two dimensions in my x we have x1 and x2", "image_path": "img_data/video_57_chunk_14.jpg"}
{"video": "video_57", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "have effectively two features two dimensions in my x we have x1 and x2 dimensions in my x we have x1 and x2 again in this new data set we have which again in this new data set we have which is slightly more complicated let's first of all remove complicated let's first of all remove the noise just to see the noise just to see the data set a bit more clearly we have data set a bit more clearly we have the blue class kind of centered in the blue class kind of centered in the middle and it is surrounded by the middle and it is surrounded by the orange glass and as we expect to see the glass and as we expect to see the decision boundary is the decision boundary is definitely heavily nonlinear it's definitely heavily nonlinear it's actually in fact circular and let's try actually in fact circular and let's try to do exactly the same thing press", "image_path": "img_data/video_57_chunk_15.jpg"}
{"video": "video_57", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "actually in fact circular and let's try to do exactly the same thing press to do exactly the same thing press the play button and see exactly what is the play button and see exactly what is happening we observe now a kind of happening we observe now a kind of different behavior we are going to start different behavior we are going to start seeing some form of loss reduction seeing some form of loss reduction the neuron is actually trying its best the neuron is actually trying its best to minimize training and test to minimize training and test losses and here we kind of see some losses and here we kind of see some kind of a decision boundary but i hope we all", "image_path": "img_data/video_57_chunk_16.jpg"}
{"video": "video_57", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "kind of a decision boundary but i hope we all boundary but i hope we all recognize that's not really the one recognize that's not really the one that we are probably going to that we are probably going to result into a very good prediction result into a very good prediction performance and very good performance and very good generalization what is to blame generalization what is to blame here and we have seen this kind of here and we have seen this kind of behavior before even in the linear behavior before even in the linear models we have studied and this is models we have studied and this is effectively what we see here is some effectively what we see here is some form of an underfeeding effect in other form of an underfeeding effect in other words the single neuron in terms of words the single neuron in terms of the number of parameters we have just", "image_path": "img_data/video_57_chunk_17.jpg"}
{"video": "video_57", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "words the single neuron in terms of the number of parameters we have just the number of parameters we have just have u three parameters here two have u three parameters here two associated with the number of dimensions associated with the number of dimensions of x plus the bias term is really not of x plus the bias term is really not able to capture the complexity which is able to capture the complexity which is required by the target function and that required by the target function and that underlying target function that governs underlying target function that governs this kind of classification problem this kind of classification problem requires probably a larger number of requires probably a larger number of parameters that's a kind of a good parameters that's a kind of a good pretext to start introducing neural pretext to start introducing neural networks now we are going to start networks now we are going to start interconnecting many neurons together in", "image_path": "img_data/video_57_chunk_18.jpg"}
{"video": "video_57", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "networks now we are going to start interconnecting many neurons together in interconnecting many neurons together in some architecture it will make sense some architecture it will make sense to all of us all right let's see we will now us all right let's see we will now use the single neuron that we have use the single neuron that we have seen as a unit i'm drawing if you seen as a unit i'm drawing if you a single box over here i'm putting a single box over here i'm putting the number one and inside that single the number one and inside that single boxes everything that you see here boxes everything that you see here above all of this blocks are inside this above all of this blocks are inside this kind of single block we are going to", "image_path": "img_data/video_57_chunk_19.jpg"}
{"video": "video_57", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "above all of this blocks are inside this kind of single block we are going to kind of single block we are going to feed to it the x that we've seen in this tensor it the x that we've seen in this tensor playround example and we are going to feed x also to get another neuron let me just draw it a bit kind of differently to have a bit more space", "image_path": "img_data/video_57_chunk_20.jpg"}
{"video": "video_57", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "kind of differently to have a bit more space and this is the second neuron the first neuron is has parameters that are inside it are is has parameters that are inside it are w1 and the associated kind of bias now w1 and the associated kind of bias now to make the little bit the math a bit to make the little bit the math a bit easier i'm not going to be dealing with easier i'm not going to be dealing with a bias term explicitly at least in this a bias term explicitly at least in this discussion i'm just going to show you discussion i'm just going to show you only", "image_path": "img_data/video_57_chunk_21.jpg"}
{"video": "video_57", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "discussion i'm just going to show you only the weight w the parameter w the weight w the parameter w vector which is two the vector which is two the dimensionality of that is two just dimensionality of that is two just the dimensionality of x is two the dimensionality of x is two the first neuron will produce an output of i first neuron will produce an output of i will call it y1 and the second neuron will produce an output which will call y2 hat these an output which will call y2 hat these are the two predictions however now these outputs predictions however now these outputs will be", "image_path": "img_data/video_57_chunk_22.jpg"}
{"video": "video_57", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "predictions however now these outputs will be combined with the help of a third combined with the help of a third neuron number three to produce the final output which three to produce the final output which we'll call y3 hatut and this neuron has its call y3 hatut and this neuron has its own kind of parameters w3 that's basically the w3 that's basically the architecture we are going to architecture we are going to study a little bit and that i study a little bit and that i think architecture this actually make", "image_path": "img_data/video_57_chunk_23.jpg"}
{"video": "video_57", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "study a little bit and that i think architecture this actually make think architecture this actually make sense we are going to definitely have a sense we are going to definitely have a scalar at the end this is a scalar this scalar at the end this is a scalar this is a scaler two scalers in the form of a is a scaler two scalers in the form of a vector at the input over vector at the input over here we see a vector this vector will here we see a vector this vector will call it let's say let's call this vector h is consist of y1 hat and y2 hat this vector will we dot", "image_path": "img_data/video_57_chunk_24.jpg"}
{"video": "video_57", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "h this vector h is consist of y1 hat and y2 hat this vector will we dot hat and y2 hat this vector will we dot product we form the dot product of h product we form the dot product of h with the w3 over here and follow with the w3 over here and follow with the sigmoidal unit the output of this the sigmoidal unit the output of this architecture is going to be this architecture is going to be this three neural architecture is going to be three neural architecture is going to be a scalar which of course is exactly what a scalar which of course is exactly what we need for binary classification and we need for binary classification and scalar between 0 and one all right scalar between 0 and one all right in terms of terminology will be in terms of terminology will be calling this stacking of neurons calling this stacking of neurons let me draw it this we'll be", "image_path": "img_data/video_57_chunk_25.jpg"}
{"video": "video_57", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "calling this stacking of neurons let me draw it this we'll be let me draw it this we'll be calling this a layer we effectively here we have two layers and more specifically this layers and more specifically this layer which is just before the layer which is just before the final stage we are going to also be final stage we are going to also be calling it body sometimes because it calling it body sometimes because it this body will consist of more than", "image_path": "img_data/video_57_chunk_26.jpg"}
{"video": "video_57", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "calling it body sometimes because it this body will consist of more than this body will consist of more than one layers typically here we just one layers typically here we just have only one and this part over here have only one and this part over here will be calling it a will be calling it a head and more specifically this head is head and more specifically this head is a sigmoidal neuron which is doing a sigmoidal neuron which is doing classification or we call it classification or we call it classification here at this moment classification here at this moment if we are to write down the if we are to write down the equation of the y3 this is sigma of w3", "image_path": "img_data/video_57_chunk_27.jpg"}
{"video": "video_57", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "sigma of w3 transpose h that's the equation of the h that's the equation of the neuron number three that is exactly neuron number three that is exactly the functional form of what the neuron 3 the functional form of what the neuron 3 is actually doing here we have sigma is actually doing here we have sigma and open parenthesis we and open parenthesis we have w13 w23 the second component of w3 transpose", "image_path": "img_data/video_57_chunk_28.jpg"}
{"video": "video_57", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "w13 w23 the second component of w3 transpose times the h1 and h2 the two components of the h vector which is components of the h vector which is nothing else and then let me write it nothing else and then let me write it the way i wrote it over here y1 hat and the way i wrote it over here y1 hat and y2 and this is now sigma of w13 y1 hat plus", "image_path": "img_data/video_57_chunk_29.jpg"}
{"video": "video_57", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "sigma of w13 y1 hat plus w23 y2 and this is nothing else now as y2 and this is nothing else now as sigma w13 well the y1 hat itself is another w13 well the y1 hat itself is another sigmoid function acting on the dot sigmoid function acting on the dot product of the first neuron that's basically my x and", "image_path": "img_data/video_57_chunk_30.jpg"}
{"video": "video_57", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "x and this parenthesis should not be there plus w23 sigmoid of w2 transpose x that is actually the expression of the output of this kind of network of the output of this kind of network of this three neuron network and there are this three neuron network and there are some kind of interesting observ ation some kind of interesting observ ation just from this kind of simple", "image_path": "img_data/video_57_chunk_31.jpg"}
{"video": "video_57", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "some kind of interesting observ ation just from this kind of simple just from this kind of simple architecture that i would to point architecture that i would to point out the first is that we definitely out the first is that we definitely see some form of nesting going on see some form of nesting going on we have some kind of a nested function we have some kind of a nested function we have sigmoid of sigmoids have sigmoids of some linear sigmoids have sigmoids of some linear combination of other combination of other sigmoids which are evidently sigmoids which are evidently nonlinear functions and if you recall nonlinear functions and if you recall from all our earlier kind of work on", "image_path": "img_data/video_57_chunk_32.jpg"}
{"video": "video_57", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "nonlinear functions and if you recall from all our earlier kind of work on from all our earlier kind of work on generalized linear models we always had generalized linear models we always had this kind of form of w transpose f of x this kind of form of w transpose f of x now these five of x's at that time came now these five of x's at that time came from the sort of wellestablished basis from the sort of wellestablished basis function such as polinomial and on function such as polinomial and on over here what we see is that these over here what we see is that these effectively are the features taking the place of these five", "image_path": "img_data/video_57_chunk_33.jpg"}
{"video": "video_57", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "features taking the place of these five functions we have seen earlier we functions we have seen earlier we have two features over here combined linearly feature u one and feature two and perhaps more two and perhaps more importantly this head this is accepting and combining features and accepting and combining features and these features have been generated from these features have been generated from within the network itself within the network itself the network is effectively", "image_path": "img_data/video_57_chunk_34.jpg"}
{"video": "video_57", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "within the network itself the network is effectively the network is effectively constructing the right features such constructing the right features such that the classification head at the very that the classification head at the very end is able to actually do this job end is able to actually do this job properly that's actually a kind of a properly that's actually a kind of a fairly important observation of fairly important observation of what we se seen earlier and comparison what we se seen earlier and comparison what we have seen earlier and what we have seen earlier and what we actually have right now the second actually have right now the second point the first one is nesting the second one", "image_path": "img_data/video_57_chunk_35.jpg"}
{"video": "video_57", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "the second one is sort of i will call it is sort of i will call it automatic feature generation and the third one which i wanted to mention is that now we wanted to mention is that now we have going to have the exactly the have going to have the exactly the same environment which we have seen same environment which we have seen earlier in our where", "image_path": "img_data/video_57_chunk_36.jpg"}
{"video": "video_57", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "same environment which we have seen earlier in our where earlier in our where parameters parameter optimization parameters parameter optimization we have evidently a binary cross entropy we have evidently a binary cross entropy and a stochastic grad descent but now and a stochastic grad descent but now everything is jointly optimized i am everything is jointly optimized i am going to be writing here the theta going to be writing here the theta vector which is consist of three other vector which is consist of three other vectors w1 w2 and w3 of these three neurons and the w3 of these three neurons and the optimizer is going to be jointly optimizer is going to be jointly optimizing the parameters just we", "image_path": "img_data/video_57_chunk_37.jpg"}
{"video": "video_57", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "optimizer is going to be jointly optimizing the parameters just we optimizing the parameters just we have seen earlier from just a single have seen earlier from just a single neuron and therefore it will be neuron and therefore it will be implementing the well known to us implementing the well known to us equation of stochastic radi that we equation of stochastic radi that we have seen earlier i'm mentioning that because earlier i'm mentioning that because very quickly the number of parameters very quickly the number of parameters that we have here will that we have here will explode here we have how many we explode here we have how many we have just basically six parameters here have just basically six parameters here plus the three bias terms which i didn't plus the three bias terms which i didn't write explicitly let's say nine this", "image_path": "img_data/video_57_chunk_38.jpg"}
{"video": "video_57", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "plus the three bias terms which i didn't write explicitly let's say nine this write explicitly let's say nine this stochastic radius n will actually be stochastic radius n will actually be searching in this nine dimensional kind searching in this nine dimensional kind of space to find this optimal theta of space to find this optimal theta star gradually as we have discussed star gradually as we have discussed before the important before the important thing is the i think number two thing is the i think number two the automatic feature generation or the automatic feature generation or the fact that the network itself", "image_path": "img_data/video_57_chunk_39.jpg"}
{"video": "video_57", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "the automatic feature generation or the fact that the network itself builds the right representations for the head or whatever the head is doing the head in this case is doing the head in this case is doing classification but we'll see other classification but we'll see other examples where the head is actually examples where the head is actually doing regression", "image_path": "img_data/video_57_chunk_40.jpg"}
{"video": "video_57", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "examples where the head is actually doing regression for the right task the task is the responsibility of the head to implement responsibility of the head to implement it and definitely we see here a it and definitely we see here a good small example but it actually good small example but it actually taught us a couple of things we have not taught us a couple of things we have not seen before to close the discussion seen before to close the discussion we'll go back to that site tensorflow we'll go back to that site tensorflow playground and recreate the architecture playground and recreate the architecture we had put together we were going to we had put together we were going to have two layers and in the first layer is going cons", "image_path": "img_data/video_57_chunk_41.jpg"}
{"video": "video_57", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "have two layers and in the first layer is going cons and in the first layer is going cons consist of two neurons both neurons consist of two neurons both neurons accept exactly the same x as it's accept exactly the same x as it's actually shown over here and the output actually shown over here and the output of this each neuron is being combined of this each neuron is being combined by the head that's the last neuron at by the head that's the last neuron at the top of the layer if you the top of the layer if you and now we are going and now we are going to sort of play again with this to sort of play again with this experiment in this kind of more experiment in this kind of more complicated ated experiment data", "image_path": "img_data/video_57_chunk_42.jpg"}
{"video": "video_57", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "experiment in this kind of more complicated ated experiment data complicated ated experiment data set that we have seen earlier that a set that we have seen earlier that a single neuron was not really able to do single neuron was not really able to do much and we are going to just press the much and we are going to just press the play button and see what play button and see what happens again the stochastic grade happens again the stochastic grade descent is now looking at this kind of a descent is now looking at this kind of a nine-dimensional space and it's continues to look we space and it's continues to look we are evidently going through many are evidently going through many epochs our training loss and test epochs our training loss and test loss seem to have been stuck at", "image_path": "img_data/video_57_chunk_43.jpg"}
{"video": "video_57", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "epochs our training loss and test loss seem to have been stuck at loss seem to have been stuck at around 0.5 not really a very good sort 0.5 not really a very good sort of behavior yet but we should also be patient yet but we should also be patient because we never i mean this because we never i mean this thing is actually running in the thing is actually running in the browser it is using yeah the browser it is using yeah the kind of v8 engine which is behind the kind of v8 engine which is behind the these browser to do the calculations in these browser to do the calculations in kind of a javascript it's not really", "image_path": "img_data/video_57_chunk_44.jpg"}
{"video": "video_57", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "these browser to do the calculations in kind of a javascript it's not really kind of a javascript it's not really the fastest implementation and since we are implementation and since we are looking in this n-dimensional space now looking in this n-dimensional space now i think we should give it some time yes i think we should give it some time yes surely enough some local minima seems to surely enough some local minima seems to have been found at this moment the loss kind of found at this moment the loss kind of dropped we start seeing some elements of dropped we start seeing some elements of a loss function being significantly a loss function being significantly now dropped and look what happens now dropped and look what happens now with a decision boundary the decision boundary the decision boundary without really us doing that much and", "image_path": "img_data/video_57_chunk_45.jpg"}
{"video": "video_57", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "boundary the decision boundary without really us doing that much and without really us doing that much and then really adding these neurons became then really adding these neurons became kind of quadratic and it's kind of almost there quadratic and it's kind of almost there in a sense that it is all we need to in a sense that it is all we need to understand is whether if i if we can understand is whether if i if we can give it some more time perhaps another give it some more time perhaps another local minima which is even better than local minima which is even better than the one that we have just experienced the one that we have just experienced will be found and we are going to will be found and we are going to just let it run and however we just let it run and however we can actually show some kind of a", "image_path": "img_data/video_57_chunk_46.jpg"}
{"video": "video_57", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "just let it run and however we can actually show some kind of a we can actually show some kind of a useful trajectory now in front of us we useful trajectory now in front of us we have seen as we are adding more have seen as we are adding more neurons then effectively we are neurons then effectively we are enhancing the modeling capabilities of enhancing the modeling capabilities of our hypothesis set we should have now our hypothesis set we should have now some analogy in our mind what we have some analogy in our mind what we have seen in the earlier as a hypothesis set seen in the earlier as a hypothesis set and the set of functions g and the set of functions g that we have discussed are is that we have discussed are is actually translate into an architecture", "image_path": "img_data/video_57_chunk_47.jpg"}
{"video": "video_57", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "that we have discussed are is actually translate into an architecture actually translate into an architecture the more complicated the architecture the more complicated the architecture becomes perhaps the more we becomes perhaps the more we increase the number of hidden layers increase the number of hidden layers or even the number of neurons which are or even the number of neurons which are in each layer then we would expect to in each layer then we would expect to see an ability to sort of fit more see an ability to sort of fit more challenging target functions let's challenging target functions let's play this with for a while let's play this with for a while let's leave it running over there and leave it running over there and then if it founds a good local minima and", "image_path": "img_data/video_57_chunk_48.jpg"}
{"video": "video_57", "start": "0:24:30", "end": "0:24:57.266667", "timestamp": "0:24:30 - 0:24:57.266667", "text": "then if it founds a good local minima and if it founds a good local minima and we can actually stop the iterations and we can actually stop the iterations and present it to you all right now from present it to you all right now from actually let it run and then from actually let it run and then from that kind of perspective i think the that kind of perspective i think the next discussion would actually be to try next discussion would actually be to try to create a mathematical abstraction of to create a mathematical abstraction of these networks that we'll be calling these networks that we'll be calling fully connected network we need to fully connected network we need to understand why we call them fully understand why we call them fully connected", "image_path": "img_data/video_57_chunk_49.jpg"}
{"video": "video_59", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "you i didn't see quite significant additional kind of significant additional kind of discussion on this kind of topic but i discussion on this kind of topic but i think i saw a couple of messages about think i saw a couple of messages about the max and typically there are the max and typically there are environment kind of changes or environment kind of changes or differences between students instructors differences between students instructors and on i think the responses we and on i think the responses we provided seems to be working sort of provided seems to be working sort of for mac computer just follow for mac computer just follow my message is there if you my message is there if you have not started that but i think most", "image_path": "img_data/video_59_chunk_0.jpg"}
{"video": "video_59", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "my message is there if you have not started that but i think most have not started that but i think most people have completed this development people have completed this development environment set up then there was this information up then there was this information theory basics which we ask you to theory basics which we ask you to sort of read a little bit about the sort of read a little bit about the reference and with computations reference and with computations could be in python computations could be in python you can use python for computations you can use python for computations doesn't have to be marked down to doesn't have to be marked down to show this calculate to show this calculate quantities or show whether or not you", "image_path": "img_data/video_59_chunk_1.jpg"}
{"video": "video_59", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "to show this calculate quantities or show whether or not you quantities or show whether or not you agree with these equalities and then the maximum life of equalities and then the maximum life of parameter estimation problem the parameter estimation problem the it kind of follows the gausian kind of it kind of follows the gausian kind of problem but with a kind of a sort of problem but with a kind of a sort of slight different change in this kind of slight different change in this kind of definition of the probability definition of the probability distribution if you watch the first distribution if you watch the first video at some point in the video you video at some point in the video you will get the log likelihood will get the log likelihood expression and what you need to", "image_path": "img_data/video_59_chunk_2.jpg"}
{"video": "video_59", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "will get the log likelihood expression and what you need to expression and what you need to do is you need to obviously either do is you need to obviously either maximize it i mean maximize the log maximize it i mean maximize the log likelihood or minimize the negative log likelihood or minimize the negative log likelihood and therefore all you need to likelihood and therefore all you need to do is to calculate the gradient do is to calculate the gradient expression the gradient expression is expression the gradient expression is also shown on this first also shown on this first video and if i see something here you will start your task one by", "image_path": "img_data/video_59_chunk_3.jpg"}
{"video": "video_59", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "if i see something here you will start your task one by here you will start your task one by producing inter arrival times you producing inter arrival times you have already a lambda and this is basically the lambda and this is basically the rate parameter of the distribution which rate parameter of the distribution which is i believe if i remember correctly was is i believe if i remember correctly was yeah it is here 100 and you will yeah it is here 100 and you will produce let's say a number of events you produce let's say a number of events you will effectively simulate out of the will effectively simulate out of the exponential distribution and you will be exponential distribution and you will be using some library to simulate it", "image_path": "img_data/video_59_chunk_4.jpg"}
{"video": "video_59", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "exponential distribution and you will be using some library to simulate it using some library to simulate it we don't really expect anything from we don't really expect anything from scratch you'll use potentially npi scratch you'll use potentially npi random apis to bring these random apis to bring these inter arrival times in as events and inter arrival times in as events and let's say you generated let's say a let's say you generated let's say a th000 events you have a data th000 events you have a data generation component from task one this generation component from task one this is the analogous way of is the analogous way of sampling from aian distribution in sampling from aian distribution in the lecture and then if you have this", "image_path": "img_data/video_59_chunk_5.jpg"}
{"video": "video_59", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "the lecture and then if you have this lecture and then if you have this since you have now the numbers the since you have now the numbers the data what you need to do is you need to data what you need to do is you need to implement a stochastic gr descent that implement a stochastic gr descent that will minimize the negative lo l the will minimize the negative lo l is a minus sign on the negative lo l is a minus sign on the expression you will see on video number expression you will see on video number one you have the expression of the one you have the expression of the negative log likelihood and of course negative log likelihood and of course the what you need for stoas descend the what you need for stoas descend is not the expression itself of the is not the expression itself of the negative l but it's gradient which is negative l but it's gradient which is also shown in the same also shown in the same video", "image_path": "img_data/video_59_chunk_6.jpg"}
{"video": "video_59", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "also shown in the same video with the exception of this minus sign with the exception of this minus sign and you will have a you will follow and you will have a you will follow kind of a similar approach to kind of a similar approach to implementing two for loops one is implementing two for loops one is going to be an outer for loop that it going to be an outer for loop that it will go over epo you can define a number of epo you can define a number of epo is as you probably heard me in this lecture you probably heard me in this lecture kind of a visitation over the whole u a kind of a visitation over the whole u a thousand samples of the data set that", "image_path": "img_data/video_59_chunk_7.jpg"}
{"video": "video_59", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "kind of a visitation over the whole u a thousand samples of the data set that thousand samples of the data set that someone gave you if it is a thousand the someone gave you if it is a thousand the number of ss you generated and then the you also generated and then the you also have the sort of a sample loop now you have the sort of a sample loop now you go through one sample you can of course go through one sample you can of course implement stoas grade descend with just implement stoas grade descend with just one sample you don't have to have a one sample you don't have to have a mini batch of greater than one it works mini batch of greater than one it works with one and you will and with one and you will and inside this kind of for loop you will inside this kind of for loop you will randomly select the sample from the", "image_path": "img_data/video_59_chunk_8.jpg"}
{"video": "video_59", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "inside this kind of for loop you will randomly select the sample from the randomly select the sample from the population you will plug in the population you will plug in the sample into the gradient expression sample into the gradient expression because if you see the gradient because if you see the gradient expression obviously has some dependency expression obviously has some dependency on the sample and also on the lambda on the sample and also on the lambda parameter and you will then have the parameter and you will then have the stochastic radi equation the parameter stochastic radi equation the parameter update equation which requires a update equation which requires a learning rate which you need to tell us learning rate which you need to tell us u what it is obviously in the u what it is obviously in the code", "image_path": "img_data/video_59_chunk_9.jpg"}
{"video": "video_59", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "u what it is obviously in the code and you will obviously need and you will obviously need to implement another line which is the to implement another line which is the loss function itself the negative l loss function itself the negative l likelihood a function itself because likelihood a function itself because we would also to see it as a plot we would also to see it as a plot at the end of this two for loops you at the end of this two for loops you will do two things one to print out the will do two things one to print out the final lambda after all this epo and final lambda after all this epo and iterations are done the second iterations are done the second for loop the inner for loop are done and", "image_path": "img_data/video_59_chunk_10.jpg"}
{"video": "video_59", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "iterations are done the second for loop the inner for loop are done and for loop the inner for loop are done and you will plot the u the sort you will plot the u the sort of the loss the negative lo over i of the loss the negative lo over i think we are asking you think we are asking you over the iteration number which is combination of the ook and combination of the ook and the samples that each the samples that each contains that's basically that", "image_path": "img_data/video_59_chunk_11.jpg"}
{"video": "video_59", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "the samples that each contains that's basically that contains that's basically that the i think the second exercise you the i think the second exercise you shouldn't have really any problem and i shouldn't have really any problem and i think from some tickets that i have think from some tickets that i have seen people are in the kind of a good seen people are in the kind of a good trajectory there and then the second the trajectory there and then the second the third final kind of problem is very third final kind of problem is very analogous to the problem that you've analogous to the problem that you've seen in the textbook the python textbook seen in the textbook the python textbook in fact it's fairly straightforward to do the extension you straightforward to do the extension you effec", "image_path": "img_data/video_59_chunk_12.jpg"}
{"video": "video_59", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "straightforward to do the extension you effec take the code and you from the take the code and you from the textbook and you augment it with the textbook and you augment it with the changes what are those changes well changes what are those changes well evidently over here we have a data set evidently over here we have a data set which is u a signer in nature you can which is u a signer in nature you can see exactly how theal data set gets see exactly how theal data set gets generated in the website and then you generated in the website and then you need to change the loss function need to change the loss function and because you're changing the loss and because you're changing the loss function all you need to do is to function all you need to do is to calculate the gradient of the new loss", "image_path": "img_data/video_59_chunk_13.jpg"}
{"video": "video_59", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "function all you need to do is to calculate the gradient of the new loss calculate the gradient of the new loss function which involves now a function which involves now a regularization term and this is the only regularization term and this is the only line that you need to change in the code line that you need to change in the code because this thing should because this thing should converge to the m is equal to 3 model converge to the m is equal to 3 model if you should converge to producing if you should converge to producing a w star vector that because of this a w star vector that because of this regularization only the first few terms regularization only the first few terms are effectively non zero and the are effectively non zero and the remaining terms are potentially", "image_path": "img_data/video_59_chunk_14.jpg"}
{"video": "video_59", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "are effectively non zero and the remaining terms are potentially remaining terms are potentially very close to zero and this is very close to zero and this is in order to show that and get the in order to show that and get the full points obviously you need to show full points obviously you need to show the impact of regularization even if the impact of regularization even if you expanding significantly larger you expanding significantly larger than three hypothesis in this case nine for hypothesis in this case nine for example then only the first three example then only the first three four terms of the w star vector are non four terms of the w star vector are non zero and the 5 6 7 8 and n are close to", "image_path": "img_data/video_59_chunk_15.jpg"}
{"video": "video_59", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "four terms of the w star vector are non zero and the 5 6 7 8 and n are close to zero and the 5 6 7 8 and n are close to zero at the end of the stochastic radi zero at the end of the stochastic radi algorithm pointing to the algorithm pointing to the hypothesis which looks more or less hypothesis which looks more or less in as well this code for in as well this code for that one you will find in your chapter 4 that one you will find in your chapter 4 of your python on your python textbook", "image_path": "img_data/video_59_chunk_16.jpg"}
{"video": "video_59", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "textbook any questions that was very helpful professor thank you problem just to confirm right i you problem just to confirm right i mean on the submission if no one has any mean on the submission if no one has any question we are submitting the question we are submitting the notebook in both bright space and also notebook in both bright space and also when we open the project right when we open the project right when we open your repository the course we open your repository the course repository", "image_path": "img_data/video_59_chunk_17.jpg"}
{"video": "video_59", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "we open your repository the course repository in the container we are actually in the container we are actually submitting the notebook in both the places the notebook in both the places and then we are also committing our and then we are also committing our changes to our repository in github changes to our repository in github correct the there is a correct the there is a web page where you can on the court a web page where you can on the court site i think i refer to it that how to site i think i refer to it that how to submitted the first job that you need submitted the first job that you need to do and there is a video inside that to do and there is a video inside that video first of all they ask you to edit", "image_path": "img_data/video_59_chunk_18.jpg"}
{"video": "video_59", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "to do and there is a video inside that video first of all they ask you to edit video first of all they ask you to edit this line 34 where your the github id of this line 34 where your the github id of your collaborators are obviously your collaborators are obviously your collaborators are the ta is the ta of collaborators are the ta is the ta of this course there's only one ta in this course there's only one ta in this course you basically take the course you basically take the github id his github id and you put it github id his github id and you put it here deleting all the remaining github here deleting all the remaining github ids quoted in this list this is for ids quoted in this list this is for your workflow when you commit your workflow when you commit your changes for the workflow to be executed changes for the workflow to be executed and check that the ta has", "image_path": "img_data/video_59_chunk_19.jpg"}
{"video": "video_59", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "changes for the workflow to be executed and check that the ta has and check that the ta has access to it and you get a green check access to it and you get a green check mark at the repository kind of action mark at the repository kind of action view on in github this is my repository github this is my repository evidently it is sitting in github let me evidently it is sitting in github let me just go quickly to that just go quickly to that location sorry what is the version sorry what is the version control here is your control here is your remotes and this is", "image_path": "img_data/video_59_chunk_20.jpg"}
{"video": "video_59", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "control here is your remotes and this is where i am what happened here it is right you have the option right you have the option to when you sign in you have the to when you sign in you have the option you will see a plus icon over option you will see a plus icon over here and you by clicking this plus here and you by clicking this plus icon you are able to make the repository", "image_path": "img_data/video_59_chunk_21.jpg"}
{"video": "video_59", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "here and you by clicking this plus icon you are able to make the repository icon you are able to make the repository your own it's a process called your own it's a process called import the other process is to clone import the other process is to clone it and then change the remote manually it and then change the remote manually at the end of the day you have at the end of the day you have the repository it's very important to for repository it's very important to for your own repository which is a clone of your own repository which is a clone of this thing potentially to be your this thing potentially to be your own and be your own obviously and own and be your own obviously and be private we ask you to make all the be private we ask you to make all the repos private and also we ask you to", "image_path": "img_data/video_59_chunk_22.jpg"}
{"video": "video_59", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "be private we ask you to make all the repos private and also we ask you to repos private and also we ask you to sort of insert the collaborator sort of insert the collaborator when the action is executed the when the action is executed the collaborators are checked and everything collaborators are checked and everything is fine you will place let's assume is fine you will place let's assume that you do the development environment that you do the development environment in docker container obviously you have in docker container obviously you have some notebook there if you work with some notebook there if you work with a docker container from within the a docker container from within the repository the notebook is likely repository the notebook is likely already under the assignment somewh it's already under the assignment somewh it's probably under the assignment one this probably under the assignment one this notebook is a dam notebook is if you", "image_path": "img_data/video_59_chunk_23.jpg"}
{"video": "video_59", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "probably under the assignment one this notebook is a dam notebook is if you notebook is a dam notebook is if you open it you will create an error because open it you will create an error because it just doesn't have any bytes inside it just doesn't have any bytes inside it's just a placeholder if this is it's just a placeholder if this is your notebook please save the output of your notebook please save the output of the notebook and the collaborator the notebook and the collaborator the ta is able to just click on it the ta is able to just click on it github will pass it and will see the github will pass it and will see the result and will grade appropriately and result and will grade appropriately and give you a grade in bright space give you a grade in bright space if you don't and you do the if you don't and you do the implementation outside of do container", "image_path": "img_data/video_59_chunk_24.jpg"}
{"video": "video_59", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "if you don't and you do the implementation outside of do container implementation outside of do container inside a collab for example environment inside a collab for example environment or some other environment you have to or some other environment you have to copy the notebook you have to copy the notebook you have to download from collab and copy it download from collab and copy it over into your github over into your github repository and also make sure that repository and also make sure that the outputs are saved again the ta the outputs are saved again the ta will just open the notebook and we just will just open the notebook and we just grade without redoing much the grade without redoing much the bottom line is that what you", "image_path": "img_data/video_59_chunk_25.jpg"}
{"video": "video_59", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "grade without redoing much the bottom line is that what you bottom line is that what you submit is you submit this url you submit is you submit this url to bright space and on to bright space and on sunday preferably at sunday preferably at 1159 p.m. before 11:59 p.m. and then you 1159 p.m. before 11:59 p.m. and then you will you'll be fine bright base only accepts urls fine bright base only accepts urls and there's only one url you submit and there's only one url you submit in each assignment because you the ta knows that", "image_path": "img_data/video_59_chunk_26.jpg"}
{"video": "video_59", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "in each assignment because you the ta knows that because you the ta knows that you're submitting an assignment at the you're submitting an assignment at the specific deadline it will go to specific deadline it will go to assignment one or two or three whatever assignment one or two or three whatever folder it is there and will del it do this can we do this all at least for this assignment in one single least for this assignment in one single ipynb file that has the python and ipynb file that has the python and the markdown all at once oh yes i the markdown all at once oh yes i mean yeah assignment one could mean yeah assignment one could potentially be just one notebook right", "image_path": "img_data/video_59_chunk_27.jpg"}
{"video": "video_59", "start": "0:14:00", "end": "0:14:22.766667", "timestamp": "0:14:00 - 0:14:22.766667", "text": "mean yeah assignment one could potentially be just one notebook right potentially be just one notebook right and this notebook will contain a mixture and this notebook will contain a mixture of python and markdown obviously you of course free to break the assignment into multiple parts break the assignment into multiple parts and multiple not notebooks that is and multiple not notebooks that is and put it under the assignment one and put it under the assignment one folder", "image_path": "img_data/video_59_chunk_28.jpg"}
{"video": "video_64", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "will take advantage of the smoothness of the underlying target function and claim the underlying target function and claim the following i can go ahead and the following i can go ahead and evaluate every single of those evaluate every single of those hypothesis that i have come up with hypothesis that i have come up with according to this kind of loss function according to this kind of loss function now very quickly realize that the now very quickly realize that the -called g9 function according we -called g9 function according we can write that according", "image_path": "img_data/video_64_chunk_0.jpg"}
{"video": "video_64", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "-called g9 function according we can write that according to the mean square error criterion the g9 function offers to us loss of y hat comma y which is equal to loss of y hat comma y which is equal to zero because every single point this", "image_path": "img_data/video_64_chunk_1.jpg"}
{"video": "video_64", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "loss of y hat comma y which is equal to zero because every single point this zero because every single point this function goes through every single point function goes through every single point in my data set this will actually in my data set this will actually be the best function according to be the best function according to this criterion and this is basically this criterion and this is basically what i will push to production to what i will push to production to start making predictions according to start making predictions according to my model if we assume that this is a my model if we assume that this is a real estate use case where the xaxis real estate use case where the xaxis represents square footage and the y ais represents square footage and the y ais prices and actually we push our", "image_path": "img_data/video_64_chunk_2.jpg"}
{"video": "video_64", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "represents square footage and the y ais prices and actually we push our prices and actually we push our g9 this function over here to g9 this function over here to production then at around production then at around 3:00 in the morning we may actually 3:00 in the morning we may actually start receiving phone calls from start receiving phone calls from upset customers these companies have upset customers these companies have invested lots of money into real estate invested lots of money into real estate using our model and they are not using our model and they are not really happy at all with the prediction really happy at all with the prediction results and in this way we results and in this way we can actually see the reason i mean", "image_path": "img_data/video_64_chunk_3.jpg"}
{"video": "video_64", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "results and in this way we can actually see the reason i mean can actually see the reason i mean i'm trying to explain why perhaps the i'm trying to explain why perhaps the g9 function despite the fact that it was g9 function despite the fact that it was minimizing our mean square or is not minimizing our mean square or is not really the sort of a good really the sort of a good hypothesis to push out this for hypothesis to push out this for example take the point over here in this example take the point over here in this point which is the x new see point which is the x new see exactly what our predictions are exactly what our predictions are according to this function according to this function g9 our prediction will actually be", "image_path": "img_data/video_64_chunk_4.jpg"}
{"video": "video_64", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "according to this function g9 our prediction will actually be g9 our prediction will actually be the y over here and all of the other the y over here and all of the other points that we have in this function points that we have in this function in this in our data set are really far in this in our data set are really far away from this predicted data away from this predicted data point this is actually an point this is actually an indication of a condition called indication of a condition called overfitting let me write this down overfitting let me write this down the g9 of x comma w with this specific par", "image_path": "img_data/video_64_chunk_5.jpg"}
{"video": "video_64", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "the g9 of x comma w with this specific par g9 of x comma w with this specific par parameters will lead however to overfitting and this condition overfitting is going to be with us overfitting is going to be with us throughout this course overfitting is going to be course overfitting is going to be a condition that is going to be a condition that is going to be fairly problematic", "image_path": "img_data/video_64_chunk_6.jpg"}
{"video": "video_64", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "a condition that is going to be fairly problematic and in order for us to be able to and in order for us to be able to address it let us just start by address it let us just start by introducing a new parameter i will be introducing a new parameter i will be calling this parameter capital m and calling this parameter capital m and capital m will actually symbolize over capital m will actually symbolize over here the maximum exponent that i have here the maximum exponent that i have over here in my monomials for this specific monomials for this specific hypothesis g0 capital m will be zero hypothesis g0 capital m will be zero for this capital m will be for this capital m will be one and", "image_path": "img_data/video_64_chunk_7.jpg"}
{"video": "video_64", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "for this capital m will be one and on and for this will be capital m and on and for this will be capital m will be nine and we'll be using this capital m nine and we'll be using this capital m as a proxy to what we'll be calling as a proxy to what we'll be calling model complexity and as you can understand complexity and as you can understand the as the m is increasing as the m is increasing as the number of parameters we have the number of parameters we have incorporated into the model and incorporated into the model and overfitting basically is the a overfitting basically is the a condition where we have put too many condition where we have put too many parameters into a model the question parameters into a model the question now become how are we going to be able", "image_path": "img_data/video_64_chunk_8.jpg"}
{"video": "video_64", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "parameters into a model the question now become how are we going to be able now become how are we going to be able to control these parameters in a to control these parameters in a semi-automatic way first we have to semi-automatic way first we have to address how we detect overfitting and address how we detect overfitting and secondly how we can control it one secondly how we can control it one way to address this overfitting is to way to address this overfitting is to detect this overfitting that is to detect this overfitting that is to split our original kind of data into split our original kind of data into two parts or three", "image_path": "img_data/video_64_chunk_9.jpg"}
{"video": "video_64", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "split our original kind of data into two parts or three but we will use the two the first two initially the first part is the -call training data set the validation and the -called test validation and the -called test data set all right we will be using set all right we will be using training and validation we are this training and validation we are this part both of these parts have typic", "image_path": "img_data/video_64_chunk_10.jpg"}
{"video": "video_64", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "training and validation we are this part both of these parts have typic labels we do have the y's in this test data set we typically we test data set we typically we don't however we actually what we don't however we actually what we do borrow is just the x's we have x and valid and y valid over here and then we", "image_path": "img_data/video_64_chunk_11.jpg"}
{"video": "video_64", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "x's we have x and valid and y valid over here and then we valid and y valid over here and then we have x train and y train coming in over there all train coming in over there all of these subsets originate from of these subsets originate from our original kind of data all right our original kind of data all right the bottom line is the following what we're trying actually to following what we're trying actually to do is to use for training the do is to use for training the model which we'll see exactly how", "image_path": "img_data/video_64_chunk_12.jpg"}
{"video": "video_64", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "do is to use for training the model which we'll see exactly how model which we'll see exactly how using only one of the subsets and using only one of the subsets and then we will be sort of exercising then we will be sort of exercising the validation data set in order for us the validation data set in order for us to produce the u losses that to produce the u losses that are pretending that we have these are pretending that we have these are the validation data are the data we have the validation data are the data we have never seen before while before we never seen before while before we were doing training across the whole were doing training across the whole data set and we were sort of shipping data set and we were sort of shipping the model over into a serving", "image_path": "img_data/video_64_chunk_13.jpg"}
{"video": "video_64", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "data set and we were sort of shipping the model over into a serving the model over into a serving pipeline order to be served in this pipeline order to be served in this specific case we will use only a sub specific case we will use only a sub of the data and serve the model of the data and serve the model just using the validation data and just using the validation data and actually when we see there some actually when we see there some relationship between the training and relationship between the training and validation losses then we will be validation losses then we will be able to detect overfeeding and this what able to detect overfeeding and this what we actually we will do now we will we actually we will do now we will go back to our site", "image_path": "img_data/video_64_chunk_14.jpg"}
{"video": "video_64", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "we actually we will do now we will go back to our site and as we discussed these are and as we discussed these are various hypothesis each one of them various hypothesis each one of them will actually be able to give us some will actually be able to give us some training loss and some kind of training loss and some kind of validation loss now i should mention validation loss now i should mention that in many textbooks the that in many textbooks the validation the test data set are being validation the test data set are being used interchangeably in this specific interchangeably in this specific figure what we have here as figure what we have here as test is what i mentioned as validation test is what i mentioned as validation where we have labels in order to produce", "image_path": "img_data/video_64_chunk_15.jpg"}
{"video": "video_64", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "test is what i mentioned as validation where we have labels in order to produce where we have labels in order to produce any loss value we do need the y as we any loss value we do need the y as we discussed the loss value is obtained discussed the loss value is obtained by comparing the y's and the y hats by comparing the y's and the y hats the y is the ground truth and our the y is the ground truth and our predictions what we have here is we predictions what we have here is we have u we are iterating over the model have u we are iterating over the model complexity in other words every distinct complexity in other words every distinct m corresponds to a distinct hypothesis m corresponds to a distinct hypothesis m is equal to z is that horizontal m is equal to z is that horizontal red hypothesis that we have seen", "image_path": "img_data/video_64_chunk_16.jpg"}
{"video": "video_64", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "m is equal to z is that horizontal red hypothesis that we have seen red hypothesis that we have seen here and on we are itating over and actually we'll see that itating over and actually we'll see that on the at this initial kind of state on the at this initial kind of state we have yet another condition which was we have yet another condition which was called underfitting in the underfitting called underfitting in the underfitting condition we are not invest at all condition we are not invest at all into the we are under representing if into the we are under representing if you the model complexity and we are not complexity and we are not really putting a lot of par really putting a lot of par parameters into the problem and we are", "image_path": "img_data/video_64_chunk_17.jpg"}
{"video": "video_64", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "really putting a lot of par parameters into the problem and we are parameters into the problem and we are under sort of feting that in that way under sort of feting that in that way we have twoo simple of a we have twoo simple of a hypothesis and as we the model hypothesis and as we the model complexity is increasing we will go complexity is increasing we will go through a relatively kind of flat region through a relatively kind of flat region over here in this specific example that over here in this specific example that is and at some point our g9 will is and at some point our g9 will actually offer to us a training loss as actually offer to us a training loss as we have seen of zero and the reason we have seen of zero and the reason is that the hypothesis goes through is that the hypothesis goes through all of the", "image_path": "img_data/video_64_chunk_18.jpg"}
{"video": "video_64", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "is that the hypothesis goes through all of the data points the training kind of data points the training kind of data points however at the same time we points however at the same time we are producing a validation loss for are producing a validation loss for every model we have now two losses one every model we have now two losses one is the training and the other the is the training and the other the validation losses will actually be validation losses will actually be fairly close to the training losses fairly close to the training losses however every time we have over feting however every time we have over feting we will exhibit a huge difference we will exhibit a huge difference between training and validation and it between training and validation and it is really difference that we'll be using", "image_path": "img_data/video_64_chunk_19.jpg"}
{"video": "video_64", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "between training and validation and it is really difference that we'll be using is really difference that we'll be using for detecting the overfeeding for detecting the overfeeding condition and straight for actually to condition and straight for actually to see why we have this kind of huge see why we have this kind of huge difference as we discussed in the g9 difference as we discussed in the g9 hypothesis as you can see other hypothesis as you can see other hypothesis let's say for example take hypothesis let's say for example take the m is equal to 3 case every the m is equal to 3 case every the red lines are actually predicting red lines are actually predicting values of y hat which are actually values of y hat which are actually very close", "image_path": "img_data/video_64_chunk_20.jpg"}
{"video": "video_64", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "values of y hat which are actually very close to the existing validation kind of to the existing validation kind of data and therefore the training and data and therefore the training and validation losses will actually be validation losses will actually be very similar the m is equal to 9 very similar the m is equal to 9 however case for certain values of however case for certain values of validation data for example take the validation data for example take the value that it is associated with value that it is associated with let's say this point is not part of let's say this point is not part of our training set or this point not our training set or this point not really part of the training set and", "image_path": "img_data/video_64_chunk_21.jpg"}
{"video": "video_64", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "our training set or this point not really part of the training set and really part of the training set and for some actually values over there for some actually values over there will be a significant kind of there will be a significant kind of difference and it's actually quite easy difference and it's actually quite easy to see why we have this kind of to see why we have this kind of significant discrepancy between training significant discrepancy between training and validation in the case where we have and validation in the case where we have the g9 hypothesis in all other the g9 hypothesis in all other hypothesis the despite in even in the hypothesis the despite in even in the case of under fitting we have a case of under fitting we have a relatively small difference between relatively small difference between training and validation because training and validation because the hypothesis that we have we are", "image_path": "img_data/video_64_chunk_22.jpg"}
{"video": "video_64", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "training and validation because the hypothesis that we have we are the hypothesis that we have we are evaluating are predicting values of y evaluating are predicting values of y which is in the neighborhood of our which is in the neighborhood of our other y values the case happens the same other y values the case happens the same way with m is equal to 1 m is equal 3 as way with m is equal to 1 m is equal 3 as we see here for m is equal to 9 there we see here for m is equal to 9 there are some regions in the x- axis and if are some regions in the x- axis and if we had validation data in these kind of we had validation data in these kind of regions we're going to have some regions we're going to have some significant overshoots or undersuits in significant overshoots or undersuits in this kind of case the validation this kind of case the validation actually loss is going to be much higher in", "image_path": "img_data/video_64_chunk_23.jpg"}
{"video": "video_64", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "actually loss is going to be much higher in is going to be much higher in that case also not worthy is the fact that case also not worthy is the fact that the absolute loss are quite high as that the absolute loss are quite high as well when we have underfeed models and well when we have underfeed models and we have a fairly wide dynamic range as we have a fairly wide dynamic range as we discussed where we can actually we discussed where we can actually pick up any model out of this range pick up any model out of this range of m values and what really makes of m values and what really makes sense for us to select is a model sense for us to select is a model that could be either three four five six that could be either three four five six or seven but out of all these kind of", "image_path": "img_data/video_64_chunk_24.jpg"}
{"video": "video_64", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "that could be either three four five six or seven but out of all these kind of or seven but out of all these kind of options only one is really the best and options only one is really the best and that best option is the m is equal to that best option is the m is equal to three case because with m is equal to 3 three case because with m is equal to 3 as we seen we only expand very few as we seen we only expand very few parameters let's say four parameters in parameters let's say four parameters in total while in the case of if we had total while in the case of if we had selected m is equal to let's say eight selected m is equal to let's say eight would have expected expanded nine would have expected expanded nine parameters in total why pay for parameters in total why pay for pretty much the same performance pretty much the same performance an expense a cost in terms of cpu or", "image_path": "img_data/video_64_chunk_25.jpg"}
{"video": "video_64", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "pretty much the same performance an expense a cost in terms of cpu or an expense a cost in terms of cpu or gpu resources during inference when we gpu resources during inference when we are serving the model why we actually are serving the model why we actually can get away with a much simpler model can get away with a much simpler model we call actually this situation we call actually this situation over here the u kind of oams razor this over here the u kind of oams razor this kind of concept where we want to kind of concept where we want to definitely operate with simple models definitely operate with simple models but not simpler we will need to now move", "image_path": "img_data/video_64_chunk_26.jpg"}
{"video": "video_64", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "simpler we will need to now move on into the following discussion of how on into the following discussion of how to address it after detection and of course the detection and of course the introduction of the u training introduction of the u training validation and test data sets we'll validation and test data sets we'll discuss here an approach of addressing discuss here an approach of addressing overfeeding that will actually lead to overfeeding that will actually lead to us being able to semi-automatically us being able to semi-automatically tune the model complexity and that tune the model complexity and that approach is actually called approach is actually called regularization and regularization is a regularization and regularization is a very big topic in machine learning out very big topic in machine learning out of all of the approaches that are trying to", "image_path": "img_data/video_64_chunk_27.jpg"}
{"video": "video_64", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "of all of the approaches that are trying to all of the approaches that are trying to regulate the model complexity if you regulate the model complexity if you we are going to be discussing we are going to be discussing the -called we decay also known as r the -called we decay also known as r regression approach and according to this approach and according to this approach we are going to be adding a penalty we are going to be adding a penalty factor into our mean square error that factor into our mean square error that it was deemed insufficient of preventing it was deemed insufficient of preventing overfitting you recall the minimization overfitting you recall the minimization of mean square error led us to the of mean square error led us to the hypothesis g", "image_path": "img_data/video_64_chunk_28.jpg"}
{"video": "video_64", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "of mean square error led us to the hypothesis g that completely minimized in fact it that completely minimized in fact it made it zero it is really the choice made it zero it is really the choice of the loss function which as we of the loss function which as we discussed is our responsibility that actually led to responsibility that actually led to overfeeding we are going to be overfeeding we are going to be changing this loss function by adding changing this loss function by adding the penalty factor called l penalty here the penalty factor called l penalty here a penalty loss this penalty loss has a penalty loss this penalty loss has this following functional form as you this following functional form as you can actually see over here it's a can actually see over here it's a scalar that we call lambda and we will scalar that we call lambda and we will be calling this type of factors hyper", "image_path": "img_data/video_64_chunk_29.jpg"}
{"video": "video_64", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "scalar that we call lambda and we will be calling this type of factors hyper be calling this type of factors hyper parameters lambda is going to determine parameters lambda is going to determine how much of the norm squar of the how much of the norm squar of the parameter vector is going to be added parameter vector is going to be added in the into the mean square error in the into the mean square error loss we have here as we let's say loss we have here as we let's say for m is equal to let's say 9 we have a for m is equal to let's say 9 we have a parameter vector w which is w 0 w1 all", "image_path": "img_data/video_64_chunk_30.jpg"}
{"video": "video_64", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "for m is equal to let's say 9 we have a parameter vector w which is w 0 w1 all parameter vector w which is w 0 w1 all the way to w9 we will calculate the norm squar w9 we will calculate the norm squar of w which is obviously w0 s + w1 s + w9 2 w which is obviously w0 s + w1 s + w9 2 and we will be adding it into the and we will be adding it into the loss that we called earlier mean loss that we called earlier mean square error loss now why this make square error loss now why this make sense for that we have to go into sense for that we have to go into the notebook and understand exactly why", "image_path": "img_data/video_64_chunk_31.jpg"}
{"video": "video_64", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "sense for that we have to go into the notebook and understand exactly why the notebook and understand exactly why the norm squared of w is really a signal the norm squared of w is really a signal to speak that it will allow us to speak that it will allow us to observe its behavior under observe its behavior under non overfitting conditions and under non overfitting conditions and under overfitting conditions and actually overfitting conditions and actually persuade ourselves that this is a good persuade ourselves that this is a good signal to pick up and use it as a as signal to pick up and use it as a regularization term our course site what we actually term our course site what we actually are going to be seeing over here is a", "image_path": "img_data/video_64_chunk_32.jpg"}
{"video": "video_64", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "term our course site what we actually are going to be seeing over here is a are going to be seeing over here is a button at the very top most of the button at the very top most of the pages of the course site are actually pages of the course site are actually python notebooks and what we actually can do notebooks and what we actually can do here is to press on that button and in here is to press on that button and in fact right mouse click to it and fact right mouse click to it and select open link in a new tab and the select open link in a new tab and the moment actually you do that you will be moment actually you do that you will be taken to the -cal google collab taken to the -cal google collab service that is a cloud resource that we service that is a cloud resource that we will be using in this course to be will be using in this course to be able to run our notebooks and you have", "image_path": "img_data/video_64_chunk_33.jpg"}
{"video": "video_64", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "will be using in this course to be able to run our notebooks and you have able to run our notebooks and you have actually two options in this course as actually two options in this course as you probably seen on a video that you probably seen on a video that discusses about your programming discusses about your programming environment the first is to use collab environment the first is to use collab and the second to actually use visual and the second to actually use visual studio code either of the two will studio code either of the two will actually be sufficient i have both of actually be sufficient i have both of them open over here you can them open over here you can actually see how both of them are actually see how both of them are actually working and for now what i actually working and for now what i will do is you need to go to run will do is you need to go to run time and press the run all it will time and press the run all it will actually execute all the cells of this", "image_path": "img_data/video_64_chunk_34.jpg"}
{"video": "video_64", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "time and press the run all it will actually execute all the cells of this actually execute all the cells of this notebook it will install some packages notebook it will install some packages and you will actually will go through and you will actually will go through this kind of notebook to answer the this kind of notebook to answer the appending question and the appending question and the appending question is how did we determine that question is how did we determine that this nor squ of w is a suitable this nor squ of w is a suitable term to add in as a regularization as a term to add in as a regularization as a loss as a penalty factor as a penalty loss as a penalty factor as a penalty loss factor in this notebook we", "image_path": "img_data/video_64_chunk_35.jpg"}
{"video": "video_64", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "loss as a penalty factor as a penalty loss factor in this notebook we loss factor in this notebook we have functions evidently we have the have functions evidently we have the soal create data set which is soal create data set which is creating our toy data set we call it creating our toy data set we call it this toy this signal data set which is this toy this signal data set which is of representing an underlying of representing an underlying relationship between x and y you relationship between x and y you actually see the u it say a synthetic actually see the u it say a synthetic data set we call synthetic data sets the data set we call synthetic data sets the data sets that allow us data sets that allow us to syn ize our training and", "image_path": "img_data/video_64_chunk_36.jpg"}
{"video": "video_64", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "data sets that allow us to syn ize our training and to syn ize our training and validation kind of data validation kind of data and that they can process some and that they can process some desired behaviors the underlying desired behaviors the underlying function we have used here is the function we have used here is the sinusoidal function and we added some sinusoidal function and we added some noise into it that is noise into it that is actually described over here and actually described over here and we have selected the set of we have selected the set of pols as the sort of underlying basis", "image_path": "img_data/video_64_chunk_37.jpg"}
{"video": "video_64", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "we have selected the set of pols as the sort of underlying basis pols as the sort of underlying basis functions that we are going to be functions that we are going to be using in order to determine the shape using in order to determine the shape and form of our hypothesis an order and form of our hypothesis an order zero polinomial is evidently able to zero polinomial is evidently able to produce a straight line it's actually produce a straight line it's actually only horizontal to the only horizontal to the x-axis degree one polinomial a x-axis degree one polinomial a straight line with any slope and straight line with any slope and intercept and on we have", "image_path": "img_data/video_64_chunk_38.jpg"}
{"video": "video_64", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "straight line with any slope and intercept and on we have intercept and on we have over here u produced a cell where we have seen let produced a cell where we have seen let me increase the font me increase the font size we have a function called size we have a function called polinomial feature that is generating all the feature that is generating all the features which are needed for our features which are needed for our regression function to be able to be regression function to be able to be detered effectively it is creating all of", "image_path": "img_data/video_64_chunk_39.jpg"}
{"video": "video_64", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "detered effectively it is creating all of effectively it is creating all of those hypothesis that i was writing those hypothesis that i was writing earlier and we have u applying earlier and we have u applying some form of transformation to create some form of transformation to create and determine if you those and determine if you those hypothesis for u our training and hypothesis for u our training and validation sets and we actually validation sets and we actually calling a function called linear calling a function called linear regression that will determine the regression that will determine the optimal set of parameters we are not optimal set of parameters we are not really interesting at this point in time", "image_path": "img_data/video_64_chunk_40.jpg"}
{"video": "video_64", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "optimal set of parameters we are not really interesting at this point in time really interesting at this point in time to explain how this optimal set of to explain how this optimal set of parameters have been determined we will parameters have been determined we will see that in a different kind of video see that in a different kind of video but for now what we are interesting to but for now what we are interesting to see is if we are to train the system see is if we are to train the system using this hypothesis as an example what using this hypothesis as an example what kind of final hypothesis therefore kind of final hypothesis therefore w vectors we are able to produce at the w vectors we are able to produce at the output and of this process output and of this process and the graph over here is actually", "image_path": "img_data/video_64_chunk_41.jpg"}
{"video": "video_64", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "output and of this process and the graph over here is actually and the graph over here is actually showing four wind square error what kind showing four wind square error what kind of w vector coefficients we are getting w vector coefficients we are getting some really small number we are getting some really small number of coefficients and we are actually also getting some really high amplitude for in our attempt to amplitude for in our attempt to actually fit u kind of", "image_path": "img_data/video_64_chunk_42.jpg"}
{"video": "video_64", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "actually fit u kind of a hypothesis that goes through every a hypothesis that goes through every simple data point we are actually the simple data point we are actually the making some really i need to make some making some really i need to make some really significant swings into the u really significant swings into the u coefficients of that kind of w vector coefficients of that kind of w vector our polinomial which is evidently has polinomial which is evidently has this w coefficients as parameters is this w coefficients as parameters is going to sort of go through every", "image_path": "img_data/video_64_chunk_43.jpg"}
{"video": "video_64", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "this w coefficients as parameters is going to sort of go through every going to sort of go through every single data point and this is single data point and this is the reason why we have to make this kind the reason why we have to make this kind of widely fluctuating w's as our of widely fluctuating w's as our coefficients let's change this kind coefficients let's change this kind of cell from m = 9 to m is = of cell from m = 9 to m is = 3 for m is = to 3 we have 3 for m is = to 3 we have now the following this is now our now the following this is now our desire our sort of produced w with a", "image_path": "img_data/video_64_chunk_44.jpg"}
{"video": "video_64", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "now the following this is now our desire our sort of produced w with a desire our sort of produced w with a much smaller model complex in other much smaller model complex in other words we see the dynamic range of the ws words we see the dynamic range of the ws that we actually need is much that we actually need is much smaller and in the case we have smaller and in the case we have overfitting our n squar in other words overfitting our n squar in other words is going to be huge and in the is going to be huge and in the case we have not an overfeeding case we have not an overfeeding condition let's say for m is equal to 3 condition let's say for m is equal to 3 our n squ w is going to be much our n squ w is going to be much smaller and this is the reason why we", "image_path": "img_data/video_64_chunk_45.jpg"}
{"video": "video_64", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "our n squ w is going to be much smaller and this is the reason why we smaller and this is the reason why we finally selected the u w norm finally selected the u w norm squared the non s of w to be our squared the non s of w to be our required desired signal it goes required desired signal it goes and possesses that kind of desired and possesses that kind of desired behavior and this what we will use behavior and this what we will use next when we have the just me square next when we have the just me square error using this w vector the one that it is possessing", "image_path": "img_data/video_64_chunk_46.jpg"}
{"video": "video_64", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "error using this w vector the one that it is possessing vector the one that it is possessing this kind of streams is going to result of streams is going to result obviously into the shown obviously into the shown overfitting hypothesis final overfitting hypothesis final hypothesis and what we have discussed hypothesis and what we have discussed earlier is to add the non square of w earlier is to add the non square of w as a regularization term and adjusted by as a regularization term and adjusted by scalar factor lambda and what we need to discuss now lambda and what we need to discuss now is that if we do that first of all what", "image_path": "img_data/video_64_chunk_47.jpg"}
{"video": "video_64", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "lambda and what we need to discuss now is that if we do that first of all what is that if we do that first of all what we get but before we do let's discuss we get but before we do let's discuss something which has to do with the hyper something which has to do with the hyper parameter optimization which is u almost parameter optimization which is u almost you can treat do an outer loop in u you can treat do an outer loop in u this problem in the for this problem in the for every distinct value of lambda we are every distinct value of lambda we are picking up a penalty factor we are picking up a penalty factor we are adding into the mean square error and we adding into the mean square error and we actually seeing the behavior actually seeing the behavior of training and test if you recall", "image_path": "img_data/video_64_chunk_48.jpg"}
{"video": "video_64", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "actually seeing the behavior of training and test if you recall of training and test if you recall earlier we had a huge gap between earlier we had a huge gap between training and validation ises and this training and validation ises and this gap is very clearly seen over here gap is very clearly seen over here when the log of lambda is actually a when the log of lambda is actually a very small number which means that very small number which means that the lambda is actually very close to the lambda is actually very close to zero when lambda is very close to zero when lambda is very close to zero we are back into the mean square error we are back into the mean square error no surprises that this is going to be no surprises that this is going to be the case where we have some really big the case where we have some really big gap between test and", "image_path": "img_data/video_64_chunk_49.jpg"}
{"video": "video_64", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "the case where we have some really big gap between test and gap between test and validation the as we start to validation the as we start to increase the lambda we see that kind of increase the lambda we see that kind of gap shrinking we have a pretty gap shrinking we have a pretty significant kind of dynamic range where significant kind of dynamic range where the test and validation losses are close the test and validation losses are close to each other which as we discuss is the to each other which as we discuss is the required outcome or the desired required outcome or the desired outcome and then after we start outcome and then after we start increasing l perhaps too much we are increasing l perhaps too much we are seeing some increase of our total seeing some increase of our total loss of our absolute loss and this", "image_path": "img_data/video_64_chunk_50.jpg"}
{"video": "video_64", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "seeing some increase of our total loss of our absolute loss and this loss of our absolute loss and this should also not come a surprice when should also not come a surprice when lambda is becoming fairly large what lambda is becoming fairly large what happens here we are putting some more here we are putting some more emphasis on the non square of w in emphasis on the non square of w in other words the non square of w is other words the non square of w is dominating the total loss and we are dominating the total loss and we are not really considering that much the not really considering that much the prediction ability of what the prediction ability of what the prediction machine we are building we", "image_path": "img_data/video_64_chunk_51.jpg"}
{"video": "video_64", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "prediction ability of what the prediction machine we are building we prediction machine we are building we not really considering the difference not really considering the difference between y hats and y and which is also between y hats and y and which is also not good we have to strike a not good we have to strike a balance and that kind of switch spot balance and that kind of switch spot is what hyper parameter optimization is what hyper parameter optimization loop is going to give us in its simplest loop is going to give us in its simplest form is going to iterate over multiple form is going to iterate over multiple values of lambda or log lambda is values of lambda or log lambda is actually shown over here and it will actually shown over here and it will produce at for every iteration produce at for every iteration a training and a validation loss and", "image_path": "img_data/video_64_chunk_52.jpg"}
{"video": "video_64", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "produce at for every iteration a training and a validation loss and a training and a validation loss and we'll store that in memory the u set we'll store that in memory the u set of parameters that actually produced out of parameters that actually produced out of this total loss if it finds later of this total loss if it finds later on that it is this losses are start on that it is this losses are start to increase he can always go back and to increase he can always go back and retrieve the best possible loss retrieve the best possible loss that we have gotten it is around here that we have gotten it is around here for that specific value of lambda and for that specific value of lambda and therefore we are always can therefore we are always can produce at the output the optimal value", "image_path": "img_data/video_64_chunk_53.jpg"}
{"video": "video_64", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "therefore we are always can produce at the output the optimal value produce at the output the optimal value of lambda and fix that as part of our of lambda and fix that as part of our model both parameters the w starts model both parameters the w starts the optimal set of parameters of our the optimal set of parameters of our hypothesis as well also the optimal hypothesis as well also the optimal hyper parameters will be produced out hyper parameters will be produced out of that exercise yeah after we have exercise yeah after we have produced the optimal hyperparameters now produced the optimal hyperparameters now and the we are actually able to just see and the we are actually able to just see what we get throughout if you this", "image_path": "img_data/video_64_chunk_54.jpg"}
{"video": "video_64", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "what we get throughout if you this throughout if you this training process at the output and training process at the output and this is basically what we get and if you this is basically what we get and if you see this is with model complexity see this is with model complexity equal to 9 m capital m is equal to 9 m capital m is equal to 9 the hyperparameter tuning and the hyperparameter tuning and the whole approach that we have adopted over whole approach that we have adopted over here allowed us to do the following here allowed us to do the following despite the fact we are expanding 10 despite the fact we are expanding 10 w's 10 param in order for us to model", "image_path": "img_data/video_64_chunk_55.jpg"}
{"video": "video_64", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "despite the fact we are expanding 10 w's 10 param in order for us to model w's 10 param in order for us to model our problem the we do not have our problem the we do not have overfitting the produced hypothesis in overfitting the produced hypothesis in fact is actually very similar to the fact is actually very similar to the hypothesis with a much smaller model hypothesis with a much smaller model complexity that we had gotten earlier complexity that we had gotten earlier for m is equal to 3 and what we have for m is equal to 3 and what we have avoided doing is we have avoided doing is we have avoided searching over all possible capital ms searching over all possible capital ms in order to find the best one instead we in order to find the best one instead we have replaced that with hyper parameter have replaced that with hyper parameter tuning which is much easier given that", "image_path": "img_data/video_64_chunk_56.jpg"}
{"video": "video_64", "start": "0:28:30", "end": "0:28:36.966667", "timestamp": "0:28:30 - 0:28:36.966667", "text": "have replaced that with hyper parameter tuning which is much easier given that tuning which is much easier given that the functional form of our loss function the functional form of our loss function is given and static", "image_path": "img_data/video_64_chunk_57.jpg"}
{"video": "video_65", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "at the heart of every intelligent agent we will find the perception agent we will find the perception subsystem as we saw in an earlier video subsystem as we saw in an earlier video the agent that where the agent was the agent that where the agent was actually moving in a very dynamic actually moving in a very dynamic environment in a kind of a self-driving environment in a kind of a self-driving car application the perception sub car application the perception sub subsystem is very essential into subsystem is very essential into enabling the agent to understand its enabling the agent to understand its surroundings today in this video surroundings today in this video we will see this subsystem in", "image_path": "img_data/video_65_chunk_0.jpg"}
{"video": "video_65", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "surroundings today in this video we will see this subsystem in we will see this subsystem in detail and we will outline what is detail and we will outline what is inside it which is really essentially a inside it which is really essentially a prediction machine we will be prediction machine we will be designing this prediction machine designing this prediction machine together and one block at a time i together and one block at a time i will start with a blog diagram the will start with a blog diagram the blog diagram that vapnik put together blog diagram that vapnik put together many decades ago in while he was many decades ago in while he was working b laboratories and this block", "image_path": "img_data/video_65_chunk_1.jpg"}
{"video": "video_65", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "working b laboratories and this block laboratories and this block diagram is really the block diagram is really the block diagram that outlines the essence of that outlines the essence of supervised learning as well we'll supervised learning as well we'll see that in a moment the vaping blog diagram is now moment the vaping blog diagram is now going to be drawn a block by block going to be drawn a block by block starting from the first block starting from the first block which i will call the data generator and this data generator is", "image_path": "img_data/video_65_chunk_2.jpg"}
{"video": "video_65", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "generator and this data generator is effectively e compasses everything effectively e compasses everything that nature can generate nature that nature can generate nature generates images sounds all sorts of images sounds all sorts of structure and unstructured data and we structure and unstructured data and we will be calling whatever it will be calling whatever it has been generated here with x with this has been generated here with x with this kind of underbar and this underbar kind of underbar and this underbar will indicate that x belongs to", "image_path": "img_data/video_65_chunk_3.jpg"}
{"video": "video_65", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "kind of underbar and this underbar will indicate that x belongs to will indicate that x belongs to the set of real numbers to the power of the set of real numbers to the power of n that's a kind of a common notation n that's a kind of a common notation that indicates that x is in fact let's that indicates that x is in fact let's say a vector with n elements the x's are going to elements the x's are going to be mapped from the second block which mapped from the second block which is actually called the is actually called the target function into", "image_path": "img_data/video_65_chunk_4.jpg"}
{"video": "video_65", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "function into a set of what we'll call labels a set of what we'll call labels why and these labels are going to why and these labels are going to belong into two types the first type we belong into two types the first type we will see a bit later is when y is a real will see a bit later is when y is a real number and in this case we'll see that number and in this case we'll see that cor this corresponds to the -called cor this corresponds to the -called regression problem and the second case is when y", "image_path": "img_data/video_65_chunk_5.jpg"}
{"video": "video_65", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "problem and the second case is when y belongs to a set of distinct belongs to a set of distinct classes let's say capital k classes let's say capital k classes total and in this case we will see that total and in this case we will see that this corresponds to the classification problem all right this is basically and let's sort of give a and let's sort of give a an example", "image_path": "img_data/video_65_chunk_6.jpg"}
{"video": "video_65", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "and let's sort of give a an example of regression and example of regression and example of classification let's assume that x is classification let's assume that x is square footage of the square footage of the house the number of bedrooms the house the number of bedrooms the number of bathrooms and why will number of bathrooms and why will correspond to the price of the house correspond to the price of the house that is a sort of a use case that is a sort of a use case that governs regression in the case of governs regression in the case of classification x could be an image and classification x could be an image and we want to understand whether this we want to understand whether this image represent a person there is a", "image_path": "img_data/video_65_chunk_7.jpg"}
{"video": "video_65", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "we want to understand whether this image represent a person there is a image represent a person there is a person in that image or not and person in that image or not and that's a classification problem it's a that's a classification problem it's a binary classification binary classification problem now there is a an infinite problem now there is a an infinite amount of data that nature can generate amount of data that nature can generate what we can what we have to do what we can what we have to do over here and is to introduce a new over here and is to introduce a new block that we will call the sampler or block that we will call the sampler or the sampling block", "image_path": "img_data/video_65_chunk_8.jpg"}
{"video": "video_65", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "block that we will call the sampler or the sampling block is effectively out of the infinite mappings effectively out of the infinite mappings of x and y samples few that we will of x and y samples few that we will actually call with this kind of actually call with this kind of calligraphic d symbol and it calligraphic d symbol and it will be called the data set and the", "image_path": "img_data/video_65_chunk_9.jpg"}
{"video": "video_65", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "calligraphic d symbol and it will be called the data set and the will be called the data set and the data set brings together examples which are pairs of x and y and where i is in the sort of set of where i is in the sort of set of numbers from one to m from now on we numbers from one to m from now on we will have m examples that the will have m examples that the sampling block is actually given to us sampling block is actually given to us and it's worth noting that the sampling and it's worth noting that the sampling we have absolutely no control about of we have absolutely no control about of the sampling or any blocks above this", "image_path": "img_data/video_65_chunk_10.jpg"}
{"video": "video_65", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "we have absolutely no control about of the sampling or any blocks above this the sampling or any blocks above this line the sampler will play a couple line the sampler will play a couple of tricks for us the first to understand of tricks for us the first to understand that let's assume the first use case that let's assume the first use case where the we are given a data set where the we are given a data set that contains 100,000 images of that contains 100,000 images of people that are healthy from the people that are healthy from the let's say the cut scans and 10,000 let's say the cut scans and 10,000 images of people that are unfortunately images of people that are unfortunately have cancerous condition", "image_path": "img_data/video_65_chunk_11.jpg"}
{"video": "video_65", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "images of people that are unfortunately have cancerous condition have cancerous condition and as you can understand the and as you can understand the predictor will actually have need to be predictor will actually have need to be very accurate in detecting the very accurate in detecting the cancerous cases this is a major cancerous cases this is a major problem in machine learning in general problem in machine learning in general and it's actually called class imbalance and it's actually called class imbalance in our data set and we will learn how in our data set and we will learn how to deal with such problematic data sets to deal with such problematic data sets shortly the second case is that of shortly the second case is that of missing data in the case of missing data", "image_path": "img_data/video_65_chunk_12.jpg"}
{"video": "video_65", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "shortly the second case is that of missing data in the case of missing data in the case of missing data we have ex's let's say back to the we have ex's let's say back to the house price prediction problem here you house price prediction problem here you have x represents a house has have x represents a house has several features the square footage several features the square footage the number of bedrooms number of the number of bedrooms number of bathrooms and for some of those bathrooms and for some of those examples some of those houses there examples some of those houses there is absolutely no information about the is absolutely no information about the number of bedrooms the square footage number of bedrooms the square footage is also missing and yes you that's is also missing and yes you that's basically the well-known missing data", "image_path": "img_data/video_65_chunk_13.jpg"}
{"video": "video_65", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "is also missing and yes you that's basically the well-known missing data basically the well-known missing data problem and this is yet another trick problem and this is yet another trick that the sambler will play for us now that we have seen what the sampler will give us what the sampler will give us let's now start designing our let's now start designing our predictor machine and first block according", "image_path": "img_data/video_65_chunk_14.jpg"}
{"video": "video_65", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "predictor machine and first block according machine and first block according to vapnik that we need to specify i to vapnik that we need to specify i will draw it right down here at the will draw it right down here at the bottom of this diagram and this actually bottom of this diagram and this actually is called hypothesis set the hypothesis set is our own guess about the nature of this guess about the nature of this target function we're trying to guess", "image_path": "img_data/video_65_chunk_15.jpg"}
{"video": "video_65", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "guess about the nature of this target function we're trying to guess target function we're trying to guess what it is that's what we need to do what it is that's what we need to do and the function had an argument x if and the function had an argument x if you remember for our hypothesis we will remember for our hypothesis we will also include a set of parameters also include a set of parameters it's going to be a parametric function it's going to be a parametric function from now on the set of parameters is from now on the set of parameters is going to be in general called w in some going to be in general called w in some instan we'll call it theta later on as instan we'll call it theta later on as well but for now let's keep it at w and well but for now let's keep it at w and actually later on in the next video", "image_path": "img_data/video_65_chunk_16.jpg"}
{"video": "video_65", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "well but for now let's keep it at w and actually later on in the next video actually later on in the next video we'll start plotting some of these we'll start plotting some of these parametric functions to understand parametric functions to understand exactly how w what kind of decrees exactly how w what kind of decrees of freedom it provides to us for of freedom it provides to us for now think about the g as something that now think about the g as something that i will ideally resemble f and how can i will ideally resemble f and how can we actually resemble a function with we actually resemble a function with have no absolutely no information about have no absolutely no information about it we'll see how that goes it we'll see how that goes let's now draw the other block let's now draw the other block which i will draw over here in the which i will draw over here in the middle", "image_path": "img_data/video_65_chunk_17.jpg"}
{"video": "video_65", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "which i will draw over here in the middle and this block i want to middle and this block i want to call this block the optimization algorithm the algorithm is going to do a couple of things is going to do a couple of things for us it will receive the data on for us it will receive the data on from the top and it will also receive re from the top and it will also receive re a hypothesis set and with the help of", "image_path": "img_data/video_65_chunk_18.jpg"}
{"video": "video_65", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "from the top and it will also receive re a hypothesis set and with the help of a hypothesis set and with the help of another block which is i will draw it another block which is i will draw it over here on the left which i will call over here on the left which i will call it the objective or loss function it will provide some information to that objective loss information to that objective loss function block and it will the that", "image_path": "img_data/video_65_chunk_19.jpg"}
{"video": "video_65", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "information to that objective loss function block and it will the that function block and it will the that block will respond back as to how block will respond back as to how well this parametric function g which is well this parametric function g which is our hypothesis matches the target our hypothesis matches the target function f and it will do its thing function f and it will do its thing we see that it's going to be an we see that it's going to be an iterative algorithm and at some point the algorithm and at some point the iterations will end and we will produce the -cal optimal parameter", "image_path": "img_data/video_65_chunk_20.jpg"}
{"video": "video_65", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "produce the -cal optimal parameter vector w and using that we will form our w and using that we will form our final hypothesis this final hypothesis we will effectively correspond to the effectively correspond to the prediction and from now on the prediction and from now on the prediction is going to be prediction is going to be called why hat the hat s for called why hat the hat s for predicted and because we are trying to", "image_path": "img_data/video_65_chunk_21.jpg"}
{"video": "video_65", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "called why hat the hat s for predicted and because we are trying to predicted and because we are trying to predict the label y for an x that we predict the label y for an x that we have never seen before we need to have never seen before we need to understand how this x is that we understand how this x is that we never seen before this is will arrive to never seen before this is will arrive to our sort of a prediction machine our sort of a prediction machine and actually it what will turn out is and actually it what will turn out is that we will need to do something in the that we will need to do something in the data set to split it up into things that data set to split it up into things that we will be using for running this we will be using for running this iterative algorithm and then things that", "image_path": "img_data/video_65_chunk_22.jpg"}
{"video": "video_65", "start": "0:11:30", "end": "0:11:44.833333", "timestamp": "0:11:30 - 0:11:44.833333", "text": "we will be using for running this iterative algorithm and then things that iterative algorithm and then things that we will do later in order for us to we will do later in order for us to test exactly how close we are in test exactly how close we are in our predictions and our approximation our predictions and our approximation to the target function", "image_path": "img_data/video_65_chunk_23.jpg"}
{"video": "video_66", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in an earlier video we show how stochastic gr descent is able to stochastic gr descent is able to optimize h the parameters and hyper optimize h the parameters and hyper parameters of u our learning problem we parameters of u our learning problem we started with a sinusoidal simple data started with a sinusoidal simple data set and we introduced during that kind set and we introduced during that kind of discussion the mean square error of discussion the mean square error which was our loss function as it which was our loss function as it turns out mean square error is was turns out mean square error is was introduced in a kind of a hand wavy f introduced in a kind of a hand wavy f fashion the some kind of fashion the some kind of geometric between our predictions and", "image_path": "img_data/video_66_chunk_0.jpg"}
{"video": "video_66", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "fashion the some kind of geometric between our predictions and geometric between our predictions and ground truth labels why but there ground truth labels why but there is quite a little bit of background is quite a little bit of background behind it in this kind of section behind it in this kind of section what we will do is we will introduce what we will do is we will introduce a foundational concept in information a foundational concept in information theory that was pioneered by cl shannon theory that was pioneered by cl shannon in b laboratories several decades ago in b laboratories several decades ago and see ultimately that mean square can and see ultimately that mean square can be derived out of this theory and how be derived out of this theory and how this all linked to not only not this all linked to not only loss functions of regression but", "image_path": "img_data/video_66_chunk_1.jpg"}
{"video": "video_66", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "this all linked to not only loss functions of regression but only loss functions of regression but also the loss functions in also the loss functions in classification i want to introduce the classification i want to introduce the concept of entropy by asking you a concept of entropy by asking you a question imagine that you are a hedge question imagine that you are a hedge fund manager and you are sort of fund manager and you are sort of someone comes to you with two pieces someone comes to you with two pieces of information the first piece of information the first piece of information goes as follows on a information goes as follows on a monday morning no holidays just a monday morning no holidays just a normal monday morning the holland", "image_path": "img_data/video_66_chunk_2.jpg"}
{"video": "video_66", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "monday morning no holidays just a normal monday morning the holland normal monday morning the holland tunnel as the inbound direction into the tunnel as the inbound direction into the city will be a parking lot lots and city will be a parking lot lots and lots of traffic and the second piece of lots of traffic and the second piece of information is that in a monday morning information is that in a monday morning in inbound again lanes into in inbound again lanes into manhattan there will be completely manhattan there will be completely traffic free no cars will be there traffic free no cars will be there the question is which of the two the question is which of the two pieces of information would you be pieces of information would you be willing to bet money on and", "image_path": "img_data/video_66_chunk_3.jpg"}
{"video": "video_66", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "pieces of information would you be willing to bet money on and willing to bet money on and we will see now which one of the two we will see now which one of the two are make sense for us to are make sense for us to select you have to select one of to select you have to select one of the two not both most students will respond i both most students will respond i will of course bet money on the high will of course bet money on the high traffic intensity piece of traffic intensity piece of information because that's what i information because that's what i believe it's going to be true but believe it's going to be true but as it turns out there is the as it turns out there is the answer is the right answer is i will of", "image_path": "img_data/video_66_chunk_4.jpg"}
{"video": "video_66", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "as it turns out there is the answer is the right answer is i will of answer is the right answer is i will of course bet money on the most course bet money on the most unlikely information and this is unlikely information and this is the most unlikely event to happen is the most unlikely event to happen is that on a monday morning there actually that on a monday morning there actually will be no cars in the inbound will be no cars in the inbound traffic into manhattan in fact cl traffic into manhattan in fact cl shanon put a formula that is actually shanon put a formula that is actually called the v information formula that called the v information formula that actually can see on the screen and i'll actually can see on the screen and i'll also be writing now in a moment and also be writing now in a moment and this formula is basically says", "image_path": "img_data/video_66_chunk_5.jpg"}
{"video": "video_66", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "also be writing now in a moment and this formula is basically says this formula is basically says that the value of information about an that the value of information about an event and this event is let's say x event and this event is let's say x is inversely pro is a log of the inverse is inversely pro is a log of the inverse of the probability of that event of the probability of that event happening in the case where we happening in the case where we have completely free inbound lanes have completely free inbound lanes into manhattan this is the into manhattan this is the probability of that occurring in probability of that occurring in monday morning is very low and monday morning is very low and therefore the value information for that", "image_path": "img_data/video_66_chunk_6.jpg"}
{"video": "video_66", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "monday morning is very low and therefore the value information for that therefore the value information for that is very high and there is kind of is very high and there is kind of some form of intuition to remember that some form of intuition to remember that and the intuition is that the u and the intuition is that the u value of some information of any value of some information of any piece of information is definitely piece of information is definitely proportional to the surprise to the proportional to the surprise to the degree of surprise that is actually this degree of surprise that is actually this information is causing us why bet money information is causing us why bet money on something that everyone believes that", "image_path": "img_data/video_66_chunk_7.jpg"}
{"video": "video_66", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "information is causing us why bet money on something that everyone believes that on something that everyone believes that it is and therefore that it is and therefore everyone accepts as a certain as a everyone accepts as a certain as a certainty that's the intuition behind certainty that's the intuition behind this kind of formula and we'll see now this kind of formula and we'll see now in a moment how this formula is used to in a moment how this formula is used to define the concept of define the concept of entropy what we have just seen now entropy what we have just seen now let me just write down the let me just write down the equation", "image_path": "img_data/video_66_chunk_8.jpg"}
{"video": "video_66", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "let me just write down the equation this is the sooc call value of information of the event x and now i think we should go back into the i think we should go back into the previous experiment that we into the previous experiment that we have seen this experiment have seen we have seen this experiment in while we are reviewing some", "image_path": "img_data/video_66_chunk_9.jpg"}
{"video": "video_66", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "have seen we have seen this experiment in while we are reviewing some in while we are reviewing some probabilistic concepts and in that probabilistic concepts and in that experiment it was if you remember the experiment it was if you remember the linux faq guide experiment where someone linux faq guide experiment where someone was actually reading the text out of was actually reading the text out of this linux guide and the text this linux guide and the text the events that we were recording back the events that we were recording back then were all the lowercase characters of the english lowercase characters of the english alphabet plus the space symbol alphabet plus the space symbol the character 27 was a space symbol and", "image_path": "img_data/video_66_chunk_10.jpg"}
{"video": "video_66", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "alphabet plus the space symbol the character 27 was a space symbol and the character 27 was a space symbol and we were effectively counting u how we were effectively counting u how frequent each and every of these random frequent each and every of these random 27 random events were occuring and 27 random events were occuring and we're quoting the probabilities we're quoting the probabilities we were calculating the probabilities which were calculating the probabilities which of course was a ratio of the number of course was a ratio of the number of events that we have observed to the events that we have observed to the total number of events we have seen in total number of events we have seen in that we have in that input that we have in that input text on the fourth column over here text on the fourth column over here what we have done is we have actually what we have done is we have actually taken this i ofx formula and we have quoted the", "image_path": "img_data/video_66_chunk_11.jpg"}
{"video": "video_66", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "taken this i ofx formula and we have quoted the formula and we have quoted the for every possible event that u for every possible event that u value of information this value of information this value of information as it turns out is given information as it turns out is given to us in a familiar to us quantity that to us in a familiar to us quantity that is actually called bits and for those is actually called bits and for those who are sort of just starting to who are sort of just starting to realize the connection with information realize the connection with information theory and computer science there are theory and computer science there are two ways that you can calculate the two ways that you can calculate the value information one with the", "image_path": "img_data/video_66_chunk_12.jpg"}
{"video": "video_66", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "two ways that you can calculate the value information one with the value information one with the natural log and in this case it will be natural log and in this case it will be given as nuts and the second is when you use the nuts and the second is when you use the log two which in this case will be log two which in this case will be the measurement is going to be the very the measurement is going to be the very familiar to you quantity called a bit the u going back to that we have for every possible event that we have for every possible event that we can meet a corresponding quantity can meet a corresponding quantity that's is actually called bits the bits", "image_path": "img_data/video_66_chunk_13.jpg"}
{"video": "video_66", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "can meet a corresponding quantity that's is actually called bits the bits that's is actually called bits the bits you can consider to be very you can consider to be very intuitively as the currency the money in intuitively as the currency the money in other words that we are going to be other words that we are going to be paying to acquire that kind of paying to acquire that kind of information that random event and information that random event and you actually you can see here that you actually you can see here that very unlikely characters such as very unlikely characters such as the character q for example or the character q for example or the character j in that specific corpus character j in that specific corpus that we have looked at are that we have looked at are carrying a quite significant number", "image_path": "img_data/video_66_chunk_14.jpg"}
{"video": "video_66", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "that we have looked at are carrying a quite significant number carrying a quite significant number of bits which is the expense that we of bits which is the expense that we have to pay to acquire that have to pay to acquire that information and very frequent information and very frequent characters such as the space symbol over characters such as the space symbol over here is carrying a very small here is carrying a very small expense the information expense the information content of a random event as we will content of a random event as we will discuss a bit later is also present our discuss a bit later is also present our daily lives in as part of", "image_path": "img_data/video_66_chunk_15.jpg"}
{"video": "video_66", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "discuss a bit later is also present our daily lives in as part of daily lives in as part of various information compression various information compression algorithms that are entirely based on algorithms that are entirely based on this fundamental principle why pay this fundamental principle why pay exactly the same number of bits to exactly the same number of bits to carry something that it is very carry something that it is very frequent while you can sort frequent while you can sort of expand far less and carry more of expand far less and carry more number of bits to for events which number of bits to for events which are much less frequent are much less frequent what is really entropy well entropy", "image_path": "img_data/video_66_chunk_16.jpg"}
{"video": "video_66", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "are much less frequent what is really entropy well entropy what is really entropy well entropy is now introduced as some kind of a is now introduced as some kind of a expectation in this case we see here the expectation in this case we see here the sample mean over of all the sample mean over of all the information that we have values of information that we have values of information we have in a probability information we have in a probability distribution and this probability distribution and this probability distribution here obviously is discrete distribution here obviously is discrete it consists of 27 possible events it consists of 27 possible events each event has a value of information each event has a value of information we've seen this in bits and the weight we've seen this in bits and the weight this weighted sum is the probability this weighted sum is the probability of observing that event that formula", "image_path": "img_data/video_66_chunk_17.jpg"}
{"video": "video_66", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "this weighted sum is the probability of observing that event that formula of observing that event that formula is fairly intuitive now and let's kind is fairly intuitive now and let's kind of try to u draw it when we have the of try to u draw it when we have the case where we have a coin tossing in case where we have a coin tossing in the coin tossing experiment we have just the coin tossing experiment we have just a bias coin that we will be tossing and a bias coin that we will be tossing and we will plotting that entropy and make we will plotting that entropy and make some kind of easy conclusions now as to some kind of easy conclusions now as to how this function kind of looks how this function kind of looks i wrote earlier over here a natural log", "image_path": "img_data/video_66_chunk_18.jpg"}
{"video": "video_66", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "how this function kind of looks i wrote earlier over here a natural log i wrote earlier over here a natural log but in fact it is log 2 and it's very but in fact it is log 2 and it's very easy to calculate anything as a base of easy to calculate anything as a base of log two log 2 let's say of x is log two log 2 let's say of x is log 10 of x ided by log 10 of 2 any log 10 of x ided by log 10 of 2 any calculator can be used to calculate the calculator can be used to calculate the log with respect to base two and now log with respect to base two and now moving on let's discuss a little bit moving on let's discuss a little bit about this experiment in this experiment about this experiment in this experiment we have a bias causing toin causing we have a bias causing toin causing we have a bent coin that generates kind", "image_path": "img_data/video_66_chunk_19.jpg"}
{"video": "video_66", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "we have a bias causing toin causing we have a bent coin that generates kind we have a bent coin that generates kind of random events let's say heads and tails events let's say heads and tails according to some distribution which according to some distribution which we all know is called the bernui we all know is called the bernui distribution these are all beri distribution these are all beri distributed events that we have distributed events that we have also seen in a review of probability also seen in a review of probability theory that type of distribution and if we are to sort distribution and if we are to sort of plot the entropy which is the degree of plot the entropy which is the degree of uncertainty carry some kind of degree", "image_path": "img_data/video_66_chunk_20.jpg"}
{"video": "video_66", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "of plot the entropy which is the degree of uncertainty carry some kind of degree of uncertainty carry some kind of degree of uncertainty with respect to our of uncertainty with respect to our events over here we will come up with a events over here we will come up with a plot this we will see we plot this and this we will see we plot this and then plot below it the then plot below it the formula this is something formula this is something that the this plot starts making some sense this plot starts making some sense now we will introduce why it is shaped now we will introduce why it is shaped the way it is shaped if we are to", "image_path": "img_data/video_66_chunk_21.jpg"}
{"video": "video_66", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "now we will introduce why it is shaped the way it is shaped if we are to the way it is shaped if we are to sort of write the equation for that sort of write the equation for that plot we will write the following plot we will write the following the entropy h of p is the some the entropy h of p is the some minus the summation over two types of minus the summation over two types of events we have here either we have a events we have here either we have a head or a tails the probability of head or a tails the probability of this event and let me just write here this event and let me just write here a little bit better this i that belongs to the set h better this i that belongs to the set h tals times log base 2 1us p of i well", "image_path": "img_data/video_66_chunk_22.jpg"}
{"video": "video_66", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "better this i that belongs to the set h tals times log base 2 1us p of i well tals times log base 2 1us p of i well that's not entirely correct because the that's not entirely correct because the minus is accounted with this with minus is accounted with this minus sign this is p of i over this minus sign this is p of i over here all right let's expand here all right let's expand it this minus the parenthesis of the probability of heads parenthesis of the probability of heads let's write this now explicitly log two let's write this now explicitly log two probability of hs plus the probability of hs plus the probability of tails well the probability of tails", "image_path": "img_data/video_66_chunk_23.jpg"}
{"video": "video_66", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "probability of hs plus the probability of tails well the probability of tails well the probability of tails is 1 minus probability of heads log two is 1 minus probability of heads log two of probability of heads we'll see this equation again heads we'll see this equation again when we are actually discussing binary when we are actually discussing binary classification and if you actually classification and if you actually plot it then over here we have the plot it then over here we have the capital h and over here we have capital h and over here we have probability of heads and we see here probability of heads and we see here the range of the probability of heads the range of the probability of heads evidently between 0 and one", "image_path": "img_data/video_66_chunk_24.jpg"}
{"video": "video_66", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "the range of the probability of heads evidently between 0 and one and when we have probability of h equal to zero then everything equal to zero then everything we have observed is all we have observed is all tals this is the sort of what the experiment will the sort of what the experiment will result on all the events and evidently result on all the events and evidently over here as you can guess these are over here as you can guess these are going to be all heads coming back to this kind of heads coming back to this kind of question about h and tunnel now you can question about h and tunnel now you can kind of get the intuition why pay any", "image_path": "img_data/video_66_chunk_25.jpg"}
{"video": "video_66", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "question about h and tunnel now you can kind of get the intuition why pay any kind of get the intuition why pay any money in bits to acquire and almost money in bits to acquire and almost actually not almost it's a certain actually not almost it's a certain event of tales you live in a world event of tales you live in a world that you never seen anything other than that you never seen anything other than tales and correspondingly why pay any money to correspondingly why pay any money to observe anything in a world of all observe anything in a world of all heads anything else on than heads you heads anything else on than heads you will actually no surprise that the will actually no surprise that the entropy in these two extremes are is", "image_path": "img_data/video_66_chunk_26.jpg"}
{"video": "video_66", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "will actually no surprise that the entropy in these two extremes are is entropy in these two extremes are is actually going to zero it's going to be actually going to zero it's going to be zero and the maximum zero and the maximum entropy is really here at this at the entropy is really here at this at the middle point and really when the middle point and really when the probability of hge is equal 1/2 this is probability of hge is equal 1/2 this is the point where you it's the maximum the point where you it's the maximum possible point where the you at that possible point where the you at that point you have the maximum possible and point you have the maximum possible and certainty as what is coming next certainty as what is coming next not surpris that this is going to be not surpris that this is going to be also be the maximum possible", "image_path": "img_data/video_66_chunk_27.jpg"}
{"video": "video_66", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "not surpris that this is going to be also be the maximum possible entropy and any deviation from this point are to the left or to the right point are to the left or to the right will have a slightly greater will have a slightly greater proportion of tails or heads and then proportion of tails or heads and then the entropy will actually start to the entropy will actually start to decrease and all the way to zero decrease and all the way to zero that is a nice kind of diagram to that is a nice kind of diagram to always have in the back of your minds in always have in the back of your minds in order for us to connect now this entropy order for us to connect now this entropy with loss function and we'll now see how", "image_path": "img_data/video_66_chunk_28.jpg"}
{"video": "video_66", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "order for us to connect now this entropy with loss function and we'll now see how with loss function and we'll now see how we do that next let's now extend the concept of next let's now extend the concept of entropy into what we actually call entropy into what we actually call relative entropy in entropy we have one relative entropy in entropy we have one probability distribution in relative probability distribution in relative entropy we have two as we can entropy we have two as we can actually see here on the page we have actually see here on the page we have probability distributions that are probability distributions that are involved in this relative entropy involved in this relative entropy formula which is p of x and q of x formula which is p of x and q of x these are two probability distributions these are two probability distributions and we will qualify them short shortly and we will qualify them short shortly when we start now putting some", "image_path": "img_data/video_66_chunk_29.jpg"}
{"video": "video_66", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "and we will qualify them short shortly when we start now putting some when we start now putting some probabilistic notions into the vaping probabilistic notions into the vaping block diagram and as you can actually block diagram and as you can actually see for the formula this in the see for the formula this in the intuition behind this formula is that intuition behind this formula is that is also called kl that is also called kl divergence the there are two divergence the there are two probability distributions if these probability distributions if these two are very close to each other in two are very close to each other in fact on top of each other then the kl fact on top of each other then the kl divergence or relative entropy is going divergence or relative entropy is going to be zero when they are really far away to be zero when they are really far away from each other they are the kale", "image_path": "img_data/video_66_chunk_30.jpg"}
{"video": "video_66", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "from each other they are the kale divergence is actually a large positive divergence is actually a large positive number or any positive number the number or any positive number the in fact the distributions could in fact the distributions could be on top of each other but they are be on top of each other but they are could be complete different shapes as could be complete different shapes as you see in the figure below and still you see in the figure below and still have a quite significant k divergence have a quite significant k divergence between the two treated in some kind between the two treated in some kind of a probabilistic distance for now of a probabilistic distance for now and this distance is going to u be very and this distance is going to u be very useful to us after we introduced the", "image_path": "img_data/video_66_chunk_31.jpg"}
{"video": "video_66", "start": "0:16:00", "end": "0:16:16.766667", "timestamp": "0:16:00 - 0:16:16.766667", "text": "and this distance is going to u be very useful to us after we introduced the useful to us after we introduced the probabilistic vaping block probabilistic vaping block version of the vaping block diagram and version of the vaping block diagram and a very foundational concept called a very foundational concept called maximum likelihood this will actually maximum likelihood this will actually will do next let's now put the probability next let's now put the probability distributions", "image_path": "img_data/video_66_chunk_32.jpg"}
{"video": "video_67", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "this data set consists of 10 examples all from y x1 y1 x2 y2 examples all from y x1 y1 x2 y2 all the way to x10 ym y10 and this is a very u simple ym y10 and this is a very u simple lature to understand we have variable lature to understand we have variable x that represents let's say a square x that represents let's say a square footage of a house and why that footage of a house and why that represent let's say the price of a house represent let's say the price of a house and we are asked to predict the price", "image_path": "img_data/video_67_chunk_0.jpg"}
{"video": "video_67", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "represent let's say the price of a house and we are asked to predict the price and we are asked to predict the price of a house for a house that we have of a house for a house that we have never seen before and in this case since never seen before and in this case since the only feature available to us is the only feature available to us is square footage which is on the x-axis square footage which is on the x-axis this is the house that we never seen this is the house that we never seen before let's say we given all of before let's say we given all of these data points but this one we're these data points but this one we're not given no one is giving this we not given no one is giving this we are going to predicting the price of the are going to predicting the price of the house for that data point where house for that data point where about how the y will actually be that is", "image_path": "img_data/video_67_chunk_1.jpg"}
{"video": "video_67", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "house for that data point where about how the y will actually be that is about how the y will actually be that is really the problem and as we said really the problem and as we said earlier in another video we will call earlier in another video we will call these predictions why hats and according these predictions why hats and according to the vapnik block diagram the block to the vapnik block diagram the block diagram we have seen in the earlier diagram we have seen in the earlier video we first thing that we have to video we first thing that we have to do is to decide on the hypothesis set to decide on the hypothesis set and i'm actually going to start every and i'm actually going to start every everyone is entitled to a valid", "image_path": "img_data/video_67_chunk_2.jpg"}
{"video": "video_67", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "and i'm actually going to start every everyone is entitled to a valid everyone is entitled to a valid hypothesis i'm going to start with a hypothesis i'm going to start with a valid hypothesis let's valid hypothesis let's assume that i believe that the function assume that i believe that the function f is that underl this data set is f is that underl this data set is this straight line i'm going to this straight line i'm going to be calling this function be calling this function g0 and i'm going to further qualify g0 and i'm going to further qualify it as g0 of x comma", "image_path": "img_data/video_67_chunk_3.jpg"}
{"video": "video_67", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "g0 and i'm going to further qualify it as g0 of x comma w is w0 and every hypothesis is of w0 and every hypothesis is of course g is able to give us a prediction why g is able to give us a prediction why hat this is basically the hat this is basically the equation of my first hypothesis is equation of my first hypothesis is it a valid hypothesis yes it is for it a valid hypothesis yes it is for many of you", "image_path": "img_data/video_67_chunk_4.jpg"}
{"video": "video_67", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "it a valid hypothesis yes it is for many of you it probably will be a you're now it probably will be a you're now thinking that this is actually a too thinking that this is actually a too simple hypothesis but just to remind simple hypothesis but just to remind you that the computer is not able you that the computer is not able to plot the computer is actually pretty to plot the computer is actually pretty stupid it will actually we need to teach stupid it will actually we need to teach the computer how to come up with a good the computer how to come up with a good enough hypothesis for our problem and enough hypothesis for our problem and for now as far as the computer is for now as far as the computer is concerned g0 is as good as any concerned g0 is as good as any hypothesis all right that is one", "image_path": "img_data/video_67_chunk_5.jpg"}
{"video": "video_67", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "concerned g0 is as good as any hypothesis all right that is one hypothesis all right that is one hypothesis let's now move on into hypothesis let's now move on into get another hypothesis let's plot get another hypothesis let's plot another hypothesis let's plot the another hypothesis let's plot the hypothesis maybe this one someone say hypothesis maybe this one someone say someone says that's another valid someone says that's another valid hypothesis i'll call this one to hypothesis i'll call this one to distinguish from the other one distinguish from the other one g1 the according to this g1 we will be according to this g1 we will be making predictions why hats this g1", "image_path": "img_data/video_67_chunk_6.jpg"}
{"video": "video_67", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "according to this g1 we will be making predictions why hats this g1 making predictions why hats this g1 is that can now be written as w0 + is that can now be written as w0 + w1x and i hope you remember the equation w1x and i hope you remember the equation of a straight line mx plus b is exactly of a straight line mx plus b is exactly the same equation now we are not really the same equation now we are not really constraining the hypothesis to be constraining the hypothesis to be parallel to the x-axis but we have parallel to the x-axis but we have introduced another parameter introduced another parameter w1 and we started using actually the w1 and we started using actually the features that someone is giving us to features that someone is giving us to draw that hypothesis to", "image_path": "img_data/video_67_chunk_7.jpg"}
{"video": "video_67", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "features that someone is giving us to draw that hypothesis to draw that hypothesis to define this hypothesis the define this hypothesis the hypothesis that we have up to now i hypothesis that we have up to now i hope you realize that these are an hope you realize that these are an infinite we can generate an infinite we can generate an infinite set of lines even just staying a set of lines even just staying a little bit only in the first little bit only in the first hypothesis let's say the first one hypothesis let's say the first one we have defined by varing w0 which we have defined by varing w0 which is the only knob that someone is", "image_path": "img_data/video_67_chunk_8.jpg"}
{"video": "video_67", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "we have defined by varing w0 which is the only knob that someone is the only knob that someone is actually giving us we can actually plot actually giving us we can actually plot an infinite number of parallel lines to an infinite number of parallel lines to the x-axis and this is the reason why i the x-axis and this is the reason why i actually call this w0 parameters because actually call this w0 parameters because we are actually going to be able to we are actually going to be able to vary them and vary them intelligently vary them and vary them intelligently later on in the same way by varing later on in the same way by varing w0 and w1 at the same time we are able w0 and w1 at the same time we are able to draw any straight line parallel to draw any straight line parallel to the x-axis not parallel to the", "image_path": "img_data/video_67_chunk_9.jpg"}
{"video": "video_67", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "to draw any straight line parallel to the x-axis not parallel to the x-axis not parallel to the x-axis any with any intercept and any x-axis any with any intercept and any slope all right any intercept and any slope all right now what we can actually can do is now what we can actually can do is we can continue and let's see that we can continue and let's see that someone suggest another hypothesis and the other hypothesis is as follows w0 + is as follows w0 + w1x + w2 x^2", "image_path": "img_data/video_67_chunk_10.jpg"}
{"video": "video_67", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "is as follows w0 + w1x + w2 x^2 now you're going to say hold on a second now you're going to say hold on a second what is really this term over here what is really this term over here let me just draw it and then kind of let me just draw it and then kind of explain it maybe the other hypothesis explain it maybe the other hypothesis will look this g2 let me call it and in this g2 let me call it and in this cas hypothesis we have actually cas hypothesis we have actually started introducing effectively new started introducing effectively new features the only thing that features the only thing that someone gave us to us in our data set is", "image_path": "img_data/video_67_chunk_11.jpg"}
{"video": "video_67", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "features the only thing that someone gave us to us in our data set is someone gave us to us in our data set is x but over here we see some function x but over here we see some function of x let's say x s or x byx and of x let's say x s or x byx and we are going to suggest that this we are going to suggest that this hypothesis is also valid and with hypothesis is also valid and with this hypothesis we are definitely this hypothesis we are definitely expecting these equations to expecting these equations to draw quadratic lines it's a quadratic lines it's a quadratic function something that could function something that could potentially be drawn for specific", "image_path": "img_data/video_67_chunk_12.jpg"}
{"video": "video_67", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "function something that could potentially be drawn for specific potentially be drawn for specific values of w0 w1 and w2 in fact we can values of w0 w1 and w2 in fact we can actually draw an infinite set of lines actually draw an infinite set of lines which are now going to be more which are now going to be more complicated than just a straight line complicated than just a straight line and we can actually continue all the and we can actually continue all the way to whyat let's say let me call this g9 of x comma w say let me call this g9 of x comma w and here i'm going to and here i'm going to have a very interesting", "image_path": "img_data/video_67_chunk_13.jpg"}
{"video": "video_67", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "and here i'm going to have a very interesting hypothesis in this hypothesis we will let me draw it hypothesis we will let me draw it could be a hypothesis that goes through all of a hypothesis that goes through all of the points that someone gave me i the points that someone gave me i do it approximately obviously and do it approximately obviously and this hypothesis is the point where we", "image_path": "img_data/video_67_chunk_14.jpg"}
{"video": "video_67", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "do it approximately obviously and this hypothesis is the point where we this hypothesis is the point where we actually stop and i think we drew actually stop and i think we drew enough of them and now we actually need enough of them and now we actually need to move now to the discussion on how are to move now to the discussion on how are we going to evaluate them some of you we going to evaluate them some of you may already have plotted in your mind may already have plotted in your mind the sinusoidal hypothesis which the sinusoidal hypothesis which is the final hypothesis but mind you is the final hypothesis but mind you that this line which is actually also that this line which is actually also shown in the notes you should not shown in the notes you should not you just forget it because this you just forget it because this green line in your notes is the target", "image_path": "img_data/video_67_chunk_15.jpg"}
{"video": "video_67", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "you just forget it because this green line in your notes is the target green line in your notes is the target function and the machine has absolutely function and the machine has absolutely no information about this target no information about this target function and than the fact that function and than the fact that it exists before we understand exactly how exists before we understand exactly how we are going to go about designing this we are going to go about designing this loss function it's our responsibility loss function it's our responsibility by the way to design the loss function by the way to design the loss function and we will need to just do some kind of and we will need to just do some kind of thinking here ultimately as we thinking here ultimately as we mentioned we need to mentioned we need to design a predictor that will", "image_path": "img_data/video_67_chunk_16.jpg"}
{"video": "video_67", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "mentioned we need to design a predictor that will design a predictor that will actually produce good results good prediction produce good results good prediction results now we need to understand what results now we need to understand what this good means but for x's that we this good means but for x's that we have never seen before let's pick one have never seen before let's pick one x over here this x this is i will x over here this x this is i will call this the -called x call this the -called x new and i just want to understand to new and i just want to understand to make you understand exactly how a make you understand exactly how a predictions will be done for predictions will be done for that x new i have various hypothesis", "image_path": "img_data/video_67_chunk_17.jpg"}
{"video": "video_67", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "predictions will be done for that x new i have various hypothesis that x new i have various hypothesis well for hypothesis g0 i will look up well for hypothesis g0 i will look up this x new i'll put this inside that this x new i'll put this inside that kind of function g0 and given the w0 kind of function g0 and given the w0 which i have right now drawn i will look up have right now drawn i will look up the y hat well the y hat is exactly the y hat well the y hat is exactly at the same place as where the w0 that's at the same place as where the w0 that's my y hat for the sort of zero my y hat for the sort of zero hypothesis that is let's say", "image_path": "img_data/video_67_chunk_18.jpg"}
{"video": "video_67", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "my y hat for the sort of zero hypothesis that is let's say hypothesis that is let's say for the y hat for the g1 for the y hat for the g1 hypothesis is approximately over here hypothesis is approximately over here this is another white hat all right this is another white hat all right and how about the g2 well you and how about the g2 well you probably guessed it is right probably guessed it is right here that's my wife hat and as far as here that's my wife hat and as far as the g9 is concerned is actually over the g9 is concerned is actually over here it is far away from everything", "image_path": "img_data/video_67_chunk_19.jpg"}
{"video": "video_67", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "the g9 is concerned is actually over here it is far away from everything else i hope it's not a too busy kind of diagram you can actually see exactly diagram you can actually see exactly what i have done over here all right what i have done over here all right every g parametric functions give every g parametric functions give us the opportunity to do as many us the opportunity to do as many predictions as we want and now predictions as we want and now that we have seen exactly how we'll do predictions", "image_path": "img_data/video_67_chunk_20.jpg"}
{"video": "video_67", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "that we have seen exactly how we'll do predictions exactly how we'll do predictions i think it's worthwhile mentioning i think it's worthwhile mentioning that what is really our that what is really our intuition as far as the a intuition as far as the a good predictor is a good intuitive way good predictor is a good intuitive way of thinking about the loss function is of thinking about the loss function is the following since someone gives us the following since someone gives us some examples maybe we can just use some examples maybe we can just use those examples that evidently have those examples that evidently have a label to gauge how well how close a label to gauge how well how close to the existing data our predictions are", "image_path": "img_data/video_67_chunk_21.jpg"}
{"video": "video_67", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "a label to gauge how well how close to the existing data our predictions are to the existing data our predictions are if they are close enough then that if they are close enough then that function should return some scaler back function should return some scaler back to us and if they are far away the to us and if they are far away the that scalar should actually be larger that scalar should actually be larger than the previous case when they were than the previous case when they were clos that's the intuitive way clos that's the intuitive way of understanding loss functions how they of understanding loss functions how they will behave let's see now what is will behave let's see now what is a good way of sort of providing a good way of sort of providing this", "image_path": "img_data/video_67_chunk_22.jpg"}
{"video": "video_67", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "a good way of sort of providing this mathematically what we describe with mathematically what we describe with words now we'll put down this thing words now we'll put down this thing in a math equation that defines this in a math equation that defines this kind of loss function definitely we kind of loss function definitely we expect this loss function that i will expect this loss function that i will symbolize with l the caligraphic l to be symbolize with l the caligraphic l to be a function of two things the first one a function of two things the first one is on one hand our predictions the y is on one hand our predictions the y hats and on the other will be the ground hats and on the other will be the ground roots or our labels y roots or our labels y and as we kind of just", "image_path": "img_data/video_67_chunk_23.jpg"}
{"video": "video_67", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "roots or our labels y and as we kind of just and as we kind of just described we expect to see over here described we expect to see over here some form of a distance between let's some form of a distance between let's say our prediction white hat for the say our prediction white hat for the specific u example that we have specific u example that we have the ath example and some form of the ath example and some form of distance between that prediction and distance between that prediction and the ground truth as we discussed", "image_path": "img_data/video_67_chunk_24.jpg"}
{"video": "video_67", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "distance between that prediction and the ground truth as we discussed the ground truth as we discussed if that difference is large then that kind difference is large then that kind of a loss function should return a large of a loss function should return a large number and if that difference is small number and if that difference is small a small number that kind of a small number that kind of goes of is compatible with what we goes of is compatible with what we had just discussed however in some had just discussed however in some instances for some specific examples instances for some specific examples the prediction is a larger number the prediction is a larger number than the ground truth and or vice versa", "image_path": "img_data/video_67_chunk_25.jpg"}
{"video": "video_67", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "the prediction is a larger number than the ground truth and or vice versa than the ground truth and or vice versa we want to at the end of the day have we want to at the end of the day have a number which is kind of positive a number which is kind of positive irrespectively of how the prediction is irrespectively of how the prediction is larger or lesser than the ground larger or lesser than the ground truth what we will do is we will truth what we will do is we will square them and we will square to square them and we will square to convert them into positive to convert them into positive numbers we have now a numbers we have now a difference for every single example", "image_path": "img_data/video_67_chunk_26.jpg"}
{"video": "video_67", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "numbers we have now a difference for every single example difference for every single example that we i care for however we that we i care for however we do not know exactly what to do with do not know exactly what to do with let's say m numbers what do let's say m numbers what do information that conveys to us information that conveys to us probably it would be much better if probably it would be much better if we are to get just a single number out we are to get just a single number out of these differences and in of these differences and in this case it's very easy to convert", "image_path": "img_data/video_67_chunk_27.jpg"}
{"video": "video_67", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "of these differences and in this case it's very easy to convert this case it's very easy to convert m differences to a single number m differences to a single number we're just going to sum over all of our we're just going to sum over all of our examples and normalize by the number of examples we normalize by the number of examples we given and this is really the loss given and this is really the loss function that we are going to use function that we are going to use from now on it made it intercept sense from now on it made it intercept sense to us this function is in fact known as to us this function is in fact known as mean squared error or", "image_path": "img_data/video_67_chunk_28.jpg"}
{"video": "video_67", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "squared error or msse and it is a standard function msse and it is a standard function for regression problems later on we will for regression problems later on we will see that this function that right now see that this function that right now was presented from a kind of intuitive was presented from a kind of intuitive perspective in fact it is derived out perspective in fact it is derived out of a concept that we'll actually call a of a concept that we'll actually call a cross entropy loss function all of cross entropy loss function all of these will actually come a bit later these will actually come a bit later but for now i hope this discussion was but for now i hope this discussion was understood good and this is", "image_path": "img_data/video_67_chunk_29.jpg"}
{"video": "video_67", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "but for now i hope this discussion was understood good and this is understood good and this is this loss functions just any this loss functions just any loss function for us will return back as loss function for us will return back as you understand a single scalar number if you understand a single scalar number if that number is large we are not going to that number is large we are not going to be doing very well because this means be doing very well because this means that we are far away from our that we are far away from our predictions if that number is very small predictions if that number is very small we are doing much better we are doing much better and something else that i wanted", "image_path": "img_data/video_67_chunk_30.jpg"}
{"video": "video_67", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "we are doing much better and something else that i wanted and something else that i wanted to mention also i should also highlight to mention also i should also highlight the fact that behind the y hatat the fact that behind the y hatat is the parameter w behind the y hat is the parameter w behind the y hat there is a hypothesis and therefore sometimes hypothesis and therefore sometimes you actually see this being written as l you actually see this being written as l of w and this is exactly identical of w and this is exactly identical concepts and identical notations", "image_path": "img_data/video_67_chunk_31.jpg"}
{"video": "video_67", "start": "0:16:00", "end": "0:16:14.466667", "timestamp": "0:16:00 - 0:16:14.466667", "text": "of w and this is exactly identical concepts and identical notations concepts and identical notations in many instances we do want to in many instances we do want to write it this because we want to write it this because we want to emphasize the fact that by changing emphasize the fact that by changing these w parameters we are actually going these w parameters we are actually going to be changing the observed law", "image_path": "img_data/video_67_chunk_32.jpg"}
{"video": "video_68", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now that we have seen how hypothesis are formed we are actually need to go formed we are actually need to go back to the vapnik block diagram and back to the vapnik block diagram and actually start addressing one key aspect actually start addressing one key aspect of it which is the optimization block of it which is the optimization block over here we've seen how the loss over here we've seen how the loss function what is the loss function is function what is the loss function is with regularization we've seen how we can regularization we've seen how we can potentially create various kind of potentially create various kind of hypothesis and but we have not really", "image_path": "img_data/video_68_chunk_0.jpg"}
{"video": "video_68", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "potentially create various kind of hypothesis and but we have not really hypothesis and but we have not really discussed how we actually got this w discussed how we actually got this w star how the optimal set of parameters star how the optimal set of parameters got to be determined and mind you that got to be determined and mind you that this whole discussion happened without this whole discussion happened without really quoting even the any really quoting even the any probabilistic kind of concept we will probabilistic kind of concept we will finish this discussion first and by finish this discussion first and by visiting an optimization alg called visiting an optimization alg called stochastic gr decent and then actually stochastic gr decent and then actually we'll go back into this diagram and we'll go back into this diagram and start introducing again the same", "image_path": "img_data/video_68_chunk_1.jpg"}
{"video": "video_68", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "we'll go back into this diagram and start introducing again the same start introducing again the same concepts but in this time we will be concepts but in this time we will be discussing this learning approach discussing this learning approach from a probabilistic standpoint from a probabilistic standpoint probabilities are going to be at the probabilities are going to be at the core of our learning agents from that core of our learning agents from that point onwards the optimization point onwards the optimization algorithm we're going to do see now algorithm we're going to do see now is called gradient descent and just is called gradient descent and just to motivate it i have actually drawn to motivate it i have actually drawn over here a simple function l of w over here a simple function l of w and this simple function is given by and this simple function is given by this expression of here is w s is a you", "image_path": "img_data/video_68_chunk_2.jpg"}
{"video": "video_68", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "and this simple function is given by this expression of here is w s is a you this expression of here is w s is a perhaps the know perhaps the simplest convex function you can simplest convex function you can actually have and just to remind actually have and just to remind everyone about the concept of a everyone about the concept of a tangent line at a point here we see tangent line at a point here we see the tangent that we touches if you the tangent that we touches if you this function at this specific this function at this specific point and the i hope you point and the i hope you everyone remembers the simple everyone remembers the simple gradients simple derivative", "image_path": "img_data/video_68_chunk_3.jpg"}
{"video": "video_68", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "everyone remembers the simple gradients simple derivative gradients simple derivative from sort of your earlier kind of from sort of your earlier kind of studies and if not we have sort of studies and if not we have sort of links in the math background links in the math background module the u this simple d module the u this simple d derivative of i actually use here derivative of i actually use here partial derivatives because typically we partial derivatives because typically we have we are going to have more than one have we are going to have more than one variable is approximated by the ratio variable is approximated by the ratio of delta l by delta w here we see of delta l by delta w here we see delta l sorry delta w", "image_path": "img_data/video_68_chunk_4.jpg"}
{"video": "video_68", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "of delta l by delta w here we see delta l sorry delta w delta l sorry delta w represents the u how much we wiggle represents the u how much we wiggle the parameter w around that kind of the parameter w around that kind of point and we observe how much the point and we observe how much the function changes as we vary the u the function changes as we vary the u the that parameter by delta w that is that parameter by delta w that is going to be approximating for going to be approximating for us the simple d derivative and we can", "image_path": "img_data/video_68_chunk_5.jpg"}
{"video": "video_68", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "going to be approximating for us the simple d derivative and we can us the simple d derivative and we can actually calculate that simple actually calculate that simple derivative in general by plugging in the l general by plugging in the l and we calculate the par partial and we calculate the par partial derivative of the w squ with respect to derivative of the w squ with respect to w and from lookup tables this is w and from lookup tables this is effectively partial derivative lookup effectively partial derivative lookup tables which are widely available and tables which are widely available and we are going to provide in the notes a we are going to provide in the notes a link for you that you are familiar", "image_path": "img_data/video_68_chunk_6.jpg"}
{"video": "video_68", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "we are going to provide in the notes a link for you that you are familiar link for you that you are familiar with such a lookup table because we'll with such a lookup table because we'll be using these lookup tables over and be using these lookup tables over and over again you can actually see that over again you can actually see that this partial derivative is evaluated to this partial derivative is evaluated to w and if you notice as we are w and if you notice as we are going to approach approaching this going to approach approaching this global minimum over here in this kind of global minimum over here in this kind of function and you can imagine that kind function and you can imagine that kind of tangent as the point is moving of tangent as the point is moving towards if you the global minimum", "image_path": "img_data/video_68_chunk_7.jpg"}
{"video": "video_68", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "of tangent as the point is moving towards if you the global minimum towards if you the global minimum location the tangent and the slope of location the tangent and the slope of that kind of tangent is becoming that kind of tangent is becoming zero we can actually given that zero we can actually given that this is the slope of that tangent this is the slope of that tangent line we are going to set the line we are going to set the partial derivative of l with respect to partial derivative of l with respect to now w star now this w star is now w star now this w star is notation wise represents the optimal notation wise represents the optimal point or the global minimal point in", "image_path": "img_data/video_68_chunk_8.jpg"}
{"video": "video_68", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "notation wise represents the optimal point or the global minimal point in point or the global minimal point in this case and equal to zero and this case and equal to zero and that's a foundational theorem in that's a foundational theorem in calculus and from that equation we can calculus and from that equation we can solve for w star and find the as we solve for w star and find the as we expected that global minima to be at expected that global minima to be at zero this is was a simple u zero this is was a simple u characterization of potentially u characterization of potentially u expected behavior from an algorith this expected behavior from an algorith this algorith will actually function as algorith will actually function as follows it will start from some", "image_path": "img_data/video_68_chunk_9.jpg"}
{"video": "video_68", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "algorith will actually function as follows it will start from some follows it will start from some initial random point and it will initial random point and it will actually gradually move in small increments gradually move in small increments towards the minimum and actually the analogy that we minimum and actually the analogy that we can actually can make is that you are on can actually can make is that you are on the top of the mountain and then there the top of the mountain and then there is a lot of fog around you and the is a lot of fog around you and the and you want to go down the and you want to go down the mountain you will have two options mountain you will have two options one is to jump into the unknown at the one is to jump into the unknown at the expense of being of getting killed and", "image_path": "img_data/video_68_chunk_10.jpg"}
{"video": "video_68", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "one is to jump into the unknown at the expense of being of getting killed and expense of being of getting killed and the second approach is to since the second approach is to since you can see a couple of feet around you can see a couple of feet around you can actually make the full a you can actually make the full a small step and when you reach that small step and when you reach that next step you see again around you make next step you see again around you make another step and the only thing that you another step and the only thing that you need to make sure is that these steps need to make sure is that these steps are leading to a lower elevation are leading to a lower elevation as your to towards your target to as your to towards your target to getting down the mountain similar things", "image_path": "img_data/video_68_chunk_11.jpg"}
{"video": "video_68", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "as your to towards your target to getting down the mountain similar things getting down the mountain similar things are actually happening over here we are actually happening over here we are going now to see how this we can are going now to see how this we can express that as an equation how this express that as an equation how this approach can be expressed as an equation approach can be expressed as an equation and explain that the equation for and explain that the equation for the gradient decent is the gradient decent is the following i am writing now the following i am writing now the equation with on leftand site has the equation with on leftand site has the next value of the parameter w this next value of the parameter w this is the w the initial w this is the", "image_path": "img_data/video_68_chunk_12.jpg"}
{"video": "video_68", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "next value of the parameter w this is the w the initial w this is the w the initial w this is the next parameter of the w is the next parameter of the w is the previous one wk minus a hyper parameter that we be wk minus a hyper parameter that we be calling the learning rate and it's actually a small number times the gradient don't be alarmed times the gradient don't be alarmed by this kind of nasty by this kind of nasty symbol and this is", "image_path": "img_data/video_68_chunk_13.jpg"}
{"video": "video_68", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "by this kind of nasty symbol and this is the gradient of the loss is the gradient of the loss function with respect to the function with respect to the parameter w or the set of parameters w parameter w or the set of parameters w when we have many parameters these when we have many parameters these are actually becoming vectors and that's are actually becoming vectors and that's why we actually go need to go into why we actually go need to go into the next level up instead of the next level up instead of a simple partial derivative we have a simple partial derivative we have a vector of partial derivatives this is vector of partial derivatives this is actually called the gradient", "image_path": "img_data/video_68_chunk_14.jpg"}
{"video": "video_68", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "vector of partial derivatives this is actually called the gradient of the loss function with respect to the function with respect to the parameter vector w and this gradient is as i said the generalization of the partial derivative is a vector where we have for a given for", "image_path": "img_data/video_68_chunk_15.jpg"}
{"video": "video_68", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "derivative is a vector where we have for a given for vector where we have for a given vector of parameters that consider say of w0 all the way to w let's say all the way to w let's say m we have the partial derivative of the loss the partial derivative of the loss function or in any case any function or in any case any function with respect to w0 all the way par with respect to w0 all the way par derivative of the loss function with derivative of the loss function with respect to", "image_path": "img_data/video_68_chunk_16.jpg"}
{"video": "video_68", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "derivative of the loss function with respect to wm that is the sort of functional wm that is the sort of functional form of the grn algorithm in other words form of the grn algorithm in other words the gradient kind of the gradient kind of generalizes the direction in the simple case where we direction in the simple case where we have a sybol a single dimension that have a sybol a single dimension that direction is given by the slope of this direction is given by the slope of this tangent line now that we have in general tangent line now that we have in general multiple directions we are going to multiple directions we are going to have multiple partial derivatives", "image_path": "img_data/video_68_chunk_17.jpg"}
{"video": "video_68", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "multiple directions we are going to have multiple partial derivatives have multiple partial derivatives if you can imagine i'm sure everyone has if you can imagine i'm sure everyone has used their hands to drink water used their hands to drink water from a water source as you are from a water source as you are forming your hand you create kind forming your hand you create kind of create a cavity trying to create a of create a cavity trying to create a cavity you have effectively two cavity you have effectively two dimensions one is along the direction of dimensions one is along the direction of your fingers and the other is the other your fingers and the other is the other direction over there perpendicular to direction over there perpendicular to this these are basically a simple", "image_path": "img_data/video_68_chunk_18.jpg"}
{"video": "video_68", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "direction over there perpendicular to this these are basically a simple this these are basically a simple convex three-dimensional function convex three-dimensional function that has been formed it has two that has been formed it has two coordinates one is a w1 and the other is coordinates one is a w1 and the other is a w2 and this is just a simple w2 and this is just a simple analogy to always remember when you're analogy to always remember when you're dealing with multivariate functions and dealing with multivariate functions and the partial derivative of using the first parameter derivative of using the first parameter and the partial derivative using the and the partial derivative using the second parameter will define that second parameter will define that direction in the three-dimensional now direction in the three-dimensional now space we'll see that in a notebook as", "image_path": "img_data/video_68_chunk_19.jpg"}
{"video": "video_68", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "direction in the three-dimensional now space we'll see that in a notebook as space we'll see that in a notebook as an example a bit later how this an example a bit later how this stochastic gr is cent or sorry gred stochastic gr is cent or sorry gred descent he has not become descent he has not become stochastic yet is able to find that stochastic yet is able to find that kind of a minimum unfortunately for us we were minimum unfortunately for us we were never going to have this simple never going to have this simple situation of convex functions our situation of convex functions our loss functions are going to be very loss functions are going to be very complicated and will involve you complicated and will involve billions and billions of parameters", "image_path": "img_data/video_68_chunk_20.jpg"}
{"video": "video_68", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "complicated and will involve you know billions and billions of parameters know billions and billions of parameters and here in this page we can and here in this page we can actually see a great simplification of actually see a great simplification of what is going to happen our algorith what is going to happen our algorith is going to start from some kind of is going to start from some kind of random point let's say this point over random point let's say this point over here and will start going down in small here and will start going down in small increments as we discussed and it actually may get discussed and it actually may get stuck into a local minima and these stuck into a local minima and these minima are plenty and actually we minima are plenty and actually we never going to have even the", "image_path": "img_data/video_68_chunk_21.jpg"}
{"video": "video_68", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "minima are plenty and actually we never going to have even the never going to have even the possibility of recognizing whether we possibility of recognizing whether we have we are got stuck in this local have we are got stuck in this local minima which could be pretty bad minima which could be pretty bad this local minima is much larger this local minima is much larger than the minima which is the global than the minima which is the global minima that is at this point over here minima that is at this point over here and in fact much larger even what and in fact much larger even what is call a good local minima we have is call a good local minima we have bad local minima and u good local minima bad local minima and u good local minima the at test we would strive to u", "image_path": "img_data/video_68_chunk_22.jpg"}
{"video": "video_68", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "bad local minima and u good local minima the at test we would strive to u the at test we would strive to u optimize to come up with a value of optimize to come up with a value of parameter that corresponds to a good parameter that corresponds to a good local minima it's very unlikely it will local minima it's very unlikely it will reach if you a global minima that reach if you a global minima that it is very difficult to find as we are it is very difficult to find as we are searching effectively in a huge searching effectively in a huge parameter space in general this is something space in general this is something we always have to keep in mind as we always have to keep in mind as we are evaluating the behavior of the evaluating the behavior of the this type of grad descent type of", "image_path": "img_data/video_68_chunk_23.jpg"}
{"video": "video_68", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "evaluating the behavior of the this type of grad descent type of this type of grad descent type of algorithms how the grad descent is it going to help us descent is it going to help us optimize efficiently the loss function or not efficiently the loss function or not and i want to address that kind of issue and i want to address that kind of issue which is associated with how heavy it which is associated with how heavy it is everyone gets to understand that is everyone gets to understand that kind of point which is important is kind of point which is important is actually the pretext for us to introduce actually the pretext for us to introduce a more efficient version of grad desent a more efficient version of grad desent will be calling later stoas gr descent", "image_path": "img_data/video_68_chunk_24.jpg"}
{"video": "video_68", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "a more efficient version of grad desent will be calling later stoas gr descent will be calling later stoas gr descent let's assume that we have a let's assume that we have a hypothesis this hypothesis are very hypothesis this hypothesis are very simple hypoth right now is a just a simple hypoth right now is a just a straight line i hope you remember the straight line i hope you remember the from high school the equation y is from high school the equation y is equal to mx plus b that's exactly the equal to mx plus b that's exactly the hypothesis we have here we have some hypothesis we have here we have some other words data that this hypothesis other words data that this hypothesis make sense for us to use and we make sense for us to use and we have two obviously we have two obviously we have two parameters here we actually having a", "image_path": "img_data/video_68_chunk_25.jpg"}
{"video": "video_68", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "have two obviously we have two parameters here we actually having a parameters here we actually having a vector of dimensionality too and i vector of dimensionality too and i want to write again this kind of want to write again this kind of equation of gred descent wk + 1 is equation of gred descent wk + 1 is equal to w k minus theta the gradient of equal to w k minus theta the gradient of the loss with respect to w and in this loss with respect to w and in this case our loss function and i hope you case our loss function and i hope you remember the loss function we have seen remember the loss function we have seen earlier is the -called mean square", "image_path": "img_data/video_68_chunk_26.jpg"}
{"video": "video_68", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "remember the loss function we have seen earlier is the -called mean square earlier is the -called mean square error the loss function is in other error the loss function is in other words one over m this summation from words one over m this summation from i is equal to 1 to m of u y hat minus y this is the i of hat minus y this is the i of prediction squared and if we replace the prediction squared and if we replace the y with our the shape and form of this", "image_path": "img_data/video_68_chunk_27.jpg"}
{"video": "video_68", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "prediction squared and if we replace the y with our the shape and form of this y with our the shape and form of this hypothesis the summation from i is equal to 1 to m of g of w x i minus y sorry yi squar and this one is definitely squar and this one is definitely equal to 1 /", "image_path": "img_data/video_68_chunk_28.jpg"}
{"video": "video_68", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "1 / m summation from i is = to 1 to m of w0 m summation from i is = to 1 to m of w0 + w1x minus y i this is x ius y i the + w1x minus y i this is x ius y i the whole thing squar what is we need whole thing squar what is we need to do now is to calculate the gradient to do now is to calculate the gradient of this loss function which is ends of this loss function which is ends up being this one with respect to the parameter one with respect to the parameter vector w which as you probably remember vector w which as you probably remember from the definition of the gradient i'm", "image_path": "img_data/video_68_chunk_29.jpg"}
{"video": "video_68", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "vector w which as you probably remember from the definition of the gradient i'm from the definition of the gradient i'm going to write it here at the top corner going to write it here at the top corner over here it is this is the over here it is this is the partial derivative of the loss with partial derivative of the loss with respect to w0 and then the partial respect to w0 and then the partial derivative of the loss with respect to derivative of the loss with respect to w1 this is the calculation we need to do w1 this is the calculation we need to do and actually we need to see how heavy and actually we need to see how heavy this calculation is at the end of the this calculation is at the end of the day we'll do the first calculation of day we'll do the first calculation of the partial derivative of l with respect the partial derivative of l with respect to w0 and it is", "image_path": "img_data/video_68_chunk_30.jpg"}
{"video": "video_68", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "to w0 and it is going to be given by the 1 / m going to be given by the 1 / m summation from i isal to 1 to m the summation from i isal to 1 to m the partial derivative w0 + w1x partial derivative w0 + w1x i minus y i 2 with respect to w0 if we actually going to be to w0 if we actually going to be evaluating partial derivative such as evaluating partial derivative such as this one", "image_path": "img_data/video_68_chunk_31.jpg"}
{"video": "video_68", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "evaluating partial derivative such as this one i think which was sufficient to go i think which was sufficient to go back to this lookup table of partial back to this lookup table of partial derivatives and recall if you an derivatives and recall if you an identity that governs the partial identity that governs the partial derivatives of composite function derivatives of composite function i'm going to just put this in a box i'm going to just put this in a box over here this identity and this over here this identity and this identity reads that the partial identity reads that the partial derivative of a composite function in derivative of a composite function in other words a function g that has other words a function g that has arguments and other function f arguments and other function f ofx with respect to", "image_path": "img_data/video_68_chunk_32.jpg"}
{"video": "video_68", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "arguments and other function f ofx with respect to x it is g prime f ofx times the partial derivative of f ofx times the partial derivative of f ofx with respect to x this is ofx with respect to x this is the definitely the this is the definitely the functional form of this kind of functional form of this kind of partial derivative that we will be using and derivative that we will be using and with a prime over here with a prime over here in many textbooks they are", "image_path": "img_data/video_68_chunk_33.jpg"}
{"video": "video_68", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "with a prime over here in many textbooks they are in many textbooks they are symbolizing the derivative of the function g over derivative of the function g over here the composite function we have is here the composite function we have is this squaring function and inside it this squaring function and inside it we have f ofx this is equivalent to we have f ofx this is equivalent to the this what is inside that kind of the this what is inside that kind of parenthesis we'll apply the formula parenthesis we'll apply the formula and see what we get is and see what we get is 2 *", "image_path": "img_data/video_68_chunk_34.jpg"}
{"video": "video_68", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "and see what we get is 2 * the w0 + w0 + w1 x i - y i times the partial derivative of w0 i times the partial derivative of w0 + w1x ius y i with respect to w0 and + w1x ius y i with respect to w0 and this is simply 2 w0 + w1 x i - y i this is simply 2 w0 + w1 x i - y i because the partial derivative of this", "image_path": "img_data/video_68_chunk_35.jpg"}
{"video": "video_68", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "this is simply 2 w0 + w1 x i - y i because the partial derivative of this because the partial derivative of this is a partial derivative of w0 with is a partial derivative of w0 with respect to w0 which is one and all the other w0 which is one and all the other terms are actually zero and therefore we terms are actually zero and therefore we don't write the one over here we end don't write the one over here we end up with this and this of course came up with this and this of course came from the fact that a partial derivative from the fact that a partial derivative of something which is squared is two of something which is squared is two * that something and this we have * that something and this we have actually already met in that u earlier actually already met in that u earlier discussion that we had about this", "image_path": "img_data/video_68_chunk_36.jpg"}
{"video": "video_68", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "actually already met in that u earlier discussion that we had about this discussion that we had about this nice convex function called nice convex function called w^2 this was basically what w^2 this was basically what we will be getting this internal term we will be getting this internal term with respect to the first parameter and with respect to the first parameter and if we replicate this for the second if we replicate this for the second parameter w1 we will actually have exactly the same thing to w0 + w 1 x i - y i partial", "image_path": "img_data/video_68_chunk_37.jpg"}
{"video": "video_68", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "w1 we will actually have exactly the same thing to w0 + w 1 x i - y i partial same thing to w0 + w 1 x i - y i partial derivative of w0 + w1x i - y i with respect to w1 of w0 + w1x i - y i with respect to w1 now in this case and evidently this is now in this case and evidently this is given by the first term times x i in this case because", "image_path": "img_data/video_68_chunk_38.jpg"}
{"video": "video_68", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "term times x i in this case because over here we have with respect to w1 over here we have with respect to w1 partial derivative of w0 with respect to partial derivative of w0 with respect to w1 is zero and then this of course w1 is zero and then this of course gives us xi and with respect to the gives us xi and with respect to the this is also another zero we end up this is also another zero we end up with this expression over with this expression over here what actually we see here is here what actually we see here is that this calculation is quite heavy in that this calculation is quite heavy in many instances we are dealing with many instances we are dealing with pretty significant data sets in terms of", "image_path": "img_data/video_68_chunk_39.jpg"}
{"video": "video_68", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "many instances we are dealing with pretty significant data sets in terms of pretty significant data sets in terms of hundreds of thousands hundreds of thousands millions if you of samples of millions if you of samples of examples and m is actually typically examples and m is actually typically very large and for every iteration of very large and for every iteration of the gradi descend we will have to the gradi descend we will have to calculate this gradient quantity that calculate this gradient quantity that involves itself a summation over all of involves itself a summation over all of our examples all of the that examples all of the that calculation here is pretty complicated calculation here is pretty complicated we have to go through the whole data", "image_path": "img_data/video_68_chunk_40.jpg"}
{"video": "video_68", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "calculation here is pretty complicated we have to go through the whole data we have to go through the whole data set to let me write this set to let me write this down we need to go through the whole data set let's call this data set of size m of course will be the m train or whatever portion of this data set we whatever portion of this data set we have allocated for training have allocated for training and this is going through", "image_path": "img_data/video_68_chunk_41.jpg"}
{"video": "video_68", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "have allocated for training and this is going through and this is going through the whole data set we call it the whole data set we call it one epoch and for each iteration of the graded descent and we actually of the graded descent and we actually have to go through", "image_path": "img_data/video_68_chunk_42.jpg"}
{"video": "video_68", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "of the graded descent and we actually have to go through many epo in order for us to be able to epo in order for us to be able to when i say many sometimes it will be when i say many sometimes it will be tens of epochs each epoch involves m tens of epochs each epoch involves m examples it is number of epochs times examples it is number of epochs times m of examples that we have to have as a m of examples that we have to have as a total number of iterations i can total number of iterations i can write here the total number of write here the total number of iterations is number of epo", "image_path": "img_data/video_68_chunk_43.jpg"}
{"video": "video_68", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "write here the total number of iterations is number of epo times number of examples which i actually called m i don't need to actually called m i don't need to write this down number of epop times m write this down number of epop times m these are the number of iterations these are the number of iterations we're going to typically have to execute we're going to typically have to execute and this is actually a very large number and this is actually a very large number when m is a very large number and when m is a very large number and they u this means that for every of this they u this means that for every of this each iteration we have to go through one each iteration we have to go through one full sort of a vision it through the", "image_path": "img_data/video_68_chunk_44.jpg"}
{"video": "video_68", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "full sort of a vision it through the sort of a vision it through the whole data set m that's a the whole data set m that's a very expensive and we will be we very expensive and we will be having to sort of find will be having to sort of find another approximate an approximation another approximate an approximation which will end up actually also which will end up actually also offering certain advantages as well offering certain advantages as well we'll see that next in the we'll see that next in the stochastic gr descent instead of having stochastic gr descent instead of having to go through the whole m example", "image_path": "img_data/video_68_chunk_45.jpg"}
{"video": "video_68", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "to go through the whole m example to calculate our gradient of the loss function with respect to w as we function with respect to w as we discussed what we will do is we will discussed what we will do is we will break down the m examples into what we break down the m examples into what we will actually call the mini will actually call the mini batches we're going to have to batches we're going to have to define a mini bat size i will be define a mini bat size i will be calling this mb and this stands for mini", "image_path": "img_data/video_68_chunk_46.jpg"}
{"video": "video_68", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "mb and this stands for mini batch but sometimes people just refer to as batch and we are going to obviously batch and we are going to obviously this mini batch is numbers that could this mini batch is numbers that could be as low as one to as high as we be as low as one to as high as we let's say 4,000 four 4,000 examples let's say 4,000 four 4,000 examples for example it could be larger than for example it could be larger than 4,000 this is typically 4,000 this is typically in this range and in that this", "image_path": "img_data/video_68_chunk_47.jpg"}
{"video": "video_68", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "4,000 this is typically in this range and in that this in this range and in that this will be a hyper parameter mini batch will be a hyper parameter mini batch is going to be a hyper parameter that we is going to be a hyper parameter that we have to optimize in our but it's have to optimize in our but it's typically but optimized as a hyper parameter and this hyper parameter you can", "image_path": "img_data/video_68_chunk_48.jpg"}
{"video": "video_68", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "and this hyper parameter you can actually use optuna or whatever any other tool that you any other tool that you actually have in order for you to actually have in order for you to sort of optimize such hyper parameters sort of optimize such hyper parameters we're going to have definitely we're going to have definitely either an assignment or another video either an assignment or another video actually talks and walks you through actually talks and walks you through this hyper parameter optimization all this hyper parameter optimization all right this is the first right this is the first step is we're going to use for the", "image_path": "img_data/video_68_chunk_49.jpg"}
{"video": "video_68", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "right this is the first step is we're going to use for the step is we're going to use for the gradient calculation only the a much gradient calculation only the a much smaller subset of our data to do smaller subset of our data to do the gradient calculation and this is the gradient calculation and this is the initial algor gr s sometimes the initial algor gr s sometimes actually called full batch and in order for us to be able to make sure that we do not have to make sure that we do not have anything systemic going on in our data what we typically", "image_path": "img_data/video_68_chunk_50.jpg"}
{"video": "video_68", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "anything systemic going on in our data what we typically going on in our data what we typically do is we are going to be sampling what do is we are going to be sampling what kind of examples we are going to be kind of examples we are going to be selecting at every moment in time to selecting at every moment in time to form our mini bats either we have to form our mini bats either we have to let me write this down we sample from the m examples to determine", "image_path": "img_data/video_68_chunk_51.jpg"}
{"video": "video_68", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "examples to determine the examples that will form the mini bats each mini bat i would say and the second thing that we do is say and the second thing that we do is or alternatively that's one way of doing or alternatively that's one way of doing it second way of actually doing it is it second way of actually doing it is that we randomize", "image_path": "img_data/video_68_chunk_52.jpg"}
{"video": "video_68", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "it second way of actually doing it is that we randomize the order of our m examples and we randomize the order of the whole data set that we have of the whole data set that we have and this me and then we basically and this me and then we basically split it up into the integer number split it up into the integer number of mini batches these two obviously are batches these two obviously are equivalent and in most cases we these", "image_path": "img_data/video_68_chunk_53.jpg"}
{"video": "video_68", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "batches these two obviously are equivalent and in most cases we these equivalent and in most cases we these are delegated to the frameworks that are delegated to the frameworks that we are sort of we are employing p we are sort of we are employing p tensor flow and on what we tensor flow and on what we have achieved here with stochastic gr have achieved here with stochastic gr descend let me just provide a descend let me just provide a picture that is always kind of useful to picture that is always kind of useful to visualize what is really going on we visualize what is really going on we have let's say the earlier hypothesis have let's say the earlier hypothesis that we have actually had seen this that we have actually had seen this w0 + w1x we have for this specific", "image_path": "img_data/video_68_chunk_54.jpg"}
{"video": "video_68", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "that we have actually had seen this w0 + w1x we have for this specific w0 + w1x we have for this specific hypothesis g of w comma x that we have hypothesis g of w comma x that we have seen earlier we have two parameters we have earlier we have two parameters we have w0 and w1 we are searching in a w0 and w1 we are searching in a two-dimensional parameter space which is two-dimensional parameter space which is actually shown here in this plane actually shown here in this plane somewhere in this plane there will be an the optimal plane there will be an the optimal w star that we would hope to will be w star that we would hope to will be able to find as of course to the able to find as of course to the constraints that we had already", "image_path": "img_data/video_68_chunk_55.jpg"}
{"video": "video_68", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "able to find as of course to the constraints that we had already constraints that we had already discussed about how easy or difficult it discussed about how easy or difficult it is but in this specific case it should is but in this specific case it should be straightforward because our loss be straightforward because our loss function is well behaved and we are function is well behaved and we are starting as we said earlier from a starting as we said earlier from a -called u -called u w0 mind you not confuse the w0 with w0 mind you not confuse the w0 with the one of the parameters of the w the one of the parameters of the w vector here is the first iteration the vector here is the first iteration the first the starting point of our w vector", "image_path": "img_data/video_68_chunk_56.jpg"}
{"video": "video_68", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "vector here is the first iteration the first the starting point of our w vector first the starting point of our w vector evidently our w vector evidently has this kind of coordinates w1 and has this kind of coordinates w1 and w2 here we have in fact i w2 here we have in fact i should have called them w0 and should have called them w0 and w1 these are our two w1 w0 and w1 these are our two coordinates this is the starting coordinates this is the starting point earlier what we have actually point earlier what we have actually seen is that there will be some there seen is that there will be some kind of a will be some kind of a underlying", "image_path": "img_data/video_68_chunk_57.jpg"}
{"video": "video_68", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "will be some kind of a underlying sort of loss function and i sort of loss function and i should have plotted that over here there should have plotted that over here there will be some minima somewhere let's assume that the loss somewhere let's assume that the loss function is such that the minima is function is such that the minima is actually located here that's our actually located here that's our starting point and we actually earlier we were going into broadly speaking very broadly going into broadly speaking very broadly speaking very approximately speaking to speaking very approximately speaking to towards this", "image_path": "img_data/video_68_chunk_58.jpg"}
{"video": "video_68", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "speaking very approximately speaking to towards this direction to find that kind of direction to find that kind of minimum the because of the fact that we were because of the fact that we were having a fairly well educated kind of having a fairly well educated kind of direction i drew it as a straight line direction i drew it as a straight line obviously it's never a straight line but obviously it's never a straight line but that's a kind of an approximation i'm that's a kind of an approximation i'm trying to convey here now we with the trying to convey here now we with the minibatch we are going to go into minibatch we are going to go into directions which are very noisy because directions which are very noisy because we are using only a small subset of our", "image_path": "img_data/video_68_chunk_59.jpg"}
{"video": "video_68", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "directions which are very noisy because we are using only a small subset of our we are using only a small subset of our data for this type of estimation data for this type of estimation of the gradient our gradient of the gradient our gradient estimates which is the directional estimates which is the directional derivative that involves derivative that involves both parameters will be fairly noisy we both parameters will be fairly noisy we may actually go into a fairly may actually go into a fairly convoluted way and we can actually convoluted way and we can actually show that still if we choose the show that still if we choose the hyperparameter called learning rate hyperparameter called learning rate as appropriately to be able to still", "image_path": "img_data/video_68_chunk_60.jpg"}
{"video": "video_68", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "hyperparameter called learning rate as appropriately to be able to still as appropriately to be able to still converts to the local minima or converts to the local minima or the global minima for that matter and the global minima for that matter and that's one thing we're going to see that's one thing we're going to see some slower i will quality training some slower i will quality training in terms of there's obviously the there's obviously the trade-off between computational trade-off between computational efficiency to see exactly how slow it efficiency to see exactly how slow it will actually be and the form will actually be and the form of the loss function but we will see", "image_path": "img_data/video_68_chunk_61.jpg"}
{"video": "video_68", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "will actually be and the form of the loss function but we will see of the loss function but we will see some kind of interl trajectory to some kind of interl trajectory to reach that kind of minimum that reach that kind of minimum that having said that the advantage that having said that the advantage that actually we will see is that there is actually we will see is that there is in many instances if you remember the in many instances if you remember the presence of many local minima in real presence of many local minima in real loss functions that noisy gradient loss functions that noisy gradient may actually help us escape from bad may actually help us escape from bad local minima and how going to find local minima and how going to find new opportunities in the search space new opportunities in the search space that noisiness help us find perhaps", "image_path": "img_data/video_68_chunk_62.jpg"}
{"video": "video_68", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "new opportunities in the search space that noisiness help us find perhaps that noisiness help us find perhaps it will help us find another perhaps it will help us find another local minima which is actually a good local minima which is actually a good local minima and that will actually be minima and that will actually be for us despite the fact that we can u for us despite the fact that we can u never reach a lower kind never reach a lower kind of local minima that sort of good of local minima that sort of good local minima will be and we will local minima will be and we will get some good performance out of it get some good performance out of it evidently the loss functions are not evidently the loss functions are not well behaved in reality that", "image_path": "img_data/video_68_chunk_63.jpg"}
{"video": "video_68", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "evidently the loss functions are not well behaved in reality that well behaved in reality that sort of stochastic version of the gr sort of stochastic version of the gr descent is going to has been found descent is going to has been found that it's performing very well in fact that it's performing very well in fact empirically what has been found is a gr empirically what has been found is a gr descent is a very bad algorithm for descent is a very bad algorithm for the typical machine learning problems the typical machine learning problems we are going to be facing we are going to be facing we are going to adopt stochastic grate going to adopt stochastic grate descent which again empirically has been descent which again empirically has been shown to perform much better for the shown to perform much better for the both computationally and also", "image_path": "img_data/video_68_chunk_64.jpg"}
{"video": "video_68", "start": "0:32:30", "end": "0:32:35.566667", "timestamp": "0:32:30 - 0:32:35.566667", "text": "shown to perform much better for the both computationally and also both computationally and also performance- wise on finding more performance- wise on finding more opportunities for us", "image_path": "img_data/video_68_chunk_65.jpg"}
{"video": "video_69", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this go back to this vap new block diagram put all probability diagram put all probability distributions that we will need in distributions that we will need in order for us to do the following for order for us to do the following for every prediction we are going to get out every prediction we are going to get out of our machine that prediction should of our machine that prediction should have some form of uncertainty all the have some form of uncertainty all the previous discussions we have done up to previous discussions we have done up to now in the regression problem we were now in the regression problem we were getting a price let's say of a house getting a price let's say of a house or any kind of real number in our or any kind of real number in our white hats now we will have not only", "image_path": "img_data/video_69_chunk_0.jpg"}
{"video": "video_69", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "or any kind of real number in our white hats now we will have not only white hats now we will have not only just the price of the house but also we just the price of the house but also we have an uncertainty of how uncertain have an uncertainty of how uncertain our predictions are most of these our predictions are most of these machines that are to be useful they need machines that are to be useful they need to have that uncertainty output it's to have that uncertainty output it's quite different to say i'm certain 100% quite different to say i'm certain 100% that the house the price of that house that the house the price of that house will be 350,000 but quite different if you 350,000 but quite different if you say that yeah there will be 350,000 but say that yeah there will be 350,000 but i'm going to add to it plus", "image_path": "img_data/video_69_chunk_1.jpg"}
{"video": "video_69", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "say that yeah there will be 350,000 but i'm going to add to it plus i'm going to add to it plus or minus 100,000 into that kind of or minus 100,000 into that kind of price that we will see also in price that we will see also in classification and that's why we start classification and that's why we start now in treating learning as a now in treating learning as a probabilistic problem we have seen in probabilistic problem we have seen in this kind of block diagram that we have this kind of block diagram that we have a data generator this data generator now a data generator this data generator now will be decorated with a marginal will be decorated with a marginal probability distribution will be calling probability distribution will be calling p of x this x over here is a samp", "image_path": "img_data/video_69_chunk_2.jpg"}
{"video": "video_69", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "probability distribution will be calling p of x this x over here is a samp p of x this x over here is a samp out of this probability generative out of this probability generative probability distribution p of x and probability distribution p of x and that x will actually go through the that x will actually go through the target function which is again unknown target function which is again unknown and it will produce at the output the and it will produce at the output the target variable y and target variable y and probabilistically we can express this probabilistically we can express this y as being obtained out of another y as being obtained out of another probability distribution which we will probability distribution which we will start calling now p data let me start calling now p data let me just change that", "image_path": "img_data/video_69_chunk_3.jpg"}
{"video": "video_69", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "start calling now p data let me just change that to be the -called p data of x and to be the -called p data of x and actually draw a little bit this box actually draw a little bit this box differently to i have some kind of space differently to i have some kind of space this is now p data of x and this probably distribution data of x and this probably distribution will be the p data of y given x this i mean the qualifier x this i mean the qualifier data over there and the probability data over there and the probability distribution is actually done for a distribution is actually done for a reason and we have adopted the ian", "image_path": "img_data/video_69_chunk_4.jpg"}
{"video": "video_69", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "distribution is actually done for a reason and we have adopted the ian reason and we have adopted the ian good felles kind of book notation for good felles kind of book notation for all the probabbly distributions we will all the probabbly distributions we will code here the sampler code here the sampler now sees both of these x's and now sees both of these x's and y's and in fact what we can actually y's and in fact what we can actually tell here is that the sampler will tell here is that the sampler will c will sample out of a joint probability c will sample out of a joint probability distribution p data of x comma y which is nothing", "image_path": "img_data/video_69_chunk_5.jpg"}
{"video": "video_69", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "distribution p data of x comma y which is nothing data of x comma y which is nothing else as we have seen in the probability else as we have seen in the probability review section as the product of p review section as the product of p data of y given x * b data of y given x * b data of x now we everything is kind of data of x now we everything is kind of consistent here in terms of the consistent here in terms of the product rule and it will the sampler product rule and it will the sampler will sample out of this joint will sample out of this joint probability distribution and give us probability distribution and give us some examples and if we are to describe", "image_path": "img_data/video_69_chunk_6.jpg"}
{"video": "video_69", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "probability distribution and give us some examples and if we are to describe some examples and if we are to describe probabilistically that kind of sampling probabilistically that kind of sampling process we went from p data of x comma y process we went from p data of x comma y and over here we can associate the and over here we can associate the -called empirical probability distribution of p data hat of x comma y and this is p data hat of x comma y and this is kind of the histogram if you kind of the histogram if you want to think about it of all the want to think about it of all the examples that the sampler gave us out examples that the sampler gave us out of this p data had that's a", "image_path": "img_data/video_69_chunk_7.jpg"}
{"video": "video_69", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "examples that the sampler gave us out of this p data had that's a of this p data had that's a very important probability solution very important probability solution because that's what is coming is because that's what is coming is involved in the input of our involved in the input of our machine and going back to this machine and going back to this machine kind of block diagram we have machine kind of block diagram we have specified the hypothesis set the specified the hypothesis set was offering to us the hypothesis set was offering to us the u hypothesis which called g of x comma w u hypothesis which called g of x comma w the x and obviously was associated", "image_path": "img_data/video_69_chunk_8.jpg"}
{"video": "video_69", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "u hypothesis which called g of x comma w the x and obviously was associated the x and obviously was associated with our data x and w was the vector with our data x and w was the vector that was encompassing all of the that was encompassing all of the parameters of our hypothesis and we have parameters of our hypothesis and we have seen several of those hypothesis in the seen several of those hypothesis in the sinusoidal regression problem we had sinusoidal regression problem we had the objective loss function and we the objective loss function and we had the optimization algorithm which is had the optimization algorithm which is the stochastic rated the stochastic rated descent and the finally we had descent and the finally we had this final hypothesis at the produced this final hypothesis at the produced at the output of this optimization at the output of this optimization procedure now instead of actually", "image_path": "img_data/video_69_chunk_9.jpg"}
{"video": "video_69", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "at the output of this optimization procedure now instead of actually procedure now instead of actually producing now a function g that it producing now a function g that it was claimed to be as close as possible was claimed to be as close as possible to the target function f that was our to the target function f that was our attempt to make it as close as possible attempt to make it as close as possible to the underlying target function f to the underlying target function f now we are going to be producing now we are going to be producing a probability distribution called p a probability distribution called p model as our hypothesis our model as our hypothesis will actually be also hypothesis will actually be also probabilistic in nature it will b p", "image_path": "img_data/video_69_chunk_10.jpg"}
{"video": "video_69", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "hypothesis will actually be also probabilistic in nature it will b p probabilistic in nature it will b p model and this p model will have model and this p model will have again in going to be a function of x and we'll in going to be a function of x and we'll have a set of parameters have a set of parameters w and in fact it's going to be w and in fact it's going to be conditional it will be y given x comma conditional it will be y given x comma w and the reason actually it is why w and the reason actually it is why given x comma w is", "image_path": "img_data/video_69_chunk_11.jpg"}
{"video": "video_69", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "w and the reason actually it is why given x comma w is we want to match that to as close as we want to match that to as close as possible to the u sort of possible to the u sort of underline probability distribution of underline probability distribution of our labels we are trying to guess labels our labels we are trying to guess labels the labels why and we are going to the labels why and we are going to offer a probabilistic model for that offer a probabilistic model for that kind of guess for that kind of guess for that kind of hypothesis therefore we will have a hypothesis therefore we will have a parametric version of that p data parametric version of that p data evidently the p data is not available to evidently the p data is not available to us therefore we have a parametric", "image_path": "img_data/video_69_chunk_12.jpg"}
{"video": "video_69", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "evidently the p data is not available to us therefore we have a parametric us therefore we have a parametric version of trying to approximate the version of trying to approximate the empirical probably distribution p data empirical probably distribution p data hat and we will see now how hat and we will see now how this p data hat and p model can come this p data hat and p model can come closer to each other in a similar closer to each other in a similar fashion as we have seen in the -called fashion as we have seen in the -called deterministic equivalent discussion we deterministic equivalent discussion we had in the regression problem and the had in the regression problem and the other thing of course we need to other thing of course we need to convert this discussion to convert this discussion to convert from a deterministic discussion convert from a deterministic discussion to a probabilistic discussion we had", "image_path": "img_data/video_69_chunk_13.jpg"}
{"video": "video_69", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "convert from a deterministic discussion to a probabilistic discussion we had to a probabilistic discussion we had we have to change this kind of loss we have to change this kind of loss function from something which is a function from something which is a function to another function which function to another function which will include the p model and p will include the p model and p data hat and if you remember we data hat and if you remember we introduced the equivalent function that we called k function that we called k divergence or relative entropy just divergence or relative entropy just in the an earlier video we will be in the an earlier video we will be start now calling this cross entropy and", "image_path": "img_data/video_69_chunk_14.jpg"}
{"video": "video_69", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "in the an earlier video we will be start now calling this cross entropy and start now calling this cross entropy and we'll symbolize with capital c and we'll symbolize with capital c and capital e cross entropy and this cross enty will have two things as parameters the ground two things as parameters the ground truth and the yat in a similar way that truth and the yat in a similar way that the mean square also had these two the mean square also had these two parameters as arguments the parameters as arguments the predictions and our ground truth and predictions and our ground truth and we'll see now how this cross entropy", "image_path": "img_data/video_69_chunk_15.jpg"}
{"video": "video_69", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "predictions and our ground truth and we'll see now how this cross entropy we'll see now how this cross entropy will be derived out of relative will be derived out of relative entropy for a specific problem entropy for a specific problem that we will have we will pose in fact that we will have we will pose in fact this crossentropy will be the this crossentropy will be the maximum related to the maximum related to the maximum likelihood we'll introduce now likelihood we'll introduce now maximum likelihood is probably one of maximum likelihood is probably one of the main lessons in this specific the main lessons in this specific sort of module that we are sort of module that we are discussing and we will do that next discussing and we will do that next using s simple data", "image_path": "img_data/video_69_chunk_16.jpg"}
{"video": "video_69", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "discussing and we will do that next using s simple data set i will be introducing now maximum set i will be introducing now maximum likelihood using a simple case likelihood using a simple case where for the moment we will be ignoring where for the moment we will be ignoring our labels y there will be no labels in this y there will be no labels in this problem in other words i'm problem in other words i'm actually what i'm doing right now is i'm actually what i'm doing right now is i'm actually giving you some kind of data actually giving you some kind of data set that it is consist of for real", "image_path": "img_data/video_69_chunk_17.jpg"}
{"video": "video_69", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "actually giving you some kind of data set that it is consist of for real set that it is consist of for real numbers this is just x's on scalar numbers this is just x's on scalar numbers on the real line let's call them the -call line let's call them the -call x1 x2 all the way let's say to x1 x2 all the way let's say to xm m real numbers on the real line and xm m real numbers on the real line and i'm asking you the following question i'm asking you the following question this question is going to be wed this question is going to be wed over here at the top the question is what is the", "image_path": "img_data/video_69_chunk_18.jpg"}
{"video": "video_69", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "over here at the top the question is what is the model that fits the data or maybe i can just write as specify model that fits the", "image_path": "img_data/video_69_chunk_19.jpg"}
{"video": "video_69", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "data or maybe i can just write as specify model that fits the dat that's a kind of a reasonable question it comes to various forms in question it comes to various forms in practice you probably have seen in practice you probably have seen it as earlier as potentially kind of it as earlier as potentially kind of a density andity estimation kind of a density andity estimation kind of histogram plotting and or sort of histogram plotting and or sort of many other questions that are associated many other questions that are associated with this question that we wrote over", "image_path": "img_data/video_69_chunk_20.jpg"}
{"video": "video_69", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "many other questions that are associated with this question that we wrote over with this question that we wrote over here and the data set is going here and the data set is going to be evidently as going to be denoted to be evidently as going to be denoted by let's say x1 xm note the kind of strange x1 xm note the kind of strange kind of x there is there in order for us kind of x there is there in order for us to sort of not to associate it this to sort of not to associate it this is not a vector that's basically a set is not a vector that's basically a set the set consist of all the data the set consist of all the data that we have and the according to the", "image_path": "img_data/video_69_chunk_21.jpg"}
{"video": "video_69", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "the set consist of all the data that we have and the according to the that we have and the according to the vening kind of blob diagram the very vening kind of blob diagram the very first thing that we have to do and first thing that we have to do and if i go actually go back to this kind of if i go actually go back to this kind of block diagram we will see that the block diagram we will see that the we don't have now labels we have u we don't have now labels we have u just x's there is an underlying u just x's there is an underlying u generative kind of distribution p data generative kind of distribution p data of x that it is sampled from the of x that it is sampled from the sampler is giving us some sampler the sampler is giving us some x's mx's that i just drw and we have now", "image_path": "img_data/video_69_chunk_22.jpg"}
{"video": "video_69", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "sampler the sampler is giving us some x's mx's that i just drw and we have now x's mx's that i just drw and we have now as input just the x's and the first as input just the x's and the first thing that we have to do is after we thing that we have to do is after we have now introduced the probabilistic have now introduced the probabilistic version of the vaping block diagram is version of the vaping block diagram is to suggest a suitable p model a suitable to suggest a suitable p model a suitable probability distribution for this probability distribution for this kind of xes if i go back to this kind of xes if i go back to this kind of diagram we have now we of diagram we have now we need to write now this kind of a form of need to write now this kind of a form of this p model i will suggest out of many this p model i will suggest out of many problem distributions all that are", "image_path": "img_data/video_69_chunk_23.jpg"}
{"video": "video_69", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "this p model i will suggest out of many problem distributions all that are problem distributions all that are appropriate for the problem and this appropriate for the problem and this problem is because x's are in the real problem is because x's are in the real line we were looking to find a line we were looking to find a probability distribution which is for probability distribution which is for continuous random variables one of them is you variables one of them is you probably guessed it the gausian probably guessed it the gausian distribution or the normal distribution or the normal distribution we have emphasized on that distribution we have emphasized on that distribution in the probability review let's write in the probability review let's write down our hypothesis", "image_path": "img_data/video_69_chunk_24.jpg"}
{"video": "video_69", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "in the probability review let's write down our hypothesis for our in other words p model is that is evidently going model is that is evidently going now to be there are no labels the p now to be there are no labels the p model is going to be just the x comma w model is going to be just the x comma w and x is not even higher dimension and x is not even higher dimension it is just a scalar there's no it is just a scalar there's no underline here but it could be of course underline here but it could be of course we need to put underline the around w we need to put underline the around w the over under the w because it's a set the over under the w because it's a set of parameters it's going to be the of parameters it's going to be the normal distribution of x given w you", "image_path": "img_data/video_69_chunk_25.jpg"}
{"video": "video_69", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "of parameters it's going to be the normal distribution of x given w you normal distribution of x given w you can write this given a set of parameters and in many instances parameters and in many instances actually we can write this as actually we can write this as a conditional given w right we a conditional given w right we have now you probably know the have now you probably know the functional form of that is 1 / square functional form of that is 1 / square root of 2 pi sigma s", "image_path": "img_data/video_69_chunk_26.jpg"}
{"video": "video_69", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "functional form of that is 1 / square root of 2 pi sigma s time e of - x - mu squ / 2 sigma squ that will actually be the our p model that will actually be the our p model and the set of parameters here as you and the set of parameters here as you can actually see is the vector w the that corresponds to the mean w the that corresponds to the mean and the variance of this ga and the variance of this ga and distribution we have two parameters in distribution we have two parameters in this kind of problem in this kind of problem and the moment we actually wrote down", "image_path": "img_data/video_69_chunk_27.jpg"}
{"video": "video_69", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "in this kind of problem and the moment we actually wrote down and the moment we actually wrote down the functional form of our probability the functional form of our probability distribution that we believe that it is distribution that we believe that it is the best the most suitable one for the best the most suitable one for describing the data the we converted this kind of data the we converted this kind of question that question now evolved into question that question now evolved into the following question i will just the following question i will just write it as a q prime write it as a q prime now this question is the following now this question is the following what are the", "image_path": "img_data/video_69_chunk_28.jpg"}
{"video": "video_69", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "are the parameters of the p model that fit the data i might just draw this gaus and distribution over here that's my p model i assume here that's my p model i assume is going to be gusen that's a nice bel is going to be gusen that's a nice bel shaped kind of curve and we can", "image_path": "img_data/video_69_chunk_29.jpg"}
{"video": "video_69", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "is going to be gusen that's a nice bel shaped kind of curve and we can shaped kind of curve and we can actually define the specific parameters that define the specific parameters that are present right in drawing this the are present right in drawing this the distribution this distribution has a distribution this distribution has a specific mean and a specific variance specific mean and a specific variance and for that specific mean and variance and for that specific mean and variance it is located over here and has this it is located over here and has this kind of uncertainty buil in into it as a uncertainty buil in into it as a probabbly distribution the now we have", "image_path": "img_data/video_69_chunk_30.jpg"}
{"video": "video_69", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "probabbly distribution the now we have distribution the now we have the this kind of following question the this kind of following question the question becomes what is how we becomes what is how we are going to find these parameters we are going to find these parameters we have now effectively a parameter search have now effectively a parameter search problem the we are not trying to find problem the we are not trying to find to try many parameters just what we to try many parameters just what we have seen in the cidal regression have seen in the cidal regression problem we were changing the w and problem we were changing the w and then the length of the vector w and come then the length of the vector w and come up with to come up with a final", "image_path": "img_data/video_69_chunk_31.jpg"}
{"video": "video_69", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "then the length of the vector w and come up with to come up with a final up with to come up with a final hypothesis now in a very similar way hypothesis now in a very similar way we have to change the parameters of the we have to change the parameters of the gausian distribution in order for us gausian distribution in order for us to provide a good fit to the data and as to provide a good fit to the data and as you can actually see here this you can actually see here this specific hypothesis with this mean specific hypothesis with this mean let's say mu zero over here is really let's say mu zero over here is really far away from the data it makes sense far away from the data it makes sense for us to have the following for us to have the following evolution of our question if we are to follow the same question if we are to follow the same principle as we have done earlier in", "image_path": "img_data/video_69_chunk_32.jpg"}
{"video": "video_69", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "question if we are to follow the same principle as we have done earlier in principle as we have done earlier in order was to convert this parameter order was to convert this parameter estimation problem into op optimization estimation problem into op optimization solution problem then it's actually solution problem then it's actually reasonable to write now this question as follows what is the objective function behind every optimization function behind every optimization problem there's must be an objective function of the optimization problem", "image_path": "img_data/video_69_chunk_33.jpg"}
{"video": "video_69", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "function of the optimization problem of the optimization problem that will result will give us a good in quotes choice of the parameters w i will call this choice w star", "image_path": "img_data/video_69_chunk_34.jpg"}
{"video": "video_69", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "parameters w i will call this choice w star of the p model these are the two subsequent questions and now we will two subsequent questions and now we will need to address this last need to address this last one we went from q to q prime to q one we went from q to q prime to q double prime that's our final kind of a double prime that's our final kind of a question that we need to now address and question that we need to now address and we'll do that by introducing that kind we'll do that by introducing that kind of objective function that makes kind of objective function that makes kind of some intuitive sense now make a following kind of", "image_path": "img_data/video_69_chunk_35.jpg"}
{"video": "video_69", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "some intuitive sense now make a following kind of assumption which is kind of essential on u that the data r i this is a very common assumption you will see that this assumption you will see that this expression in many kind of statistics expression in many kind of statistics textbooks and iid stands for independent and", "image_path": "img_data/video_69_chunk_36.jpg"}
{"video": "video_69", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "and identically distributed independent distributed means that a data subsequent distributed means that a data subsequent samples out of this underlying probabbly samples out of this underlying probabbly distribution again we do not know the distribution again we do not know the p data hatut the one that we that p data hatut the one that we that generated this kind of data and in generated this kind of data and in fact this p data hat fact this p data hat probably will be some kind of probably will be some kind of probability distribution in this kind of", "image_path": "img_data/video_69_chunk_37.jpg"}
{"video": "video_69", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "probably will be some kind of probability distribution in this kind of probability distribution in this kind of neighborhood or where the data neighborhood or where the data are the data in other words were are the data in other words were generated independently from each other generated independently from each other subsequent samples that's the subsequent samples that's the independent and they identically independent and they identically distributed means that all the data came distributed means that all the data came from one probability distribution we do from one probability distribution we do not know its shape and form the only not know its shape and form the only thing of course we have at our disposal thing of course we have at our disposal is our p model and that we have to is our p model and that we have to provide as a hypothesis and now we'll", "image_path": "img_data/video_69_chunk_38.jpg"}
{"video": "video_69", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "is our p model and that we have to provide as a hypothesis and now we'll to provide as a hypothesis and now we'll need to define this kind of objective need to define this kind of objective function after we make this kind of function after we make this kind of distribution the p model however distribution the p model however can the moment we have lots of data can the moment we have lots of data can be written as follows we can be written as follows we can actually write the p model the moment we have someone is giving us all this kind of data will be", "image_path": "img_data/video_69_chunk_39.jpg"}
{"video": "video_69", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "model the moment we have someone is giving us all this kind of data will be the u probability of generating this kind of set of samples given our kind of set of samples given our w and this is a function that we will w and this is a function that we will call likelihood function for the it's actually it's a function because this is function because this is only a function of the parameters vector", "image_path": "img_data/video_69_chunk_40.jpg"}
{"video": "video_69", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "function because this is only a function of the parameters vector only a function of the parameters vector w only of the sec of the set of w only of the sec of the set of parameters w and describes how well the data x1 to xm ar let's say capture", "image_path": "img_data/video_69_chunk_41.jpg"}
{"video": "video_69", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "xm ar let's say capture ed very informal discussion at this point with such hypothesis the likelihood function is hypothesis the likelihood function is end up being a function of only the w end up being a function of only the w as the as we want in any case from as the as we want in any case from our objective function and it is our objective function and it is grounded the it is grounded", "image_path": "img_data/video_69_chunk_42.jpg"}
{"video": "video_69", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "grounded the it is grounded by the hypothesis and i think we should have now some discussion as to why this is now some discussion as to why this is kind of an important kind of an important sentence because the likelihood function sentence because the likelihood function should not be confused with something should not be confused with something that can provide to us an estimate of a that can provide to us an estimate of a probability of any of this kind of probability of any of this kind of samples evidently the probability of samples evidently the probability of observing sample x2 is zero", "image_path": "img_data/video_69_chunk_43.jpg"}
{"video": "video_69", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "samples evidently the probability of observing sample x2 is zero observing sample x2 is zero because x is a continuous random because x is a continuous random variable and if you go back to that variable and if you go back to that discussion we had in the probability discussion we had in the probability review section that's definitely a review section that's definitely a probability of zero we have an infinite probability of zero we have an infinite in other words a possibilities of in other words a possibilities of generating any of these real numbers generating any of these real numbers x1 to xm and therefore the corresponding x1 to xm and therefore the corresponding probability zero however the likelihood probability zero however the likelihood is not a probability the likelihood is not a probability the likelihood function is actually capturing how function is actually capturing how likely that sample is to come from a", "image_path": "img_data/video_69_chunk_44.jpg"}
{"video": "video_69", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "function is actually capturing how likely that sample is to come from a likely that sample is to come from a probability distribution we call here a probability distribution we call here a p model let's see that this how p model let's see that this how this function can be written and then this function can be written and then we'll make some discussion about the we'll make some discussion about the likelihood of every of these each one of likelihood of every of these each one of those samples we made an assumption those samples we made an assumption that the data are independent and if that the data are independent and if you go back to the discussion we had you go back to the discussion we had about the product rule then you can about the product rule then you can realize that we can now write this realize that we can now write this p model", "image_path": "img_data/video_69_chunk_45.jpg"}
{"video": "video_69", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "realize that we can now write this p model which is effectively the b model of x1 to xm given w as the product of x1 given w because of the independence x1 given w because of the independence inception times the product of p inception times the product of p model of x2 given w and", "image_path": "img_data/video_69_chunk_46.jpg"}
{"video": "video_69", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "inception times the product of p model of x2 given w and on and this product then i think it's worthwhile spending some time to each worthwhile spending some time to each one of those terms each one of the those one of those terms each one of the those terms can be completely evaluated and terms can be completely evaluated and it's very easy to see how it can be it's very easy to see how it can be completely evaluated we know x1 we completely evaluated we know x1 we can plug it in here we have assume specific hypothesis here we have assume specific hypothesis in this specific case it is music 0 and in this specific case it is music 0 and sigma 0 squ we can plug mu0 and sigma", "image_path": "img_data/video_69_chunk_47.jpg"}
{"video": "video_69", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "in this specific case it is music 0 and sigma 0 squ we can plug mu0 and sigma 0 squ we can plug mu0 and sigma 0 squ and over here and each of these 0 squ and over here and each of these are is a number is a floating point are is a number is a floating point number it's a number that is going number it's a number that is going to give us how likely each data to give us how likely each data point is with respect to this point is with respect to this specific p model and as you can actually specific p model and as you can actually all recognize if we have a all recognize if we have a specific hypothesis located over here specific hypothesis located over here this u likelihood that this x1", "image_path": "img_data/video_69_chunk_48.jpg"}
{"video": "video_69", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "specific hypothesis located over here this u likelihood that this x1 this u likelihood that this x1 came out of this was generated out of came out of this was generated out of this probability distribution is a very small distribution is a very small number because it's really far number because it's really far away from it in the left tail of it far away from it in the left tail of it and the same goes with x2 and the same goes with x2 and the same goes for all of the samples in this same goes for all of the samples in this specific case let me just write specific case let me just write over here that each one of them", "image_path": "img_data/video_69_chunk_49.jpg"}
{"video": "video_69", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "specific case let me just write over here that each one of them of the terms can be evaluated to a number it's going to be some very low number it's going to be some very low number say 10 theus 100 for x1 number say 10 theus 100 for x1 maybe 10 theus 98 for x2 and 10 to maybe 10 theus 98 for x2 and 10 to the minus sort of 60 for xm over the minus sort of 60 for xm over here and as you can actually see", "image_path": "img_data/video_69_chunk_50.jpg"}
{"video": "video_69", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "the minus sort of 60 for xm over here and as you can actually see here and as you can actually see if we start moving this propably if we start moving this propably distribution around and probably distribution around and probably we'll see how this prob distribution can we'll see how this prob distribution can be moved to the right direction which is be moved to the right direction which is left in this figure then we can actually start figure then we can actually start seeing how this function start to seeing how this function start to increase we have a certain hopes to increase we have a certain hopes to make this function that we call make this function that we call likelihood function our objective likelihood function our objective function to continue let's write", "image_path": "img_data/video_69_chunk_51.jpg"}
{"video": "video_69", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "to continue let's write now ml continue let's write down the now ml continue let's write down the p model again the likelihood function p model again the likelihood function i will call this likelihood function i will call this likelihood function calligraphic l the calligraphic l was also a l the calligraphic l was also a letter that it was used to describe letter that it was used to describe the loss function we'll see now when the loss function we'll see now when we move a bit forward that we with we move a bit forward that we with there's a sort of connection between the there's a sort of connection between the two we'll this is going to be just two we'll this is going to be just the lo the likelihood at this", "image_path": "img_data/video_69_chunk_52.jpg"}
{"video": "video_69", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "two we'll this is going to be just the lo the likelihood at this point likelihood function and again function is a function of parameters the parameters w and the moment we have a specific w and the moment we have a specific hypothesis we can write it as a p", "image_path": "img_data/video_69_chunk_53.jpg"}
{"video": "video_69", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "w and the moment we have a specific hypothesis we can write it as a p model as a product sorry as product we've seen this product from let's say from i is equal to 1 to m the p model of x i given w and as we saw this each one of these w and as we saw this each one of these could be a very low number and what", "image_path": "img_data/video_69_chunk_54.jpg"}
{"video": "video_69", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "w and as we saw this each one of these could be a very low number and what could be a very low number and what we need to do now next is to discuss a we need to do now next is to discuss a little bit about who is going to compute little bit about who is going to compute it evidently computer is going to it evidently computer is going to compute it for us in order for us to compute it for us in order for us to start optimizing it in other words start optimizing it in other words trying to find the optimal set of trying to find the optimal set of parameters that will maximize it and we parameters that will maximize it and we saw that the maximization is going to saw that the maximization is going to be happening probably by moving it in be happening probably by moving it in the right direction someone has to the right direction someone has to tell us where to move it shall we tell us where to move it shall we move it to the right here or shall we move it to the right here or shall we move it to the left and the move it to the left and the evidently the good direction for us is", "image_path": "img_data/video_69_chunk_55.jpg"}
{"video": "video_69", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "move it to the left and the evidently the good direction for us is evidently the good direction for us is the left direction over here i will the left direction over here i will bring it closer to the data that's bring it closer to the data that's one thing and the second the computer one thing and the second the computer probably if it starts calculating this probably if it starts calculating this kind of small numbers 10 the minus 100 kind of small numbers 10 the minus 100 10us 90 and 10us 80 then will go into 10us 90 and 10us 80 then will go into an underflow very quickly for a an underflow very quickly for a large data sets or even small data sets large data sets or even small data sets in this kind of case what we do is we in this kind of case what we do is we do all the calculations in the log do all the calculations in the log domain we will be from now on", "image_path": "img_data/video_69_chunk_56.jpg"}
{"video": "video_69", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "do all the calculations in the log domain we will be from now on domain we will be from now on discussing about the log likelihood function is evidently going to be provided evidently going to be provided to us very easily from the likelihood to us very easily from the likelihood function if we use the function if we use the formula log of a is equal to log of a formula log of a is equal to log of a plus log of p which is hopefully known to", "image_path": "img_data/video_69_chunk_57.jpg"}
{"video": "video_69", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "plus log of p which is hopefully known to log of p which is hopefully known to everyone instead of a product we will everyone instead of a product we will be getting a sum instead of a product be getting a sum instead of a product over here we will be having the p over here we will be having the p model sorry it's going to model sorry it's going to be i'll be using the natural log i be i'll be using the natural log i can just write it over here with natural can just write it over here with natural log but in general will be the any log but in general will be the any log with any base and i'm using the sort of natural base and i'm using the sort of natural logs here because we have a gausian", "image_path": "img_data/video_69_chunk_58.jpg"}
{"video": "video_69", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "base and i'm using the sort of natural logs here because we have a gausian logs here because we have a gausian and the gausian have exponents and the gausian have exponents and therefore the natural log is going to be therefore the natural log is going to be a res to something which is kind of easy a res to something which is kind of easy for us to absorb all right it's going to be absorb all right it's going to be summation from i equal to 1 2 m of the summation from i equal to 1 2 m of the ln of the p model of x i given w and if you do the math and if you do all this kind of and if you do all this kind of replacement and put a natural", "image_path": "img_data/video_69_chunk_59.jpg"}
{"video": "video_69", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "and if you do all this kind of replacement and put a natural replacement and put a natural log in front of every of these kind log in front of every of these kind of terms you will come to the same to of terms you will come to the same to the following close form to the following close form expression about log likelihood expression about log likelihood i'm can actually write here that the log i'm can actually write here that the log likelihood of the parameters likelihood of the parameters u w it's a function of the parameters w u w it's a function of the parameters w it is minus 1 / in this specific case it is minus 1 / in this specific case of the p model to sigma 0", "image_path": "img_data/video_69_chunk_60.jpg"}
{"video": "video_69", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "it is minus 1 / in this specific case of the p model to sigma 0 of the p model to sigma 0 s summation let's say from i is equal to summation let's say from i is equal to 1 to m of x i minus mu squar minus m / 2 ln sigma", "image_path": "img_data/video_69_chunk_61.jpg"}
{"video": "video_69", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "mu squar minus m / 2 ln sigma 2us m / 2 ln of 2 pi this is the 2us m / 2 ln of 2 pi this is the clause form expression about log clause form expression about log likelihood and i understand this is a likelihood and i understand this is a kind of a complicated expression and kind of a complicated expression and that's why i think it's worthwhile that's why i think it's worthwhile plotting it and we have a notebook plotting it and we have a notebook that we will go through that plots if that we will go through that plots if you this log livelihood and the you this log livelihood and the only thing i want to mention now only thing i want to mention now before kind of plotting it is that in", "image_path": "img_data/video_69_chunk_62.jpg"}
{"video": "video_69", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "only thing i want to mention now before kind of plotting it is that in before kind of plotting it is that in many of the optimization algorithms that many of the optimization algorithms that we will be using a descent type of algorithms they are descent type of algorithms they are minimizers instead of maximizing this minimizers instead of maximizing this log likelihood which is evidently is log likelihood which is evidently is going to be resulting into parameters going to be resulting into parameters which are close in this area h we are which are close in this area h we are going to be minimizing its negative this going to be minimizing its negative this exactly equivalent we will be exactly equivalent we will be let me write this down instead of", "image_path": "img_data/video_69_chunk_63.jpg"}
{"video": "video_69", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "of maximizing the log likelihood of w and maximizing the log likelihood of w and we you can actually are entitled if i we you can actually are entitled if i may say to sort of use the log may say to sort of use the log in front of the likelihood function it in front of the likelihood function it will result in exactly the same will result in exactly the same parameter vector w because the log is parameter vector w because the log is monotonic function instead of monotonic function instead of maximizing the log li of w we will be maximizing the log li of w we will be", "image_path": "img_data/video_69_chunk_64.jpg"}
{"video": "video_69", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "maximizing the log li of w we will be minimizing the negative log likelihood that we will call be calling n and symbolize it this nl calling n and symbolize it this nl with no calligraphic soltion with no calligraphic soltion the negative look", "image_path": "img_data/video_69_chunk_65.jpg"}
{"video": "video_69", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "with no calligraphic soltion the negative look that's the shape of that will actually be as follows it will that will actually be as follows it will be a nice a very well behaving be a nice a very well behaving function in this case because we see function in this case because we see that is we are going to have some that is we are going to have some convexity in our function we were convexity in our function we were hoping to have the ne log likelihood hoping to have the ne log likelihood function over just one of the parameters", "image_path": "img_data/video_69_chunk_66.jpg"}
{"video": "video_69", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "hoping to have the ne log likelihood function over just one of the parameters function over just one of the parameters if i obviously to plot this i have if i obviously to plot this i have two parameters i need a two parameters i need a three-dimensional kind of plot but if i three-dimensional kind of plot but if i just set this sigma square to something just set this sigma square to something to some kind of constant number then to some kind of constant number then i have a an and ploted as a function i have a an and ploted as a function of only the mean then i will get i of only the mean then i will get something that it will look will get something that it will look something that the sort of negative log likelihood function sort of negative log likelihood function is going to be well behaving in this is going to be well behaving in this case we will be minimizing it we will", "image_path": "img_data/video_69_chunk_67.jpg"}
{"video": "video_69", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "is going to be well behaving in this case we will be minimizing it we will case we will be minimizing it we will try with some kind of algorithm to find try with some kind of algorithm to find the minimum that minimum will be the minimum that minimum will be calling new start of course we'll repeat calling new start of course we'll repeat exactly the same exercise with sigma exactly the same exercise with sigma squar in fact the two parameters have to squar in fact the two parameters have to be jointly optimized and therefore we be jointly optimized and therefore we will be by minimizing the negative log likelihood the negative log likelihood we will get we get the", "image_path": "img_data/video_69_chunk_68.jpg"}
{"video": "video_69", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "we get the optimal set of parameters w star is equal to new star and sigma star squar this is effectively what is squar this is effectively what is going to happen now is we're going to happen now is we're going to be using exactly the same be using exactly the same optimization al the stochastic rend", "image_path": "img_data/video_69_chunk_69.jpg"}
{"video": "video_69", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "be using exactly the same optimization al the stochastic rend optimization al the stochastic rend stoas rend will move with a small stoas rend will move with a small steps and will position our p model very close to the data it will find for us the specific new star and find for us the specific new star and the specific parameter sigma star squar the specific parameter sigma star squar and in fact we will see that now and in fact we will see that now we're talking about a distribution of the -called p", "image_path": "img_data/video_69_chunk_70.jpg"}
{"video": "video_69", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "we're talking about a distribution of the -called p model and this distribution should be very close to the unknown to us p very close to the unknown to us p data hatut and this p data hut data hatut and this p data hut distribution is going to be distribution is going to be effectively connect that these two effectively connect that these two distributions with what we have distributions with what we have discussed earlier as relative entropy discussed earlier as relative entropy we will be minimizing the we will be minimizing the probabilistic distance by this kind probabilistic distance by this kind of approach between p model and p data", "image_path": "img_data/video_69_chunk_71.jpg"}
{"video": "video_69", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "probabilistic distance by this kind of approach between p model and p data of approach between p model and p data cut let me just write it cut let me just write it down for the negative lo likelihood will need to be connected to relative entropy also known as kl", "image_path": "img_data/video_69_chunk_72.jpg"}
{"video": "video_69", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "also known as kl divergence and divergence kl divergence and relative entropy is one and the same relative entropy is one and the same thing let's try to make this kind of thing let's try to make this kind of connection and ultimately also connect connection and ultimately also connect also with the cross entropy because we also with the cross entropy because we said cross entropy is going to be our said cross entropy is going to be our final destination in terms of loss we final destination in terms of loss we need to do some kind of manipulation need to do some kind of manipulation in order to go from the negative log in order to go from the negative log likelihood to relative entropy and then likelihood to relative entropy and then to cross entropy that's what we'll do to cross entropy that's what we'll do next in our side we actually see the kind", "image_path": "img_data/video_69_chunk_73.jpg"}
{"video": "video_69", "start": "0:37:00", "end": "0:37:30", "timestamp": "0:37:00 - 0:37:30", "text": "in our side we actually see the kind of equation version of what we have just of equation version of what we have just gone through and we started with a gone through and we started with a set of samples on the rear line and we set of samples on the rear line and we posted this as an optimization problem posted this as an optimization problem this optimization problem is shown here this optimization problem is shown here as the optimal set of parameters w that as the optimal set of parameters w that will result out of the argmax operator will result out of the argmax operator over all set of parameters w of the p over all set of parameters w of the p model of our set of samples that model of our set of samples that someone gave us given the set", "image_path": "img_data/video_69_chunk_74.jpg"}
{"video": "video_69", "start": "0:37:30", "end": "0:38:00", "timestamp": "0:37:30 - 0:38:00", "text": "model of our set of samples that someone gave us given the set someone gave us given the set of parameters w that we have assumed in of parameters w that we have assumed in our hypothesis and we worked in the log our hypothesis and we worked in the log domain as we discussed converted that domain as we discussed converted that into objective function into a sum and into objective function into a sum and we have seen that we have actually we have seen that we have actually reached this point and we have replaced reached this point and we have replaced with p model with a gausian p model and with p model with a gausian p model and we have reached this point we have reached this point where we have now a negative log where we have now a negative log likelihood that we need to optimize and in fact", "image_path": "img_data/video_69_chunk_75.jpg"}
{"video": "video_69", "start": "0:38:00", "end": "0:38:30", "timestamp": "0:38:00 - 0:38:30", "text": "likelihood that we need to optimize and in fact that we need to optimize and in fact this negative lo likelihood can be this negative lo likelihood can be written very generically as the written very generically as the expectation instead of a sample mean expectation instead of a sample mean we have an expectation over all of our we have an expectation over all of our samples x that someone gave us out samples x that someone gave us out of this p data hat distribution of this p data hat distribution the next step in our sort of the next step in our sort of journey around maximum likelihood is to journey around maximum likelihood is to connect that objective function the", "image_path": "img_data/video_69_chunk_76.jpg"}
{"video": "video_69", "start": "0:38:30", "end": "0:39:00", "timestamp": "0:38:30 - 0:39:00", "text": "connect that objective function the that objective function the negative lo objective function with negative lo objective function with the kl divergence this is the kl divergence this is the definition that we have seen earlier definition that we have seen earlier regarding kl divergence let's write regarding kl divergence let's write it down i've seen that let's see w nl is", "image_path": "img_data/video_69_chunk_77.jpg"}
{"video": "video_69", "start": "0:39:00", "end": "0:39:30", "timestamp": "0:39:00 - 0:39:30", "text": "w nl is minus it's actually ar me over all set of parameters w of a quantity that it is actually quantity that it is actually minus the expectation over all sets of numbers expectation over all sets of numbers that can be produced out of that can be produced out of my p data distribution log of a p", "image_path": "img_data/video_69_chunk_78.jpg"}
{"video": "video_69", "start": "0:39:30", "end": "0:40:00", "timestamp": "0:39:30 - 0:40:00", "text": "distribution log of a p model x given w this is this was the discussion we just had and now we are going to we just had and now we are going to borrow the definition of kl borrow the definition of kl divergence from the relative entropy divergence from the relative entropy discussion this is just purely discussion this is just purely definition but now instead of p and q definition but now instead of p and q distrib tions we are going to plug in", "image_path": "img_data/video_69_chunk_79.jpg"}
{"video": "video_69", "start": "0:40:00", "end": "0:40:30", "timestamp": "0:40:00 - 0:40:30", "text": "definition but now instead of p and q distrib tions we are going to plug in distrib tions we are going to plug in the corresponding distributions involved the corresponding distributions involved in the ving block diagram this is the in the ving block diagram this is the notation of kl divergence of two notation of kl divergence of two distributions p data hat and p model distributions p data hat and p model that we saw that are involved in our that we saw that are involved in our learning process and by definition k divergence process and by definition k divergence is the expectation over x distributed according to the p data x distributed according to the p data hat distribution of log of p data", "image_path": "img_data/video_69_chunk_80.jpg"}
{"video": "video_69", "start": "0:40:30", "end": "0:41:00", "timestamp": "0:40:30 - 0:41:00", "text": "distribution of log of p data hatut of x minus log of my p model of x given w this is by definition and actually definition and actually we can observe if we are to write in we can observe if we are to write in other words our objective function as other words our objective function as in terms of kl divergence we note", "image_path": "img_data/video_69_chunk_81.jpg"}
{"video": "video_69", "start": "0:41:00", "end": "0:41:30", "timestamp": "0:41:00 - 0:41:30", "text": "other words our objective function as in terms of kl divergence we note in terms of kl divergence we note that only the second term that only the second term involves our set of parameters involves our set of parameters w the first term this term over here w the first term this term over here is independent of the model and therefore we can see that the negative lo lihood minimization problem", "image_path": "img_data/video_69_chunk_82.jpg"}
{"video": "video_69", "start": "0:41:30", "end": "0:42:00", "timestamp": "0:41:30 - 0:42:00", "text": "the negative lo lihood minimization problem negative lo lihood minimization problem and this k divergence can be and this k divergence can be immediately connected we can actually immediately connected we can actually see that minimizing kl divergence effectively minimizes also identically", "image_path": "img_data/video_69_chunk_83.jpg"}
{"video": "video_69", "start": "0:42:00", "end": "0:42:30", "timestamp": "0:42:00 - 0:42:30", "text": "minimizes also identically the negative log likelihood because scale diverence if likelihood because scale diverence if you ignore this term is identical to the you ignore this term is identical to the expression we got with the expression we got with the minimization problem i we posst to minimization problem i we posst to find the optimal set of parameters w find the optimal set of parameters w this is what we call earlier also a w this is what we call earlier also a w star let me also call it w star here the", "image_path": "img_data/video_69_chunk_84.jpg"}
{"video": "video_69", "start": "0:42:30", "end": "0:43:00", "timestamp": "0:42:30 - 0:43:00", "text": "this is what we call earlier also a w star let me also call it w star here the star let me also call it w star here the outcome of this minimization and because kl p data hat p model is expected value of x the data hat of", "image_path": "img_data/video_69_chunk_85.jpg"}
{"video": "video_69", "start": "0:43:00", "end": "0:43:30", "timestamp": "0:43:00 - 0:43:30", "text": "the data hat of minus of the log of the p minus of the log of the p model f given w that's one thing and now what we need to do is to introduce a now what we need to do is to introduce a new definition that we call cross a new definition that we call cross entropy and this cross entropy is going to be ultimately connected to the kl", "image_path": "img_data/video_69_chunk_86.jpg"}
{"video": "video_69", "start": "0:43:30", "end": "0:44:00", "timestamp": "0:43:30 - 0:44:00", "text": "and this cross entropy is going to be ultimately connected to the kl ultimately connected to the kl divergence or relative entropy by divergence or relative entropy by definition and without making any definition and without making any assumptions and therefore it will be assumptions and therefore it will be ultimately connected to the problem of ultimately connected to the problem of maximizing minimizing sorry the maximizing minimizing sorry the negative l root function the negative l root function the cross entropy that is going to be u it's cross entropy that is going to be u it's a joint entropy will be symbolized by c and will entropy will be symbolized by c and will be the joint the entropy of the joint", "image_path": "img_data/video_69_chunk_87.jpg"}
{"video": "video_69", "start": "0:44:00", "end": "0:44:30", "timestamp": "0:44:00 - 0:44:30", "text": "entropy will be symbolized by c and will be the joint the entropy of the joint be the joint the entropy of the joint distribution the joint entropy of distribution the joint entropy of p data hatut and p model to use our distributions that we model to use our distributions that we have we are they are involved in have we are they are involved in the learning process and this is going the learning process and this is going to be evidently according to this definition to the sc of the p", "image_path": "img_data/video_69_chunk_88.jpg"}
{"video": "video_69", "start": "0:44:30", "end": "0:45:00", "timestamp": "0:44:30 - 0:45:00", "text": "sc of the p data hat p model plus the entropy of the marginal distribution p data hat this is marginal distribution p data hat this is the definition of cross the definition of cross entropy the given that this entropy the given that this term now is independent for our supervised learning independent for our supervised learning problem and constant given the fact problem and constant given the fact that we have been", "image_path": "img_data/video_69_chunk_89.jpg"}
{"video": "video_69", "start": "0:45:00", "end": "0:45:30", "timestamp": "0:45:00 - 0:45:30", "text": "problem and constant given the fact that we have been given a set of data given we have been given a set of data points x this is now constant we can conclude that the only thing that the cross entropy only thing that the cross entropy in our specific supervised learning in our specific supervised learning problem this is a very big star over problem this is a very big star over here where star is that for", "image_path": "img_data/video_69_chunk_90.jpg"}
{"video": "video_69", "start": "0:45:30", "end": "0:46:00", "timestamp": "0:45:30 - 0:46:00", "text": "is that for supervised well i don't to say supervised lar because we don't see any supervised lar because we don't see any we i don't have any labels at this point but we will call labels at this point but we will call that a bit call out this fact a bit that a bit call out this fact a bit later for our problem our learning problem that problem our learning problem that is only this is equivalent to kl is only this is equivalent to kl diver", "image_path": "img_data/video_69_chunk_91.jpg"}
{"video": "video_69", "start": "0:46:00", "end": "0:46:30", "timestamp": "0:46:00 - 0:46:30", "text": "is only this is equivalent to kl diver either we optimize cross entropy given the data that someone is giving given the data that someone is giving us and the fact that we don't have us and the fact that we don't have any uncertainty on those data because any uncertainty on those data because they are given that this we they are given that this we have we don't have any dependency on have we don't have any dependency on parameters or any of that sort this parameters or any of that sort this is our kl divergence is equivalent is our kl divergence is equivalent to cross entropy from now on we'll be", "image_path": "img_data/video_69_chunk_92.jpg"}
{"video": "video_69", "start": "0:46:30", "end": "0:47:00", "timestamp": "0:46:30 - 0:47:00", "text": "is our kl divergence is equivalent to cross entropy from now on we'll be to cross entropy from now on we'll be calling the loss function the calling the loss function the objective function that is from now on our marginal objective i will call it a margin the marginal maximum likelihood or", "image_path": "img_data/video_69_chunk_93.jpg"}
{"video": "video_69", "start": "0:47:00", "end": "0:47:30", "timestamp": "0:47:00 - 0:47:30", "text": "objective i will call it a margin the marginal maximum likelihood or minimum negative lri objective is going to be equivalent to objective is going to be equivalent to cross entropy and we'll be symbolizing cross entropy since symbolizing cross entropy since the in this specific problem we'll be", "image_path": "img_data/video_69_chunk_94.jpg"}
{"video": "video_69", "start": "0:47:30", "end": "0:48:00", "timestamp": "0:47:30 - 0:48:00", "text": "symbolizing cross entropy since the in this specific problem we'll be the in this specific problem we'll be symbolizing it without any label symbolizing it without any label we'll just leave it as such the we'll just leave it as such the cross entropy of the data hatut and p model for now we'll just hatut and p model for now we'll just call it as cross entropy of w because we call it as cross entropy of w because we know that only one this is a function know that only one this is a function of only the parameters w later when we of only the parameters w later when we introduce back our labels then we will introduce back our labels then we will be calling it cross entropy of y be calling it cross entropy of y comma y but for now we can just leave it", "image_path": "img_data/video_69_chunk_95.jpg"}
{"video": "video_69", "start": "0:48:00", "end": "0:48:30", "timestamp": "0:48:00 - 0:48:30", "text": "be calling it cross entropy of y comma y but for now we can just leave it comma y but for now we can just leave it as such and therefore we hope that by as such and therefore we hope that by minimizing it we will be u this minimizing it we will be u this minimization of the cross entropy this minimization of the cross entropy in other words the minimization of the in other words the minimization of the distance between p data hat p model is distance between p data hat p model is an intuitively now understood sort of an intuitively now understood sort of concept and we will the time it has come concept and we will the time it has come to actually see some summary in terms to actually see some summary in terms of the not we actually have built over of the not we actually have built over here and this notebook can be seen on", "image_path": "img_data/video_69_chunk_96.jpg"}
{"video": "video_69", "start": "0:48:30", "end": "0:49:00", "timestamp": "0:48:30 - 0:49:00", "text": "of the not we actually have built over here and this notebook can be seen on here and this notebook can be seen on our kind of course site following our kind of course site following this kind of discussion we have this kind of discussion we have some data that someone is actually some data that someone is actually giving us this is the data that we have giving us this is the data that we have that someone is giving us that someone is giving us evidently these are shown as indicate evidently these are shown as indicate numbers but can be floating point numbers but can be floating point numbers and we have now two different p numbers and we have now two different p models plotted here gausian p models we models plotted here gausian p models we need to select which one makes sense for need to select which one makes sense for us for the data that someone gave us", "image_path": "img_data/video_69_chunk_97.jpg"}
{"video": "video_69", "start": "0:49:00", "end": "0:49:30", "timestamp": "0:49:00 - 0:49:30", "text": "need to select which one makes sense for us for the data that someone gave us for the data that someone gave us and we will be doing that by and we will be doing that by calculating as we have done earlier the calculating as we have done earlier the log likelihood of this the data log likelihood of this the data that was given we have that was given we have two hypothesis mu1 and mu2 two hypothesis mu1 and mu2 correspondingly mu1 and sigma 1 squar or correspondingly mu1 and sigma 1 squar or standard deviation sigma 1 and mu2 and standard deviation sigma 1 and mu2 and sigma 2 squ or standard deviation sigma 2 squ or standard deviation sigma 2 for each of these two standard dev 2 for each of these two standard dev varations we actually can go ahead and varations we actually can go ahead and calculate the log likelihood which is", "image_path": "img_data/video_69_chunk_98.jpg"}
{"video": "video_69", "start": "0:49:30", "end": "0:50:00", "timestamp": "0:49:30 - 0:50:00", "text": "varations we actually can go ahead and calculate the log likelihood which is calculate the log likelihood which is evidently will be that summation we have evidently will be that summation we have seen earlier over the set of sampol the seen earlier over the set of sampol the m sampol that someone gave us we can m sampol that someone gave us we can actually see the two hypothesis and we actually see the two hypothesis and we can actually produce the log can actually produce the log likelihood we can actually select likelihood we can actually select the maximum or we can equivalent select the maximum or we can equivalent select the minimum of the negative lo the minimum of the negative lo likelihood in this case we don't the likelihood in this case we don't the negative lo likelihood but just the lo negative lo likelihood but just the lo likelihood and evidently the hypothesis", "image_path": "img_data/video_69_chunk_99.jpg"}
{"video": "video_69", "start": "0:50:00", "end": "0:50:30", "timestamp": "0:50:00 - 0:50:30", "text": "negative lo likelihood but just the lo likelihood and evidently the hypothesis likelihood and evidently the hypothesis that corresponds to min - that corresponds to min - 33.3 corresponds to the hypothesis of 33.3 corresponds to the hypothesis of where mean 7 standard deviation of three where mean 7 standard deviation of three this one will be the maximum this one will be the maximum log likelihood and that's going to be log likelihood and that's going to be the better hypothesis out of the two the better hypothesis out of the two remember we only offered here two remember we only offered here two hypothesis and we just need to select hypothesis and we just need to select one of the two we are what we have done here is we", "image_path": "img_data/video_69_chunk_100.jpg"}
{"video": "video_69", "start": "0:50:30", "end": "0:51:00", "timestamp": "0:50:30 - 0:51:00", "text": "one of the two we are what we have done here is we are what we have done here is we have plotted this negative log have plotted this negative log function as a function of only the function as a function of only the mean by keeping the standard deviation mean by keeping the standard deviation or variance constant and actually we can or variance constant and actually we can see the form that we have plotted see the form that we have plotted earlier regarding our shape and form earlier regarding our shape and form of this convex negative logitude of this convex negative logitude function we can actually see that function we can actually see that it's very straightforward in this it's very straightforward in this specific problem of course to specific problem of course to minimize to min to minimize it and", "image_path": "img_data/video_69_chunk_101.jpg"}
{"video": "video_69", "start": "0:51:00", "end": "0:51:30", "timestamp": "0:51:00 - 0:51:30", "text": "specific problem of course to minimize to min to minimize it and minimize to min to minimize it and therefore we will need to come up with therefore we will need to come up with an algorithm that is not going to be an algorithm that is not going to be other than the stochastic grade descent other than the stochastic grade descent again that will minimize our again that will minimize our function we'll see that in function we'll see that in a sort of a complete notebook when we a sort of a complete notebook when we are facing again now the regression are facing again now the regression problem now we need to go into this same problem now we need to go into this same discussion again but in this time we discussion again but in this time we will introduce labels and we'll see will introduce labels and we'll see how this objective function", "image_path": "img_data/video_69_chunk_102.jpg"}
{"video": "video_69", "start": "0:51:30", "end": "0:52:00", "timestamp": "0:51:30 - 0:52:00", "text": "will introduce labels and we'll see how this objective function how this objective function is which is called conditional max is which is called conditional max maximum likelihood is turns out to maximum likelihood is turns out to be in terms of pure equation of the be in terms of pure equation of the loss function for conditional models loss function for conditional models everything we have discussed on marginal everything we have discussed on marginal models applies equally to conditional models applies equally to conditional models and the conditional models we models and the conditional models we have a probability distribution of our p have a probability distribution of our p model of y given x therefore all we model of y given x therefore all we need to do is go to the very last need to do is go to the very last expression we had about either cross expression we had about either cross entropy or negative log likelihood and", "image_path": "img_data/video_69_chunk_103.jpg"}
{"video": "video_69", "start": "0:52:00", "end": "0:52:30", "timestamp": "0:52:00 - 0:52:30", "text": "expression we had about either cross entropy or negative log likelihood and entropy or negative log likelihood and replace the p module of x given y with a replace the p module of x given y with a p model of y given x and p model of y given x and w and in this way we actually w and in this way we actually arrived in exactly the same equation and arrived in exactly the same equation and all we need to do now is minimize the all we need to do now is minimize the cross entropy for the conditional cross entropy for the conditional maximum likelihood this way we will u maximum likelihood this way we will u adopting that kind of formula we adopting that kind of formula we can actually go ahead and solve the", "image_path": "img_data/video_69_chunk_104.jpg"}
{"video": "video_69", "start": "0:52:30", "end": "0:53:00", "timestamp": "0:52:30 - 0:53:00", "text": "adopting that kind of formula we can actually go ahead and solve the can actually go ahead and solve the regression problem and not only regression problem and not only present as we said in the very beginning present as we said in the very beginning prediction a why hat but that prediction a why hat but that prediction also is comes together prediction also is comes together with some the confidence that we have with some the confidence that we have about it which is essential to any about it which is essential to any prediction machine we'll do that for prediction machine we'll do that for the regression problem and then we will the regression problem and then we will meet again when we have a very meet again when we have a very important diagram to describe very important diagram to describe which captures the essence of the which captures the essence of the regression setting in the with", "image_path": "img_data/video_69_chunk_105.jpg"}
{"video": "video_69", "start": "0:53:00", "end": "0:53:30", "timestamp": "0:53:00 - 0:53:30", "text": "which captures the essence of the regression setting in the with regression setting in the with the probabbly distributions with the probabbly distributions present earlier we have seen the present earlier we have seen the sinusoidal data set of someone's giving sinusoidal data set of someone's giving us a number m of examples that describe us a number m of examples that describe the relationship u of between x and y the relationship u of between x and y and what we have actually done is and what we have actually done is that we had come up with a sort of a that we had come up with a sort of a hypothesis function g that will allow us hypothesis function g that will allow us to make predictions before what we to make predictions before what we had is we had some kind of an x zero", "image_path": "img_data/video_69_chunk_106.jpg"}
{"video": "video_69", "start": "0:53:30", "end": "0:54:00", "timestamp": "0:53:30 - 0:54:00", "text": "to make predictions before what we had is we had some kind of an x zero had is we had some kind of an x zero over here or let me call this x new that over here or let me call this x new that we have never seen before and using if we have never seen before and using if you the final hypothesis we you the final hypothesis we were able to determine a y were able to determine a y hat new this y is evidently a floating new this y is evidently a floating point number a real number let's say point number a real number let's say price of the house and it was price of the house and it was no uncertainty was actually possible no uncertainty was actually possible that to report as far as this prediction", "image_path": "img_data/video_69_chunk_107.jpg"}
{"video": "video_69", "start": "0:54:00", "end": "0:54:30", "timestamp": "0:54:00 - 0:54:30", "text": "no uncertainty was actually possible that to report as far as this prediction that to report as far as this prediction is concerned now with a new is concerned now with a new probabilistic kind of setting something probabilistic kind of setting something interesting is happening we need to interesting is happening we need to instead of a hypothesis that will be a instead of a hypothesis that will be a function we'll have a hypothesis that function we'll have a hypothesis that will be a probability distribution and will be a probability distribution and this hypothesis will be the this hypothesis will be the corresponding p model which is going to be the model which is going to be the conditional p model of x comma conditional p model of x comma w and this for this spefic w and this for this spefic example we be using a normal", "image_path": "img_data/video_69_chunk_108.jpg"}
{"video": "video_69", "start": "0:54:30", "end": "0:55:00", "timestamp": "0:54:30 - 0:55:00", "text": "w and this for this spefic example we be using a normal example we be using a normal distribution this normal distribution is distribution this normal distribution is going to have obviously has a mean and a going to have obviously has a mean and a variance i'm going to just quote here variance i'm going to just quote here the normal distribution where we have the normal distribution where we have the y given x comma and as w we're the y given x comma and as w we're going to have here a mean that mean is going to have here a mean that mean is going to be some let me write it going to be some let me write it below it we have a bit more space", "image_path": "img_data/video_69_chunk_109.jpg"}
{"video": "video_69", "start": "0:55:00", "end": "0:55:30", "timestamp": "0:55:00 - 0:55:30", "text": "going to be some let me write it below it we have a bit more space it will be normal of y given x comma the w normal of y given x comma the w is going to be g of x comma w and then i'm going to have x comma w and then i'm going to have some variance that will actually capture the variance that will actually capture the uncertainty what we have now with uncertainty what we have now with this kind of p model we have the", "image_path": "img_data/video_69_chunk_110.jpg"}
{"video": "video_69", "start": "0:55:30", "end": "0:56:00", "timestamp": "0:55:30 - 0:56:00", "text": "uncertainty what we have now with this kind of p model we have the this kind of p model we have the conditional mean write it down that the conditional mean of our p model captures the regression function what we have seen earlier this regression function or hypothesis", "image_path": "img_data/video_69_chunk_111.jpg"}
{"video": "video_69", "start": "0:56:00", "end": "0:56:30", "timestamp": "0:56:00 - 0:56:30", "text": "function what we have seen earlier this regression function or hypothesis regression function or hypothesis was the x comma w and the and this and in fact what i can actually do here to avoid any what i can actually do here to avoid any kind of confusion i can call this w kind of confusion i can call this w theta to avoid any confusion between the theta to avoid any confusion between the parameters of our p model the parameters of our p model the probability listing model and the probability listing model and the parameters that get to define the shape parameters that get to define the shape and form of this function this was", "image_path": "img_data/video_69_chunk_112.jpg"}
{"video": "video_69", "start": "0:56:30", "end": "0:57:00", "timestamp": "0:56:30 - 0:57:00", "text": "parameters that get to define the shape and form of this function this was and form of this function this was our now g of x comma w which is our now g of x comma w which is our final hypothesis therefore i will i final hypothesis therefore i will i can def define it as kind of a w can def define it as kind of a w star and evidently this came out of star and evidently this came out of some optimization procedure which is some optimization procedure which is going to be the stochastic r descent and going to be the stochastic r descent and this g of x comma w star is going to this g of x comma w star is going to be now our conditional mean i call it be now our conditional mean i call it conditional mean because it is the", "image_path": "img_data/video_69_chunk_113.jpg"}
{"video": "video_69", "start": "0:57:00", "end": "0:57:30", "timestamp": "0:57:00 - 0:57:30", "text": "be now our conditional mean i call it conditional mean because it is the conditional mean because it is the mean of a conditional probability mean of a conditional probability distribution if i am to plot this distribution if i am to plot this conditional prob distribution if i give conditional prob distribution if i give you an x new and i ask you to make a you an x new and i ask you to make a prediction well this conditional prediction well this conditional probability distribution will be some probability distribution will be some kind of gausian that will be centered around gausian that will be centered around this mean this will be the mean of this mean this will be the mean of the condition of the normal of the condition of the normal model here the gausian model and normal model here the gausian model and there will be some kind of a there will be some kind of a variance which i call it here as kind", "image_path": "img_data/video_69_chunk_114.jpg"}
{"video": "video_69", "start": "0:57:30", "end": "0:58:00", "timestamp": "0:57:30 - 0:58:00", "text": "there will be some kind of a variance which i call it here as kind variance which i call it here as kind of sigma squar to capture the inherent squar to capture the inherent variability of my data here i just variability of my data here i just ploted for a few x's ploted for a few x's represent my data set but definitely the represent my data set but definitely the data set is could potentially be far data set is could potentially be far more uncertain than the one i just drew uncertain than the one i just drew and that sigma square will actually and that sigma square will actually capture the inherent variability of y", "image_path": "img_data/video_69_chunk_115.jpg"}
{"video": "video_69", "start": "0:58:00", "end": "0:58:30", "timestamp": "0:58:00 - 0:58:30", "text": "and that sigma square will actually capture the inherent variability of y capture the inherent variability of y of my ground truth labels my of my ground truth labels my ground truth labels are sort of ground truth labels are sort of described with this variance described with this variance the parameters theta of the p model the parameters theta of the p model capture the only uncertain thing that it capture the only uncertain thing that it is over here in any conditional prob is over here in any conditional prob distribution which is the y because distribution which is the y because everything after the given symbol is everything after the given symbol is given therefore it's not random thea given therefore it's not random thea captures the uncertainty the mean", "image_path": "img_data/video_69_chunk_116.jpg"}
{"video": "video_69", "start": "0:58:30", "end": "0:59:00", "timestamp": "0:58:30 - 0:59:00", "text": "given therefore it's not random thea captures the uncertainty the mean captures the uncertainty the mean and the uncertainty and the variance and the uncertainty and the variance of this gausian model for my labels of this gausian model for my labels and that's why the gausian distribution and that's why the gausian distribution is not over the x- axis as it was before is not over the x- axis as it was before in when we had been discussing the in when we had been discussing the marginal problem but it is now flipped marginal problem but it is now flipped and along the y ais in any and along the y ais in any conditional pro probabilistic model conditional pro probabilistic model i think this is a very important i think this is a very important kind of diagram and is actually kind of diagram and is actually also shown here in your site there", "image_path": "img_data/video_69_chunk_117.jpg"}
{"video": "video_69", "start": "0:59:00", "end": "0:59:30", "timestamp": "0:59:00 - 0:59:30", "text": "kind of diagram and is actually also shown here in your site there also shown here in your site there are some kind of notational kind of are some kind of notational kind of differences but that's the essence we differences but that's the essence we have some kind of a data the have some kind of a data the examples that someone gave data the examples that someone gave us carry some kind of uncertainty over us carry some kind of uncertainty over the y axis and for a given value of x we the y axis and for a given value of x we call it x new we have a probability a p call it x new we have a probability a p model that will describe model that will describe the conditional probability the conditional probability distribution that's a ga np model as", "image_path": "img_data/video_69_chunk_118.jpg"}
{"video": "video_69", "start": "0:59:30", "end": "1:00:00", "timestamp": "0:59:30 - 1:00:00", "text": "the conditional probability distribution that's a ga np model as distribution that's a ga np model as you can see here whose mean is the you can see here whose mean is the right on top of this regression function right on top of this regression function the regression function itself is a the regression function itself is a function that will involve again some function that will involve again some form of features the it's a form of features the it's a parameterized function over the set of parameterized function over the set of parameters w that we have seen earlier parameters w that we have seen earlier in the deterministic version of this in the deterministic version of this problem and here it is kind of shown problem and here it is kind of shown as this kind of conditional mean and now", "image_path": "img_data/video_69_chunk_119.jpg"}
{"video": "video_69", "start": "1:00:00", "end": "1:00:30", "timestamp": "1:00:00 - 1:00:30", "text": "problem and here it is kind of shown as this kind of conditional mean and now as this kind of conditional mean and now we are able to do predictions we are able to do predictions we are going to be able to do predictions by going to be able to do predictions by reporting the mean which is going to be reporting the mean which is going to be our estimate of the house price let's our estimate of the house price let's say 350,000 but also we can respond 350,000 but also we can respond report a confidence interval that it report a confidence interval that it will be driven obviously by that will be driven obviously by that variance of this gausian distribution variance of this gausian distribution let's say plus or minus let's say plus or minus $50,000 that's basically the ence $50,000 that's basically the ence of the conditional model of the that of the conditional model of the that is present in the in any in the", "image_path": "img_data/video_69_chunk_120.jpg"}
{"video": "video_69", "start": "1:00:30", "end": "1:01:00", "timestamp": "1:00:30 - 1:01:00", "text": "of the conditional model of the that is present in the in any in the is present in the in any in the regression setting and we'll see again regression setting and we'll see again now soon the equivalent model for now soon the equivalent model for the classification setting in a in the next setting in a in the next video it's also worth noting here that video it's also worth noting here that all of the parameters that we see that all of the parameters that we see that require for us to actually go and require for us to actually go and optimize the parameters both the optimize the parameters both the all of the theta parameters all of the theta parameters the theta parameter is evidently the theta parameter is evidently the parameters of our p model that includes parameters of our p model that includes the mean and the", "image_path": "img_data/video_69_chunk_121.jpg"}
{"video": "video_69", "start": "1:01:00", "end": "1:01:24", "timestamp": "1:01:00 - 1:01:24", "text": "parameters of our p model that includes the mean and the variance all of these parameters have variance all of these parameters have to be optimized by a stochastic rated to be optimized by a stochastic rated descent the stochastic gr descent the stochastic gr descent instead of just divide u coming up instead of just divide u coming up with the values w that the -called w with the values w that the -called w starts that will plot the mean it also starts that will plot the mean it also has to come up with an optimal has to come up with an optimal variance that it will determine that variance that it will determine that kind of uncer", "image_path": "img_data/video_69_chunk_122.jpg"}
{"video": "video_70", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in an earlier video we actually introduced the course and went through introduced the course and went through the syllabus in this video we will start the syllabus in this video we will start with the following question can with the following question can anyone spot a strange thing about this anyone spot a strange thing about this boat this picture was taken in boat this picture was taken in 1844 in a port in europe and it was actually if you europe and it was actually if you take a closer look you can actually see take a closer look you can actually see that this boat has both masss and new that this boat has both masss and new technology at the time called the steam", "image_path": "img_data/video_70_chunk_0.jpg"}
{"video": "video_70", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "that this boat has both masss and new technology at the time called the steam technology at the time called the steam engine and the main reason was that engine and the main reason was that probably people did not trust that new probably people did not trust that new technology to cross the pond and they technology to cross the pond and they wanted definitely to avoid this ship wanted definitely to avoid this ship to being sunk for the u midway to being sunk for the u midway from the europe to america we're actually in a very much america we're actually in a very much similar situation with the other new similar situation with the other new technology called artificial technology called artificial intelligence although we have deployed", "image_path": "img_data/video_70_chunk_1.jpg"}
{"video": "video_70", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "technology called artificial intelligence although we have deployed intelligence although we have deployed many ai sl machine learning systems many ai sl machine learning systems out there they are limited to nonm out there they are limited to nonm mission crtical applications what are mission crtical applications what are those mission crtical applications we're those mission crtical applications we're referring to here well our networks all referring to here well our networks all of our networks the grid network of our networks the grid network the telecommunication network all the telecommunication network all sorts of mission critical sorts of mission critical infrastructures are they have control infrastructures are they have control mechanism which are still based on mechanism which are still based on the rule based system and", "image_path": "img_data/video_70_chunk_2.jpg"}
{"video": "video_70", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "mechanism which are still based on the rule based system and the rule based system and specifically designed by humans to specifically designed by humans to take care of all possible situations take care of all possible situations that this infrastructures will that this infrastructures will actually face in the vast field of healthcare face in the vast field of healthcare doctors are definitely using doctors are definitely using intelligence systems the -called intelligence systems the -called augmented intelligence systems such augmented intelligence systems such as those that are processing as those that are processing x-rays and cat scans to detect x-rays and cat scans to detect abnormalities surgeons are actually", "image_path": "img_data/video_70_chunk_3.jpg"}
{"video": "video_70", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "x-rays and cat scans to detect abnormalities surgeons are actually abnormalities surgeons are actually using robotic surgical machines in using robotic surgical machines in operating theaters but when life is operating theaters but when life is at stake there's always a human behind at stake there's always a human behind the scenes to monitor and intervene in the scenes to monitor and intervene in robotic systems perhaps we are seeing robotic systems perhaps we are seeing the first application of autopilot the first application of autopilot technology here we see a simulated technology here we see a simulated environment of a highway and a robot environment of a highway and a robot equipped with quite a bit of sensing equipped with quite a bit of sensing and quite advanced perception algorithms", "image_path": "img_data/video_70_chunk_4.jpg"}
{"video": "video_70", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "equipped with quite a bit of sensing and quite advanced perception algorithms and quite advanced perception algorithms that we will come up in detail a bit that we will come up in detail a bit later is having a task to maintain later is having a task to maintain some kind of a constant speed of 40 some kind of a constant speed of 40 something 50 m hour and it's sort something 50 m hour and it's sort of moving along in this kind of highway of moving along in this kind of highway obviously has to avoid to crash to obviously has to avoid to crash to other agents also maintain some kind other agents also maintain some kind of a helpful sort of pleasant travel", "image_path": "img_data/video_70_chunk_5.jpg"}
{"video": "video_70", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "other agents also maintain some kind of a helpful sort of pleasant travel of a helpful sort of pleasant travel for all the inhabitants of the for all the inhabitants of the vehicle we have certain kind of vehicle we have certain kind of constraints of acceleration deceleration constraints of acceleration deceleration jerk and on and when it comes to jerk and on and when it comes to moving in this kind of moving in this kind of lane and comes to another vehicle which lane and comes to another vehicle which actually going slower it will actually going slower it will actually has the logic and the ability to has the logic and the ability to change lanes and very smoothly and lanes and very smoothly and then move back to the center lane then move back to the center lane which is also the safest lane in that", "image_path": "img_data/video_70_chunk_6.jpg"}
{"video": "video_70", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "then move back to the center lane which is also the safest lane in that which is also the safest lane in that environment this kind of environment this kind of environment is and this kind of environment is and this kind of ability that we just saw it kind of ability that we just saw it kind of gives the impression of an agent that gives the impression of an agent that it is actually quite it is actually quite intelligent however for all the intelligent however for all the control tasks we have that we have control tasks we have that we have implemented here it's worth noting implemented here it's worth noting that most of the control tasks are", "image_path": "img_data/video_70_chunk_7.jpg"}
{"video": "video_70", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "implemented here it's worth noting that most of the control tasks are that most of the control tasks are effectively glorified if the nail effectively glorified if the nail statements if there is no car coming on statements if there is no car coming on the left lane then i will move the left lane then i will move then i'll check again for other then i'll check again for other conditions have i passed through the conditions have i passed through the kind of car and can of go back to my kind of car and can of go back to my center lane and on in this center lane and on in this environments which are pretty much easy environments which are pretty much easy because imagine the difference because imagine the difference between a highway driving and an urban between a highway driving and an urban driving we have already deployed ai", "image_path": "img_data/video_70_chunk_8.jpg"}
{"video": "video_70", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "between a highway driving and an urban driving we have already deployed ai driving we have already deployed ai systems that are responsible for systems that are responsible for certain controls but the underlying certain controls but the underlying control logic is still the control logic is still the control logic of an if then else statement that we of an if then else statement that we have seen and we had for have seen and we had for the -called expert systems in the -called expert systems in the 80s based on that demo that we 80s based on that demo that we actually saw what we will do next actually saw what we will do next is we actually draw a block diagram that is we actually draw a block diagram that kind of abstracts what we actually see kind of abstracts what we actually see here on this kind of video and this here on this kind of video and this block diagram will have two", "image_path": "img_data/video_70_chunk_9.jpg"}
{"video": "video_70", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "here on this kind of video and this block diagram will have two block diagram will have two components the m main component here components the m main component here is the environment which we are not is the environment which we are not going to be dissecting other than going to be dissecting other than claiming that the environment contains a claiming that the environment contains a state and the state is state and the state is definitely not known to definitely not known to us imagine for example as you are us imagine for example as you are driving at what kind of sort of driving at what kind of sort of state do you have a visibility of what state do you have a visibility of what happens to when the cars ahead of you happens to when the cars ahead of you obviously not it's the only thing", "image_path": "img_data/video_70_chunk_10.jpg"}
{"video": "video_70", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "happens to when the cars ahead of you obviously not it's the only thing obviously not it's the only thing that is some form of partial that is some form of partial state the -called partial observed state the -called partial observed environments nevertheless the environments nevertheless the environment will encapsulate its global environment will encapsulate its global state and some portion of that could state and some portion of that could potentially be u retrievable via the -called of u retrievable via the -called of u sensors these sensors are potentially sensors these sensors are potentially cameras we have plenty of cameras in cameras we have plenty of cameras in this kind of sort of robots this kind of sort of robots with a", "image_path": "img_data/video_70_chunk_11.jpg"}
{"video": "video_70", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "this kind of sort of robots with a motion we have let's say radars let motion we have let's say radars let me call this as a symbol and we have also this as a symbol and we have also liers let me call this riders with liers let me call this riders with this symbol and all of these this symbol and all of these kind of sensors are kind of in many kind of sensors are kind of in many instances fused together we will not be dealing with all together we will not be dealing with all the details in this course but they are what is", "image_path": "img_data/video_70_chunk_12.jpg"}
{"video": "video_70", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "details in this course but they are what is course but they are what is happening is at the output of this kind happening is at the output of this kind of fusion we are going to be of fusion we are going to be processing all this kind of sensing data processing all this kind of sensing data this symbol will be using here is x this symbol will be using here is x and we'll put an underbar under it in and we'll put an underbar under it in order for us to designate that this x is order for us to designate that this x is a highly dimensional quantity in general a highly dimensional quantity in general we'll discuss about dimensionality in a we'll discuss about dimensionality in a moment and the first subsystem that we moment and the first subsystem that we will be dealing in this course is called will be dealing in this course is called the perception", "image_path": "img_data/video_70_chunk_13.jpg"}
{"video": "video_70", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "will be dealing in this course is called the perception subsystem and this perception subsystem will take this x and will subsystem will take this x and will produce a quantity that will be coiling produce a quantity that will be coiling from now on y hat yut in general is also from now on y hat yut in general is also going to be multi-dimensional but we'll start multi-dimensional but we'll start with a simple use case where it is with a simple use case where it is actually a scalar a bit later in another actually a scalar a bit later in another video now for example to give you an video now for example to give you an idea about the u types of xes that we", "image_path": "img_data/video_70_chunk_14.jpg"}
{"video": "video_70", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "video now for example to give you an idea about the u types of xes that we idea about the u types of xes that we are going to be observing we can for example observe observing we can for example observe video streams as transmitted by these streams as transmitted by these cameras we have typically around eight cameras we have typically around eight cameras in every self-driving car cameras in every self-driving car we have some image we has width of w we have some image we has width of w pixels and some height of h pixels and some height of h pixels and since every pixel is since every pixel is independent of u each other and they're independent of u each other and they're assessing kind of different parts of the", "image_path": "img_data/video_70_chunk_15.jpg"}
{"video": "video_70", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "independent of u each other and they're assessing kind of different parts of the assessing kind of different parts of the image then we have effectively image then we have effectively what we'll be calling from now one what we'll be calling from now one n is the number of dimensions o of x we can actually see that x we can actually see that designation wise notation wise it will designation wise notation wise it will be x belongs to r to the power of be x belongs to r to the power of n that's what it means x is a", "image_path": "img_data/video_70_chunk_16.jpg"}
{"video": "video_70", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "be x belongs to r to the power of n that's what it means x is a n that's what it means x is a let's call it a vector of n dimensions let's call it a vector of n dimensions and this n dimensions will be in for the and this n dimensions will be in for the case of a single frame over here case of a single frame over here will be w * will be w * h as every pixel is independent of each other w pixel is independent of each other w * h dimension some pretty large * h dimension some pretty large dimensions are present already just from dimensions are present already just from the single image kind of the single image kind of perspective and we will see now that perspective and we will see now that and also i can write this as let's", "image_path": "img_data/video_70_chunk_17.jpg"}
{"video": "video_70", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "perspective and we will see now that and also i can write this as let's and also i can write this as let's say a vector from x1 to xn for example n you can imagine xn for example n you can imagine typically let's say that w is 640 typically let's say that w is 640 pixels just 640 pixels and h is another pixels just 640 pixels and h is another 640 pixels you can imagine that n 640 pixels you can imagine that n could be in the many could be in the many thousands and what will be the y thousands and what will be the y hat well let's imagine that this a hat well let's imagine that this a pixel over here sorry this picture", "image_path": "img_data/video_70_chunk_18.jpg"}
{"video": "video_70", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "hat well let's imagine that this a pixel over here sorry this picture pixel over here sorry this picture over here has taken the shot of let's over here has taken the shot of let's say a pedestrian and our task is a simple pedestrian and our task is a simple classification task where we have as one the task where we have as one the pedestrian being present in that kind of pedestrian being present in that kind of scene or r zero the pedestrian is scene or r zero the pedestrian is not present let me just throw a dog or not present let me just throw a dog or whatever in the scene in the specific case our y", "image_path": "img_data/video_70_chunk_19.jpg"}
{"video": "video_70", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "or whatever in the scene in the specific case our y scene in the specific case our y hat will be a discrete random hat will be a discrete random variable that predicts whether the variable that predicts whether the pedestrian is present or not pedestrian is present or not that's that will be the simplest that's that will be the simplest case of a white hat that will come case of a white hat that will come across now perceptions are systems across now perceptions are systems are very heavy in kind of deep learning are very heavy in kind of deep learning technology there's a lot of lots of technology there's a lot of lots of complexity in there and i will spend complexity in there and i will spend considerable time trying to get all considerable time trying to get all the networking and deep networking", "image_path": "img_data/video_70_chunk_20.jpg"}
{"video": "video_70", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "considerable time trying to get all the networking and deep networking the networking and deep networking kind of understood but they are the understood but they are the -called they have a problem and -called they have a problem and they are typically kind of fairly they are typically kind of fairly reflexive is a term that reflexive is a term that we actually use to probably you we actually use to probably you sort of have experienced some kind sort of have experienced some kind of reflex such as sneezing and of reflex such as sneezing and obviously you did but in obviously you did but in the fashion over here in this in the fashion over here in this ception subsystem fashion over here", "image_path": "img_data/video_70_chunk_21.jpg"}
{"video": "video_70", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "in the fashion over here in this ception subsystem fashion over here ception subsystem fashion over here reflexive means that in one frame we can reflexive means that in one frame we can observe this kind of pedestrian but in a observe this kind of pedestrian but in a subsequent kind of frame let's say a subsequent kind of frame let's say a random lighting condition changes light random lighting condition changes light is a very heavy determinant of is a very heavy determinant of performance of computer vision systems performance of computer vision systems we don't despite the fact that the we don't despite the fact that the pedestrian is actually there and hasn't pedestrian is actually there and hasn't really moved from one millisecond to the really moved from one millisecond to the next what we will need to next what we will need to employ is what we call the a", "image_path": "img_data/video_70_chunk_22.jpg"}
{"video": "video_70", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "next what we will need to employ is what we call the a employ is what we call the a probabilistic reasoning over time probabilistic vision over time also time probabilistic vision over time also known as tracking with this kind of systems tracking with this kind of systems what we do is we process these frames what we do is we process these frames in sequence as they're coming out of in sequence as they're coming out of this perception subsystem and we are correcting subsystem and we are correcting tracking if you the objects and", "image_path": "img_data/video_70_chunk_23.jpg"}
{"video": "video_70", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "subsystem and we are correcting tracking if you the objects and tracking if you the objects and then we correcting from this kind of then we correcting from this kind of reflexive or spous type of errors they reflexive or spous type of errors they are actually making the ability for are actually making the ability for us to track is fairly pronounced if you us to track is fairly pronounced if you think about our ability to fill the think about our ability to fill the gaps for example if you can imagine gaps for example if you can imagine that i'm rotating myself let's say that i'm rotating myself let's say 360° you can actually or better go 360° you can actually or better go beyond behind an occlusion beyond behind an occlusion temporarily you can actually infer that", "image_path": "img_data/video_70_chunk_24.jpg"}
{"video": "video_70", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "beyond behind an occlusion temporarily you can actually infer that temporarily you can actually infer that i'm actually behind that kind of i'm actually behind that kind of occlusion and therefore our occlusion and therefore our ability to just look at this ability to just look at this systems this system is built in our systems this system is built in our kind of brains on how we actually track kind of brains on how we actually track objects and how we actually fill the objects and how we actually fill the gaps it's not only perception that we gaps it's not only perception that we fill the gaps u we use this in fill the gaps u we use this in perception kind of problems we use this perception kind of problems we use this kind of probabilistic re over time we kind of probabilistic re over time we also use it in language as i'm also use it in language as i'm actually writing over here your", "image_path": "img_data/video_70_chunk_25.jpg"}
{"video": "video_70", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "also use it in language as i'm actually writing over here your actually writing over here your sometimes my writing is not really sometimes my writing is not really high quality you can if i omit a high quality you can if i omit a sentence or sorry a word you can sort of sentence or sorry a word you can sort of fill the gaps for that specific word fill the gaps for that specific word and you can infer what i was trying to and you can infer what i was trying to say there the next step say there the next step downwards is down this kind of chain is downwards is down this kind of chain is what we call now logical reasoning", "image_path": "img_data/video_70_chunk_26.jpg"}
{"video": "video_70", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "logical reasoning is typically assisted by what we call here the assisted by what we call here the knowledge base and we'll see some examples of all that in a moment and we receive this kind of white tilda over here -cal kind of white tilda over here -cal corrected kind of", "image_path": "img_data/video_70_chunk_27.jpg"}
{"video": "video_70", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "kind of white tilda over here -cal corrected kind of prediction and what we actually prediction and what we actually can do in this probabilistic can do in this probabilistic reasoning over time together in reasoning over time together in combination with kind of logical combination with kind of logical reasoning is to create symbol symbolic reasoning is to create symbol symbolic abstractions symbolic abstractions symbolic abstractions can work in combination with neuron can work in combination with neuron networks the -cal neuros symbolic networks the -cal neuros symbolic systems to create symbolic systems to create symbolic representations of the of our world and representations of the of our world and greatly accelerate inference time or", "image_path": "img_data/video_70_chunk_28.jpg"}
{"video": "video_70", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "greatly accelerate inference time or accelerate inference time or can actually help us to do inference can actually help us to do inference even in the absence of any sensing even in the absence of any sensing information we'll see that in a information we'll see that in a moment how this is could potentially be moment how this is could potentially be done as we discussing the done as we discussing the various ways that we actually can various ways that we actually can implement ai and the fourth subsystem implement ai and the fourth subsystem is the -called planning", "image_path": "img_data/video_70_chunk_29.jpg"}
{"video": "video_70", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "implement ai and the fourth subsystem is the -called planning and in the this box over here i forgot to mention is called knowledge base and it acts as a kind of a database of all of our knowledge at this of all of our knowledge at this moment in time that includes percepts moment in time that includes percepts any logical reasoning that we have any logical reasoning that we have done in terms of inference rules done in terms of inference rules of the world and many others", "image_path": "img_data/video_70_chunk_30.jpg"}
{"video": "video_70", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "done in terms of inference rules of the world and many others of the world and many others planning is the next subsystem down planning is the next subsystem down and in the planning obviously we have determined now we have a task to solve determined now we have a task to solve and we have let's say to go from and we have let's say to go from place a to place b obviously everyone place a to place b obviously everyone has used google maps as a driving has used google maps as a driving directions you everyone actually have directions you everyone actually have used a planning system where google used a planning system where google maps is let's say planning a route for", "image_path": "img_data/video_70_chunk_31.jpg"}
{"video": "video_70", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "used a planning system where google maps is let's say planning a route for maps is let's say planning a route for you and at any moment in time it you and at any moment in time it depending let's say of traffic depending let's say of traffic conditions it will also be able to conditions it will also be able to replan and planning is in this replan and planning is in this case happens with some kind of case happens with some kind of interaction but in many problems we are interaction but in many problems we are dealing with planning that without dealing with planning that without really interacting with the really interacting with the environment and if i distinguish environment and if i distinguish planning without and with interactions planning without and with interactions the one that with interactions", "image_path": "img_data/video_70_chunk_32.jpg"}
{"video": "video_70", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "planning without and with interactions the one that with interactions the one that with interactions it can potentially it is it can be it can potentially it is it can be called reinforcement learning called reinforcement learning reinforcement learning you will see how reinforcement learning is implemented reinforcement learning is implemented and in this reinforcement learning we and in this reinforcement learning we have definitely communication with", "image_path": "img_data/video_70_chunk_33.jpg"}
{"video": "video_70", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "and in this reinforcement learning we have definitely communication with have definitely communication with the environment and in fact u this communication is in many senses one way communication is in many senses one way from the environment to us and this from the environment to us and this ar over here is called reward and of course we need to design if you a reward function inside our if you a reward function inside our agent in order to sort of receive", "image_path": "img_data/video_70_chunk_34.jpg"}
{"video": "video_70", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "if you a reward function inside our agent in order to sort of receive agent in order to sort of receive the right reward at that right moment the right reward at that right moment to lead us to the optimal sort of trajectory such as in optimal sort of trajectory such as in robotic applications in many robotic applications in many instances we'll see that also how instances we'll see that also how this is functioning in a moment all this is functioning in a moment all right and then finally we have a plan right and then finally we have a plan and the last kind of block over and the last kind of block over here is called a controller and the job here is called a controller and the job of the controller is knowing exactly all of the controller is knowing exactly all the details of the robot in this case all the actuator", "image_path": "img_data/video_70_chunk_35.jpg"}
{"video": "video_70", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "the details of the robot in this case all the actuator robot in this case all the actuator details what kind of sort details what kind of sort of individual component controllers has of individual component controllers has wheels it has propeller wheels it has propeller whatever depends on the robot it whatever depends on the robot it executes the plan and effectively takes actions in plan and effectively takes actions in our specific example we saw the actions our specific example we saw the actions where for example the sting angle where for example the sting angle theta and some form of acceleration", "image_path": "img_data/video_70_chunk_36.jpg"}
{"video": "video_70", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "where for example the sting angle theta and some form of acceleration theta and some form of acceleration deceleration a in general this will be a in general this will be the actions and what happens now another cycle is going to be cycle is going to be repeated when the robot is taken action repeated when the robot is taken action the state of the environment is changed the state of the environment is changed let's say the robot move by a certain kind of distance move by a certain kind of distance this change the state of the environment this change the state of the environment this leads to new information about that", "image_path": "img_data/video_70_chunk_37.jpg"}
{"video": "video_70", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "this change the state of the environment this leads to new information about that this leads to new information about that kind of state coming in our agent and kind of state coming in our agent and all these kind of subse kind of blocks all these kind of subse kind of blocks are going to be executed over and are going to be executed over and over again until the environment kind over again until the environment kind of terminates when we reach our of terminates when we reach our destination whatever the task we have destination whatever the task we have the plan we have is fully executed but the second system that i would to show you is kind of an would to show you is kind of an considered to be a bit more evolved than", "image_path": "img_data/video_70_chunk_38.jpg"}
{"video": "video_70", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "would to show you is kind of an considered to be a bit more evolved than considered to be a bit more evolved than the previous eom motion kind of ai the previous eom motion kind of ai agent we still see a robot moving in agent we still see a robot moving in a kind of a closed world the pictures is a kind of a closed world the pictures is pixelated in this case because we pixelated in this case because we have sh this kind of video at the have sh this kind of video at the sort of output of that kind of sensor and output of that kind of sensor and this actually sensor is sort of we this actually sensor is sort of we are down sampling quite considerably are down sampling quite considerably from the camera to produce input from the camera to produce input in this kind of neural network neural", "image_path": "img_data/video_70_chunk_39.jpg"}
{"video": "video_70", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "from the camera to produce input in this kind of neural network neural in this kind of neural network neural networks are not known to require networks are not known to require 4k video in order for them to 4k video in order for them to perform well and also computational to perform well and also computational computationally this actually going to computationally this actually going to be a very heavy sort of a load for be a very heavy sort of a load for the neural network to do the neural network to do the role of this kind of agent is to role of this kind of agent is to move around in this close environment move around in this close environment without really crashing but the without really crashing but the architecture inside it is a bit architecture inside it is a bit different and the one that we have seen different and the one that we have seen earlier instead of me drawing all", "image_path": "img_data/video_70_chunk_40.jpg"}
{"video": "video_70", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "different and the one that we have seen earlier instead of me drawing all earlier instead of me drawing all over this kind of block diagram again over this kind of block diagram again what i'm going to draw here is only what i'm going to draw here is only the agent u kind of a block diagram from the agent u kind of a block diagram from left to right the whole agent is quite considerable uron network this is a deep", "image_path": "img_data/video_70_chunk_41.jpg"}
{"video": "video_70", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "quite considerable uron network this is a deep network that accepts the row information or the sensing information or the sensing information the x under bar that we have information the x under bar that we have seen earlier and directly spits earlier and directly spits out the steering direction and the out the steering direction and the acceleration deceleration alpha now we have an end deceleration alpha now we have an end to endend system that is doing all", "image_path": "img_data/video_70_chunk_42.jpg"}
{"video": "video_70", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "deceleration alpha now we have an end to endend system that is doing all to endend system that is doing all the tasks that the previous subsystems the tasks that the previous subsystems the individual subs subsystems were the individual subs subsystems were actually doing including perception actually doing including perception probabilistic reion over time control probabilistic reion over time control and on planning and on this is and on planning and on this is a second kind of system that we a second kind of system that we can actually have this is called can actually have this is called let's say imitation learning we have plenty for of lear learning approaches and on", "image_path": "img_data/video_70_chunk_43.jpg"}
{"video": "video_70", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "imitation learning we have plenty for of lear learning approaches and on of lear learning approaches and on how we actually can train this kind of how we actually can train this kind of neural networks and these robots neural networks and these robots to behave the way we want but in to behave the way we want but in this kind of imitation learning what we this kind of imitation learning what we actually have done is we have actually have done is we have gotten kind of training data which are a gotten kind of training data which are a bit different than the training data bit different than the training data we were requiring from the earliest we were requiring from the earliest system here we have actually used system here we have actually used humans in the", "image_path": "img_data/video_70_chunk_44.jpg"}
{"video": "video_70", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "in the loop to drive hum humans in the loop to drive hum humans in the loop they drive the robot in the exactly the same environment that's kind of important and therefore we actually can pick up their own steering directions", "image_path": "img_data/video_70_chunk_45.jpg"}
{"video": "video_70", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "important and therefore we actually can pick up their own steering directions pick up their own steering directions and accelerations decelerations and couple them with u decelerations and couple them with u this is now my the y hat and this is now my the y hat and our training data were effectively the our training data were effectively the combinations of images and the y's combinations of images and the y's which are the vector theta and a which are the vector theta and a these were basically the types of these were basically the types of data we have to accumulate we have to", "image_path": "img_data/video_70_chunk_46.jpg"}
{"video": "video_70", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "these were basically the types of data we have to accumulate we have to data we have to accumulate we have to accumulate mountains of this kind accumulate mountains of this kind of data not only data that describe of data not only data that describe the sort of how well the you the sort of how well the driver that is know the driver that is performing well in around this kind performing well in around this kind of environment but also gather of environment but also gather data of corrections and this actually data of corrections and this actually was deemed to be a very was deemed to be a very important for training if you important for training if you an agent that will be able to not only", "image_path": "img_data/video_70_chunk_47.jpg"}
{"video": "video_70", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "important for training if you an agent that will be able to not only an agent that will be able to not only behave well in but also to behave well in but also to be able to correct any mistakes that be able to correct any mistakes that they were making that they were making that's the second kind of subsystem that's the second kind of subsystem that we have u sort of that we have u sort of thought to present and these kind of thought to present and these kind of humans in the loop are increasingly kind humans in the loop are increasingly kind of present in many ai systems we have of present in many ai systems we have seen it even recently with large seen it even recently with large language models and how we can get language models and how we can get them to behave and perform much", "image_path": "img_data/video_70_chunk_48.jpg"}
{"video": "video_70", "start": "0:24:30", "end": "0:24:58.833333", "timestamp": "0:24:30 - 0:24:58.833333", "text": "language models and how we can get them to behave and perform much them to behave and perform much better than without any human feedback better than without any human feedback a final video on the a final video on the what could potentially be the case in what could potentially be the case in robotics in a few years time this is in robotics in a few years time this is in the lab obviously and obviously there is the lab obviously and obviously there is a person behind the scenes with a person behind the scenes with a big red button ready to press to big red button ready to press it the moment something goes wrong", "image_path": "img_data/video_70_chunk_49.jpg"}
{"video": "video_71", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "another area which is actually quite important in artificial intelligence is important in artificial intelligence is that of simulation and the role of that of simulation and the role of simulation that plays in ai today simulation that plays in ai today should not really be overlooked what should not really be overlooked what we actually see here in this image we actually see here in this image are a three-dimensional representation are a three-dimensional representation of a specific patients arteries and of a specific patients arteries and a company called heart flow saw a company called heart flow saw this kind of opportunity a few years this kind of opportunity a few years back it's a multi dollar company i", "image_path": "img_data/video_71_chunk_0.jpg"}
{"video": "video_71", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "this kind of opportunity a few years back it's a multi dollar company i back it's a multi dollar company i believe right now and what they did believe right now and what they did is they took the cut scans and using is they took the cut scans and using those cut scans from hospitals where those cut scans from hospitals where patients were actually going to patients were actually going to understand whether they needed a heart understand whether they needed a heart operation or not and they converted a operation or not and they converted a three-dimensional model of the patient's three-dimensional model of the patient's heart with all the pressure heart with all the pressure pressures being simulated with pressures being simulated with computation of fluid dynamics based on computation of fluid dynamics based on this threedimensional this threedimensional model and that kind of in side helped", "image_path": "img_data/video_71_chunk_1.jpg"}
{"video": "video_71", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "this threedimensional model and that kind of in side helped model and that kind of in side helped helps quite a bit the doctors who want helps quite a bit the doctors who want to make a decision as to whether this to make a decision as to whether this patient needs to have a st procedure patient needs to have a st procedure right away or some other kind of right away or some other kind of intervention in many instances intervention in many instances simulation is being used not to simulation is being used not to therefore design the ai algorithms therefore design the ai algorithms but to provide world models for those but to provide world models for those algorithms to be able to algorithms to be able to provide some kind of augumented", "image_path": "img_data/video_71_chunk_2.jpg"}
{"video": "video_71", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "algorithms to be able to provide some kind of augumented provide some kind of augumented intelligence into fields that we are intelligence into fields that we are either are unable to perform either are unable to perform actually a physical modeling for actually a physical modeling for example designing self-driving kind of cars and designing self-driving kind of cars and all these kind of algorithms that are all these kind of algorithms that are involved is almost impossible to do it involved is almost impossible to do it by crashing cars all the time you by crashing cars all the time you have to have a very accurate simulated have to have a very accurate simulated environments here we actually see a environments here we actually see a different scenario where the different scenario where the simulator is in fact the product", "image_path": "img_data/video_71_chunk_3.jpg"}
{"video": "video_71", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "different scenario where the simulator is in fact the product a few words now on how we actually make progress in this field and in this progress in this field and in this kind of diagram what we try to represent kind of diagram what we try to represent is really three axis here the first axis is really three axis here the first axis is this kind of neuroscience a is this kind of neuroscience a psychology axis and this represents tens psychology axis and this represents tens of years of research in this kind of years of research in this kind of domain anything from functional domain anything from functional decomposition of the brain trying to decomposition of the brain trying to get how the brain kind of work trying to get how the brain kind of work trying to understand how the brain works which is understand how the brain works which is obviously a very difficult kind of task", "image_path": "img_data/video_71_chunk_4.jpg"}
{"video": "video_71", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "understand how the brain works which is obviously a very difficult kind of task obviously a very difficult kind of task but certain point discoveries have been but certain point discoveries have been modeled in what we call now the modeled in what we call now the methods kind of access over the years methods kind of access over the years specifically after the reintroduction of specifically after the reintroduction of neural networks in around 2010 kind of neural networks in around 2010 kind of time frame with the deep time frame with the deep networks present there in many of the networks present there in many of the tasks and also discoveries that have happened discoveries that have happened over there over the years such as the over there over the years such as the fact that neurons that are sort of", "image_path": "img_data/video_71_chunk_5.jpg"}
{"video": "video_71", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "over there over the years such as the fact that neurons that are sort of fact that neurons that are sort of processing our vision are firing processing our vision are firing locally they have led to certain models locally they have led to certain models that we call actually here methods in that we call actually here methods in this axis a lot of researchers are this axis a lot of researchers are actually picking up ideas from this axis actually picking up ideas from this axis and they are modeling providing and they are modeling providing abstractions into this axis even take abstractions into this axis even take for example the neuron is a for example the neuron is a very complicated organism in very complicated organism in reality but it was modeled in a f reality but it was modeled in a f really simple kind of block diagram of", "image_path": "img_data/video_71_chunk_6.jpg"}
{"video": "video_71", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "reality but it was modeled in a f really simple kind of block diagram of really simple kind of block diagram of let's say a dot product followed by let's say a dot product followed by nonlinearity as we will see similar kind nonlinearity as we will see similar kind of modeling approaches of trying to get of modeling approaches of trying to get let's say some model of episodic memory let's say some model of episodic memory or working memory already are present in or working memory already are present in this kind of domain that in this kind of domain that we call machine learning and artificial we call machine learning and artificial intelligent methods the course will intelligent methods the course will be heavy on this kind of axis but at the be heavy on this kind of axis but at the same time no progress will have been same time no progress will have been made unless we i invest quite a bit on", "image_path": "img_data/video_71_chunk_7.jpg"}
{"video": "video_71", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "same time no progress will have been made unless we i invest quite a bit on made unless we i invest quite a bit on frameworks and platforms and we have frameworks and platforms and we have frameworks that are software frameworks that are software frameworks and hardware frameworks and in the and hardware frameworks and in the hardware side companies such as nvidia hardware side companies such as nvidia amd intel and others u that are amd intel and others u that are nowadays building application specific nowadays building application specific ic to handle the computational ic to handle the computational workload of machine learning and ai are workload of machine learning and ai are very much active in this kind of axis", "image_path": "img_data/video_71_chunk_8.jpg"}
{"video": "video_71", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "workload of machine learning and ai are very much active in this kind of axis very much active in this kind of axis software frameworks such as t of low software frameworks such as t of low cara spy torch jacks and many others cara spy torch jacks and many others have sort of appeared to allow have sort of appeared to allow these hardware machines to be able to these hardware machines to be able to be fully utilized and in this kind be fully utilized and in this kind of course we will definitely experience of course we will definitely experience such frameworks and we'll be using in such frameworks and we'll be using in some instances graphical processing some instances graphical processing units gpus which are available in units gpus which are available in the cloud for us if i am to map", "image_path": "img_data/video_71_chunk_9.jpg"}
{"video": "video_71", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "units gpus which are available in the cloud for us if i am to map the cloud for us if i am to map the course kind of intersection in this the course kind of intersection in this kind of diagram that's definitely this kind of diagram that's definitely this kind of a green space over there that kind of a green space over there that we will be acting on and we will be acting on and trying to understand what really are the trying to understand what really are the methods that we should be using for a methods that we should be using for a specific task and how we're going to go specific task and how we're going to go about implementing them obviously there about implementing them obviously there are far more than four approaches to ai are far more than four approaches to ai these are just categories of", "image_path": "img_data/video_71_chunk_10.jpg"}
{"video": "video_71", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "are far more than four approaches to ai these are just categories of these are just categories of approaches and the lots of combinations approaches and the lots of combinations that can actually happen between that can actually happen between those and recently we have seen quite a those and recently we have seen quite a lot of progress in combining for example lot of progress in combining for example neuros symbolic reasoning the four approaches are reasoning the four approaches are presented here the sort of turing test presented here the sort of turing test or test driven approach the cognitive or test driven approach the cognitive model approach the syllogism and the model approach the syllogism and the rational agent approach are treated here rational agent approach are treated here as a", "image_path": "img_data/video_71_chunk_11.jpg"}
{"video": "video_71", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "rational agent approach are treated here as a indicative categories and we should all indicative categories and we should all be critical that at the end of the day be critical that at the end of the day there's not just a single cal bullet there's not just a single cal bullet that can actually provide a solution to that can actually provide a solution to general ai when the t test approach we have a ai when the t test approach we have a machine behind the scenes and unknown to machine behind the scenes and unknown to human we converse with the machine if human we converse with the machine if the human is persuaded that's not the human is persuaded that's not the machine then we can claim that machine then we can claim that machine pass the urine test and there have been", "image_path": "img_data/video_71_chunk_12.jpg"}
{"video": "video_71", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "machine then we can claim that machine pass the urine test and there have been pass the urine test and there have been various kind of attempts to define various kind of attempts to define if you prices along this here if you prices along this here you see a couple of videos along those you see a couple of videos along those lines on for example the amazon alexa lines on for example the amazon alexa price and there are many of price and there are many of others the capabilities that we others the capabilities that we need to have to pass that unit test is need to have to pass that unit test is obviously be able to converse in a obviously be able to converse in a natural language to be able to device natural language to be able to device reasoning and to sort of to", "image_path": "img_data/video_71_chunk_13.jpg"}
{"video": "video_71", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "natural language to be able to device reasoning and to sort of to reasoning and to sort of to create analogies to be able to analogies to be able to recognize objects and to understand recognize objects and to understand scenes to be able also to u move and scenes to be able also to u move and interact with other agents humans interact with other agents humans are lots of progress in robotics in are lots of progress in robotics in actuators sensing and on in actuators sensing and on initially the tuning machine has initially the tuning machine has started with a just conversational kind started with a just conversational kind of a chatboard type of agent", "image_path": "img_data/video_71_chunk_14.jpg"}
{"video": "video_71", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "started with a just conversational kind of a chatboard type of agent of a chatboard type of agent but then has expanded into other domains but then has expanded into other domains u that are appropriate for e motion kind u that are appropriate for e motion kind of agents the approach that actually agents the approach that actually i want to stay a little bit on is the i want to stay a little bit on is the -called cognitive model approach and -called cognitive model approach and along this i wanted to just show along this i wanted to just show you a couple of objects that i have you a couple of objects that i have brought over here these are let's say two objects", "image_path": "img_data/video_71_chunk_15.jpg"}
{"video": "video_71", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "brought over here these are let's say two objects here these are let's say two objects that are let's say present in the scene that are let's say present in the scene a pyramid and a cylinder and i want to kind of start cylinder and i want to kind of start by asking the following question if by asking the following question if you are asked the following question you are asked the following question what object is on the left of the pyramid", "image_path": "img_data/video_71_chunk_16.jpg"}
{"video": "video_71", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "is on the left of the pyramid how about going sort of trying to come up with a reasonable answer the come up with a reasonable answer the moment you have a scene this moment you have a scene this and definitely i mean the first step and definitely i mean the first step is to recognize the entities that are present in this entities that are present in this kind of naturally sort of posted kind of naturally sort of posted question the in order for us question the in order for us to be able to understand if you", "image_path": "img_data/video_71_chunk_17.jpg"}
{"video": "video_71", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "question the in order for us to be able to understand if you to be able to understand if you scenes we have to build a world model of scenes we have to build a world model of the scene that we have we are the scene that we have we are facing i'm just going to go ahead and facing i'm just going to go ahead and define a coordinate system let's define a coordinate system let's the coordinate system be defined here the coordinate system be defined here at the top corner of this pad at the top corner of this pad this could be sort of south and this will be sort of south and this will be or maybe instead of south let me just or maybe instead of south let me just call it a the let's say x and y axises and we", "image_path": "img_data/video_71_chunk_18.jpg"}
{"video": "video_71", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "call it a the let's say x and y axises and we the let's say x and y axises and we also have a z-axis although the also have a z-axis although the experiment is not supposed to be experiment is not supposed to be three dimensional all right we have a three dimensional all right we have a 0 comma 0 coordinate over 0 comma 0 coordinate over here and the first thing i think we here and the first thing i think we should happen every time you're facing a scene happen every time you're facing a scene this let me put it this object this way is to recognize", "image_path": "img_data/video_71_chunk_19.jpg"}
{"video": "video_71", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "this let me put it this object this way is to recognize the location of objects and what they what those objects are definitely you we are going to be involved into recognizing objects be involved into recognizing objects in other words to put a kind of a in other words to put a kind of a bounding box around each object and to be able to", "image_path": "img_data/video_71_chunk_20.jpg"}
{"video": "video_71", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "bounding box around each object and to be able to around each object and to be able to detect it and claim that yes this object detect it and claim that yes this object is located in the centroid this is the is located in the centroid this is the centroid where the object is centroid where the object is located and this object is actually located and this object is actually called a pyramid and we are certain let's say with 95% probability that's the that's 95% probability that's the object that it is there and the same the object that it is there and the same thing we actually will do with the thing we actually will do with the let me put it a bit", "image_path": "img_data/video_71_chunk_21.jpg"}
{"video": "video_71", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "thing we actually will do with the let me put it a bit further and this is a cylinder and we are 80% certain that this will be a cylinder that this will be a cylinder these are the first if you these are the first if you perception subsystem it should be invoked to subsystem it should be invoked to give us if you some centroid give us if you some centroid where these kind of objects are", "image_path": "img_data/video_71_chunk_22.jpg"}
{"video": "video_71", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "give us if you some centroid where these kind of objects are where these kind of objects are located and the second thing is that we located and the second thing is that we have to establish via the coordinate system some relations between objects", "image_path": "img_data/video_71_chunk_23.jpg"}
{"video": "video_71", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "relations between objects in other words we have to interpret the left and how we can actually claim that the cylinder is actually on the that the cylinder is actually on the left of the pyramid probably some form of transformation needs to happen that will connect", "image_path": "img_data/video_71_chunk_24.jpg"}
{"video": "video_71", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "happen that will connect coordinates of centroids and predicates such as left from etc all of this kind of a question etc all of this kind of a question it is definitely answerable with kind it is definitely answerable with kind of today's kind of technology of today's kind of technology and what we just discussed over", "image_path": "img_data/video_71_chunk_25.jpg"}
{"video": "video_71", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "of today's kind of technology and what we just discussed over and what we just discussed over here we actually built a simple model of here we actually built a simple model of the world is this plane over here and this kind of two plane over here and this kind of two objects that are we are able to objects that are we are able to actually get some answers from that actually get some answers from that kind of world by invoking several kind of world by invoking several subsystems that we will be designing in subsystems that we will be designing in fact there are sort of in the fact there are sort of in the space of vqa or let me write this space of vqa or let me write this down see", "image_path": "img_data/video_71_chunk_26.jpg"}
{"video": "video_71", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "space of vqa or let me write this down see visual quering and answer or the domain called answer or the domain called vqa in artificial intelligence for a tooling that will help us answer these type of questions", "image_path": "img_data/video_71_chunk_27.jpg"}
{"video": "video_71", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "tooling that will help us answer these type of questions one of them is called relational networks rn and we have if you a rn and we have if you a project on the course side that project on the course side that actually deals with this specifically actually deals with this specifically even using this kind of primitive even using this kind of primitive objects in a scene to come up with this objects in a scene to come up with this kind of answers what we actually have kind of answers what we actually have developed here is some form of a developed here is some form of a primitive form of a cognitive model of primitive form of a cognitive model of the world and this is what the", "image_path": "img_data/video_71_chunk_28.jpg"}
{"video": "video_71", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "primitive form of a cognitive model of the world and this is what the of the world and this is what the cognitive model approach invokes cognitive model approach invokes and entails back to the kind of and entails back to the kind of site we have looked at some kind site we have looked at some kind of a primitive version of a primitive version of that and another approach that and another approach that we have in ai is a -called logical we have in ai is a -called logical approach or s symbolism type of approach or s symbolism type of approach and this approach if i go back to that and this approach if i go back to that kind of earlier kind of scene that we kind of earlier kind of scene that we had with the cogn cognitive kind of", "image_path": "img_data/video_71_chunk_29.jpg"}
{"video": "video_71", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "kind of earlier kind of scene that we had with the cogn cognitive kind of had with the cogn cognitive kind of model we see exactly this kind of model we see exactly this kind of objects and in this specific case our objects and in this specific case our perception system will perception system will definitely u need to do something definitely u need to do something more not only just be able to detect the more not only just be able to detect the objects but also to determine whether objects but also to determine whether these kind of objects are these kind of objects are unique in u in a coordinate system unique in u in a coordinate system that we have here and uniqueness can be", "image_path": "img_data/video_71_chunk_30.jpg"}
{"video": "video_71", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "unique in u in a coordinate system that we have here and uniqueness can be that we have here and uniqueness can be determined by u some form of determined by u some form of localization constraints and what should happen constraints and what should happen is that every each one of those kind is that every each one of those kind of objects will be assigned a kind of objects will be assigned a kind of a globally unique id in other words we'll be treating each id in other words we'll be treating each of these objects a symbol that kind of symbol may be associated with some grid world", "image_path": "img_data/video_71_chunk_31.jpg"}
{"video": "video_71", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "symbol that kind of symbol may be associated with some grid world associated with some grid world this grid world is could be expanded this grid world is could be expanded all over the plane over here but here we all over the plane over here but here we see just three locations in this grid see just three locations in this grid world let's say we have location 11 one world let's say we have location 11 one location l12 and location l13 where location l12 and location l13 where this is the first cell on this axis this is the first cell on this axis and this is one two three cells and this is one two three cells three coordinates on this other axis on three coordinates on this other axis on the axis y the on the axis y and with a symbolic kind of", "image_path": "img_data/video_71_chunk_32.jpg"}
{"video": "video_71", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "the on the axis y and with a symbolic kind of and with a symbolic kind of representations what actually can see is representations what actually can see is you can assign some propositional you can assign some propositional kind of symbols such as kind of symbols such as p and we can actually for pyramid p and we can actually for pyramid and we can qualify it with if you and we can qualify it with if you the location and we can assign some the location and we can assign some form of a true or false statement for form of a true or false statement for this the same thing we actually can do this the same thing we actually can do here for the u i'm sorry this is not p this is c for", "image_path": "img_data/video_71_chunk_33.jpg"}
{"video": "video_71", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "here for the u i'm sorry this is not p this is c for i'm sorry this is not p this is c for cylinder and the location for p cylinder and the location for p is 13 that's true and this now become symbols true and this now become symbols that are present and in a that are present and in a kind of a symbolic representation and kind of a symbolic representation and they can be involved in as they can be involved in as sentences in knowledge bases in our sentences in knowledge bases in our knowledge base we can actually use knowledge base we can actually use using our knowledge base we will learn", "image_path": "img_data/video_71_chunk_34.jpg"}
{"video": "video_71", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "knowledge base we can actually use using our knowledge base we will learn using our knowledge base we will learn in another video how we actually can in another video how we actually can make logical inferences regarding make logical inferences regarding relationships between those relationships between those symbols and that's that is the symbols and that's that is the other approach of sort of the other approach of sort of dealing with ai problems the dealing with ai problems the syllogist kind of based approach and syllogist kind of based approach and finally what we have is finally what we have is probably the one that we will be probably the one that we will be spending quite a lot of time spending quite a lot of time is the -called rational agent", "image_path": "img_data/video_71_chunk_35.jpg"}
{"video": "video_71", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "spending quite a lot of time is the -called rational agent is the -called rational agent approach and in the rational agent approach and in the rational agent approach we have an agent that approach we have an agent that finally at the end of a day we need to finally at the end of a day we need to satisfy an objective and that objective sometimes is called utility function sometimes is called utility function sometimes we'll meet it as value sometimes we'll meet it as value functions or for value functions or for value functions about its state or its actions that about its state or its actions that they actually took the they actually took the international kind of agent approach we international kind of agent approach we always going to have to solve an", "image_path": "img_data/video_71_chunk_36.jpg"}
{"video": "video_71", "start": "0:18:30", "end": "0:18:54.866667", "timestamp": "0:18:30 - 0:18:54.866667", "text": "international kind of agent approach we always going to have to solve an always going to have to solve an optimization problem and the agent optimization problem and the agent is going to sort of move and act is going to sort of move and act in this kind of environment in this kind of environment in this kind of optimal way that is the this kind of optimal way that is the rational agent and we have a rational agent and we have a whole utility theory behind it that we whole utility theory behind it that we will get to be exposed in some parts of will get to be exposed in some parts of this course", "image_path": "img_data/video_71_chunk_37.jpg"}
{"video": "video_72", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "we have seen earlier the regression problem where earlier the regression problem where we have effectively to model a we have effectively to model a conditional probability distribution at conditional probability distribution at the output of our predictor now we the output of our predictor now we actually switching to a new task it's a actually switching to a new task it's a classification task where again we will classification task where again we will solve this task using the vaping block solve this task using the vaping block diagram again we will need to model our diagram again we will need to model our predictor with a conditional probability predictor with a conditional probability distribution but in classification our distribution but in classification our target variables are distinct and target variables are distinct and discrete random variables rather than discrete random variables rather than continues in this kind of setting", "image_path": "img_data/video_72_chunk_0.jpg"}
{"video": "video_72", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "discrete random variables rather than continues in this kind of setting continues in this kind of setting i'll motivate the classification task i'll motivate the classification task with a simple use case is actually going with a simple use case is actually going to be called the radar problem and to be called the radar problem and we'll that's what we will start with we'll that's what we will start with next in this setting the use case the next in this setting the use case the application we will see is a application we will see is a well-known application back in the well-known application back in the second world war the battle of england", "image_path": "img_data/video_72_chunk_1.jpg"}
{"video": "video_72", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "second world war the battle of england the battle of england was w primarily from the erection was w primarily from the erection of these towers this was actually called the towers this was actually called the radar towers whose job is was to radar towers whose job is was to transmit a signal towards the france where signal towards the france where from france the nazi airplanes were from france the nazi airplanes were coming in to bomb london and the every time that the london and the every time that the this waveform was impinging into some", "image_path": "img_data/video_72_chunk_2.jpg"}
{"video": "video_72", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "london and the every time that the this waveform was impinging into some this waveform was impinging into some large object on the sky a plane it large object on the sky a plane it was returning back into what we call was returning back into what we call the radar receiver and there was kind of a human operator over there in on human operator over there in on the on each kind of tower with an access the on each kind of tower with an access to a kind of a telephone device to a kind of a telephone device over there and every time that there was over there and every time that there was a strong return", "image_path": "img_data/video_72_chunk_3.jpg"}
{"video": "video_72", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "over there and every time that there was a strong return of a strong signal that was received of a strong signal that was received in the radar kind of a receiver antenna in the radar kind of a receiver antenna it was he was calling london and it was he was calling london and millions of people were well the sirens millions of people were well the sirens were sounding and millions of people were sounding and millions of people were actually running to the tubes were actually running to the tubes stations to tube stations to save stations to tube stations to save their lives that was the application and their lives that was the application and this application is evidently present this application is evidently present in any modern car today that has in any modern car today that has exactly the same ability to have", "image_path": "img_data/video_72_chunk_4.jpg"}
{"video": "video_72", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "in any modern car today that has exactly the same ability to have exactly the same ability to have raiders located in the let's say in raiders located in the let's say in the front of the car to send exactly the front of the car to send exactly the same signals and every time that the same signals and every time that you have a feature in your car that it's you have a feature in your car that it's automatic what is called automatic what is called automatic distance keeping to the vehicle in distance keeping to the vehicle in front of you that is exactly the same front of you that is exactly the same thing it was returning back that thing it was returning back that reflection from the car in front and reflection from the car in front and controller is actually trying to keep controller is actually trying to keep the velocities between your car and", "image_path": "img_data/video_72_chunk_5.jpg"}
{"video": "video_72", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "controller is actually trying to keep the velocities between your car and the velocities between your car and the car in front of you constant and the car in front of you constant and therefore maintain a desired distance therefore maintain a desired distance between the two cars but anyway i would between the two cars but anyway i would motivate it with this sort of motivate it with this sort of application which is the world war ii application which is the world war ii application because also back then a lot application because also back then a lot of the terminology that we will cover of the terminology that we will cover today it was also first today it was also first invented and just to see exactly invented and just to see exactly what is actually happening here we have what is actually happening here we have a some kind of a time", "image_path": "img_data/video_72_chunk_6.jpg"}
{"video": "video_72", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "what is actually happening here we have a some kind of a time a some kind of a time plot of a quantity that will be calling plot of a quantity that will be calling x and it will be designating for us the x and it will be designating for us the -call -call signal strength of or received signal strength or power and very simplistically we have or power and very simplistically we have some fluctuation received power when we have fluctuation received power when we have a return a strong return from a plane a return a strong return from a plane and in nights where we don't the attacks and in nights where we don't the attacks were usually during the night", "image_path": "img_data/video_72_chunk_7.jpg"}
{"video": "video_72", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "and in nights where we don't the attacks were usually during the night were usually during the night that when we have no return we that when we have no return we still have some fluctuating signal power still have some fluctuating signal power much smaller than that and our receiver that and our receiver is going to have just one knob that knob is going to have just one knob that knob is going to be called the threshold will is going to be called the threshold will symbolize it with w it's a scalar value symbolize it with w it's a scalar value that it is anything that exceeds it that it is anything that exceeds it is going to", "image_path": "img_data/video_72_chunk_8.jpg"}
{"video": "video_72", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "that it is anything that exceeds it is going to let's assume that this threshold value is set to this kind of level everything is set to this kind of level everything that exceeds it is going to be called that exceeds it is going to be called the predicted that is well not predicted that is well not predicted is we're going to sound the predicted is we're going to sound the alarm and anything that it is not alarm and anything that it is not exceeding it we are going to not exceeding it we are going to not alarm anyone alert anyone and we alarm anyone alert anyone and we will call that the no attack case", "image_path": "img_data/video_72_chunk_9.jpg"}
{"video": "video_72", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "alarm anyone alert anyone and we will call that the no attack case will call that the no attack case our problem is a binary our problem is a binary classification problem what we call classification problem what we call binary because our now target variable binary because our now target variable as compared to that regression can only as compared to that regression can only take two values one we will call this take two values one we will call this the -called positive condition the -called positive condition the attack is going on and zero we will be attack is going on and zero we will be calling this the -cal calling this the -cal negative condition and our job negative condition and our job here now that we are in the", "image_path": "img_data/video_72_chunk_10.jpg"}
{"video": "video_72", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "negative condition and our job here now that we are in the here now that we are in the 2020s we are going to solve this problem 2020s we are going to solve this problem of determining the value of that kind of determining the value of that kind of threshold using kind of machine of threshold using kind of machine learning approaches we have what learning approaches we have what we will do is we will task a military we will do is we will task a military person over here to sit and observe what person over here to sit and observe what is really happening every single day is really happening every single day first night records the signal", "image_path": "img_data/video_72_chunk_11.jpg"}
{"video": "video_72", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "is really happening every single day first night records the signal first night records the signal strength x1 and records also whether strength x1 and records also whether attack has happened or not x2 the same attack has happened or not x2 the same way and on if they survive we will way and on if they survive we will keep them if they not we have to keep them if they not we have to replace them with somebody else xm replace them with somebody else xm ym that's our let's say we have ym that's our let's say we have aggregated m examples of what is has aggregated m examples of what is has happened at our receiver and as before", "image_path": "img_data/video_72_chunk_12.jpg"}
{"video": "video_72", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "aggregated m examples of what is has happened at our receiver and as before happened at our receiver and as before we progress into specifying what is we progress into specifying what is really the problem statement i just really the problem statement i just want to u convey some kind of an want to u convey some kind of an intuitive fashion of what is happening intuitive fashion of what is happening this was the soal attack signal this was the soal attack signal strength and this was the no attack signal strength the sort of time series of all these kind of time series of all these kind of observations night after night and we observations night after night and we have", "image_path": "img_data/video_72_chunk_13.jpg"}
{"video": "video_72", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "observations night after night and we have the problem to solve is to come up the problem to solve is to come up with an optimal value for the threshold with an optimal value for the threshold you can understand that the threshold is you can understand that the threshold is quite critical in determining whether quite critical in determining whether how the system operates if we set the how the system operates if we set the threshold too high this means that we threshold too high this means that we will be missing quite a lot of will be missing quite a lot of attacks and evidently people will die if attacks and evidently people will die if we set the threshold too low this we set the threshold too low this means that we will be", "image_path": "img_data/video_72_chunk_14.jpg"}
{"video": "video_72", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "we set the threshold too low this means that we will be means that we will be waking up everyone unnecessarily the first night they unnecessarily the first night they will believe us and return will believe us and return to their beds disappointed somehow to their beds disappointed somehow or relieved the second night they or relieved the second night they will still believe us they will go down will still believe us they will go down to the tube but after the third night to the tube but after the third night they will stop believing us and when a they will stop believing us and when a real att happens people will also real att happens people will also die", "image_path": "img_data/video_72_chunk_15.jpg"}
{"video": "video_72", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "real att happens people will also die the system will lose complete the system will lose complete credibility the choice of this w is credibility the choice of this w is quite critical our is quite critical our predictor the y hat that we have to predictor the y hat that we have to predict at this moment in time we will predict at this moment in time we will treat this as a one and zero later will treat this as a one and zero later will become a number between zero and one become a number between zero and one is also going to return our prediction predictor is going also to return one or zero for the", "image_path": "img_data/video_72_chunk_16.jpg"}
{"video": "video_72", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "to return our prediction predictor is going also to return one or zero for the going also to return one or zero for the positive and negative condition respectively negative condition respectively that is the problem statement how to that is the problem statement how to set up this w optimally and as you can understand optimally and as you can understand i can just after just before in i can just after just before in the regression kind of problem we had the regression kind of problem we had introduced the probabilistic nature introduced the probabilistic nature of every predictor what we will of every predictor what we will do", "image_path": "img_data/video_72_chunk_17.jpg"}
{"video": "video_72", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "of every predictor what we will do now is we will draw two probability distributions and let me just draw distributions and let me just draw this probability distribution let's say this is a probability distribution of x we have here on the x- distribution of x we have here on the x- axis our signal strength", "image_path": "img_data/video_72_chunk_18.jpg"}
{"video": "video_72", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "distribution of x we have here on the x- axis our signal strength axis our signal strength and one of them is going to be and one of them is going to be called the probability of called the probability of x and when no attack is happening and x and when no attack is happening and this is the probability of x when the this is the probability of x when the attack is happening and as you can see attack is happening and as you can see these two prob distributions can be these two prob distributions can be easily obtained u as histograms by just easily obtained u as histograms by just going back to our data tape going back to our data tape the data set that we have created", "image_path": "img_data/video_72_chunk_19.jpg"}
{"video": "video_72", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "going back to our data tape the data set that we have created the data set that we have created and recorded and select all the rows and recorded and select all the rows where the y target variable is zero and where the y target variable is zero and obtain this histogram and visit all the obtain this histogram and visit all the corresponding rows where the y is equal corresponding rows where the y is equal to one and plot this corresponding to one and plot this corresponding histogram in terms of plotting the histogram in terms of plotting the histograms this is definitely a histograms this is definitely a very i will call it a easy exercise and very i will call it a easy exercise and as you probably notice from the shape of as you probably notice from the shape of these histograms we have not really made these histograms we have not really made any assumption with respect to u", "image_path": "img_data/video_72_chunk_20.jpg"}
{"video": "video_72", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "these histograms we have not really made any assumption with respect to u any assumption with respect to u gaussianity or nothing that they gaussianity or nothing that they are definitely plotted as non- gausian are definitely plotted as non- gausian type of probability distributions and the moment i have distributions and the moment i have and also another sort of evident and also another sort of evident that thing that is actually going that thing that is actually going on with this problem is that there is a on with this problem is that there is a very strong overlap between the two very strong overlap between the two histograms when we have the signal histograms when we have the signal strength between no attacks and attacks strength between no attacks and attacks as you can see also here from the time", "image_path": "img_data/video_72_chunk_21.jpg"}
{"video": "video_72", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "strength between no attacks and attacks as you can see also here from the time as you can see also here from the time series plot there's a quite series plot there's a quite significant overlap between the two and significant overlap between the two and that's of course due to properties of that's of course due to properties of radio wave propagation the radio wave propagation the waveform emitted from this kind of radar waveform emitted from this kind of radar station can be pinched on the sea station can be pinched on the sea surface and go into some kind of surface and go into some kind of other transverse other kind of paths the other transverse other kind of paths the -call multipath fading situation -call multipath fading situation some of you may have observed fading some of you may have observed fading while lening to analog radio stations", "image_path": "img_data/video_72_chunk_22.jpg"}
{"video": "video_72", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "some of you may have observed fading while lening to analog radio stations while lening to analog radio stations such as am and fm back in the old such as am and fm back in the old days this were the only radio stations days this were the only radio stations that we had access to and also you that we had access to and also you may have sort of experienced exactly may have sort of experienced exactly the same situation where you are the same situation where you are going in and out of coverage holes using going in and out of coverage holes using your cellular devices the important your cellular devices the important thing about the sort of thing about the sort of u overlap is that let's assume that i", "image_path": "img_data/video_72_chunk_23.jpg"}
{"video": "video_72", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "u overlap is that let's assume that i u overlap is that let's assume that i have selected here a value for my have selected here a value for my threshold w and the moment i have if you a w and the moment i have if you a threshold w i can start clear threshold w i can start clear defining certain areas under those defining certain areas under those probability distributions that will be probability distributions that will be of great interest to me i want to of great interest to me i want to shade the first probability tail shade the first probability tail which is this one i want also to shade this", "image_path": "img_data/video_72_chunk_24.jpg"}
{"video": "video_72", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "which is this one i want also to shade this and area and also want to say with horizontal stripes this horizontal stripes this area these three areas are i think area these three areas are i think will be quite important will be quite important now the overlap means that since now the overlap means that since there's no absolutely clear there's no absolutely clear separation between the two prob separation between the two prob distributions we are always going to distributions we are always going to make some form of mistake we always make some form of mistake we always going to have mistakes going to have mistakes and in fact we can clearly distinguish", "image_path": "img_data/video_72_chunk_25.jpg"}
{"video": "video_72", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "going to have mistakes and in fact we can clearly distinguish and in fact we can clearly distinguish four conditions and we will tabulate them with what we call the confusion with what we call the confusion matrix on the one axis of the matrix on the one axis of the confusion matrix we'll be assigning this confusion matrix we'll be assigning this axis to the -call ground truth and axis to the -call ground truth and the other to the prediction the yut the other to the prediction the yut and when the y hat", "image_path": "img_data/video_72_chunk_26.jpg"}
{"video": "video_72", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "the other to the prediction the yut and when the y hat is positive and negative and this is positive and negative and this positive and negative agrees to the positive and negative agrees to the with the ground truth then we will be with the ground truth then we will be calling this correspondingly true calling this correspondingly true positive and true negative in fact it's actually quite common to write first the letter the common to write first the letter the second letter as a pneumonic rule second letter as a pneumonic rule write first the second letter and put a write first the second letter and put a word the letter t in front every time word the letter t in front every time the that you have agreement with ground", "image_path": "img_data/video_72_chunk_27.jpg"}
{"video": "video_72", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "word the letter t in front every time the that you have agreement with ground the that you have agreement with ground truth in the case where you predict truth in the case where you predict there is an attack going on but you are there is an attack going on but you are wrong you are prepending it with the wrong you are prepending it with the letter f stands for false we letter f stands for false we have false positive here and here we have false positive here and here we have also false negative we are have also false negative we are predicting negative but we are wrong and predicting negative but we are wrong and therefore we have the -called false therefore we have the -called false negative events the overlap as compared to the case which is overlap as compared to the case which is quite unrealistic in practice where we", "image_path": "img_data/video_72_chunk_28.jpg"}
{"video": "video_72", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "quite unrealistic in practice where we quite unrealistic in practice where we have some form of significant separation have some form of significant separation between the two histograms between the two histograms this and therefore it's easy to this and therefore it's easy to select something of a threshold w that select something of a threshold w that will separate the two conditions will separate the two conditions perfectly the -cal linearly separable perfectly the -cal linearly separable case is not present here we case is not present here we are going to let me just delete are going to let me just delete it to avoid any confusion is not it to avoid any confusion is not present here we always going to have", "image_path": "img_data/video_72_chunk_29.jpg"}
{"video": "video_72", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "it to avoid any confusion is not present here we always going to have present here we always going to have in other words these two type of in other words these two type of events present in our problem and events present in our problem and before i describe what was actually before i describe what was actually happening every time we get this happening every time we get this threshold w to not be set optimally when threshold w to not be set optimally when the threshold w is set too high the threshold w is set too high then we are missing the events that are missing the events that are actually attacks that are actually going actually attacks that are actually going will happen and therefore we are", "image_path": "img_data/video_72_chunk_30.jpg"}
{"video": "video_72", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "actually attacks that are actually going will happen and therefore we are will happen and therefore we are going to be increasing our force going to be increasing our force negative we are predicting that no negative we are predicting that no attacks is happening while in fact they attacks is happening while in fact they are and if the w is set too low we are and if the w is set too low we are going to be increasing the false are going to be increasing the false positive rate and in fact we will be positive rate and in fact we will be alerting sort of the people to go down to the cube but the people to go down to the cube but unfortunately no attack is actually unfortunately no attack is actually going", "image_path": "img_data/video_72_chunk_31.jpg"}
{"video": "video_72", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "unfortunately no attack is actually going on now that we have recognized that we always going to make mistake as always going to make mistake as manifested by this confusion matrix", "image_path": "img_data/video_72_chunk_32.jpg"}
{"video": "video_72", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "manifested by this confusion matrix we are interested to just qualify these mistakes and quantify those these mistakes and quantify those mistakes by just understanding the mistakes by just understanding the probability of making a probability of making a mistake this is definitely the mistake this is definitely the probability of when we make where probability of when we make where our prediction why hat our prediction why hat is not equal to the ground ruth y is not equal to the ground ruth y and this is happens in two instances the", "image_path": "img_data/video_72_chunk_33.jpg"}
{"video": "video_72", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "is not equal to the ground ruth y and this is happens in two instances the and this is happens in two instances the first instance is when we first instance is when we have the probability when we make the have the probability when we make the prediction that no attack is happening prediction that no attack is happening when in fact there is an attack when in fact there is an attack happening plus the events when we are making the opposite claim and i think it's worthwhile now trying to understand what is", "image_path": "img_data/video_72_chunk_34.jpg"}
{"video": "video_72", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "claim and i think it's worthwhile now trying to understand what is now trying to understand what is happening in this what are those happening in this what are those probabilities and how they related to probabilities and how they related to these histan we have plotted the moment these histan we have plotted the moment we have specified the threshold location over specified the threshold location over here w we have split the region into two here w we have split the region into two parts the first region is called r0 and parts the first region is called r0 and the second region is called r1 and now that we have this region names i think the region names i think the region names are also intuitively are also intuitively kind of understood because this zero", "image_path": "img_data/video_72_chunk_35.jpg"}
{"video": "video_72", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "are also intuitively kind of understood because this zero kind of understood because this zero index here corresponds to the case where index here corresponds to the case where we declare anything as we said below the we declare anything as we said below the threshold w is there's no attack threshold w is there's no attack we are predicting no attack that's we are predicting no attack that's the what the zero is here and anything the what the zero is here and anything above the w we have we predicting attack above the w we have we predicting attack is happening and that's why the one is happening and that's why the one is there we can actually start putting there we can actually start putting this probabilities quantifying these this probabilities quantifying these probabilities based on the area under probabilities based on the area under the those cures and we i hope you all remember that", "image_path": "img_data/video_72_chunk_36.jpg"}
{"video": "video_72", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "the those cures and we i hope you all remember that and we i hope you all remember that probability for continuous random probability for continuous random variables such a signal strength over variables such a signal strength over here is sort of manifested by such here is sort of manifested by such kind of areas what i'm going to do kind of areas what i'm going to do now is i'm going to declare that this now is i'm going to declare that this probability the first probability over probability the first probability over here is equal to the here is equal to the probability that i am making a probability that i am making a prediction such as my prediction such as my x belongs to the region", "image_path": "img_data/video_72_chunk_37.jpg"}
{"video": "video_72", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "prediction such as my x belongs to the region x belongs to the region r0 when in fact the ground truth is r0 when in fact the ground truth is one i converted the y is equal to one i converted the y is equal to zero to all the events which are zero to all the events which are to the left of w all the events where to the left of w all the events where x belongs to this kind of region are x belongs to this kind of region are zero and the do the same belong to the region r1 same belong to the region r1 when y is equal to", "image_path": "img_data/video_72_chunk_38.jpg"}
{"video": "video_72", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "same belong to the region r1 when y is equal to z all events of x greater than w in other words all x's which are belonging other words all x's which are belonging to the region r1 it is exactly to the region r1 it is exactly equivalent to those convention equivalent to those convention that i had about what is y hat is that i had about what is y hat is equal to one and what y hat is equal to one and what y hat is equal to zero this one the probability that zero this one the probability that x belongs to the region x belongs to the region r0 when y is one corresponds to the left tail of", "image_path": "img_data/video_72_chunk_39.jpg"}
{"video": "video_72", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "r0 when y is one corresponds to the left tail of y is one corresponds to the left tail of this probability distribution this probability distribution this probability distribution but only the probability distribution but only the left tail you can see here that the left tail you can see here that the whole probability distribution here the whole probability distribution here the whole histogram is p of x comm y = to 1 whole histogram is p of x comm y = to 1 but here i'm interesting only for the x but here i'm interesting only for the x equal to r0 it's the summation of equal to r0 it's the summation of this vertically st striped and this vertically st striped and this bubbly kind of area over there i am bubbly kind of area over there i am going to write it as an integral", "image_path": "img_data/video_72_chunk_40.jpg"}
{"video": "video_72", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "bubbly kind of area over there i am going to write it as an integral probability of x comma y is = to 1 dx and i'm going to take this and 1 dx and i'm going to take this and actually do exactly the same for the actually do exactly the same for the r1 probability of x comma y isal to 0 dx r1 probability of x comma y isal to 0 dx and now if i actually start relating and now if i actually start relating what i actually wrote here with the what i actually wrote here with the counts that i have count it i can count counts that i have count it i can count through a random realization of this through a random realization of this experiment from my kind of a", "image_path": "img_data/video_72_chunk_41.jpg"}
{"video": "video_72", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "through a random realization of this experiment from my kind of a experiment from my kind of a looking at the predictor output and looking at the predictor output and looking at the ground truth over here i looking at the ground truth over here i can understand that this is corresponds can understand that this is corresponds to the false negative to the false negative rate and this corresponds to the false rate and this corresponds to the false positive rate and definitely this is the false negative because i am actually false negative because i am actually predicting that no attack is happening predicting that no attack is happening and in fact i'm wrong and the", "image_path": "img_data/video_72_chunk_42.jpg"}
{"video": "video_72", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "predicting that no attack is happening and in fact i'm wrong and the and in fact i'm wrong and the corresponding here case where i'm corresponding here case where i'm predicting that the attack is happening predicting that the attack is happening and in fact i'm also wrong the false and in fact i'm also wrong the false positive and the false neg the false positive and the false neg the false negative and the false posi are related negative and the false posi are related to the entries of the confusion matrix to the entries of the confusion matrix here that are definitely present here that are definitely present and countable using this kind of and countable using this kind of histograms and as we discussed our role histograms and as we discussed our role here is to find the optimal w and i here is to find the optimal w and i just want you to understand how visually just want you to understand how visually we can be persuaded that is in fact we can be persuaded that is in fact there is an optimal w and that optimal w", "image_path": "img_data/video_72_chunk_43.jpg"}
{"video": "video_72", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "we can be persuaded that is in fact there is an optimal w and that optimal w there is an optimal w and that optimal w will minimize the probability of making will minimize the probability of making the mistakes you cannot make it the mistakes you cannot make it zero because as we discussed we do not zero because as we discussed we do not face in this situation linearly face in this situation linearly separable data set but at the very least separable data set but at the very least we can minimize the summation of false we can minimize the summation of false positives and false negative positives and false negative events and imagine that you are events and imagine that you are moving that w in the left side over moving that w in the left side over here trying to move this line to the", "image_path": "img_data/video_72_chunk_44.jpg"}
{"video": "video_72", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "moving that w in the left side over here trying to move this line to the here trying to move this line to the left and look what's happening as left and look what's happening as you're moving into the left gradually you're moving into the left gradually you will come maybe two snapshots are enough to see what is happening in the first value of happening in the first value of this w to the left of the previous kind this w to the left of the previous kind of w what we are actually achieving of w what we are actually achieving is we are going to whatever we are losing in terms", "image_path": "img_data/video_72_chunk_45.jpg"}
{"video": "video_72", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "are going to whatever we are losing in terms going to whatever we are losing in terms of area out of our this vertically of area out of our this vertically striped area we will be gaining in terms striped area we will be gaining in terms of the horizontally striped area having of the horizontally striped area having said that we actually start seeing said that we actually start seeing reduction of this bubbly area and this reduction of this bubbly area and this area will start to be reduced and area will start to be reduced and reduced up to the point reduced and reduced up to the point where we reach what will be where we reach what will be calling the w star the optimal w and thus optimal w is the w that", "image_path": "img_data/video_72_chunk_46.jpg"}
{"video": "video_72", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "star the optimal w and thus optimal w is the w that and thus optimal w is the w that minimized this probability of mistake minimized this probability of mistake simply because in that location the simply because in that location the bubbly area got eliminated bubbly area got eliminated completely and the summation of the completely and the summation of the therefore of the false positives and therefore of the false positives and false negatives that included it false negatives that included it is the minimum possible actually we can write that possible actually we can write that sort of optimizing", "image_path": "img_data/video_72_chunk_47.jpg"}
{"video": "video_72", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "possible actually we can write that sort of optimizing the w towards w star will be done using the well-known to us stochastic gr descent algorithm that", "image_path": "img_data/video_72_chunk_48.jpg"}
{"video": "video_72", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "algorithm that minimizes the probability of mistakes probability of making a mistake the misclassification error or also the misclassification error or also called misclassification error eight this will be done now that we have some kind will be done now that we have some kind of a visual motivation of what we're of a visual motivation of what we're trying to achieve here we now need to", "image_path": "img_data/video_72_chunk_49.jpg"}
{"video": "video_72", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "of a visual motivation of what we're trying to achieve here we now need to trying to achieve here we now need to understand how we can also motivate understand how we can also motivate in the next discussion an objective in the next discussion an objective function and that it is going to be function and that it is going to be suitable for our problem here suitable for our problem here which is the classification problem in a which is the classification problem in a similar way as we have done with the u similar way as we have done with the u earlier loss function we have used earlier loss function we have used initially was called mean square error initially was called mean square error and then it was also called cross and then it was also called cross entropy we'll do that entropy we'll do that next the come up with this objective", "image_path": "img_data/video_72_chunk_50.jpg"}
{"video": "video_72", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "entropy we'll do that next the come up with this objective next the come up with this objective function before we go and discuss that function before we go and discuss that let's review the -called classification review the -called classification metrics the classification metrics that metrics the classification metrics that there a couple of classification there a couple of classification metrics will be of interest to us they metrics will be of interest to us they will be entirely based of course on the will be entirely based of course on the previously described confusion matrix previously described confusion matrix and the first matric i want to address and the first matric i want to address is called the true positive is called the true positive rate the second metric is well the", "image_path": "img_data/video_72_chunk_51.jpg"}
{"video": "video_72", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "is called the true positive rate the second metric is well the rate the second metric is well the dr postive rate is comes with many names dr postive rate is comes with many names and many of them have been sort of and many of them have been sort of originating from various kind of domains originating from various kind of domains electrical engineering computer electrical engineering computer science and others in computer science and others in computer science this also is called science this also is called recall in electrical engineering this is recall in electrical engineering this is also called probability of detection and also called probability of detection and many other domains quote it as many other domains quote it as sensitivity it's one and the same thing and i just", "image_path": "img_data/video_72_chunk_52.jpg"}
{"video": "video_72", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "sensitivity it's one and the same thing and i just it's one and the same thing and i just want to mention all of them just in case want to mention all of them just in case you come up with come across one of you come up with come across one of the many this is the ratio the of the many this is the ratio between true positive and true positive plus false positive and true positive plus false negative this is a ratio that is negative this is a ratio that is definitely going to be of concern definitely going to be of concern to us and of interest to us every to us and of interest to us every time we have to evaluate a classifier time we have to evaluate a classifier and the second metrix that i want to and the second metrix that i want to quote and have some discussion about", "image_path": "img_data/video_72_chunk_53.jpg"}
{"video": "video_72", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "and the second metrix that i want to quote and have some discussion about quote and have some discussion about those metrics a bit later is a -called those metrics a bit later is a -called precision and this precision is another precision and this precision is another ratio of true positive ide by true ratio of true positive ide by true positive plus false positive and if you positive plus false positive and if you follow the this video where we have follow the this video where we have plotted these histograms in the binary plotted these histograms in the binary classifier when the -called the radar classifier when the -called the radar problem and you probably understood problem and you probably understood the tradeoff that exists between false the tradeoff that exists between false positives and false negatives as we were", "image_path": "img_data/video_72_chunk_54.jpg"}
{"video": "video_72", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "the tradeoff that exists between false positives and false negatives as we were positives and false negatives as we were moving in fact the as we were moving in fact the as we were moving the value of the threshold w moving the value of the threshold w we were changing the areas under we were changing the areas under those two histograms and of course those two histograms and of course here we were trading all false here we were trading all false positive or false negatives in our positive or false negatives in our attempt to find this optimal kind of w attempt to find this optimal kind of w in a very similar way we can actually in a very similar way we can actually claim that now that we have the those claim that now that we have the those metrics the tradeoff between false positives and", "image_path": "img_data/video_72_chunk_55.jpg"}
{"video": "video_72", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "metrics the tradeoff between false positives and the tradeoff between false positives and false negatives is evident over here u false negatives is evident over here u in the following trade off in the following trade off let me write it this let me write it this down we can say that down we can say that because as w changes there is a tradeoff between", "image_path": "img_data/video_72_chunk_56.jpg"}
{"video": "video_72", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "tradeoff between false positives and false negatives we can actually claim that negatives we can actually claim that there is a tradeoff between recall and precision because recall and precision because recall and precision everything is exactly the same in terms everything is exactly the same in terms of numerator and portion of the of numerator and portion of the denominator but only the false pos denominator but only the false pos and false negatives are present there", "image_path": "img_data/video_72_chunk_57.jpg"}
{"video": "video_72", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "denominator but only the false pos and false negatives are present there and false negatives are present there this is actually an important tradeoff this is actually an important tradeoff that will be of great interest to us that will be of great interest to us as we will always finding ourselves as we will always finding ourselves making that kind of tradeoff for making that kind of tradeoff for classif classification architectures we classif classification architectures we will be designing soon the other will be designing soon the other metric it's not really a different metric it's not really a different metric but it's a way to present metric but it's a way to present performance metrics classification performance metrics classification metrix is this what we call the", "image_path": "img_data/video_72_chunk_58.jpg"}
{"video": "video_72", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "performance metrics classification metrix is this what we call the metrix is this what we call the -cal receiver operating characteristic -cal receiver operating characteristic and we actually call it receiver and we actually call it receiver operating characteristic from those operating characteristic from those days in the 40s when they were deploying days in the 40s when they were deploying this kind of radars and i will describe it as the radars and i will describe it as the curve that we can plot by changing curve that we can plot by changing the threshold w in the x-axis over here the threshold w in the x-axis over here it is the false positive it is the false positive rate also known as a false alarm from rate also known as a false alarm from those days the probab ility of the", "image_path": "img_data/video_72_chunk_59.jpg"}
{"video": "video_72", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "rate also known as a false alarm from those days the probab ility of the those days the probab ility of the probability of false alarm pfa and the y probability of false alarm pfa and the y ais is called recall evidently the same ais is called recall evidently the same thing as a true positive rate and thing as a true positive rate and definitely we have a probability of definitely we have a probability of false positive rate that goes from one false positive rate that goes from one from 0 to one and the probability of from 0 to one and the probability of recall true positive rate that goes recall true positive rate that goes again from 0 to one because there are again from 0 to one because there are probabilities and therefore we'll find probabilities and therefore we'll find this cur of constraint by those values", "image_path": "img_data/video_72_chunk_60.jpg"}
{"video": "video_72", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "probabilities and therefore we'll find this cur of constraint by those values and the u as we change the threshold we will be able to plot to draw such cures some of these curves are going to cures some of these curves are going to be this let me plot three cases case let's say a case b and case c and i think it's reasonable to", "image_path": "img_data/video_72_chunk_61.jpg"}
{"video": "video_72", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "a case b and case c and i think it's reasonable to and i think it's reasonable to understand now what is the best understand now what is the best possible classifier we can ever design possible classifier we can ever design which is of course not achievable which is of course not achievable right now and it's not achievable in right now and it's not achievable in any case we have such cases of any case we have such cases of overlap between positive and negative overlap between positive and negative classes and that point over here is classes and that point over here is this is the sort of go of this is the sort of go of ideal and unrealistic operating point", "image_path": "img_data/video_72_chunk_62.jpg"}
{"video": "video_72", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "this is the sort of go of ideal and unrealistic operating point this care as we discussed is called receiver operating characteristic for r and every and each and every curve is being plotted by having a being plotted by having a classifier and tuning if you it's classifier and tuning if you it's par ameters adjusting its parameters the", "image_path": "img_data/video_72_chunk_63.jpg"}
{"video": "video_72", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "par ameters adjusting its parameters the ameters adjusting its parameters the threshold more specifically and same threshold more specifically and same here this is supposed to be a diagonal here this is supposed to be a diagonal line 45° and let's compare between 45° and let's compare between three different classifiers which one do we classifiers which one do we believe that is actually the best one believe that is actually the best one and it's actually a very and it's actually a very straightforward kind of answer if we straightforward kind of answer if we draw let's say a horizontal kind of l draw let's say a horizontal kind of l line the classifier a is offering", "image_path": "img_data/video_72_chunk_64.jpg"}
{"video": "video_72", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "draw let's say a horizontal kind of l line the classifier a is offering line the classifier a is offering exactly the same performance as exactly the same performance as the recall but at a much reduced probability recall but at a much reduced probability of false alarm or false positive rate as of false alarm or false positive rate as compared to classifier compared to classifier b and of course much better than b and of course much better than classifier c and therefore either we classifier c and therefore either we draw a horizontal line or actually you draw a horizontal line or actually you can actually draw a vertical line we can actually draw a vertical line we can make the same argument a is offered a make the same argument a is offered a much better probability of true", "image_path": "img_data/video_72_chunk_65.jpg"}
{"video": "video_72", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "make the same argument a is offered a much better probability of true much better probability of true positive as compared to b and as positive as compared to b and as compared to c for the same false compared to c for the same false positive rate and therefore we can positive rate and therefore we can actually write this kind of preference actually write this kind of preference relationship in this specific case relationship in this specific case recall the recall and force positive recall the recall and force positive are involved in plotting this kind of are involved in plotting this kind of curve to give us a if you a curve to give us a if you a graphical view of how the classifier is graphical view of how the classifier is behaving in various kind of operating", "image_path": "img_data/video_72_chunk_66.jpg"}
{"video": "video_72", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "graphical view of how the classifier is behaving in various kind of operating is behaving in various kind of operating conditions and when we tune this conditions and when we tune this classifier and we choosing the w the classifier and we choosing the w the specific w start that we have seen specific w start that we have seen earlier we effectively operate at a earlier we effectively operate at a specific operating point at that point specific operating point at that point we will be sort of constantly we will be sort of constantly operating and we will in many operating and we will in many senses we will need to make different senses we will need to make different tradeoffs between positives and tradeoffs between positives and false positives and true positives in false positives and true positives in various applications let me write this down", "image_path": "img_data/video_72_chunk_67.jpg"}
{"video": "video_72", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "various applications let me write this down applications let me write this down that each point in the roc corresponds to a unique setting of the threshold", "image_path": "img_data/video_72_chunk_68.jpg"}
{"video": "video_72", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "setting of the threshold w that is as kind of a short summary of a couple of short summary of a couple of classification metrics that will be of classification metrics that will be of interest to us and the receiver interest to us and the receiver operating curve and we will also have operating curve and we will also have another curve called recall versus another curve called recall versus precision this curve will be introduced precision this curve will be introduced in another video and we'll be in another video and we'll be discussed then in an early video we saw how maximum", "image_path": "img_data/video_72_chunk_69.jpg"}
{"video": "video_72", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "discussed then in an early video we saw how maximum in an early video we saw how maximum likelihood was motivating every likelihood was motivating every the underlying you objective the underlying you objective function of every prediction problem function of every prediction problem and binary classification will not be and binary classification will not be an exception and we started with the exception and we started with the regression problem and we saw how regression problem and we saw how maximum likelihood and cross entropy are maximum likelihood and cross entropy are ultimately connected now we recognize ultimately connected now we recognize the functional form of the -call the functional form of the -call binary crossentropy loss function which binary crossentropy loss function which is for spe specifically for our is for spe specifically for our binary classification problem and we", "image_path": "img_data/video_72_chunk_70.jpg"}
{"video": "video_72", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "is for spe specifically for our binary classification problem and we binary classification problem and we will be motivating this by recognizing will be motivating this by recognizing that in regression we had a p mod that in regression we had a p mod which actually was gausian in binary which actually was gausian in binary classification we going to need to classification we going to need to actually have a probabilistic model a actually have a probabilistic model a probability distribution which is really probability distribution which is really appropriate for our discrete appropriate for our discrete random variables that are the are random variables that are the are wise our form of myp model of let's wise our form of myp model of let's say y given an x comma", "image_path": "img_data/video_72_chunk_71.jpg"}
{"video": "video_72", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "wise our form of myp model of let's say y given an x comma w this y over here is definitely a discrete and in fact binary i have seen already in the discussion of the seen already in the discussion of the entropy video coing and i know that entropy video coing and i know that at that point i have quoted beri at that point i have quoted beri distribution as the appropriate propability", "image_path": "img_data/video_72_chunk_72.jpg"}
{"video": "video_72", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "distribution as the appropriate propability the appropriate propability distribution for our model and the distribution for our model and the beri distribution let me write it with beri distribution let me write it with all words over here is given as y hat to the power of y 1 - 1 - y the^ of 1 - y let's spend some time y the^ of 1 - y let's spend some time kind of understanding this if kind of understanding this if my ground truth is", "image_path": "img_data/video_72_chunk_73.jpg"}
{"video": "video_72", "start": "0:37:00", "end": "0:37:30", "timestamp": "0:37:00 - 0:37:30", "text": "kind of understanding this if my ground truth is one this p model of y had of y sorry given had of y sorry given x comma w is simply y because the y is one this y because the y is one this is zero and therefore this term hole is zero and therefore this term hole is one the only thing that remains is y one the only thing that remains is y and in fact this is a very important and in fact this is a very important conclusion in a sense that from now", "image_path": "img_data/video_72_chunk_74.jpg"}
{"video": "video_72", "start": "0:37:30", "end": "0:38:00", "timestamp": "0:37:30 - 0:38:00", "text": "conclusion in a sense that from now in a sense that from now on in binary classification all i on in binary classification all i need to produce at the output is this need to produce at the output is this a single floating point number a single floating point number between zero and one it's a between zero and one it's a probability it's going to be zero probability it's going to be zero between z and one for sure i'll be between z and one for sure i'll be calling this probability when y is calling this probability when y is equal to 1 the probability of y is equal to 1 the probability of y is equal to 1 given x comma w in fact i can to 1 given x comma w in fact i can write it as", "image_path": "img_data/video_72_chunk_75.jpg"}
{"video": "video_72", "start": "0:38:00", "end": "0:38:30", "timestamp": "0:38:00 - 0:38:30", "text": "to 1 given x comma w in fact i can write it as a the model and this is actually going to be called the posterior probability and we recognize the term posterior already we recognize the term posterior already we recognized it when we had this recognized it when we had this probability review lecture and in probability review lecture and in that kind of video we have actually seen that kind of video we have actually seen that posterior it means after we get that posterior it means after we get to observe the ais our data what can to observe the ais our data what can we say about our target variable y in", "image_path": "img_data/video_72_chunk_76.jpg"}
{"video": "video_72", "start": "0:38:30", "end": "0:39:00", "timestamp": "0:38:30 - 0:39:00", "text": "to observe the ais our data what can we say about our target variable y in we say about our target variable y in this case the posterior probability this case the posterior probability distribution is going to be called yck distribution is going to be called yck that's that is what our classifier that's that is what our classifier is going to from now on going to be is going to from now on going to be producing and that this posterior being producing and that this posterior being a probability means that we are have a probability means that we are have inherent the ability to provide an inherent the ability to provide an uncertainty about our pred uncertainty about our pred prediction we are going to let's say", "image_path": "img_data/video_72_chunk_77.jpg"}
{"video": "video_72", "start": "0:39:00", "end": "0:39:30", "timestamp": "0:39:00 - 0:39:30", "text": "uncertainty about our pred prediction we are going to let's say prediction we are going to let's say report the 0.82 as let's say the output of the 0.82 as let's say the output of the positive class and this means that we positive class and this means that we are going to be 82% certain that we have are going to be 82% certain that we have a positive event at the output of our a positive event at the output of our classif fire and if we have already classif fire and if we have already reported the output of a positive event reported the output of a positive event then when we are dealing with the then when we are dealing with the negative case our p model over here that negative case our p model over here that we have selected of y given x comma w we have selected of y given x comma w will be simply be 1 minus y", "image_path": "img_data/video_72_chunk_78.jpg"}
{"video": "video_72", "start": "0:39:30", "end": "0:40:00", "timestamp": "0:39:30 - 0:40:00", "text": "we have selected of y given x comma w will be simply be 1 minus y will be simply be 1 minus y because with y is equal to zero then because with y is equal to zero then this becomes one and this becomes one and this becomes one minus y hat immediately we can minus y hat immediately we can actually get the corresponding p model for get the corresponding p model for the c of the negative case in fact i the c of the negative case in fact i don't need to write anything else in don't need to write anything else in this point maybe i can do maybe i can write p point maybe i can do maybe i can write p model of y is equal to z given x comma w", "image_path": "img_data/video_72_chunk_79.jpg"}
{"video": "video_72", "start": "0:40:00", "end": "0:40:30", "timestamp": "0:40:00 - 0:40:30", "text": "point maybe i can do maybe i can write p model of y is equal to z given x comma w model of y is equal to z given x comma w and this was shown to be 1 - y i and this was shown to be 1 - y i don't even need to produce a vector don't even need to produce a vector in the output of my binary classifier in the output of my binary classifier just with one value from that value i just with one value from that value i can deterministically obtain the other can deterministically obtain the other for the negative class if i have for the negative class if i have produced 0.8 for the positive class then produced 0.8 for the positive class then the probability of the negative class the probability of the negative class is 0.2 that's kind of an important", "image_path": "img_data/video_72_chunk_80.jpg"}
{"video": "video_72", "start": "0:40:30", "end": "0:41:00", "timestamp": "0:40:30 - 0:41:00", "text": "is 0.2 that's kind of an important that's kind of an important conclusion of this kind of discussion conclusion of this kind of discussion and the beri distribution which is and the beri distribution which is obviously very appropriate for binary obviously very appropriate for binary events at the 0o one or the -cal coin events at the 0o one or the -cal coin dossing distribution of heads versus dossing distribution of heads versus tales and if i remember the tales and if i remember the maximum lack kind of discussion and the maximum lack kind of discussion and the -called cross entropy in that kind of -called cross entropy in that kind of cross entropy i had the following cross entropy i had the following formula which is equally applicable to", "image_path": "img_data/video_72_chunk_81.jpg"}
{"video": "video_72", "start": "0:41:00", "end": "0:41:30", "timestamp": "0:41:00 - 0:41:30", "text": "cross entropy i had the following formula which is equally applicable to formula which is equally applicable to any predictor i had a y cat comma y is any predictor i had a y cat comma y is minus the expectation of x comma expectation of x comma y my examples according to the -call p y my examples according to the -call p data high distribution log p model of y given x comma w and this is definitely the result that", "image_path": "img_data/video_72_chunk_82.jpg"}
{"video": "video_72", "start": "0:41:30", "end": "0:42:00", "timestamp": "0:41:30 - 0:42:00", "text": "w and this is definitely the result that is definitely the result that we have seen earlier and just want to we have seen earlier and just want to contrast that against what contrast that against what we have seen earlier we had an we have seen earlier we had an underlying probability distribution underlying probability distribution where our data is and definitely where our data is and definitely this p data had distribution is this p data had distribution is effectively the table that we have effectively the table that we have seen earlier the distribution that governs earlier the distribution that governs the training data or the data that the training data or the data that we have recorded with all our kind we have recorded with all our kind of ground truths the bet data high", "image_path": "img_data/video_72_chunk_83.jpg"}
{"video": "video_72", "start": "0:42:00", "end": "0:42:30", "timestamp": "0:42:00 - 0:42:30", "text": "we have recorded with all our kind of ground truths the bet data high of ground truths the bet data high distribution is present in here no distribution is present in here no changes what we have seen earlier no changes what we have seen earlier it's just a different type of data and it's just a different type of data and i'm actually interested to go ahead i'm actually interested to go ahead and calculate this term over here and calculate this term over here and this term can be trivially calculated as this term can be trivially calculated as log of my p model is why to^ of y log of my p model is why to^ of y * 1 - y the 1 - y or", "image_path": "img_data/video_72_chunk_84.jpg"}
{"video": "video_72", "start": "0:42:30", "end": "0:43:00", "timestamp": "0:42:30 - 0:43:00", "text": "log of my p model is why to^ of y * 1 - y the 1 - y or * 1 - y the 1 - y or log y to the^ of y plus log of 1 - y to log y to the^ of y plus log of 1 - y to the^ of 1 - y and this is u of course the^ of 1 - y and this is u of course making the use of the log of the making the use of the log of the product is the summation of the logs product is the summation of the logs identity and this is uses another identity and this is uses another identity this is y log of y hat + 1 - y", "image_path": "img_data/video_72_chunk_85.jpg"}
{"video": "video_72", "start": "0:43:00", "end": "0:43:30", "timestamp": "0:43:00 - 0:43:30", "text": "identity and this is uses another identity this is y log of y hat + 1 - y identity this is y log of y hat + 1 - y log of 1 - y hat and for those who log of 1 - y hat and for those who remember the discussion of the entropy remember the discussion of the entropy we have seen a sort of identical kind of we have seen a sort of identical kind of term in the presentation of the term in the presentation of the binary kind of entropy graph the binary kind of entropy graph the coin tossing during the coin tossing during the coin tossing experiment let's plug this in into experiment let's plug this in into this formula and at the same time we this formula and at the same time we will replace the expectation with a will replace the expectation with a sample mean from these two we can", "image_path": "img_data/video_72_chunk_86.jpg"}
{"video": "video_72", "start": "0:43:30", "end": "0:44:00", "timestamp": "0:43:30 - 0:44:00", "text": "will replace the expectation with a sample mean from these two we can sample mean from these two we can conclude that the cross entropy the conclude that the cross entropy the binary cross entropy i'm just going binary cross entropy i'm just going to put a capital b in front to put a capital b in front to distinguish for the cross entropy for distinguish for the cross entropy for multiclass problems it is y hat comma multiclass problems it is y hat comma y minus 1 / m summation from i is equal y minus 1 / m summation from i is equal to 1 to m of", "image_path": "img_data/video_72_chunk_87.jpg"}
{"video": "video_72", "start": "0:44:00", "end": "0:44:30", "timestamp": "0:44:00 - 0:44:30", "text": "to 1 to m of yi log of yi hat plus 1 - y i log of 1 - y i hat definitely this is a very easy to evaluate this is definitely a evaluate this is definitely a scaler which is indicates to me whether scaler which is indicates to me whether how well am i doing and this is the loss function that is", "image_path": "img_data/video_72_chunk_88.jpg"}
{"video": "video_72", "start": "0:44:30", "end": "0:45:00", "timestamp": "0:44:30 - 0:45:00", "text": "how well am i doing and this is the loss function that is and this is the loss function that is going to govern from now on our going to govern from now on our binary classification problem and binary classification problem and we are need to use it as an objective we are need to use it as an objective function and when we minimize it function and when we minimize it equivalently we will be reducing the equivalently we will be reducing the probabilistic distance between the p probabilistic distance between the p data hack that is present in my data hack that is present in my data set that is actually given to me data set that is actually given to me and the p model which as we discussed and the p model which as we discussed here is of the beri distribution here is of the beri distribution and now we can just go ahead and plot", "image_path": "img_data/video_72_chunk_89.jpg"}
{"video": "video_72", "start": "0:45:00", "end": "0:45:30", "timestamp": "0:45:00 - 0:45:30", "text": "here is of the beri distribution and now we can just go ahead and plot and now we can just go ahead and plot that in fact let me just go and plot that in fact let me just go and plot this term and this term can be plotted over here the x-axis i'm going to plot it against the y hat the y is for the against the y hat the y is for the probability of the positive and this is my binary positive and this is my binary cross entropy in fact i will call this cross entropy in fact i will call this term the -called inner term to avoid", "image_path": "img_data/video_72_chunk_90.jpg"}
{"video": "video_72", "start": "0:45:30", "end": "0:46:00", "timestamp": "0:45:30 - 0:46:00", "text": "cross entropy in fact i will call this term the -called inner term to avoid term the -called inner term to avoid confusion with something that involves confusion with something that involves an averaging over many of those inner an averaging over many of those inner terms for each of the terms for each of the examples and each of my predictions examples and each of my predictions i have obviously a term over here i have obviously a term over here a real number i can trivially calculate a real number i can trivially calculate let me call it the inner term over let me call it the inner term over here in the y ais and if i plot this here in the y ais and if i plot this term then i will get something that is term then i will get something that is going to look", "image_path": "img_data/video_72_chunk_91.jpg"}
{"video": "video_72", "start": "0:46:00", "end": "0:46:30", "timestamp": "0:46:00 - 0:46:30", "text": "term then i will get something that is going to look this the probability let's interpret this graph when the y interpret this graph when the y height is one means that i am 100% height is one means that i am 100% certain that this is a positive event certain that this is a positive event that has happened and this means that has happened and this means that the probability this curve the probability this curve corresponds with i forgot to mention corresponds with i forgot to mention that this curve corresponds is plotted that this curve corresponds is plotted when y is equal to 1 for the specific", "image_path": "img_data/video_72_chunk_92.jpg"}
{"video": "video_72", "start": "0:46:30", "end": "0:47:00", "timestamp": "0:46:30 - 0:47:00", "text": "that this curve corresponds is plotted when y is equal to 1 for the specific when y is equal to 1 for the specific ground truth when my ground truth agrees ground truth when my ground truth agrees with me then i'm expected to get with me then i'm expected to get the inner term to be zero as intuitively the inner term to be zero as intuitively understood i'm 100% in agreement with understood i'm 100% in agreement with the ground truth here the on the ground truth here the on the other hand when i'm predicting the y the other hand when i'm predicting the y had to be 0 point let's say 05 over had to be 0 point let's say 05 over here and this means that i'm here and this means that i'm predicting the positive to be", "image_path": "img_data/video_72_chunk_93.jpg"}
{"video": "video_72", "start": "0:47:00", "end": "0:47:30", "timestamp": "0:47:00 - 0:47:30", "text": "here and this means that i'm predicting the positive to be predicting the positive to be 0.05 and in other words i'm 0.05 and in other words i'm predicting the negative to be predicting the negative to be 0.95 i'm predicting that a no attack has 0.95 i'm predicting that a no attack has happened with 95% happened with 95% confidence then i'm actually going to confidence then i'm actually going to and my ground ruth disagrees with me i and my ground ruth disagrees with me i am going to incure a huge am going to incure a huge loss i can actually sufficient to loss i can actually sufficient to say here that bc", "image_path": "img_data/video_72_chunk_94.jpg"}
{"video": "video_72", "start": "0:47:30", "end": "0:48:00", "timestamp": "0:47:30 - 0:48:00", "text": "say here that bc penalizes confident wrong decisions such as 0.95 but wrong decisions and actually can draw also the kind of corresponding care", "image_path": "img_data/video_72_chunk_95.jpg"}
{"video": "video_72", "start": "0:48:00", "end": "0:48:30", "timestamp": "0:48:00 - 0:48:30", "text": "decisions and actually can draw also the kind of corresponding care for y is equal to z zero and it should be symmetric obviously in the handwritten kind of way obviously in the handwritten kind of way this is they are not symmetric here but this is they are not symmetric here but this is kind of quite important to this is kind of quite important to realize the behavior of binary cross realize the behavior of binary cross penalizing confident wrong decisions now we have everything that we need to draw a block diagram and this", "image_path": "img_data/video_72_chunk_96.jpg"}
{"video": "video_72", "start": "0:48:30", "end": "0:49:00", "timestamp": "0:48:30 - 0:49:00", "text": "now we have everything that we need to draw a block diagram and this need to draw a block diagram and this block diagram is applicable in fact block diagram is applicable in fact to all sorts of predictors and we have to all sorts of predictors and we have already seen regression now in already seen regression now in classification let me just throw the classification let me just throw the predictor as this box over here we have seen that in the regression setting a linear model a dot product in setting a linear model a dot product in other words between the parameter", "image_path": "img_data/video_72_chunk_97.jpg"}
{"video": "video_72", "start": "0:49:00", "end": "0:49:30", "timestamp": "0:49:00 - 0:49:30", "text": "setting a linear model a dot product in other words between the parameter other words between the parameter vector and features and we also seeing vector and features and we also seeing here a classifier but we have not here a classifier but we have not really discussed yet the functional form really discussed yet the functional form of what the classifier will actually be of what the classifier will actually be but whatever this classifier is but whatever this classifier is going to this is either for going to this is either for aggression or classification and input x is going to come in general it has many dimensions a y", "image_path": "img_data/video_72_chunk_98.jpg"}
{"video": "video_72", "start": "0:49:30", "end": "0:50:00", "timestamp": "0:49:30 - 0:50:00", "text": "to come in general it has many dimensions a y in general it has many dimensions a y height is going to be produced at the height is going to be produced at the output and over here we're going to output and over here we're going to have the let's assume that this is have the let's assume that this is now for u i mean let's call it a loss function u i mean let's call it a loss function and the loss function will be the mean and the loss function will be the mean square error or the cross square error or the cross entropy in fact the we will show that entropy in fact the we will show that the cross entropy is able to the cross entropy is able to accommodate both mean square error as", "image_path": "img_data/video_72_chunk_99.jpg"}
{"video": "video_72", "start": "0:50:00", "end": "0:50:30", "timestamp": "0:50:00 - 0:50:30", "text": "the cross entropy is able to accommodate both mean square error as accommodate both mean square error as well and as well also the binary well and as well also the binary concenter we have just seen the concenter we have just seen the output of this kind of loss function output of this kind of loss function this loss function should have some this loss function should have some knowledge of the ground ruths y and knowledge of the ground ruths y and we are going to obtain a scalar number we are going to obtain a scalar number of the loss for and then the scalar number is going to be effectively fed into a", "image_path": "img_data/video_72_chunk_100.jpg"}
{"video": "video_72", "start": "0:50:30", "end": "0:51:00", "timestamp": "0:50:30 - 0:51:00", "text": "number is going to be effectively fed into a is going to be effectively fed into a block that it will calculate the values block that it will calculate the values of the gradient of the loss with respect of the gradient of the loss with respect to the parameter vector w and this is part of the stochastic gr descent algorithm we have seen and the descent algorithm we have seen and the output of this block is going to be output of this block is going to be called the param parameter update that we will accept a learning rate that", "image_path": "img_data/video_72_chunk_101.jpg"}
{"video": "video_72", "start": "0:51:00", "end": "0:51:30", "timestamp": "0:51:00 - 0:51:30", "text": "that we will accept a learning rate that we actually call ea this will also accept as a high ea this will also accept as a high parameter the mini batch that we have parameter the mini batch that we have actually called mb and it will the parameter update mb and it will the parameter update formula will provide for us and will formula will provide for us and will update for us the vector w of all the update for us the vector w of all the parameters involved inside this parameters involved inside this predictor this is a very generic predictor this is a very generic block diagram that we allow us to", "image_path": "img_data/video_72_chunk_102.jpg"}
{"video": "video_72", "start": "0:51:30", "end": "0:51:41.666667", "timestamp": "0:51:30 - 0:51:41.666667", "text": "predictor this is a very generic block diagram that we allow us to block diagram that we allow us to train and therefore optimize any machine any prediction machine we have seen up to prediction machine we have seen up to this point", "image_path": "img_data/video_72_chunk_103.jpg"}
{"video": "video_73", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "we are in the trajectory where we are going to obtain a blog diagram of our going to obtain a blog diagram of our first classifier binary classifier that first classifier binary classifier that is but before we do i think it's is but before we do i think it's worthwhile kind of thinking a bit about worthwhile kind of thinking a bit about the two general frameworks which are the two general frameworks which are present in the design of this present in the design of this probabilistic models that will give probabilistic models that will give us the functional form of this binary us the functional form of this binary classifier in general there are two classifier in general there are two frameworks the first", "image_path": "img_data/video_73_chunk_0.jpg"}
{"video": "video_73", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "classifier in general there are two frameworks the first one both of these frameworks involve the posterior probability but in a kind of a posterior probability but in a kind of a different way the first framework is different way the first framework is called discriminative and the second one is called generate and the difference between the two is quite important and they are", "image_path": "img_data/video_73_chunk_1.jpg"}
{"video": "video_73", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "generate and the difference between the two is quite important and they are the two is quite important and they are although they both effectively are although they both effectively are modeling in a different way at the modeling in a different way at the posterior let me call this posterior let me call this posterior y of the class small letter k given y of the class small letter k given x which as we have seen in the x which as we have seen in the probability review section this probability review section this posterior is the x given yk * p of yk", "image_path": "img_data/video_73_chunk_2.jpg"}
{"video": "video_73", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "is the x given yk * p of yk and divided by p ofx this is the posterior and by p ofx this is the posterior and these two frameworks as we will see kind these two frameworks as we will see kind of model it a bit differently the of model it a bit differently the discriminative framework we discriminative framework we will be discussing in extensively in will be discussing in extensively in this binary classifier block this binary classifier block diagram is the one that will involve diagram is the one that will involve direct modeling of the posterior the", "image_path": "img_data/video_73_chunk_3.jpg"}
{"video": "video_73", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "diagram is the one that will involve direct modeling of the posterior the direct modeling of the posterior the discriminative i call it the first discriminative i call it the first framework and the second framework and the second framework the first framework discriminative methods model the posterior directly", "image_path": "img_data/video_73_chunk_4.jpg"}
{"video": "video_73", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "posterior directly we get this posterior from the block diagram itself and while the block diagram itself and while the generative ones are effectively model generative ones are effectively model the posterior in parts in its from each the posterior in parts in its from each kind of components we'll first deal kind of components we'll first deal with the -called discriminative with the -called discriminative classifiers and i want to classifiers and i want to connect the earlier discussion connect the earlier discussion we had about the radar problem in we had about the radar problem in that kind of video we have introduced a", "image_path": "img_data/video_73_chunk_5.jpg"}
{"video": "video_73", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "we had about the radar problem in that kind of video we have introduced a that kind of video we have introduced a problem where we had we problem where we had we actually went and drew all the areas actually went and drew all the areas under the two probability distributions under the two probability distributions that were gave raise to the probability that were gave raise to the probability of mistake i want to flip the coin of mistake i want to flip the coin now and actually h sort of discuss the now and actually h sort of discuss the probability of being correct not the probability of being correct not the misclassification error but the u misclassification error but the u when we have the -called when we have the -called true positive events and trying to true positive events and trying to maximiz imize them instead of trying to", "image_path": "img_data/video_73_chunk_6.jpg"}
{"video": "video_73", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "true positive events and trying to maximiz imize them instead of trying to maximiz imize them instead of trying to minimize them is classification error minimize them is classification error i want to kind of come up with some i want to kind of come up with some reasonably intuitive answer to the reasonably intuitive answer to the following question which i'm writing following question which i'm writing over here why the importance on the posterior p of yk", "image_path": "img_data/video_73_chunk_7.jpg"}
{"video": "video_73", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "posterior p of yk given x and how this is related to our performance metrix if i kind of repeat this kind of discussion but from as i said from of discussion but from as i said from the probability of being correct", "image_path": "img_data/video_73_chunk_8.jpg"}
{"video": "video_73", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "of discussion but from as i said from the probability of being correct the probability of being correct in this kind of classification problem in this kind of classification problem we have again this kind of two integrals we have again this kind of two integrals but these two integrals now capture the but these two integrals now capture the correct events this is when we have events this is when we have probability of y is equal to 0 dx plus probability of y is equal to 0 dx plus another integral r1 probability of x another integral r1 probability of x comma y = to 1 dx these are effectively comma y = to 1 dx these are effectively the flipped areas that from the ones the flipped areas that from the ones we have actually drew if you want going we have actually drew if you want going to go ahead and review that kind of", "image_path": "img_data/video_73_chunk_9.jpg"}
{"video": "video_73", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "we have actually drew if you want going to go ahead and review that kind of to go ahead and review that kind of video that will actually be video that will actually be helpful which is of course equal to a helpful which is of course equal to a summation in general for this is now summation in general for this is now the general case where we have the general case where we have capital one to capital k in this capital one to capital k in this specific capital k is equal to specific capital k is equal to two but this formula that i'm two but this formula that i'm actually writing here is going to be actually writing here is going to be general for capital k general for capital k classes or of the integral over the", "image_path": "img_data/video_73_chunk_10.jpg"}
{"video": "video_73", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "general for capital k classes or of the integral over the classes or of the integral over the regions rk of the i'm replacing the joint rk of the i'm replacing the joint with the posterior times the marginal and now it's actually a bit more evident how maximizing p more evident how maximizing p correct effectively involves effectively involves maximizing the posterior because", "image_path": "img_data/video_73_chunk_11.jpg"}
{"video": "video_73", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "effectively involves maximizing the posterior because maximizing the posterior because this term over here is common and independent of the assignment of x to the label yk now we have connected the", "image_path": "img_data/video_73_chunk_12.jpg"}
{"video": "video_73", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "label yk now we have connected the yk now we have connected the direct connection effectively that of direct connection effectively that of the probability of being correct and the probability of being correct and the maximization trying to maximize the maximization trying to maximize the probability of being correct effectively probability of being correct effectively means maximizing the posterior means maximizing the posterior probability let me write that probability let me write that down because it's kind of down because it's kind of important maximizing p correct is equivalent to maximizing p of y k yk given x", "image_path": "img_data/video_73_chunk_13.jpg"}
{"video": "video_73", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "correct is equivalent to maximizing p of y k yk given x maximizing p of y k yk given x effectively this points to the following if we are to plot the following if we are to plot the properity distribution of the posterior distribution of the posterior actually we will see here we'll see actually we will see here we'll see something that let's plot the posterior probility distribution we went from distributions at such as this if you", "image_path": "img_data/video_73_chunk_14.jpg"}
{"video": "video_73", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "distribution we went from distributions at such as this if you distributions at such as this if you remember back in the remember back in the u in the discussion of the binary classifier something that we have seen of x and this is the probability of x and this is the probability of x comma y this was the probability of x comma y this was the probability of x comma y is equal to zer and this a comma y is equal to zer and this a probability of x comma y isal to 1 probability of x comma y isal to 1 and if we are to plot the posterior", "image_path": "img_data/video_73_chunk_15.jpg"}
{"video": "video_73", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "probability of x comma y isal to 1 and if we are to plot the posterior and if we are to plot the posterior probability distribution we will distribution we will be coming up with something that it be coming up with something that it will look this in general this is a very general this in general this is a very general kind of plot this is one to make sure that we do not exceed", "image_path": "img_data/video_73_chunk_16.jpg"}
{"video": "video_73", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "plot this is one to make sure that we do not exceed the one probability of one this is the one probability of one this is the probability of y is equal to 0 given x probability of y is equal to 0 given x and this is the probability of y is = to and this is the probability of y is = to 1 given x and the histograms that we have the histograms that we have actually the histograms that the actually the histograms that the posterior hisrs actually have come up posterior hisrs actually have come up with for a given x0 that", "image_path": "img_data/video_73_chunk_17.jpg"}
{"video": "video_73", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "posterior hisrs actually have come up with for a given x0 that is coming to us as a let's say a new x that we would as a let's say a new x that we would to classify as positive or negative to classify as positive or negative let's say x new that we have never seen let's say x new that we have never seen before touches these two curves in this before touches these two curves in this kind of two points i actually we can actually see points i actually we can actually see here that these two points correspond to", "image_path": "img_data/video_73_chunk_18.jpg"}
{"video": "video_73", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "points i actually we can actually see here that these two points correspond to the discrete this is the for the zeroth class and this is for the zeroth class and this is for the let's say class one this is the probability mass one this is the probability mass function of the posterior function of the posterior distribution at the output of our distribution at the output of our predictor this is the p of y is equal predictor this is the p of y is equal 0 given x new and this the probability of y is", "image_path": "img_data/video_73_chunk_19.jpg"}
{"video": "video_73", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "0 given x new and this the probability of y is new and this the probability of y is equal to 1 given x new and what we have just recognized over here is that all we have recognized over here is that all we have to do is we always pick the pro the class that gives pick the pro the class that gives us the maximum posterior probability us the maximum posterior probability output and rest assured if we do that we output and rest assured if we do that we are", "image_path": "img_data/video_73_chunk_20.jpg"}
{"video": "video_73", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "output and rest assured if we do that we are maximizing the probability of being maximizing the probability of being correct this discussion kind of correct this discussion kind of resulted into this kind of intuitive resulted into this kind of intuitive conclusion but it was not really evident conclusion but it was not really evident initially how the posteriors and the initially how the posteriors and the probability of being correct are related continuing now for the discussion we just had on the discriminative kind of classifiers i directly model the predict the posterior directly model the predict the posterior probability", "image_path": "img_data/video_73_chunk_21.jpg"}
{"video": "video_73", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "directly model the predict the posterior probability we can actually write the posterior we can actually write the posterior probability as follows the y let's say probability as follows the y let's say is equal to one given x is the is equal to one given x is the probability of x given y is = to 1 time probability of x given y is = to 1 time the probability of y is = to the probability of y is = to 1 divided by the probability of x but we 1 divided by the probability of x but we will write this probability of x given will write this probability of x given we have let's say two we have let's say two classes or as the probability of y is classes or as the probability of y is equal to zero probability of x given y to 0 *", "image_path": "img_data/video_73_chunk_22.jpg"}
{"video": "video_73", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "equal to zero probability of x given y to 0 * zero probability of x given y to 0 * probability of y is equal to 0 we are probability of y is equal to 0 we are using here the sum rule of probability using here the sum rule of probability that we have reviewed plus the probability of x reviewed plus the probability of x given y is = to 1 * the probability of y given y is = to 1 * the probability of y is equal 1 if you divide both terms by the if we divide both terms by the if we divide both terms with a pro with the", "image_path": "img_data/video_73_chunk_23.jpg"}
{"video": "video_73", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "terms with a pro with the numerator we will come up with the numerator we will come up with the following expression 1 / following expression 1 / 1+ the probability of x given y is equal to 1 probability of y is equal to 1 * ided by probability of y is equal to 1 * ided by probability of x given y is equal to 0 probability of x given y is equal to 0 probability of y is equal to 0 to the minus1 of y is equal to 0 to the minus1 and", "image_path": "img_data/video_73_chunk_24.jpg"}
{"video": "video_73", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "of y is equal to 0 to the minus1 and this is now related to the probability of odds because the probability of odds because the probability of odds the not the probability was the odds if the not the probability was the odds if we now write down the we now write down the odds is the ratio of this divided by of divided by this for example in", "image_path": "img_data/video_73_chunk_25.jpg"}
{"video": "video_73", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "of this divided by of divided by this for example in by of divided by this for example in a horse race where we have a horse that a horse race where we have a horse that runs 100 races and wins 25 times and runs 100 races and wins 25 times and loses the other 75 times the probability loses the other 75 times the probability of winning is 25 over 100 that's well of winning is 25 over 100 that's well known to us 25% but the odds are 25 over 75 or 33% 25% but the odds are 25 over 75 or 33% or one win to three losses this is or one win to three losses this is what we have actually defined over here what we have actually defined over here in terms of our", "image_path": "img_data/video_73_chunk_26.jpg"}
{"video": "video_73", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "what we have actually defined over here in terms of our odds the probability of winning to odds the probability of winning to the probability of losing that's effectively losing that's effectively if we can assign this to a positive if we can assign this to a positive number if we assign model it as a number if we assign model it as a posst number typically we use the e posst number typically we use the e to the power of some kind of positive number a to of some kind of positive number a to model that then we can and", "image_path": "img_data/video_73_chunk_27.jpg"}
{"video": "video_73", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "of some kind of positive number a to model that then we can and model that then we can and have effectively these two have effectively these two expressions and from these two expressions and from these two expressions we can actually write now expressions we can actually write now the form of the posterior probability the form of the posterior probability distribution that is the probability distribution that is the probability of y is equal to 1 given of y is equal to 1 given x is 1/ 1 + cus a and this is a wellknown function that a and this is a wellknown function that is actually called the sigmoid function", "image_path": "img_data/video_73_chunk_28.jpg"}
{"video": "video_73", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "a and this is a wellknown function that is actually called the sigmoid function because it is when we actually plot this function it will look something that over here will be 0.5 and over here will be the one and it will look a sigmoid that will give will take as sigmoid that will give will take as input a and will provide sigma of a and", "image_path": "img_data/video_73_chunk_29.jpg"}
{"video": "video_73", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "sigmoid that will give will take as input a and will provide sigma of a and input a and will provide sigma of a and all the output is going to be all the output is going to be constrained between 0 and constrained between 0 and one we have effectively came up with this kind of effectively came up with this kind of expression of the sort of a pro expression of the sort of a pro posterior probability distribution at posterior probability distribution at the output of a sigmoidal unit with the output of a sigmoidal unit with having as argument some kind of input a having as argument some kind of input a now if and this is kind of motivates the", "image_path": "img_data/video_73_chunk_30.jpg"}
{"video": "video_73", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "now if and this is kind of motivates the and this is kind of motivates the kind of logistic regression if a is a kind of logistic regression if a is a linear combination is a linear combination is a linear combination of features let's say a is w transpose f of x we have seen both of them", "image_path": "img_data/video_73_chunk_31.jpg"}
{"video": "video_73", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "features let's say a is w transpose f of x we have seen both of them x we have seen both of them notations in our linear regression notations in our linear regression kind of example then this model of the posterior is called logistic regression which is a wellknown and fairly popular way to do binary fairly popular way to do binary classification effectively over here", "image_path": "img_data/video_73_chunk_32.jpg"}
{"video": "video_73", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "fairly popular way to do binary classification effectively over here classification effectively over here we have the assigned the to the log ods assigned the to the log ods another way actually of seeing it is another way actually of seeing it is that if we take here the log of that if we take here the log of the odds in other words the log of the", "image_path": "img_data/video_73_chunk_33.jpg"}
{"video": "video_73", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "the log of the probability of x comma y is = 1 divided by the probability of = 1 divided by the probability of x comma y is equal to 0 this is going x comma y is equal to 0 this is going to be effectively a and if this a is equal to w transpose a and if this a is equal to w transpose f of x this is the form of u logistic x this is the form of u logistic regression the logistic regression is", "image_path": "img_data/video_73_chunk_34.jpg"}
{"video": "video_73", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "x this is the form of u logistic regression the logistic regression is regression the logistic regression is followed by the is actually followed by the is actually implemented using the following diagram implemented using the following diagram as it's actually indicated here first as it's actually indicated here first form a linear combination of form a linear combination of features and this is the dot product features and this is the dot product in other words between the feature in other words between the feature vector and the parameters of the of vector and the parameters of the of our model w and then pass that our model w and then pass that through a sigmoidal unit in order to through a sigmoidal unit in order to obtain a posterior probability at the", "image_path": "img_data/video_73_chunk_35.jpg"}
{"video": "video_73", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "through a sigmoidal unit in order to obtain a posterior probability at the obtain a posterior probability at the output we kind of obtain the logistic output we kind of obtain the logistic regression kind of block diagram for regression kind of block diagram for kind of a first principle the block kind of a first principle the block diagram is going to be w transpose f of x where we have taken x and very similar x where we have taken x and very similar to what we have seen in to what we have seen in logistic regression we went logistic regression we went through a featu riser to obtain i of", "image_path": "img_data/video_73_chunk_36.jpg"}
{"video": "video_73", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "through a featu riser to obtain i of x which we have used in this kind of dot product to form a scalar a and dot product to form a scalar a and this scalar a is at the input of a this scalar a is at the input of a sigmoidal unit sigma and that y hat rest assur is sigma and that y hat rest assur is going to be the probability of y is going to be the probability of y is equal to 1 given x this is our equal to 1 given x this is our first classifier that we will be calling a", "image_path": "img_data/video_73_chunk_37.jpg"}
{"video": "video_73", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "classifier that we will be calling a generalized linear model and we call it generalized because model and we call it generalized because of the nonlinear unit which is of the nonlinear unit which is definitely nonlinear because the every definitely nonlinear because the every sigmoidal unit can take any number from sigmoidal unit can take any number from minus one million let's say to plus one minus one million let's say to plus one million but it compresses that into a million but it compresses that into a dynamic r between 0 and one we dynamic r between 0 and one we definitely want the output to be 0 and 1 definitely want the output to be 0 and 1 because we have interpreted the output", "image_path": "img_data/video_73_chunk_38.jpg"}
{"video": "video_73", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "definitely want the output to be 0 and 1 because we have interpreted the output because we have interpreted the output as a pro as a posterior probability but as a pro as a posterior probability but definitely there a nonlinear unit definitely there a nonlinear unit this kind of long nan transformation this kind of long nan transformation from a to the posterior and that's from a to the posterior and that's why we call it generalized but why we call it generalized but definitely it's a linear model in a definitely it's a linear model in a sense that it is one of the sense that it is one of the blocks of the diagram involves a blocks of the diagram involves a linear unit a linear combination of the linear unit a linear combination of the features fx all we need to do now is to attach to", "image_path": "img_data/video_73_chunk_39.jpg"}
{"video": "video_73", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "all we need to do now is to attach to it two things the first it two things the first is the binary cross entropy is the binary cross entropy loss which will accept also the loss which will accept also the ground truth y this binary cross entropy ground truth y this binary cross entropy loss will be feeding the well known to us stochastic graded descent kind of us stochastic graded descent kind of algorithm with it's kind of parameter", "image_path": "img_data/video_73_chunk_40.jpg"}
{"video": "video_73", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "us stochastic graded descent kind of algorithm with it's kind of parameter algorithm with it's kind of parameter gradient calculation and parameter update will feed the w and will update the w at every w and will update the w at every iteration this block diagram is no iteration this block diagram is no surprise to us by now we have seen it surprise to us by now we have seen it many times in both linear regression and many times in both linear regression and now classification and the only thing now classification and the only thing that remains to be done over here is", "image_path": "img_data/video_73_chunk_41.jpg"}
{"video": "video_73", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "now classification and the only thing that remains to be done over here is that remains to be done over here is to come up with a expression of to come up with a expression of the cross entropy loss the binary cross entropy loss the binary cross entropy loss with respect to the set of loss with respect to the set of parameters w and this can be shown to be parameters w and this can be shown to be u some form such as thisal to 1 to m of y ius y i pi of", "image_path": "img_data/video_73_chunk_42.jpg"}
{"video": "video_73", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "ius y i pi of xi now we have some expression about xi now we have some expression about the gradient that we need in order the gradient that we need in order for us to implement sastic grade descent for us to implement sastic grade descent and now we will see in a notebook how and now we will see in a notebook how the stoas r descent is powering logistic the stoas r descent is powering logistic regressor and in fact it's al also regressor and in fact it's al also worthwhile commenting on how the name worthwhile commenting on how the name kind of logistic regression came to be kind of logistic regression came to be attached to this kind of block attached to this kind of block diagram and in fact if we diagram and in fact if we treat this binary classification problem", "image_path": "img_data/video_73_chunk_43.jpg"}
{"video": "video_73", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "diagram and in fact if we treat this binary classification problem treat this binary classification problem a regression problem and we plot a regression problem and we plot over here the u -called xis versus over here the u -called xis versus the y similarly what we have seen in the y similarly what we have seen in many regression problems then many regression problems then definitely r y is discrete random definitely r y is discrete random variable and take values between zero variable and take values between zero let's say and one and certainly for let's say and one and certainly for in this kind of neighborhood we'll see in this kind of neighborhood we'll see many assignments to zero and in this", "image_path": "img_data/video_73_chunk_44.jpg"}
{"video": "video_73", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "in this kind of neighborhood we'll see many assignments to zero and in this many assignments to zero and in this neighborhood remember the radar problem neighborhood remember the radar problem high signal strength low signal strength high signal strength low signal strength high signal strength mostly we will get high signal strength mostly we will get a positive prediction of our a positive prediction of our attacks and over here we're going to attacks and over here we're going to have a negative prediction of our have a negative prediction of our attacks here we're actually plotting attacks here we're actually plotting here the ground truths and if we here the ground truths and if we are to do regression to are to if we are to do regression to fit this data in a very similar way as", "image_path": "img_data/video_73_chunk_45.jpg"}
{"video": "video_73", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "are to if we are to do regression to fit this data in a very similar way as fit this data in a very similar way as we have done earlier probably will come we have done earlier probably will come up with a kind of a straight line that up with a kind of a straight line that tries to maximize some objective kind tries to maximize some objective kind of function this is straight of function this is straight line regression is not going to be very line regression is not going to be very appropriate for a classification problem appropriate for a classification problem because we are expecting our predictor because we are expecting our predictor to produce values always between zero to produce values always between zero and one therefore what we do here is and one therefore what we do here is we are applying the sigmoidal this is we are applying the sigmoidal this is effectively the line that generates", "image_path": "img_data/video_73_chunk_46.jpg"}
{"video": "video_73", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "we are applying the sigmoidal this is effectively the line that generates is effectively the line that generates the a when we have no features no the a when we have no features no featurization in this specific syle featurization in this specific syle example and the sigmoidal unit will example and the sigmoidal unit will match effectively the it has a linear effectively the it has a linear component over here and will compress component over here and will compress everything between zero and one everything between zero and one that's another way of kind of that's another way of kind of graphically remembering logistic graphically remembering logistic regression as an attempt to do regression as an attempt to do regression but at the same time regression but at the same time with a kind of a compressive", "image_path": "img_data/video_73_chunk_47.jpg"}
{"video": "video_73", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "regression but at the same time with a kind of a compressive with a kind of a compressive step now what we will do to step now what we will do to conclude a little bit the topic of conclude a little bit the topic of classification is to just in passing classification is to just in passing quote a couple of things about the quote a couple of things about the second framework that i have mentioned second framework that i have mentioned the soal generative classification the soal generative classification frameworks in the generative frameworks in the generative classification framework we're again classification framework we're again going to be task to calculate the going to be task to calculate the posterior probability", "image_path": "img_data/video_73_chunk_48.jpg"}
{"video": "video_73", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "going to be task to calculate the posterior probability that we see here for in general kind of k here for in general kind of k glasses and this posterior is going to be evidently given by this general kind be evidently given by this general kind of formula in the generative approach we will do two steps instead of will do two steps instead of coming up with the block diagram that coming up with the block diagram that generates that from directly and", "image_path": "img_data/video_73_chunk_49.jpg"}
{"video": "video_73", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "coming up with the block diagram that generates that from directly and generates that from directly and models the posterior directly as we have models the posterior directly as we have done with logistic aggression we will done with logistic aggression we will first do two steps one is to estimate the likelihood and the second is to estimate the", "image_path": "img_data/video_73_chunk_50.jpg"}
{"video": "video_73", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "the marginal and n come to some degree of approximation because it's actually of approximation because it's actually typically a very expensive for large dimensions for expensive for large dimensions for larg gen this is a very expensive calculation for we will typically involve some", "image_path": "img_data/video_73_chunk_51.jpg"}
{"video": "video_73", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "for we will typically involve some form of approximation for calculating this approximation for calculating this denominator over here of the posterior the knife base is a famous generative classification method and we actually classification method and we actually going to see that when we come to going to see that when we come to language modeling and some other tasks language modeling and some other tasks later on in some videos i will", "image_path": "img_data/video_73_chunk_52.jpg"}
{"video": "video_73", "start": "0:26:30", "end": "0:26:42.766667", "timestamp": "0:26:30 - 0:26:42.766667", "text": "language modeling and some other tasks later on in some videos i will later on in some videos i will take a rain check to discuss it at that take a rain check to discuss it at that moment and revisit if you the moment and revisit if you the generative classification framework and generative classification framework and the discussion of on bas is not really the discussion of on bas is not really essential right now for us to progress", "image_path": "img_data/video_73_chunk_53.jpg"}
{"video": "video_76", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we will actually see some definitions about covariance and some definitions about covariance and correlation matrixes which are actually correlation matrixes which are actually coming very frequently they are met very coming very frequently they are met very frequently in data science let me frequently in data science let me sort of describe the topic this is sort of describe the topic this is covariance and core correlation matrices we'll cover both let's assume that i have a vector x let's assume that i have a vector x that is actually given by that is actually given by nx1 x2 xn these are the elements of the", "image_path": "img_data/video_76_chunk_0.jpg"}
{"video": "video_76", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "that is actually given by nx1 x2 xn these are the elements of the nx1 x2 xn these are the elements of the vector and let's assume that x is and let's assume that x is sort of again sort of use this kind of sort of again sort of use this kind of notation belongs to the n dimensional notation belongs to the n dimensional space of real numbers space of real numbers and the dimensionality of x is and the dimensionality of x is generated here is n by one it's a column generated here is n by one it's a column vector we can actually form we can actually form the correlation matrix let me symbolize", "image_path": "img_data/video_76_chunk_1.jpg"}
{"video": "video_76", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "the correlation matrix let me symbolize correlation matrix let me symbolize it as rx here as an expectation of the product of x transpose the product being if i kind of expand it and i have if i kind of expand it and i have here expectation of x1", "image_path": "img_data/video_76_chunk_2.jpg"}
{"video": "video_76", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "x1 x2 dot xn and the transpose of that is now is a and the transpose of that is now is a row vector it's x1 x2 row vector it's x1 x2 x n and this one hook actually can be written as the expectation of the x1 transpose the x1 squared then we have the", "image_path": "img_data/video_76_chunk_3.jpg"}
{"video": "video_76", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "the x1 squared then we have the expectation based in other words goes expectation based in other words goes inside and we have x1 x2 inside and we have x1 x2 and over here we have an expectation x1 and over here we have an expectation x1 x n and all of this kind of diagonal kind of and all of this kind of diagonal kind of terms are sort of can be guest i guess this is now xn squared and over here i have an expectation of", "image_path": "img_data/video_76_chunk_4.jpg"}
{"video": "video_76", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "squared and over here i have an expectation of and over here i have an expectation of xn x1 and on all of this can be filled and on all of this can be filled in but i hope you got the idea that in but i hope you got the idea that this is basically a correlation this is basically a correlation matrix i mean the correlation matrix captures all second", "image_path": "img_data/video_76_chunk_5.jpg"}
{"video": "video_76", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "all second moments between paul possible pairs of components of the random vector", "image_path": "img_data/video_76_chunk_6.jpg"}
{"video": "video_76", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "of the random vector that is basically what a correlation matrix is in of course is a very useful matrix is in of course is a very useful quantity because we have seen it in many because we have seen it in many instances in data science we want to instances in data science we want to understand what is the correlation understand what is the correlation between one feature and another now in between one feature and another now in many probability distributions we also many probability distributions we also have met the -called covenience matrix have met the -called covenience matrix and the covariance matrix can be defined and the covariance matrix can be defined now and it's actually related to the now and it's actually related to the correlation matrix as follows let me correlation matrix as follows let me call this cx and let me give you the", "image_path": "img_data/video_76_chunk_7.jpg"}
{"video": "video_76", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "correlation matrix as follows let me call this cx and let me give you the call this cx and let me give you the definition is an expectation of now we have to subtract the mean of now we have to subtract the mean let's call it the x minus let's call it the x minus mu x times x minus nu x transpose minus nu x transpose this is expectation let me do the this is expectation let me do the multiplication x transpose multiplication x transpose minus x mu x minus nu x", "image_path": "img_data/video_76_chunk_8.jpg"}
{"video": "video_76", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "minus x mu x minus nu x the m6 transpose plus nu x new x the m6 transpose plus nu x new x transpose and this is now let's take the and this is now let's take the expectation inside this is expectation inside this is expectation over off of x transpose expensive list off of x transpose expensive list excess transpose minus expectation of x excess transpose minus expectation of x times in your x transpose minus times in your x transpose minus nu x", "image_path": "img_data/video_76_chunk_9.jpg"}
{"video": "video_76", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "times in your x transpose minus nu x expectation of x transpose expectation of x transpose plus whether x these are all numbers a deterministic number this is deterministic number this is basically nu x new x transpose all right let's look at this now we have let's look at this now we have here this expectation of x is nu x this expectation of x is nu x and this is the new x", "image_path": "img_data/video_76_chunk_10.jpg"}
{"video": "video_76", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "this expectation of x is nu x and this is the new x and this is the new x transpose all right we have here mu x minus mu all right we have here mu x minus mu x min x transpose we have here x min x transpose we have here plus nu x mu x transpose this term plus nu x mu x transpose this term and this term goes away and i have and this term goes away and i have finally expectation of x transpose finally expectation of x transpose minus nu x new x transpose which of course this one is the", "image_path": "img_data/video_76_chunk_11.jpg"}
{"video": "video_76", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "which of course this one is the correlation matrix we actually can write that c x the we actually can write that c x the covariance matrix is equal to the covariance matrix is equal to the correlation matrix minus nu x new x correlation matrix minus nu x new x transpose that is if you the kind of a key that is if you the kind of a key relationship and we definitely we can also have and we definitely we can also have a recursive way of estimating this kind of matrices", "image_path": "img_data/video_76_chunk_12.jpg"}
{"video": "video_76", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "a recursive way of estimating this kind of matrices estimating this kind of matrices let's assume that i have the following let's assume that i have the following end of problem and for those who just end of problem and for those who just give you an application give you an application of from real life when it is actually of from real life when it is actually useful let's assume that i have let's useful let's assume that i have let's say a smartphone today's kind of say a smartphone today's kind of smartphones has a kind of a variety of smartphones has a kind of a variety of microphones they have typically two over microphones they have typically two over here and potentially a third one here and potentially a third one over there they actually form a kind", "image_path": "img_data/video_76_chunk_13.jpg"}
{"video": "video_76", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "here and potentially a third one over there they actually form a kind over there they actually form a kind of an array of microphones where of an array of microphones where typically array of microphones typically array of microphones in both smartphones and also in smart in both smartphones and also in smart speakers we also have an array of speakers we also have an array of microphones around if you microphones around if you a circular kind of array a circular kind of array in this when you have arrays these when you have arrays these linear rays or circular arrays you linear rays or circular arrays you may actually do some kind of noise may actually do some kind of noise cancellation all of the devices which cancellation all of the devices which are actually doing cancellation are", "image_path": "img_data/video_76_chunk_14.jpg"}
{"video": "video_76", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "cancellation all of the devices which are actually doing cancellation are actually doing cancellation are doing this type of array processing in doing this type of array processing in order to do noise cancellation they need order to do noise cancellation they need an accurate estimate and up-to-date an accurate estimate and up-to-date online estimate if you of the online estimate if you of the correlation matrix let's assume that correlation matrix let's assume that i'm actually looking now at the for the i'm actually looking now at the for the following kind of notation i'm actually following kind of notation i'm actually going to be defining as p going to be defining as p the sample index", "image_path": "img_data/video_76_chunk_15.jpg"}
{"video": "video_76", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "the sample index i mean the current sample index and i can actually have from the rx let me call it now at this specific moment in time at this specific moment in time i will have i can write it as 1 over i can write it as 1 over p summation from i is equal to 1 to p x i", "image_path": "img_data/video_76_chunk_16.jpg"}
{"video": "video_76", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "summation from i is equal to 1 to p x i from i is equal to 1 to p x i but i suppose the beef sample has now come in and i the beef sample has now come in and i actually can use all the samples i have actually can use all the samples i have received up to this moment to come up received up to this moment to come up with an estimate of the correlation with an estimate of the correlation matrix we would to make to write matrix we would to make to write an expression where this kind of an expression where this kind of estimate is going to be written in a estimate is going to be written in a recursive way with p minus 1 are the previous kind of with p minus 1 are the previous kind of as a previous estimate we can", "image_path": "img_data/video_76_chunk_17.jpg"}
{"video": "video_76", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "with p minus 1 are the previous kind of as a previous estimate we can as a previous estimate we can actually write this as 1 over p and i can take this summation and i can take this summation and actually break it into two parts from i actually break it into two parts from i is equal to one to p minus one is equal to one to p minus one this x i transpose plus xp transpose and i hope everyone plus xp transpose and i hope everyone agrees that this is basically equivalent agrees that this is basically equivalent to this and i can actually write it as now 1 and i can actually write it as now 1 over p this thing is p minus 1", "image_path": "img_data/video_76_chunk_18.jpg"}
{"video": "video_76", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "and i can actually write it as now 1 over p this thing is p minus 1 over p this thing is p minus 1 rx using the previous p minus 1 samples using the previous p minus 1 samples because lxp minus 1 is 1 over p minus 1 because lxp minus 1 is 1 over p minus 1 of this and we have xp transpose and we have xp transpose and therefore i can write it rxp and therefore i can write it rxp is equal to rxp minus 1. is equal to rxp minus 1. plus 1 over p", "image_path": "img_data/video_76_chunk_19.jpg"}
{"video": "video_76", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "plus 1 over p xp transpose minus r x t minus 1. this is basically the instantaneous", "image_path": "img_data/video_76_chunk_20.jpg"}
{"video": "video_76", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "the instantaneous estimate of the rx paste if you on the current sample and this is the estimate of the previous", "image_path": "img_data/video_76_chunk_21.jpg"}
{"video": "video_76", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "and this is the estimate of the previous foreign correlation matrix estimate if we have a batch if you a estimation process where if you a estimation process where we have access to all the data evidently we have access to all the data evidently we can use all the data to come up with we can use all the data to come up with the correlation matrix and we can the correlation matrix and we can actually create if you some kind of actually create if you some kind of a time series plot over here", "image_path": "img_data/video_76_chunk_22.jpg"}
{"video": "video_76", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "actually create if you some kind of a time series plot over here a time series plot over here where rx is the elements of rx are where rx is the elements of rx are uploaded on this kind of axis the values uploaded on this kind of axis the values of rx uploaded on this kind of axis of rx uploaded on this kind of axis if for example we had the sort of batch if for example we had the sort of batch estimate of the sample covariance matrix estimate of the sample covariance matrix to be let's say value which is to be let's say value which is independent of the y of the i because we independent of the y of the i because we only obtain it at the very end if you only obtain it at the very end if you then the online estimates may then the online estimates may actually start fairly noisy and then", "image_path": "img_data/video_76_chunk_23.jpg"}
{"video": "video_76", "start": "0:12:00", "end": "0:12:10.733333", "timestamp": "0:12:00 - 0:12:10.733333", "text": "then the online estimates may actually start fairly noisy and then actually start fairly noisy and then gradually will converge over time to gradually will converge over time to this value this is just one of the this value this is just one of the elements of this covariance matrix", "image_path": "img_data/video_76_chunk_24.jpg"}
{"video": "video_79", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "in this video we will visit some of the perhaps the most visit some of the perhaps the most important rules of probability and we important rules of probability and we will kind of try to derive them kind of try to derive them experiment this experiment involves two experiment this experiment involves two errors or containers you want to call it this is the two containers i will call this container zero will call this container zero and container one and these containers they have inside", "image_path": "img_data/video_79_chunk_0.jpg"}
{"video": "video_79", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "and container one and these containers they have inside these containers they have inside some widgets balls or whatever some widgets balls or whatever i will use these symbols for i will use these symbols for they have six of these widgets and of these widgets and two of those widgets this container zero two of those widgets this container zero and container one has this widget and container one has this widget and three of those two types of widgets", "image_path": "img_data/video_79_chunk_1.jpg"}
{"video": "video_79", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "and three of those two types of widgets two types of widgets you can the experiment goes as you can the experiment goes as follows this is the experiment follows this is the experiment the first stage of the experiment the first stage of the experiment involves two a peak randomly a container or error and the second part of the experiment is after you part of the experiment is after you selected", "image_path": "img_data/video_79_chunk_2.jpg"}
{"video": "video_79", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "part of the experiment is after you selected the container you pick you sample one of those you pick you sample one of those widgets out of that container widgets out of that container you select your sample not select you select your sample not select your sample and object your sample and object widget out of this container with replacement", "image_path": "img_data/video_79_chunk_3.jpg"}
{"video": "video_79", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "out of this container with replacement this with replacement what is really the meaning of the width really the meaning of the width replacement with replacement means replacement with replacement means that after you sample an object you put your hand inside you pick an object out hand inside you pick an object out of this container you have selected in step one container you have selected in step one you look at it you determine what it is you look at it you determine what it is you write it down that i sampled object that i sampled object the white or black and the white or black and you put it back inside the container", "image_path": "img_data/video_79_chunk_4.jpg"}
{"video": "video_79", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "the white or black and you put it back inside the container you put it back inside the container that is with replacement you are that is with replacement you are effectively replacing effectively replacing this object back into the container we will call this random the container we will denote it with a the container we will denote it with a variable y [music] [music] and we know that the y could be because and we know that the y could be because we have only two containers could be we have only two containers could be either zero or one also x is the letter we will choose to", "image_path": "img_data/video_79_chunk_5.jpg"}
{"video": "video_79", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "either zero or one also x is the letter we will choose to also x is the letter we will choose to [music] [music] denote the widget and we have two types of widgets the and we have two types of widgets the white widget and the black widget we could actually say this is one this is zero whatever one this is zero whatever it doesn't really matter we cannot it doesn't really matter we cannot really do calculations with really do calculations with this kind of symbols over there although this kind of symbols over there although potentially we could but we replace them", "image_path": "img_data/video_79_chunk_6.jpg"}
{"video": "video_79", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "this kind of symbols over there although potentially we could but we replace them potentially we could but we replace them with one and zero equivalently one and zero equivalently that's basically the x and y that's basically the x and y both are effectively binary in this specific effectively binary in this specific experiment of us let's assume now experiment of us let's assume now that someone actually gave you that someone actually gave you a probability distribution a probability distribution of y and because y is discrete obviously you because y is discrete obviously you would expect to see some discrete global distribution let's", "image_path": "img_data/video_79_chunk_7.jpg"}
{"video": "video_79", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "would expect to see some discrete global distribution let's some discrete global distribution let's assume that probability of assume that probability of selecting the sort of container 0 is 40 percent and the probability of selecting and the probability of selecting container 1 is 60 or obviously because the sum 60 or obviously because the sum being a probability distribution should being a probability distribution should be equal to 1.0 and ask you to plot the probability", "image_path": "img_data/video_79_chunk_8.jpg"}
{"video": "video_79", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "and ask you to plot the probability plot the probability distribution for x and i guess x also is discrete and i guess x also is discrete in this case it's 0 and 1. in this case it's 0 and 1. obviously without really i mean the obviously without really i mean the only avenue to come up with this only avenue to come up with this probability distribution for x is to run probability distribution for x is to run this experiment multiple times experiment multiple times and in this specific case we will and in this specific case we will run the experiment let's say n times", "image_path": "img_data/video_79_chunk_9.jpg"}
{"video": "video_79", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "and in this specific case we will run the experiment let's say n times run the experiment let's say n times this thing here is executed n this thing here is executed n times this process well we have effectively a probability well we have effectively a probability a space that involves two variables a space that involves two variables x here and y is here on this kind of x here and y is here on this kind of axis this is 4x is equal to 0 and 1 is 4x is equal to 0 and 1 and correspondingly for y is equal to 0 and correspondingly for y is equal to 0 and 1.", "image_path": "img_data/video_79_chunk_10.jpg"}
{"video": "video_79", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "and correspondingly for y is equal to 0 and 1. and we are at any experiment round and we are at any experiment round we are getting one of those points we are getting one of those points obviously this point for example obviously this point for example corresponds to first selecting the earn zero first selecting the earn zero and then drawing out of this urn and then drawing out of this urn the object zero which corresponds to the object zero which corresponds to black widget if we count the number of events that falls one of the number of events that falls one of those", "image_path": "img_data/video_79_chunk_11.jpg"}
{"video": "video_79", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "the number of events that falls one of those quadrants over here we will let's quadrants over here we will let's call them these number of events in this quadrant these number of events in this quadrant related to x and y the first index will be what was the first index will be what was the x and the second index will have been x and the second index will have been there will be the y is as a convention will be the y is as a convention the indexing is not necessarily the indexing is not necessarily important zero one in this case it kind of makes zero one in this case it kind of makes sense", "image_path": "img_data/video_79_chunk_12.jpg"}
{"video": "video_79", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "zero one in this case it kind of makes sense to column this and 0 and 1 0. and now we can actually start kind of quantifying what would actually be the probability what would actually be the probability distribution of x because now we have the data in front of because now we have the data in front of us the probability that x is zero the probability that x is zero is given by summing these two", "image_path": "img_data/video_79_chunk_13.jpg"}
{"video": "video_79", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "the probability that x is zero is given by summing these two is given by summing these two events n 0 and n 0 1 effective it events n 0 and n 0 1 effective it is really this area over here is really this area over here it is exactly of what aaron was of it is exactly of what aaron was of course selected first and we can actually write that first and we can actually write that i'm not sure exactly which one is larger i'm not sure exactly which one is larger which one is smaller i have not really counted smaller i have not really counted strictly speaking the events here but strictly speaking the events here but just want to write down just want to write down the expressions n 0 plus n the expressions n 0 plus n 0 1 divided by n and the probability of", "image_path": "img_data/video_79_chunk_14.jpg"}
{"video": "video_79", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "the expressions n 0 plus n 0 1 divided by n and the probability of 0 1 divided by n and the probability of x is equal to 1 is going to be this area over here n is going to be this area over here n 1 0 plus n 1 divided by 1 0 plus n 1 divided by n where capital n was the number of n where capital n was the number of times that we have executed this experiment over and over executed this experiment over and over again now what if someone asked us to quantify now what if someone asked us to quantify another probability distribution which another probability distribution which in this case is the conditional in this case is the conditional probability distribution probability distribution of p of y given x", "image_path": "img_data/video_79_chunk_15.jpg"}
{"video": "video_79", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "probability distribution of p of y given x well this is the probability of well this is the probability of selecting the here given what we are actually observing given what we are actually observing after we pick an object out of it after we pick an object out of it i mean obviously we can think i mean obviously we can think about this probability distribution about this probability distribution as having if you four components as having if you four components and it's not of course", "image_path": "img_data/video_79_chunk_16.jpg"}
{"video": "video_79", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "as having if you four components and it's not of course and it's not of course difficult to see that this is the difficult to see that this is the probability of y is equal to zero given x is equal to zero given x is equal to zero probability of y is equal to one probability of y is equal to one given x is equal to zero and then of given x is equal to zero and then of course the corresponding course the corresponding probabilities for the other choice of the other choice of the other observation of the widget the other observation of the widget we have effectively", "image_path": "img_data/video_79_chunk_17.jpg"}
{"video": "video_79", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "the other observation of the widget we have effectively four components and i believe it's four components and i believe it's easy to start quantifying those easy to start quantifying those components what will be the probability components what will be the probability of zero given x is equal to zero first is given x is equal to zero first is this component actually would be the actually would be the n zero divided by because x is always equal to 0 is always equal to 0 this will actually be the sum", "image_path": "img_data/video_79_chunk_18.jpg"}
{"video": "video_79", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "is always equal to 0 this will actually be the sum this will actually be the sum just we have done over here 0 just we have done over here 0 plus n 0 1. plus n 0 1. this component would actually be this component would actually be correspondingly 0 1 divided by n 0 correspondingly 0 1 divided by n 0 plus n 0 1. 0 plus n 0 1. and now this component over here is and now this component over here is going to be n one zero", "image_path": "img_data/video_79_chunk_19.jpg"}
{"video": "video_79", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "n one zero divided by n 1 0 plus n 1 and similarly plus n 1 and similarly this guy will be n 1 this guy will be n 1 divided by n one zero plus n divided by n one zero plus n one this is fairly easy to see by just observing if you this diagram just observing if you this diagram over here and write it down this", "image_path": "img_data/video_79_chunk_20.jpg"}
{"video": "video_79", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "over here and write it down this and write it down this these relationships from every these relationships from every from the counters from the count that we have seen and as you probably already have noticed already we have an ability to couple this type of two an ability to couple this type of two queries to the sum and product rules to the sum and product rules of probability because", "image_path": "img_data/video_79_chunk_21.jpg"}
{"video": "video_79", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "to the sum and product rules of probability because of probability because take for example the first query that we take for example the first query that we asked what is the probability asked what is the probability distribution of x you can very clearly see here that you can very clearly see here that in this specific case we have in this specific case we have a summation involved a summation involved and if you write down the sum rule that sum rule was effectively the probability distribution of x probability distribution of x is equal to the sum over y", "image_path": "img_data/video_79_chunk_22.jpg"}
{"video": "video_79", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "probability distribution of x is equal to the sum over y is equal to the sum over y of p of x comma y we are marginalizing out my summing we are marginalizing out my summing over all possibilities y over the joint over all possibilities y over the joint distribution and this is exactly what we see here we and this is exactly what we see here we sum over all possibilities sum over all possibilities of x that we can actually get of x that we can actually get sorry with overall possibilities of sorry with overall possibilities of why we actually can get why we actually can get and therefore we are obtaining the and therefore we are obtaining the probability distribution of x", "image_path": "img_data/video_79_chunk_23.jpg"}
{"video": "video_79", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "and therefore we are obtaining the probability distribution of x probability distribution of x this effectively is the same as the sum rule that we have is the same as the sum rule that we have tasked to start sort of not tasked to start sort of not necessarily deriving but that's just necessarily deriving but that's just observing out of this kind of experiment observing out of this kind of experiment and this is a very straightforward fluorite rule that we can be derived fluorite rule that we can be derived out of this sort of exercise we have just done sort of exercise we have just done now this other rule which is the product now this other rule which is the product rule takes a little bit more work", "image_path": "img_data/video_79_chunk_24.jpg"}
{"video": "video_79", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "now this other rule which is the product rule takes a little bit more work rule takes a little bit more work and that's what we will do next and that's what we will do next for the product rule we'll start with for the product rule we'll start with this relationship over here and we'll try to relationship over here and we'll try to do a substitution which is obviously evident which is obviously evident for that i need to use a different for that i need to use a different kind of pages i run out of space here kind of pages i run out of space here i'm just going to carry over i'm just going to carry over and what i just wrote here p of and what i just wrote here p of y given x one of the y given x one of the of these four", "image_path": "img_data/video_79_chunk_25.jpg"}
{"video": "video_79", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "y given x one of the of these four was the probability of y is equal to zero co a given x is equal to zero is one of those four components which was actually given by components which was actually given by n zero divided by n zero plus n zero one now we can actually under try to understand what is really", "image_path": "img_data/video_79_chunk_26.jpg"}
{"video": "video_79", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "now we can actually under try to understand what is really under try to understand what is really the n zero from this relationship we can solve for n zero is going to be p of y is equal to zero given x is equal to zero given x is equal to zero and zero plus n zero one and just and zero plus n zero one and just leave it now this without really leave it now this without really proceeding any further now if we write the p of x is equal to zero", "image_path": "img_data/video_79_chunk_27.jpg"}
{"video": "video_79", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "now if we write the p of x is equal to zero comma y is equal to 0 then obviously comma y is equal to 0 then obviously if you can recall from the previous kind if you can recall from the previous kind of diagram these are all the events that are in these are all the events that are in this quadrant divided by n which is the number of divided by n which is the number of experiments we have run experiments we have run this is basically n 0 divided by n this is basically n 0 divided by n and we can actually replace n 0 from and we can actually replace n 0 from here it's going to be equal to here it's going to be equal to p of y is equal to 0 given x is equal to p of y is equal to 0 given x is equal to 0 m 0 plus n 0 1", "image_path": "img_data/video_79_chunk_28.jpg"}
{"video": "video_79", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "0 m 0 plus n 0 1 divided by n well look at this guy over here look at this guy over here is exactly what we actually called the p is exactly what we actually called the p is equal to x is equal to zero is equal to x is equal to zero probability effectively we can probability effectively we can write this expression as p of y write this expression as p of y is equal to zero given x is equal to zero given x is equal to zero times p of x is equal to 0 from", "image_path": "img_data/video_79_chunk_29.jpg"}
{"video": "video_79", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "zero times p of x is equal to 0 from p of x is equal to 0 from this diagram over here and this from this diagram over here and this is now something that starts to remind us if something that starts to remind us if you the product rule you the product rule as an expression and if we are to repeat as an expression and if we are to repeat the same exercise for the same exercise for the others we will rapidly come to the others we will rapidly come to the conclusion and this is the other expression was and this is the other expression was just writing it down y is equal to one writing it down y is equal to one given x is equal to zero times p of x is given x is equal to zero times p of x is equal to zero", "image_path": "img_data/video_79_chunk_30.jpg"}
{"video": "video_79", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "given x is equal to zero times p of x is equal to zero and similarly for the others x and similarly for the others x is equal to one comma y is equal to zero is equal to one comma y is equal to zero p of y is equal to zero given x is equal p of y is equal to zero given x is equal to one pi of x is equal to one pi of x is equal to one and lastly p of y is equal to one and lastly p of y is equal to one comma x is equal to one this is p comma x is equal to one this is p of y is equal to 1 given x is equal to 1 of y is equal to 1 given x is equal to 1 times p of x is equal to 1. we times p of x is equal to 1. we actually have shown", "image_path": "img_data/video_79_chunk_31.jpg"}
{"video": "video_79", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "times p of x is equal to 1. we actually have shown that these expressions are true that these expressions are true for any single possibility of x and y possibility of x and y and then we can safely conclude that the and then we can safely conclude that the in general p of x comma y in general p of x comma y is equal to p of y given x is equal to p of y given x times p of x which is times p of x which is the product rule", "image_path": "img_data/video_79_chunk_32.jpg"}
{"video": "video_79", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "times p of x which is the product rule and of course we now know but that because of p of x comma y is equal to p of y comma x is equal to p of y comma x we can actually write another version of we can actually write another version of this kind of product rule which is this kind of product rule which is p of y comma x is equal to p of y comma x is equal to p of x given y times p of y of x given y times p of y and then if we take these two equations and then if we take these two equations here we come up with something which is very", "image_path": "img_data/video_79_chunk_33.jpg"}
{"video": "video_79", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "here we come up with something which is very we come up with something which is very important and essential important and essential in everything that we will be discussing in everything that we will be discussing in the biasing kind of learning in the biasing kind of learning which is the base rule of p of y which is the base rule of p of y given x is equal to the p given x is equal to the p of x given y times p of y divided by p of x and this is a very generic one of x and this is a very generic one the very generic expression direct the very generic expression direct consequence of the product rule consequence of the product rule that we will be and we can actually", "image_path": "img_data/video_79_chunk_34.jpg"}
{"video": "video_79", "start": "0:17:30", "end": "0:17:39.733333", "timestamp": "0:17:30 - 0:17:39.733333", "text": "consequence of the product rule that we will be and we can actually that we will be and we can actually will now start giving semantics to x and y start giving semantics to x and y a bit in a different video", "image_path": "img_data/video_79_chunk_35.jpg"}
{"video": "video_81", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "now we are going to move from for those who haven't really reviewed for those who haven't really reviewed this introduction to binary this introduction to binary classification i think you should be for watching i think you should be for watching this video on object detection metrics this video on object detection metrics because it's a continuation based on because it's a continuation based on the principles kind of established there the principles kind of established there the in this video we are going to the in this video we are going to look at closely to what really is the difference closely to what really is the difference between an object detector between an object detector and the binary classifier and an object", "image_path": "img_data/video_81_chunk_0.jpg"}
{"video": "video_81", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "between an object detector and the binary classifier and an object and the binary classifier and an object detector effectively detector effectively is not is something in addition to a is not is something in addition to a binary classifier it is a banner binary classifier it is a banner classifier and a regressor because both of them are and a regressor because both of them are needed to do object detection and just to sort of object detection and just to sort of recall what we mean by object detection we what we mean by object detection we definitely have an image this the object detection an image this the object detection kind of problem we are given an image kind of problem we are given an image pretty much has some height and some", "image_path": "img_data/video_81_chunk_1.jpg"}
{"video": "video_81", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "kind of problem we are given an image pretty much has some height and some pretty much has some height and some width and our job here is to effectively and our job here is to effectively do two things one is to classify do two things one is to classify that in this region of the image there that in this region of the image there is if you a class for example person here for example person here and also specify the parameters of the and also specify the parameters of the bounding box usually the parameters of bounding box usually the parameters of the bounded box are given by either this the bounded box are given by either this pixel coordinates or this and this pixel coordinates or this and this pixel coordinates or this actually given by", "image_path": "img_data/video_81_chunk_2.jpg"}
{"video": "video_81", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "pixel coordinates or this and this pixel coordinates or this actually given by coordinates or this actually given by this pixel coordinates this pixel coordinates and with some [music] [music] height and width information height and width information for the bounding box for the bounding box b i will be calling b the bounding box b i will be calling b the bounding box capital b the boundary box for the capital b the boundary box for the from now on and of course in order for from now on and of course in order for an object detector to be trained an object detector to be trained if the black is the output of the object if the black is the output of the object detector it needs training data", "image_path": "img_data/video_81_chunk_3.jpg"}
{"video": "video_81", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "if the black is the output of the object detector it needs training data detector it needs training data and the training data are coming in the and the training data are coming in the form of bounding boxes which we will call the bounding boxes which we will call the -called -called p ground truth and the whole notion if you the performance metrics in the performance metrics in object detection is some kind of metric object detection is some kind of metric we need to derive that we need to derive that we kind of represents to us how closely kind of represents to us how closely the bounding box that the detector", "image_path": "img_data/video_81_chunk_4.jpg"}
{"video": "video_81", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "kind of represents to us how closely the bounding box that the detector the bounding box that the detector produces is to the ground truth bounding box and is to the ground truth bounding box and in general there are in general there are three i will call it events of three i will call it events of interest in the as we observe interest in the as we observe the object detector output should be the object detector output should be concerned very similar to what we have concerned very similar to what we have been saying in the binary classifier and saying in the binary classifier and this is i will call it the true this is i will call it the true portion condition where we have if you portion condition where we have if you a person that is kind of correctly", "image_path": "img_data/video_81_chunk_5.jpg"}
{"video": "video_81", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "a person that is kind of correctly a person that is kind of correctly detected and this was the ground truth bounding box this was the ground truth bounding box and the classifier correctly and the classifier correctly represented to us the right class represented to us the right class this we will call the true positive event the true positive event the i as a substitute will be explained the i as a substitute will be explained a bit later then we may have and of course later then we may have and of course this is detection it is part of a larger", "image_path": "img_data/video_81_chunk_6.jpg"}
{"video": "video_81", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "this is detection it is part of a larger is detection it is part of a larger image the second possibility the second possibility is that you may have some is that you may have some object that unfortunately the classifier miss classified as a different class and may have actually potentially multiple may have actually potentially multiple of those bounding boxes in all these", "image_path": "img_data/video_81_chunk_7.jpg"}
{"video": "video_81", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "may have actually potentially multiple of those bounding boxes in all these of those bounding boxes in all these images present from multiple classes at the same time from multiple classes at the same time but we just here outline the first just one outline the first just one to make sure that we are sort of to make sure that we are sort of understanding the concept here we have a ground the concept here we have a ground ruth and the ground truth obviously there is and the ground truth obviously there is going to be a ground truth was", "image_path": "img_data/video_81_chunk_8.jpg"}
{"video": "video_81", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "and the ground truth obviously there is going to be a ground truth was going to be a ground truth was associated with car the class car and then this is obviously is called a false positive and i will call j and then of course we and i will call j and then of course we have the third possibility where we have this the third possibility where we have this is our image there is if you a class of interest over here but unfortunately we have not really", "image_path": "img_data/video_81_chunk_9.jpg"}
{"video": "video_81", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "over here but unfortunately we have not really but unfortunately we have not really detected it and this which is what we will be and this which is what we will be calling a false negative calling a false negative k small letter k we will be calling this k small letter k we will be calling this the bounding box the predicted boundary the bounding box the predicted boundary box and as we said this one is our and as we said this one is our the ground truth the gt kind of the ground truth the gt kind of bounding box the metric that we will be using i the metric that we will be using i for now want to i'll call it represent the closeness i'll call it represent the closeness associated with prediction predicted", "image_path": "img_data/video_81_chunk_10.jpg"}
{"video": "video_81", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "i'll call it represent the closeness associated with prediction predicted associated with prediction predicted by the box as related to the ground by the box as related to the ground truth is called intersection over union is called intersection over union in the intersection of a union metric in the intersection of a union metric is effectively defined as the ratio is effectively defined as the ratio of the area as the name of the area as the name implies of bp the predicted implies of bp the predicted intersection b d t divided by the area of b predicted union b c d", "image_path": "img_data/video_81_chunk_11.jpg"}
{"video": "video_81", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "divided by the area of b predicted union b c d of b predicted union b c d if may actually represent it as if may actually represent it as a two bounding boxes this will be the seo and in fact this will be the seo and in fact i may have done in the past i may have done in the past put one in black and the other one to be the i will call it intersect the ground tools divided by", "image_path": "img_data/video_81_chunk_12.jpg"}
{"video": "video_81", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "intersect the ground tools divided by union and if i may say the intersection definitely these the intersection definitely these two bounding boxes on the numerator is two bounding boxes on the numerator is this area over here and sorry the union this area over here and sorry the union is definitely this area over there and it's not really", "image_path": "img_data/video_81_chunk_13.jpg"}
{"video": "video_81", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "this area over there and it's not really difficult to see that as this difficult to see that as this two bounded boxes can come on top of two bounded boxes can come on top of each other in the perfect case that the in the perfect case that the or completely separated that the or completely separated that the interference sorry the intersection over interference sorry the intersection over union could be as small as zero and as large could be as small as zero and as large as one and i think at this point it's worth noting that the intersection of the union makes some", "image_path": "img_data/video_81_chunk_14.jpg"}
{"video": "video_81", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "noting that the intersection of the union makes some intersection of the union makes some sense to use it as one of the components of to use it as one of the components of building up if you this building up if you this object detection metric object detection metric that we are after only when we are able that we are after only when we are able to compare bounding boxes of the same class bounding boxes of the same class of the target class let's say in this of the target class let's say in this specific case it makes sense only to consider the true it makes sense only to consider the true positives when we compare positives when we compare bounding boxes because it doesn't really bounding boxes because it doesn't really make a lot of sense to compare bounded", "image_path": "img_data/video_81_chunk_15.jpg"}
{"video": "video_81", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "bounding boxes because it doesn't really make a lot of sense to compare bounded make a lot of sense to compare bounded boxes when we have an incorrect boxes when we have an incorrect classification and over here we actually have no and over here we actually have no bounding box to compare bounding box to compare and now that we have actually seen the and now that we have actually seen the iou i think it's worth considering a iou i think it's worth considering a couple of cases of interest again in the case cases of interest again in the case where we have i definitely detected the right class i definitely detected the right class in the case where we have a person and kind of detected the person with some form of", "image_path": "img_data/video_81_chunk_16.jpg"}
{"video": "video_81", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "and kind of detected the person with some form of significant kind of overlap between the two boxes and the second case where we have again the person with in the ground again the person with in the ground truth situation here is the person situation here is the person nicely bounded by the ground truth box nicely bounded by the ground truth box but our detector is definitely doing he's a bit", "image_path": "img_data/video_81_chunk_17.jpg"}
{"video": "video_81", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "our detector is definitely doing he's a bit is definitely doing he's a bit far away from it despite the fact far away from it despite the fact that we can actually label this as a that we can actually label this as a person these are the two cases of interest here and i think by defining if you what is actually by defining if you what is actually called the iou threshold called the iou threshold we are able to actually consider we are able to actually consider those which have significant overlap those which have significant overlap which means that the iou is above a which means that the iou is above a certain threshold i will call this iou is greater than", "image_path": "img_data/video_81_chunk_18.jpg"}
{"video": "video_81", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "certain threshold i will call this iou is greater than i will call this iou is greater than a threshold and i will denote a small threshold and i will denote a small letter t as if you a true positive as if you a true positive event and in the case where i have an event and in the case where i have an iou less than the threshold then i will less than the threshold then i will consider this to be a false positive and typically we are considering as true positive", "image_path": "img_data/video_81_chunk_19.jpg"}
{"video": "video_81", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "and typically we are considering as true positive events with that have iou events with that have iou greater let's say either something greater let's say either something 0.75 or 0.5 these are the values that he typically sort of takes and now because we have sort of takes and now because we have the introduce if you this threshold the introduce if you this threshold effectively now we need to we can take the a we need to we can take the a training data set", "image_path": "img_data/video_81_chunk_20.jpg"}
{"video": "video_81", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "we need to we can take the a training data set or and kind of trying to understand what or and kind of trying to understand what is really going on with respect to an output going on with respect to an output from the detector in fact when i say from the detector in fact when i say trained in the settlement of say the trained in the settlement of say the test data set that we usually use to engage the that we usually use to engage the performance of the object detector you may recall that object detector you may recall that in the binary classification in the binary classification introductory video we have actually seen and defined here we have actually seen and defined here the precision quantity entity", "image_path": "img_data/video_81_chunk_21.jpg"}
{"video": "video_81", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "we have actually seen and defined here the precision quantity entity the precision quantity entity and in fact we definitely have seen and in fact we definitely have seen and understood the trade-offs using and understood the trade-offs using if you this probabilistic kind of diagram here probabilistic kind of diagram here with the probability distributions that with the probability distributions that had us between false positives and false had us between false positives and false negatives and this is of course is reflected in and this is of course is reflected in the sort of interplay between recall and sort of interplay between recall and precision now that we have seen all that now that we have seen all that let's now try to define some terms", "image_path": "img_data/video_81_chunk_22.jpg"}
{"video": "video_81", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "now that we have seen all that let's now try to define some terms let's now try to define some terms let's assume that i have if you let's assume that i have if you a data set that produced data set that produced n d which is the number of detections i when i feed the data observing at the end of the day at the end of the day a number of bounding boxes a number of bounding boxes this is the nd they're nd detections", "image_path": "img_data/video_81_chunk_23.jpg"}
{"video": "video_81", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "a number of bounding boxes this is the nd they're nd detections this is the nd they're nd detections and obviously the nd we can write the nd and obviously the nd we can write the nd to be as part of the discussion we have as part of the discussion we have seen just a few moments ago in the iou just a few moments ago in the iou we actually can say that here we don't we actually can say that here we don't get any detection effectively the number of detections effectively the number of detections could be either because of true positive could be either because of true positive events or false positive events or false positive events we can actually write it as nd is we can actually write it as nd is equal to ntp plus n fp", "image_path": "img_data/video_81_chunk_24.jpg"}
{"video": "video_81", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "ntp plus n fp and because the metric we're interested in is per class if you metric in is per class if you metric and also across classes but first we and also across classes but first we need to develop the upper class metric need to develop the upper class metric we are going to effectively need to we are going to effectively need to take a class and actually understand take a class and actually understand what are the number of ground truths what are the number of ground truths that we have for a specific class as you that we have for a specific class as you can see here for the class person there is in fact for the class person there is in fact the number of ground tours that we have", "image_path": "img_data/video_81_chunk_25.jpg"}
{"video": "video_81", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "for the class person there is in fact the number of ground tours that we have the number of ground tours that we have are the true positives are the true positives plus the force negative let's plus the force negative let's write down this equation as well write down this equation as well and we write it as n g t is equal to ntp and we write it as n g t is equal to ntp plus lfn and now we're able to lfn and now we're able to go ahead and sort of write down the go ahead and sort of write down the precision and the recall the precision and the recall the precision is and as a reminder the precision was", "image_path": "img_data/video_81_chunk_26.jpg"}
{"video": "video_81", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "is and as a reminder the precision was definitely something that's associated with the something that's associated with the throughput is divided by throughput is divided by throughput class force positive but over here we class force positive but over here we have to effectively do the have to effectively do the write down this equation with the write down this equation with the specific counts that we have specific counts that we have because at the end of the day we are because at the end of the day we are trying to find a kind of a computational trying to find a kind of a computational technique in order to calculate this technique in order to calculate this kind of metric let's write kind of metric let's write it is basically a summation of all it is basically a summation of all the true positive events and the true positive events and the propositions i use the index i", "image_path": "img_data/video_81_chunk_27.jpg"}
{"video": "video_81", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "the true positive events and the propositions i use the index i propositions i use the index i it is tp i from i is equal to 1 to ntp is equal to 1 to ntp divided by the again the same thing divided by the again the same thing from y is equal to 1 to from y is equal to 1 to t ntp i plus the false positives well the four the false positives well the four suppose this are the nd minus and dp summation for the four minus and dp summation for the four force that we use the index j", "image_path": "img_data/video_81_chunk_28.jpg"}
{"video": "video_81", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "minus and dp summation for the four force that we use the index j force that we use the index j force positive j from j is equal to 1 force positive j from j is equal to 1 to n d minus n tp that is the precision if you equation nothing precision if you equation nothing really changes here just basically we put some what we have just basically we put some what we have observed at the output of this test observed at the output of this test and here we have the recall and the recall definitely is associated with the probability", "image_path": "img_data/video_81_chunk_29.jpg"}
{"video": "video_81", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "and the recall definitely is associated with the probability with the probability sorry the true positives and sorry the true positives and the false negatives now we actually have again now we actually have again the same numerator summation the same numerator summation from i is equal to 1 to ntp from i is equal to 1 to ntp tpi divided by from i is equal to 1 to ntp from i is equal to 1 to ntp dpi plus now for that we use the dpi plus now for that we use the k index is equal to 1", "image_path": "img_data/video_81_chunk_30.jpg"}
{"video": "video_81", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "dpi plus now for that we use the k index is equal to 1 k index is equal to 1 to n f negative where the n of negative is n g t minus ntp of all of the negative of all of the negative events and now we are done we have events and now we are done we have written down all these two relationships written down all these two relationships that we need to process them and understand how we can process them and understand how we can go move into the direction of calculating this metric", "image_path": "img_data/video_81_chunk_31.jpg"}
{"video": "video_81", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "go move into the direction of calculating this metric as a side note we talked a little bit about this threshold t which is called the iou threshold t which is called the iou threshold and now we can actually see that how and now we can actually see that how this threshold t can actually enter this threshold t can actually enter these formulas of precision recall this these formulas of precision recall this iiut is entering those by determining the is entering those by determining the populations of effectively populations of effectively ntp and enforced positives ntp and enforced positives now i think it's worthwhile", "image_path": "img_data/video_81_chunk_32.jpg"}
{"video": "video_81", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "ntp and enforced positives now i think it's worthwhile now i think it's worthwhile reviewing what we have said about this reviewing what we have said about this threshold iou threshold t and we can threshold iou threshold t and we can consider the iu threshold t as a kind of a signal the iu threshold t as a kind of a signal flow kind of diagram how it is applied flow kind of diagram how it is applied we have at the input and the number of we have at the input and the number of detections and we can imagine this being and we can imagine this being effectively splitted by a filter whose effectively splitted by a filter whose parameter is smaller t could be parameter is smaller t could be seventy-five percent fifty percent and seventy-five percent fifty percent and on only i'll use only detections only i'll use only detections without", "image_path": "img_data/video_81_chunk_33.jpg"}
{"video": "video_81", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "only i'll use only detections without use above 75 let's say use above 75 let's say are going to result in two are going to result in two true positives and the rest are going to true positives and the rest are going to result into false positives result into false positives and the false negatives and the false negatives is unaffected by t because in the false negative t because in the false negative situation as we have seen there is absolutely no have seen there is absolutely no detection and therefore detection and therefore there's no point of comparing there's no point of comparing something we don't have against", "image_path": "img_data/video_81_chunk_34.jpg"}
{"video": "video_81", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "there's no point of comparing something we don't have against something we don't have against if you an iou threshold i can if you an iou threshold i can actually go and definitely insert this as a and definitely insert this as a parameter in all the two parameter in all the two positives and false positives and for kind of make my equation this for precision and recall and for precision and recall and now i need to consider as i said a now i need to consider as i said a kind of a second stage kind of a second stage another filter and this filter what it another filter and this filter what it will actually do", "image_path": "img_data/video_81_chunk_35.jpg"}
{"video": "video_81", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "another filter and this filter what it will actually do is it will take the number of true is it will take the number of true posters that survived if you posters that survived if you this iou threshold comparison this iou threshold comparison and it will actually make another and it will actually make another comparison and the second comparison and the second comparison actually make is for those true positives they make is for those true positives they will actually ask the questions are will actually ask the questions are is the confidence inter in the is the confidence inter in the confidence interval greater than as another fresco greater than as another fresco called tau i will call this the called tau i will call this the confidence", "image_path": "img_data/video_81_chunk_36.jpg"}
{"video": "video_81", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "called tau i will call this the confidence threshold the first confidence effectively allows the true positives to be allows the true positives to be filtered further and only a fraction filtered further and only a fraction of those would survive of those would survive and as you can see as the larger the and as you can see as the larger the highest this confidence become for example if we confidence become for example if we have 80 confidence that this class is a 80 confidence that this class is a person versus something", "image_path": "img_data/video_81_chunk_37.jpg"}
{"video": "video_81", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "person versus something smaller as a threshold you can smaller as a threshold you can understand that a much smaller fraction understand that a much smaller fraction of detections will survive in this kind of detections will survive in this kind of stage the definitely the number of true positives that survive number of true positives that survive over here decreasing function over here decreasing function of the threshold out and of the threshold out and we can actually put here greater and we can actually put here greater than tau and in order for us to any anything which is", "image_path": "img_data/video_81_chunk_38.jpg"}
{"video": "video_81", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "tau and in order for us to any anything which is in order for us to any anything which is effectively less than tau will actually effectively less than tau will actually be classified as negatives and although we would expect the although we would expect the positives other two posters or false positives other two posters or false positives to actually have this positives to actually have this reduced to reduce with respect to reduced to reduce with respect to a higher kind of confidence threshold a higher kind of confidence threshold we are expecting the negatives in this we are expecting the negatives in this case that a false negative to actually", "image_path": "img_data/video_81_chunk_39.jpg"}
{"video": "video_81", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "we are expecting the negatives in this case that a false negative to actually case that a false negative to actually have an exactly opposite kind of behavior and exactly opposite kind of behavior and actually increase with an increasing threshold with an increasing threshold we can actually go and add the tau we can actually go and add the tau parameter here effectively this entity of t comma tau effectively this entity of t comma tau is not a function but it is a number is not a function but it is a number of event revived into this kind of event revived into this kind of branch over here and correspondingly for the false and correspondingly for the false positives and the false negatives as we positives and the false negatives as we said false negatives that doesn't have", "image_path": "img_data/video_81_chunk_40.jpg"}
{"video": "video_81", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "positives and the false negatives as we said false negatives that doesn't have said false negatives that doesn't have any dependence of t and now we can write our and now we can write our final precision and recall equations final precision and recall equations that now actually include everything that we have actually include everything that we have done up to now the final if you expressions are the final if you expressions are exactly the same as the ones but exactly the same as the ones but instead of having just the numbers that we have gotten at the input", "image_path": "img_data/video_81_chunk_41.jpg"}
{"video": "video_81", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "just the numbers that we have gotten at the input numbers that we have gotten at the input of this kind of filtering process of this kind of filtering process in fact we have numbers that we in fact we have numbers that we are going to get at the very end of this filtering process the very end of this filtering process either because of the iou either because of the iou threshold or because of the confidence threshold or because of the confidence threshold the final expressions are very the final expressions are very quickly precision is equal to the summation from i is equal to 1 to n", "image_path": "img_data/video_81_chunk_42.jpg"}
{"video": "video_81", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "is equal to the summation from i is equal to 1 to n i is equal to 1 to n tp of t comma tau tpi of the comma tau and summation from is equal to one summation from is equal to one tp of the corner of the comma doubt tp of the corner of the comma doubt tpi of the comma tau plus", "image_path": "img_data/video_81_chunk_43.jpg"}
{"video": "video_81", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "tp of the corner of the comma doubt tpi of the comma tau plus tpi of the comma tau plus summation of from j is equal to one summation of from j is equal to one and then we have here nd minus i would call it's actually here i would call it's actually here it's not ntp but it's actually a small letter of entity it is n and t p of t comma tau", "image_path": "img_data/video_81_chunk_44.jpg"}
{"video": "video_81", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "it is n and t p of t comma tau f p of j t comma this is the expression for the this is the expression for the precision and then of course we have the of course we have the corresponding expression", "image_path": "img_data/video_81_chunk_45.jpg"}
{"video": "video_81", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "expression and the corresponding for the encode is i'm not sure if it is obviously it's very evident now but it should be it's going to be a summation from y is it's going to be a summation from y is equal to 1 20p of t comma tau t p i of t comma tau t p i of t comma tau this term again plus", "image_path": "img_data/video_81_chunk_46.jpg"}
{"video": "video_81", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "t p i of t comma tau this term again plus this term again plus the corresponding now false negatives but false negative of k of false negative of k of just t just the confidence threshold or the reasons we explain this threshold or the reasons we explain this is the corresponding is the corresponding final equation about recall", "image_path": "img_data/video_81_chunk_47.jpg"}
{"video": "video_81", "start": "0:24:00", "end": "0:24:04.800000", "timestamp": "0:24:00 - 0:24:04.800000", "text": "is the corresponding final equation about recall", "image_path": "img_data/video_81_chunk_48.jpg"}
{"video": "video_82", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "as far as object detection metrics is concerned i think it's destructive to just see i think it's destructive to just see the binary classification the binary classification as and represent it as if you a diagram this as if you a diagram this where we have if you the complete we have if you the complete universe of events that actually can happen of events that actually can happen and detections that then actually can and detections that then actually can happen with this actually line happen with this actually line kind of represents the separation", "image_path": "img_data/video_82_chunk_0.jpg"}
{"video": "video_82", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "happen with this actually line kind of represents the separation kind of represents the separation between what we call between what we call condition negative from condition positive and this line could be anywhere over here and just throw it over here and just throw it as a vertical line and on one side as a vertical line and on one side obviously we have events that are the negative events truly the negative events truly negative events this is basically the", "image_path": "img_data/video_82_chunk_1.jpg"}
{"video": "video_82", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "the negative events truly negative events this is basically the negative events this is basically the two positives or effectively the positive events are the ones that satisfy if you the condition that satisfy if you the condition that is to be tested and the negative events they don't and the negative events they don't we can also plot a boundary here we can also plot a boundary here which is represented by which is represented by this circular kind of line this circular kind of line that effectively separates", "image_path": "img_data/video_82_chunk_2.jpg"}
{"video": "video_82", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "this circular kind of line that effectively separates that effectively separates the correct classifications that the correct classifications that a classifier did from the incorrect one a classifier did from the incorrect one obviously here if we have a ground obviously here if we have a ground truth of positive and we can say that within this positive and we can say that within this boundary this is the two positive events one two three the two positive events one two three four events over here four events over here and on the other side there will be the and on the other side there will be the true negative events true negative events correspondingly and the second", "image_path": "img_data/video_82_chunk_3.jpg"}
{"video": "video_82", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "true negative events correspondingly and the second correspondingly and the second and the other two regions and the other two regions on this side we have a ground of on this side we have a ground of positive but unfortunately the classifier but unfortunately the classifier did not classify this as a positive did not classify this as a positive effectively it classified it as a effectively it classified it as a negative and therefore made a mistake negative and therefore made a mistake which we called false negative and the other also the on the other side also we have a on the other side also we have a negative", "image_path": "img_data/video_82_chunk_4.jpg"}
{"video": "video_82", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "on the other side also we have a negative event as a ground truth but effectively event as a ground truth but effectively the classifier told us that it was a positive and told us that it was a positive and therefore we have in this region outside of this in this region outside of this classifier boundary all the false classifier boundary all the false positive events now let's look at an interesting use case to understand what we call the case to understand what we call the probabilistic notion of binary classification", "image_path": "img_data/video_82_chunk_5.jpg"}
{"video": "video_82", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "notion of binary classification binary classification let's look at a use case where we let's look at a use case where we have i'll just draw it over here in the draw it over here in the on the side we have a device it's on the side we have a device it's actually called radar actually called radar and the job of the device is to transmit and the job of the device is to transmit a waveform that it will a waveform that it will aiming to detect incoming planes aiming to detect incoming planes and the", "image_path": "img_data/video_82_chunk_6.jpg"}
{"video": "video_82", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "aiming to detect incoming planes and the ability of the radiator definitely ability of the radiator definitely coming on planes is actually this due to the actually this due to the fact that the specific waveforms have some really specific waveforms have some really nice autocorrelation properties it is nice autocorrelation properties it is impinging into incoming a plane and when impinging into incoming a plane and when there is in plane in fact there then the signal in plane in fact there then the signal is going to be reflected is going to be reflected and come back to the receiver and we and come back to the receiver and we will call this reflected signal will call this reflected signal x and if there is absolutely", "image_path": "img_data/video_82_chunk_7.jpg"}
{"video": "video_82", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "will call this reflected signal x and if there is absolutely x and if there is absolutely no plane then obviously there should no plane then obviously there should be some other return be some other return maybe the signal is transmitted maybe the signal is transmitted some far away mountain some far away mountain or some other kind of or some other kind of obstacle and then there may be a obstacle and then there may be a sort of flying flocks of birds sort of flying flocks of birds or whatever and then there will be another much and then there will be another much weaker if you return that comes weaker if you return that comes back into the receiver and the job really into the receiver and the job really of the receiver the later receiver is to of the receiver the later receiver is to sound the alarm when the plane is coming", "image_path": "img_data/video_82_chunk_8.jpg"}
{"video": "video_82", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "of the receiver the later receiver is to sound the alarm when the plane is coming sound the alarm when the plane is coming in or not if it doesn't in or not if it doesn't in with a very high sort of with a very high sort of performance the metrics of which we will see in a the metrics of which we will see in a moment what we call what manifests kind of a good what manifests kind of a good performance if we are to draw probabilistically to view this problem in a kind of a view this problem in a kind of a bit more probabilistic setting bit more probabilistic setting we could actually draw a diagram", "image_path": "img_data/video_82_chunk_9.jpg"}
{"video": "video_82", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "bit more probabilistic setting we could actually draw a diagram we could actually draw a diagram of what we call the signal strength i mean we can the signal strength i mean we can actually sit their way actually sit their way sit there as a kind of a radar operator sit there as a kind of a radar operator monitoring if you a computer screen monitoring if you a computer screen and actually draw what we actually and actually draw what we actually see as returns if i have on this see as returns if i have on this axis i have this what is called the axis i have this what is called the radar received signal", "image_path": "img_data/video_82_chunk_10.jpg"}
{"video": "video_82", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "received signal strength let's call it i will be observing probably something that it will something that it will cost consist of two sort of possibilities sometimes i'm on a sort of possibilities sometimes i'm on a day with no claims are actually coming day with no claims are actually coming i will be probably observing some kind i will be probably observing some kind of probability distribution this", "image_path": "img_data/video_82_chunk_11.jpg"}
{"video": "video_82", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "i will be probably observing some kind of probability distribution this and on a day where indeed in instances where we actually see in instances where we actually see something coming i will plot something this to represent that kind of signal strength then there is some kind of a strength then there is some kind of a rational behind it because obviously when planes behind it because obviously when planes are not really coming are not really coming the sigma strength will actually be on the sigma strength will actually be on the left of when they do the left of when they do because the average signal because the average signal strength i'm expecting to be much lower strength i'm expecting to be much lower when they are not", "image_path": "img_data/video_82_chunk_12.jpg"}
{"video": "video_82", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "strength i'm expecting to be much lower when they are not than when they are there must be some than when they are there must be some i mean there is some kind of thinking about it and as is some kind of thinking about it and as a reminder of in binary classification we have in binary classification we have effectively have y as nothing as a label y have y as nothing as a label y which is obviously a binary random which is obviously a binary random variable which takes the value of one variable which takes the value of one if i will call it plane if i will call it plane is incoming", "image_path": "img_data/video_82_chunk_13.jpg"}
{"video": "video_82", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "if i will call it plane is incoming which is a the -called positive which is a the -called positive condition here and zero if plane is not coming that's basically our label and we have now x and y our label and we have now x and y and in fact we can create if you even a data set where create if you even a data set where values x and y pairs and is actually observed over", "image_path": "img_data/video_82_chunk_14.jpg"}
{"video": "video_82", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "values x and y pairs and is actually observed over y pairs and is actually observed over time and then we our job is really to we our job is really to sort of design something that we will sort of design something that we will call the threshold and that we will deal in a moment but and that we will deal in a moment but before we do let's try to put some let's see what let's try to put some let's see what this exact at the y axis will be called i would to call axis will be called i would to call this y axis p of x comma y because effectively we have", "image_path": "img_data/video_82_chunk_15.jpg"}
{"video": "video_82", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "p of x comma y because effectively we have because effectively we have a multi-modal distribution as a multi-modal distribution as signals a signal strength signals a signal strength and this multi-modal distribution has if and this multi-modal distribution has if you two components when a plane is not coming when a plane is not coming we can actually call this component we can actually call this component p of y is equal to zero comma x and when p of y is equal to zero comma x and when the plane is coming we can call it the plane is coming we can call it this we have y is equal to one this we have y is equal to one comma x that kind of a joint distribution of", "image_path": "img_data/video_82_chunk_16.jpg"}
{"video": "video_82", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "comma x that kind of a joint distribution of that kind of a joint distribution of x and y and obviously we will not be able to and obviously we will not be able to distinguish them as distinct probability distributions as distinct probability distributions but only by observing two different but only by observing two different conditions separately only then we can actually separately only then we can actually plot them in reality what's going to happen we in reality what's going to happen we will be observing this kind of will be observing this kind of joint durability distribution and that is basically what we will see", "image_path": "img_data/video_82_chunk_17.jpg"}
{"video": "video_82", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "and that is basically what we will see on aggregate and then our job really aggregate and then our job really there and as you can see before we go there and as you can see before we go into the threshold kind of discussion into the threshold kind of discussion as you can see these two as you can see these two distributions are not separable distributions are not separable in a sense that they are not really in a sense that they are not really even they are spilling over even they are spilling over at the tail of this distribution at the tail of this distribution spills over into the other and vice versa you would the other and vice versa you would expect", "image_path": "img_data/video_82_chunk_18.jpg"}
{"video": "video_82", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "the other and vice versa you would expect to see in this kind of region in to see in this kind of region in terms of if you are to sort of monitor what if you are to sort of monitor what is happening and create if you a data set you and create if you a data set you would expect to see something that you will to see something that you will expect to see a lot of expect to see a lot of probability distributions probability distributions sorry a lot of events that are kind of sorry a lot of events that are kind of negative and as soon as we have start seeing and as soon as we have start seeing some perhaps not this some perhaps not this as soon as we start going towards this", "image_path": "img_data/video_82_chunk_19.jpg"}
{"video": "video_82", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "some perhaps not this as soon as we start going towards this as soon as we start going towards this on the right you would expect to see on the right you would expect to see some positive some more positive events some more positive events but also some negative events as well but also some negative events as well but definitely more positive towards but definitely more positive towards that there is kind of this kind of that there is kind of this kind of region where we expect to see some a mixture where we expect to see some a mixture if you of both positive and if you of both positive and negative events because of this kind of overlapped", "image_path": "img_data/video_82_chunk_20.jpg"}
{"video": "video_82", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "negative events because of this kind of overlapped because of this kind of overlapped sort of overlapping a situation sort of overlapping a situation what is really the job of the what is really the job of the designer here the classification designer is ready to the classification designer is ready to come up with a scalar quantity in mind you are a scalar quantity in mind you are dealing with a scalar case here x is dealing with a scalar case here x is a scalar it just indicates the received signal it just indicates the received signal strength which is in watts or some other in watts or some other measurement that's only matter right now measurement that's only matter right now but the job really of the designer", "image_path": "img_data/video_82_chunk_21.jpg"}
{"video": "video_82", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "measurement that's only matter right now but the job really of the designer but the job really of the designer is really to come up with some is really to come up with some threshold in that threshold i will call this threshold i will call this threshold x-hat and i'll just write it also xdh that's a threshold of signal strength that's a threshold of signal strength and above this kind of threshold and above this kind of threshold below this threshold i will be below this threshold i will be declaring that no plane is coming", "image_path": "img_data/video_82_chunk_22.jpg"}
{"video": "video_82", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "below this threshold i will be declaring that no plane is coming and above this threshold i will be declaring exactly the opposite declaring exactly the opposite the y hat is equal to one the y hat is equal to one and that is going to be my classifier and that is going to be my classifier the job really is to the job really is to ensure binary classifications how to ensure binary classifications how to design properly this kind of threshold design properly this kind of threshold x-hat and i think it's very x-hat and i think it's very useful to actually observe useful to actually observe right now that the moment we", "image_path": "img_data/video_82_chunk_23.jpg"}
{"video": "video_82", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "useful to actually observe right now that the moment we right now that the moment we have it here a threshold have it here a threshold my x-axis over here has been my x-axis over here has been sort of divided into regions r0 sort of divided into regions r0 and r1 the momentum i actually and r1 the momentum i actually introduced in a threshold i'm splitting introduced in a threshold i'm splitting this x axis into these two regions where this x axis into these two regions where the r zero region is on the left of the threshold and the is on the left of the threshold and the r1 regions is our region is on the right of the our region is on the right of the threshold now that the threshold kind of", "image_path": "img_data/video_82_chunk_24.jpg"}
{"video": "video_82", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "threshold now that the threshold kind of now that the threshold kind of divided this line into two regions i can divided this line into two regions i can actually start writing down the i can actually start writing down the probability of making a mistake probability of making a mistake the probability of making a mistake has the probability of making a mistake has obviously a probability obviously a probability of y hat is different than of y hat is different than y and the probability y and the probability this probability has effectively two this probability has effectively two components one component components one component is for events that are is for events that are on the left side of the threshold and on the left side of the threshold and event which is in the region are zero", "image_path": "img_data/video_82_chunk_25.jpg"}
{"video": "video_82", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "on the left side of the threshold and event which is in the region are zero event which is in the region are zero and events which are on the right side and events which are on the right side of the threshold chain region are one of the threshold chain region are one effectively the probability of making effectively the probability of making the mistake on the left side is the probability on the left side is the probability of y is equal to 1 given that y-hat is equal to zero and is equal to zero and also the probability of on the other and also the probability of on the other side of y is equal to zero", "image_path": "img_data/video_82_chunk_26.jpg"}
{"video": "video_82", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "side of y is equal to zero of y is equal to zero given the y-hat is equal to one given the y-hat is equal to one these are the two components of the these are the two components of the mistake that i can actually of the mistake that i can actually make which i can actually write this which i can actually write this component as p of y is equal to one given that x is equal to one given that x belongs to region are 0 plus the belongs to region are 0 plus the probability of y is equal to 0 when x is", "image_path": "img_data/video_82_chunk_27.jpg"}
{"video": "video_82", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "of y is equal to 0 when x is in the region r1 that's that is the two components that i have and in fact if i components that i have and in fact if i look closely i can start drawing these two components i can start drawing these two components where the probability where the probability of y is equal to 0 given that x is equal of y is equal to 0 given that x is equal to r1 is definitely this tail over here and as you can see this is because", "image_path": "img_data/video_82_chunk_28.jpg"}
{"video": "video_82", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "and as you can see this is because this region this x is limited by region this x is limited by the tail of this distribution and correspondingly this guy over here is associated with this large scale over is associated with this large scale over here where x is limited by the region are zero but", "image_path": "img_data/video_82_chunk_29.jpg"}
{"video": "video_82", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "where x is limited by the region are zero but the by the region are zero but still under the probability of x is equal under the probability of x is equal to one condition this tail i will divide it into this tail i will divide it into two sub regions some tails i will call them not some tails i will call them not religious tales and this is one of them and the other and this is one of them and the other one is there this guy over here the there this guy over here the summation of these two summation of these two and why i divided them up it will and why i divided them up it will become kind of apparent in a moment become kind of apparent in a moment and now definitely we can actually write and now definitely we can actually write this sum", "image_path": "img_data/video_82_chunk_30.jpg"}
{"video": "video_82", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "and now definitely we can actually write this sum as each of these kind of terms as the as each of these kind of terms as the integral over the region r0 integral over the region r0 of the of this tail over here of p of y is equal to 1 comma x dx of y is equal to 1 comma x dx and this guy over here we can actually and this guy over here we can actually write it as the integral write it as the integral over the region r1 of p y is equal to over the region r1 of p y is equal to zero", "image_path": "img_data/video_82_chunk_31.jpg"}
{"video": "video_82", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "over the region r1 of p y is equal to zero comma x dx and as we can actually see now this tail over here this area under this tail over here this area under this tail is the total component the top of the is the total component the top of the probability of being a mistake due to probability of being a mistake due to the effectively the false negatives in the sense that in terms of ground truth there are going to be some positive", "image_path": "img_data/video_82_chunk_32.jpg"}
{"video": "video_82", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "truth there are going to be some positive going to be some positive events of planes coming in this kind events of planes coming in this kind of area over here especially in this of area over here especially in this neighborhood of this kind of thresholds on the left side this kind of thresholds on the left side but we are firstly categorize them as negative these are the false negative these are the false negatives and this region over here with this region over here with this horizontal kind of lines it's actually horizontal kind of lines it's actually very easy to see that yes also in this area of the that yes also in this area of the threshold on the right hand side there threshold on the right hand side there will be", "image_path": "img_data/video_82_chunk_33.jpg"}
{"video": "video_82", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "threshold on the right hand side there will be negative events that are possible negative events that are possible but we are always categorize them as but we are always categorize them as positive this will be the false positive this will be the false positive and using these two false positions and using these two false positions and false negatives now we can actually false negatives now we can actually start making now this start making now this association between the previous diagram association between the previous diagram and this diagram and we can make some interesting diagram and we can make some interesting conclusions but just by visual conclusions but just by visual inspection of this diagram take for example the case where diagram take for example the case where we move the threshold line to the left", "image_path": "img_data/video_82_chunk_34.jpg"}
{"video": "video_82", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "we move the threshold line to the left move the threshold line to the left then out of the total false negative then out of the total false negative component which can be impacted component which can be impacted definitely this component under this curve component under this curve this area over here is definitely going this area over here is definitely going to be reduced in terms of false i be reduced in terms of false i mean is going to move from the first negative is going to move from the first negative region into the false positive region", "image_path": "img_data/video_82_chunk_35.jpg"}
{"video": "video_82", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "is going to move from the first negative region into the false positive region into the false positive region but this part of the false negative but this part of the false negative is actually going to be is actually going to be is going to be zero the is going to be zero the because the threshold now is going to be because the threshold now is going to be right here this region here from this region here from this region became this little region and region became this little region and it's not really difficult to imagine it's not really difficult to imagine that there is specifically that there is specifically a threshold that goes through this point a threshold that goes through this point over here", "image_path": "img_data/video_82_chunk_36.jpg"}
{"video": "video_82", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "a threshold that goes through this point over here and this i will call it the x and this i will call it the x hat star which seems to be offering if you the seems to be offering if you the best situation in this specific example best situation in this specific example because effectively this whole region because effectively this whole region has been eliminated of false negatives has been eliminated of false negatives and as we said before and as we said before the total region here the total region here in these both of these tails in these both of these tails effectively by moving that threshold got", "image_path": "img_data/video_82_chunk_37.jpg"}
{"video": "video_82", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "in these both of these tails effectively by moving that threshold got effectively by moving that threshold got unaffected therefore definitely we have therefore definitely we have achieved a threshold with this threshold a threshold with this threshold a situation where the false negatives situation where the false negatives are kind of minimized there is are kind of minimized there is if you the total probability of if you the total probability of mistake is minimized because effectively we is minimized because effectively we minimized the total area under both of these the total area under both of these distributions that are associated with", "image_path": "img_data/video_82_chunk_38.jpg"}
{"video": "video_82", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "the total area under both of these distributions that are associated with distributions that are associated with the total probability of mistake and probability of mistake and by effectively going back to this by effectively going back to this diagram over here we can now start over here we can now start understanding how this classifier boundary can actually this classifier boundary can actually change it will actually change by varying the it will actually change by varying the threshold by varying the threshold we can by varying the threshold we can actually move this boundary actually move this boundary in this kind of space in this kind of space and i think it is", "image_path": "img_data/video_82_chunk_39.jpg"}
{"video": "video_82", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "in this kind of space and i think it is definitely the threshold as it was shown definitely the threshold as it was shown before was actually able to vary before was actually able to vary the trade-off between false positive and the trade-off between false positive and false negatives and in banner classification generally and in banner classification generally in classifiers we have actually defined in classifiers we have actually defined two ratios that capture if you this two ratios that capture if you this trade-off the first ratio is called recall the first ratio is called recall and i'm going to mention it over here and i'm going to mention it over here recall also known as sensitivity", "image_path": "img_data/video_82_chunk_40.jpg"}
{"video": "video_82", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "recall also known as sensitivity and it's the ratio between true positives divided by true positive divided by true positive plus false negative and that's one plus false negative and that's one area and obviously a series call and area and obviously a series call and sensitivity have to do sensitivity have to do with the condition positive with the condition positive we call it the performance and we call it the performance and as you can if you remember as you can if you remember the false negatives that are associated the false negatives that are associated with the", "image_path": "img_data/video_82_chunk_41.jpg"}
{"video": "video_82", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "the false negatives that are associated with the recoil and sensitivity formula are recoil and sensitivity formula are associated with this tail over here the other on the other side there is if you on the other side there is if you the what is called precision which is given by the true positive divided by true positive plus false positive and as you can see it has a lot to do", "image_path": "img_data/video_82_chunk_42.jpg"}
{"video": "video_82", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "and as you can see it has a lot to do with this erroneous of these false posts that this erroneous of these false posts that are actually present are actually present or equivalently with this or equivalently with this area over here under this area over here under this tail of the distribution and there is another ratio of interest that we will call interest that we will call a false positive rate and this false a false positive rate and this false positive has the equivalent on the left side of has the equivalent on the left side of what they called was on the right side", "image_path": "img_data/video_82_chunk_43.jpg"}
{"video": "video_82", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "has the equivalent on the left side of what they called was on the right side what they called was on the right side which is effectively the true effectively the true negatives divided by negatives divided by sorry the false positives false sorry the false positives false positivity is false positive divided by the false is false positive divided by the false positive plus to negative and this is the first positive rate and this is the first positive rate and we can also these two associate these we can also these two associate these two", "image_path": "img_data/video_82_chunk_44.jpg"}
{"video": "video_82", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "we can also these two associate these two with a curve that we will call the with a curve that we will call the receiver operating curve receiver operating curve and the receiver operating curve will and the receiver operating curve will actually be the curve that relates the be the curve that relates the false positive rate or false alarm false positive rate or false alarm probability with the recall probability with the recall and also we can make some kind of and also we can make some kind of intuitive kind of conclusions with respect to", "image_path": "img_data/video_82_chunk_45.jpg"}
{"video": "video_82", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "intuitive kind of conclusions with respect to kind of conclusions with respect to the shape of this curve the receiver operating curve the receiver operating curve and the receiver operating curve and the receiver operating curve definitely seems to be definitely seems to be leading to better classifiers the taller it goes faster taller it goes faster as really the ideal point is that we can as really the ideal point is that we can have a recall of 1.0", "image_path": "img_data/video_82_chunk_46.jpg"}
{"video": "video_82", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "have a recall of 1.0 this is the 1.0 point over here this is the 1.0 point over here with zero false alarm probability or with zero false alarm probability or false positive rate positive false positive rate that is definitely the ideal point that is definitely the ideal point that we can actually have and that we can actually have and this is a bit higher than what it should this is a bit higher than what it should be but i hope you get the point but i hope you get the point this is the ideal", "image_path": "img_data/video_82_chunk_47.jpg"}
{"video": "video_82", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "but i hope you get the point this is the ideal situation that it's impossible to design a classifier impossible to design a classifier this but the higher you go this but the higher you go and approach this kind of point the and approach this kind of point the better the classifier becomes better the classifier becomes and a very bad classifier would and a very bad classifier would actually be this kind of a straight line actually be this kind of a straight line where effectively you have the where effectively you have the classifier where the false positive rate classifier where the false positive rate and the true positive rate are the same and the true positive rate are the same that's obviously a very bad classifier even obviously a very bad classifier even worse classifier is that", "image_path": "img_data/video_82_chunk_48.jpg"}
{"video": "video_82", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "obviously a very bad classifier even worse classifier is that worse classifier is that when you have curves this which is when you have curves this which is kind of obviously a real a really bad situation obviously a real a really bad situation therefore for you to design therefore for you to design something this and as you can see the receiver operating as you can see the receiver operating curve is definitely going to it actually can is definitely going to it actually can be plotted by varying the threshold based on this kind varying the threshold based on this kind of previous discussion of previous discussion every single point that we have in the", "image_path": "img_data/video_82_chunk_49.jpg"}
{"video": "video_82", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "of previous discussion every single point that we have in the every single point that we have in the curve is correspond corresponds effectively to is correspond corresponds effectively to a unique threshold that the classifier is operating with that the classifier is operating with and that's called also sometimes the operating point for each threshold you have you are getting based on this earlier discussion a false based on this earlier discussion a false positive rate and a false negative rate and then", "image_path": "img_data/video_82_chunk_50.jpg"}
{"video": "video_82", "start": "0:25:30", "end": "0:25:40.633333", "timestamp": "0:25:30 - 0:25:40.633333", "text": "positive rate and a false negative rate and then rate and a false negative rate and then you can vary as you value the threshold you can vary as you value the threshold you can actually get this receiver operating actually get this receiver operating curve to plot", "image_path": "img_data/video_82_chunk_51.jpg"}
{"video": "video_83", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "we'd to introduce the concept of convolution now which is a foundational convolution now which is a foundational signal processing operation as it has signal processing operation as it has been for many decades now been for many decades now been used to study and enable the study of used to study and enable the study of linear time invariant systems by time invariant systems by giving if you further definition of giving if you further definition of what we call an impulse response", "image_path": "img_data/video_83_chunk_0.jpg"}
{"video": "video_83", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "what we call an impulse response the let's assume that we have a the let's assume that we have a linear time invariant system linear time invariant system this linear timing value system is going this linear timing value system is going to be triggered by an impulse and that impulse is going by an impulse and that impulse is going to be represented in with just a one over here and all other values being over here and all other values being zero and really the axis here and really the axis here is effectively we can actually call", "image_path": "img_data/video_83_chunk_1.jpg"}
{"video": "video_83", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "and really the axis here is effectively we can actually call is effectively we can actually call it with delta of n and this is the n with delta of n and this is the n axis this is basically a digital time or axis this is basically a digital time or discrete time samples only in one time samples only in one sample right here at zero there is if sample right here at zero there is if you an input signal and this simple signal an input signal and this simple signal goes through the linear time invariant goes through the linear time invariant system and at the output we observe to actually and at the output we observe to actually get what we actually are going to be calling", "image_path": "img_data/video_83_chunk_2.jpg"}
{"video": "video_83", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "get what we actually are going to be calling what we actually are going to be calling some form of impulse response some form of impulse response some form of response that we will be some form of response that we will be calling impulse response because calling impulse response because we are treating it with the specific we are treating it with the specific impulse here maybe that system is going to result into some is going to result into some response it could be anything and response it could be anything and i think the impulse response over here i think the impulse response over here is going to be very important for us because very important for us because for any study of any system because it", "image_path": "img_data/video_83_chunk_3.jpg"}
{"video": "video_83", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "very important for us because for any study of any system because it for any study of any system because it will be able to with it to will be able to with it to understand a lot of the parameters understand a lot of the parameters and properties if you of this time and properties if you of this time linear time invariant system we will use the impulse response now to actually introduce the to actually introduce the operation of convolution in this operation of convolution in this one-dimensional time domain if you fashion first time domain if you fashion first and then we will introduce and then we will introduce with in the convolutional neural", "image_path": "img_data/video_83_chunk_4.jpg"}
{"video": "video_83", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "and then we will introduce with in the convolutional neural with in the convolutional neural networks we will introduce a networks we will introduce a two-dimensional kind of equivalent we two-dimensional kind of equivalent we are actually going now to have we are actually going now to have a specific input signal and that input signal and that input signal we will call it exophane and we are going also to have a system that has an impulse response system", "image_path": "img_data/video_83_chunk_5.jpg"}
{"video": "video_83", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "that has an impulse response system that has an impulse response h of n and let's kind of draw them over here and let's kind of draw them over here that this is basically the that this is basically the exophant it will be at over here it will be one over here it will be maybe i can make it a bit smaller because we have some a bit smaller because we have some one of the larger numbers to make a one of the larger numbers to make a point this is one", "image_path": "img_data/video_83_chunk_6.jpg"}
{"video": "video_83", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "this is one over here we have a two and finally we have three and this is basically exophant and also we also have minus one at four", "image_path": "img_data/video_83_chunk_7.jpg"}
{"video": "video_83", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "and also we also have minus one at four that's x of n and then we this extra fan is going to go through this system is going to go through this system that we will call lti linear time that we will call lti linear time invariant system that invariant system that has an input response this system has an input response this system has an impulse response i will write it here first of all i will write it here first of all the that is gonna this exoplanet is gonna come in", "image_path": "img_data/video_83_chunk_8.jpg"}
{"video": "video_83", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "the that is gonna this exoplanet is gonna come in y often is gonna come out but the lti which is a system of interest here has which is a system of interest here has an h of n which i will draw it here an h of n which i will draw it here and this impulse response and this impulse response has one over here it has well it's a bit longer since that will go on this", "image_path": "img_data/video_83_chunk_9.jpg"}
{"video": "video_83", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "well it's a bit longer since that will go on this minus one and over here it has two this is h of n and this is n and what we are asked and this is n and what we are asked to do is to calculate to do is to calculate y of n from first principles y of n from first principles actually using effectively the actually using effectively the principle of impulse response what we principle of impulse response what we have if you a response", "image_path": "img_data/video_83_chunk_10.jpg"}
{"video": "video_83", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "principle of impulse response what we have if you a response have if you a response and then this response was causing an and then this response was causing an output the output of the lta system and what we call the an impulse response what we call the an impulse response effectively we can use exactly the same effectively we can use exactly the same concept as before and for each of concept as before and for each of these in input impulses that the system will in input impulses that the system will actually see and we can actually calculate what would and we can actually calculate what would actually be the output for each of those the output for each of those one after the other and as will turn out", "image_path": "img_data/video_83_chunk_11.jpg"}
{"video": "video_83", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "the output for each of those one after the other and as will turn out one after the other and as will turn out the final y of n is going to be a superposition of all is going to be a superposition of all the impulses impulse responses that are triggered impulse responses that are triggered by these impulses of the input signal all right here we of the input signal all right here we are we can start drawing the first we can start drawing the first impulse is arriving that's basically is arriving that's basically at this can be it is a at this can be it is a minus one and we have a value of", "image_path": "img_data/video_83_chunk_12.jpg"}
{"video": "video_83", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "at this can be it is a minus one and we have a value of minus one and we have a value of one and this is effectively the input and effectively the input and it will actually go through the system and we will have at the output sum response which is let's see what it will actually be first let's see what it will actually be first of all it starts at minus one of all it starts at minus one at index minus one and it's at index minus one and it's actually equivalent to this one by", "image_path": "img_data/video_83_chunk_13.jpg"}
{"video": "video_83", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "at index minus one and it's actually equivalent to this one by actually equivalent to this one by shifted by in time by minus one it is going to be minus one it is going to be exactly identical to the impulse and exactly identical to the impulse and smaller system of the system by shifted by -1 of the system by shifted by -1 it's going to be over here is going to be the one over here is going to be -1 and over here is going to be", "image_path": "img_data/video_83_chunk_14.jpg"}
{"video": "video_83", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "and over here is going to be at 1 is going to be 2. and this is going to be the output and similarly we can actually get let's assume we can actually get let's assume again the same system is going to be now again the same system is going to be now be hit with this impulse 2 hit with this impulse 2 and then after the symbol this and then after the symbol this the system will be hit with this impulse", "image_path": "img_data/video_83_chunk_15.jpg"}
{"video": "video_83", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "and then after the symbol this the system will be hit with this impulse the system will be hit with this impulse that this one at zero over here and this one at zero over here and is going to be creating a version of the impulse creating a version of the impulse response of course scaled by the amplitude of this impulse over the amplitude of this impulse over here the only impulse now that hits the only impulse now that hits the system is this one and the output if i just and the output if i just [music]", "image_path": "img_data/video_83_chunk_16.jpg"}
{"video": "video_83", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "and the output if i just [music] [music] draw the output over here this one draw the output over here this one it will be at two and then it will be minus 2 and will be 2 times minus 2 and will be 2 times minus 2 sorry 2 times 2 is equal to 4 sorry 2 times 2 is equal to 4 here we are maybe here it's going to be formed that is the output of the system that is the output of the system in fact i may need to draw it exactly", "image_path": "img_data/video_83_chunk_17.jpg"}
{"video": "video_83", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "in fact i may need to draw it exactly below the other one and keep if you some notion of time here because i may some notion of time here because i may i want to make a point the first i want to make a point the first one is going to be 2 then over here will be -2 and over here", "image_path": "img_data/video_83_chunk_18.jpg"}
{"video": "video_83", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "-2 and over here it will be 4. all right if i repeat the same thing all right if i repeat the same thing for all other two impulses i don't want to all other two impulses i don't want to do that necessarily right now but i guess you can understand right now but i guess you can understand that each of these other ones will also that each of these other ones will also create scaled versions of this create scaled versions of this i will have at the end of the day i will have at the end of the day in the output y of n will in the output y of n will consist of", "image_path": "img_data/video_83_chunk_19.jpg"}
{"video": "video_83", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "in the output y of n will consist of effectively four components that's effectively four components that's component number one component number one responsible for that game responsible for that game was produced out of this impulse was produced out of this impulse component number two was component number two was produced out of this impulse produced out of this impulse three and four and on i will three and four and on i will effectively have a superposition and i'm going to write it this i'm going to have effectively a i'm going to have effectively a superposition which is going to be superposition which is going to be effectively", "image_path": "img_data/video_83_chunk_20.jpg"}
{"video": "video_83", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "superposition which is going to be effectively an addition operation or i would call it an addition operation or i would call it a summation operation all these signals will be added and effectively to create if you the y offense at to create if you the y offense at the output y of n is going to be the summation y of n is going to be the summation over all of these components if you do the calculations at the end of the day then i'm going to get something that", "image_path": "img_data/video_83_chunk_21.jpg"}
{"video": "video_83", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "of the day then i'm going to get something that then i'm going to get something that will actually look as a final y of n it will look it will probably look something that if you do the calculation it will be if you do the calculation it will be this is now y of n over here i'm going to have a over n over here i'm going to have a minus sorry a one at minus one sorry a one at minus one i'm gonna have also a one at zero", "image_path": "img_data/video_83_chunk_22.jpg"}
{"video": "video_83", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "sorry a one at minus one i'm gonna have also a one at zero over here i'm going to have a three i'm going to have a zero on a three i'm going to have a seven and a four i'm going to have a minus 2 that would actually be my a minus 2 that would actually be my final y", "image_path": "img_data/video_83_chunk_23.jpg"}
{"video": "video_83", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "a minus 2 that would actually be my final y of n and this y of n is this sum we will call it a convolution sound and the formula for the convolutional sum will actually be sum will actually be the formula that is known as a convolution operation known as a convolution operation from let's say [music]", "image_path": "img_data/video_83_chunk_24.jpg"}
{"video": "video_83", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "from let's say [music] [music] m is equal to minus infinity to plus infinity x of m times h of n minus m that is the of n minus m that is the explanation of how convolution explanation of how convolution is effectively defined from is effectively defined from effortfully reaching this formula effortfully reaching this formula from this which results effectively from the which results effectively from the superposition of all these components", "image_path": "img_data/video_83_chunk_25.jpg"}
{"video": "video_83", "start": "0:13:00", "end": "0:13:11.233333", "timestamp": "0:13:00 - 0:13:11.233333", "text": "which results effectively from the superposition of all these components superposition of all these components each of these components were actually each of these components were actually formed from each of the impulses that the input each of the impulses that the input signal consists of", "image_path": "img_data/video_83_chunk_26.jpg"}
{"video": "video_87", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "this is a probably propagation example of a simple function this function has two simple function this function has two variables and is given by variables and is given by f of x comma y is equal to f of x comma y is equal to x plus sigma of y where sigma is the x plus sigma of y where sigma is the sigmoid function divided by sigma of x plus divided by sigma of x plus y squared", "image_path": "img_data/video_87_chunk_0.jpg"}
{"video": "video_87", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "divided by sigma of x plus y squared and i think the simplest and i think the simplest this simple kind of function has this simple kind of function has embedded another function which is the embedded another function which is the sigmoid function let's say of sigmoid function let's say of x which is given by the expression 1 x which is given by the expression 1 divided by 1 plus e to the minus x 1 plus e to the minus x and this is effectively and this is effectively no coincidence that this is a function no coincidence that this is a function it was actually chosen as it was actually chosen as a sigmoid functions is sometimes a", "image_path": "img_data/video_87_chunk_1.jpg"}
{"video": "video_87", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "it was actually chosen as a sigmoid functions is sometimes a sigmoid functions is sometimes a common activation function in neural networks activation function in neural networks and our job here is to and our job here is to calculate the gradient calculate the gradient of f with respect to the parameters x of f with respect to the parameters x and y and which is actually given this is a vector with two components the partial derivative of x of f with the partial derivative of x of f with respect to x and the partial derivative of f with", "image_path": "img_data/video_87_chunk_2.jpg"}
{"video": "video_87", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "respect to x and the partial derivative of f with and the partial derivative of f with respect to y this function f of x comma y has just two parameters and we can actually represent this and we can actually represent this function with a computational graph with a computational graph this computational graph with we start this computational graph with we start right here at the bottom right here at the bottom with the inputs of this function x with the inputs of this function x and y and we can actually gradually build this and we can actually gradually build this computational graph by drawing it", "image_path": "img_data/video_87_chunk_3.jpg"}
{"video": "video_87", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "computational graph by drawing it this x and y will actually be sound together we call each of these be sound together we call each of these operations this elementary operation operations this elementary operation gates and with inputs and outputs and we start calling this and outputs and we start calling this each and every of these each and every of these outputs a suitable variable name outputs a suitable variable name for example x p y that stands for x", "image_path": "img_data/video_87_chunk_4.jpg"}
{"video": "video_87", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "outputs a suitable variable name for example x p y that stands for x for example x p y that stands for x plus y for example in this case plus y for example in this case then we continue the y will actually then we continue the y will actually go through a sigma function and at the output we will obtain the and at the output we will obtain the sig y variable the x plus y as you can see here will also go through", "image_path": "img_data/video_87_chunk_5.jpg"}
{"video": "video_87", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "variable the x plus y as you can see here will also go through as you can see here will also go through the squaring operation the squaring operation at the output of that squaring operation at the output of that squaring operation we will have a variable let's say xp y variable let's say xp y sqr the x now we'll also go through a sigmoid the sigmoid is the sigma of x that we see here", "image_path": "img_data/video_87_chunk_6.jpg"}
{"video": "video_87", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "a sigmoid the sigmoid is the sigma of x that we see here is the sigma of x that we see here and it will sum the output of that will actually call it the output of that will actually call it the sig x and that c gx will be summed with the x p y sqr variable in this gate over here the sig y plus x as we see in the numerator", "image_path": "img_data/video_87_chunk_7.jpg"}
{"video": "video_87", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "the sig y plus x as we see in the numerator plus x as we see in the numerator will actually form denominator will actually form denominator numerators via an addition this addition is going to happen here x plus sigma y that's the gate right here and it will actually give us the and it will actually give us the numerator we will call it num", "image_path": "img_data/video_87_chunk_8.jpg"}
{"video": "video_87", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "and it will actually give us the numerator we will call it num numerator we will call it num and we will all we need to do is to form and we will all we need to do is to form the ratio of the two and we'll form this the ratio of the two and we'll form this ratio by taking the inverse of what is effectively the denominator and multiplied", "image_path": "img_data/video_87_chunk_9.jpg"}
{"video": "video_87", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "and multiplied with the numerator to form effectively what is our function effectively what is our function output f i hope this is a output f i hope this is a computational graph is clear computational graph is clear with consisting of some elementary gates with consisting of some elementary gates and it's also called data flow diagram and it's also called data flow diagram as it actually takes as it actually takes x and y from the input and produces f at x and y from the input and produces f at the output and the back propagation operation now", "image_path": "img_data/video_87_chunk_10.jpg"}
{"video": "video_87", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "the output and the back propagation operation now and the back propagation operation now involves two passes it's a computational two passes it's a computational technique we just presented the all the we just presented the all the steps the first pass is the -called forward pass that pass is the -called forward pass that goes from the input x and y all the way the input x and y all the way up stream to compute and the value of up stream to compute and the value of f and this is actually called the forward and this is actually called the forward pass and after the four of pass is completed", "image_path": "img_data/video_87_chunk_11.jpg"}
{"video": "video_87", "start": "0:06:00", "end": "0:06:30", "timestamp": "0:06:00 - 0:06:30", "text": "and after the four of pass is completed we will execute the -called backward pass and this is the pass that will actually compute stage by stage compute stage by stage gate by gate upstream and downstream gate by gate upstream and downstream gradients that we will go through gradients that we will go through when we reach that point let's first when we reach that point let's first execute the", "image_path": "img_data/video_87_chunk_12.jpg"}
{"video": "video_87", "start": "0:06:30", "end": "0:07:00", "timestamp": "0:06:30 - 0:07:00", "text": "when we reach that point let's first execute the easiest path of the two which is the easiest path of the two which is the forward pass for it we the forward pass for it we will just need to start from something from just need to start from something from somewhere which is start from the input somewhere which is start from the input here we will execute first the sigmoid of we will execute first the sigmoid of y we could accept executed this gate but for now let's executed this gate but for now let's write down first how we obtain the sig y first how we obtain the sig y at the output here it is effectively at the output here it is effectively obviously the one over 1 plus", "image_path": "img_data/video_87_chunk_13.jpg"}
{"video": "video_87", "start": "0:07:00", "end": "0:07:30", "timestamp": "0:07:00 - 0:07:30", "text": "obviously the one over 1 plus e to the power of minus y e to the power of minus y and that is equation number one and that is equation number one the second equation that we can the second equation that we can actually do very easily now actually do very easily now is we can immediately get the numerator is we can immediately get the numerator we can say num but the output of this we can say num but the output of this kind of gate is x plus what we just computed and this is the second equation the", "image_path": "img_data/video_87_chunk_14.jpg"}
{"video": "video_87", "start": "0:07:30", "end": "0:08:00", "timestamp": "0:07:30 - 0:08:00", "text": "and this is the second equation the third equation is we can take the sigmoid of x is we can take the sigmoid of x and preparing for form in the and preparing for form in the denominator sig x is equal again very similar to the sig x is equal again very similar to the sig y one plus exp of minus x one plus exp of minus x that's the third equation and then that's the third equation and then we just need to populate this part over we just need to populate this part over here we start with the x split", "image_path": "img_data/video_87_chunk_15.jpg"}
{"video": "video_87", "start": "0:08:00", "end": "0:08:30", "timestamp": "0:08:00 - 0:08:30", "text": "we just need to populate this part over here we start with the x split here we start with the x split x plus y x p y is equal to x plus y x p y is equal to x plus y and this is number four x plus y and this is number four we continue to squaring it x p we continue to squaring it x p y s q r this is x p y to the power of two x p y to the power of two which is we can write it down this which is we can write it down this a bit more kind of programmatic way of a bit more kind of programmatic way of writing things programmatic way of specifying programmatic way of specifying input and output", "image_path": "img_data/video_87_chunk_16.jpg"}
{"video": "video_87", "start": "0:08:30", "end": "0:09:00", "timestamp": "0:08:30 - 0:09:00", "text": "programmatic way of specifying input and output variables and now we will be able now to variables and now we will be able now to form the denominator is going to be and we call it as the norm is going to be equal to x sig x as you can see here sig x as you can see here sig x plus x b y square", "image_path": "img_data/video_87_chunk_17.jpg"}
{"video": "video_87", "start": "0:09:00", "end": "0:09:30", "timestamp": "0:09:00 - 0:09:30", "text": "x as you can see here sig x plus x b y square and this is equation number six and now we are actually able to form the and now we are actually able to form the if we're going to add a variable here if we're going to add a variable here which we will call in the num and the in the inverse of the denominator in other words is 1 over the norm", "image_path": "img_data/video_87_chunk_18.jpg"}
{"video": "video_87", "start": "0:09:30", "end": "0:10:00", "timestamp": "0:09:30 - 0:10:00", "text": "in other words is 1 over the norm and this is equation 7 and finally f and this is equation 7 and finally f is equal to obviously is equal to obviously the multiplication of num times in the norm and this is equation eight which is and this is equation eight which is really the ending equation for the forward pass", "image_path": "img_data/video_87_chunk_19.jpg"}
{"video": "video_87", "start": "0:10:00", "end": "0:10:30", "timestamp": "0:10:00 - 0:10:30", "text": "ending equation for the forward pass let's look at the chain rule which is one of the more i'll call it foundational elements more i'll call it foundational elements in calculus and it's considered to be a and it's considered to be a background material here for this background material here for this course and the chain rule for this course and the chain rule really connects in terms of partial derivatives connects in terms of partial derivatives variables which are associated with a variables which are associated with a block diagram looks this block diagram looks this we have an input variable x and the", "image_path": "img_data/video_87_chunk_20.jpg"}
{"video": "video_87", "start": "0:10:30", "end": "0:11:00", "timestamp": "0:10:30 - 0:11:00", "text": "block diagram looks this we have an input variable x and the we have an input variable x and the m-dimensional space with m-components in m-dimensional space with m-components in other words it goes through a function g2 or a gate it goes through a function g2 or a gate as we will call it in backward radiation as we will call it in backward radiation it produces another vector y it produces another vector y in the n-dimensional space in the n-dimensional space and that vector is we have another function g1 we have another function g1 is producing a scalar z and the problem is producing a scalar z and the problem we have now is how we can actually have", "image_path": "img_data/video_87_chunk_21.jpg"}
{"video": "video_87", "start": "0:11:00", "end": "0:11:30", "timestamp": "0:11:00 - 0:11:30", "text": "we have now is how we can actually have is how we can actually have some connection between the variability of x between the variability of x and to the variability of z this is and to the variability of z this is actually represented with a partial actually represented with a partial derivative of z with respect to x of z with respect to x and this according to the chain rule is and this according to the chain rule is actually given by the partial derivative actually given by the partial derivative of z with respect to y of z with respect to y times the partial derivative of y with times the partial derivative of y with respect to x and effectively intuitively this means and effectively intuitively this means that variability from x all the way to z", "image_path": "img_data/video_87_chunk_22.jpg"}
{"video": "video_87", "start": "0:11:30", "end": "0:12:00", "timestamp": "0:11:30 - 0:12:00", "text": "that variability from x all the way to z that variability from x all the way to z it's a product of the availability that it's a product of the availability that the x is actually causing to the that the x is actually causing to the variable x and in turn sorry that the and in turn sorry that the variable x is causing the variable y and variable x is causing the variable y and that in turn is goes into the variable z that in turn is goes into the variable z and that is really the single and that is really the single dimensional kind of case dimensional kind of case and it can be readily extended to the and it can be readily extended to the multi-dimensional case multi-dimensional case in other words when we have m different", "image_path": "img_data/video_87_chunk_23.jpg"}
{"video": "video_87", "start": "0:12:00", "end": "0:12:30", "timestamp": "0:12:00 - 0:12:30", "text": "multi-dimensional case in other words when we have m different in other words when we have m different than n different than one than n different than one and in this case we have and in this case we have derivatives that involve scalars and derivatives that involve scalars and vectors in fact you can start writing exactly in fact you can start writing exactly the same rule as we have done in the scalar kind rule as we have done in the scalar kind of case and here we have the derivative of z and here we have the derivative of z with respect just one component of x with respect just one component of x this is going to be equal to the this is going to be equal to the derivative of z with respect to the derivative of z with respect to the intermediate variable y over here intermediate variable y over here which is a vector times the derivative", "image_path": "img_data/video_87_chunk_24.jpg"}
{"video": "video_87", "start": "0:12:30", "end": "0:13:00", "timestamp": "0:12:30 - 0:13:00", "text": "intermediate variable y over here which is a vector times the derivative which is a vector times the derivative of that of the vector with respect to of that of the vector with respect to the scalar x1 which is the first the scalar x1 which is the first component of x out of the m components total x out of the m components total and you can actually write the and you can actually write the derivative of a scalar with respect to derivative of a scalar with respect to a vector as the row vector a vector as the row vector of the derivative of scalar with respect of the derivative of scalar with respect to each and every of the components of this and every of the components of this vector and y has n components and y has n components are being listed here", "image_path": "img_data/video_87_chunk_25.jpg"}
{"video": "video_87", "start": "0:13:00", "end": "0:13:30", "timestamp": "0:13:00 - 0:13:30", "text": "and y has n components are being listed here are being listed here times the derivative of the vector y times the derivative of the vector y with respect to the scalar x1 to the with respect to the scalar x1 to the component of interest in this specific component of interest in this specific case we have again another column vector this time again another column vector this time with respect to that at least with respect to that at least the associated if you derivatives the associated if you derivatives of all the components of all the components of y notice that a comp a single of y notice that a comp a single component of x will affect all the components of the will affect all the components of the vector y via the function via the kaplan", "image_path": "img_data/video_87_chunk_26.jpg"}
{"video": "video_87", "start": "0:13:30", "end": "0:14:00", "timestamp": "0:13:30 - 0:14:00", "text": "vector y via the function via the kaplan via the function via the kaplan function g2 and this vector of row vector times a and this vector of row vector times a column vector is nothing else column vector is nothing else than a product a dot product and we can than a product a dot product and we can actually write the result actually write the result which ends up being effectively a scalar which ends up being effectively a scalar this i think that it's actually i think that it's actually straightforward to understand straightforward to understand the direct extension between the old the direct extension between the old scalar case and the scalar with respect to", "image_path": "img_data/video_87_chunk_27.jpg"}
{"video": "video_87", "start": "0:14:00", "end": "0:14:30", "timestamp": "0:14:00 - 0:14:30", "text": "scalar case and the scalar with respect to scalar with respect to one of the components of vector x1 to one of the components of vector x1 and i think when it comes to how come and i think when it comes to how come this is a column vector a row vector and this is a column vector a row vector and this is ends up being a column vector ends up being a column vector i think it will be understood a bit i think it will be understood a bit later as and when we introduce the later as and when we introduce the general case in a moment what is really the in a moment what is really the general case is when we general case the general case is when we have the output z with respect to all", "image_path": "img_data/video_87_chunk_28.jpg"}
{"video": "video_87", "start": "0:14:30", "end": "0:15:00", "timestamp": "0:14:30 - 0:15:00", "text": "have the output z with respect to all the output z with respect to all components of x and this will effectively is and this will effectively is a row vector of the scalar z with a row vector of the scalar z with respect to all components of respect to all components of x 1 x 2 all the way to x m very x 1 x 2 all the way to x m very similar to what we have seen similar to what we have seen just a few moments ago just a few moments ago and therefore we can actually readily and therefore we can actually readily plug all the results that we get from plug all the results that we get from each of these kind of components each of these kind of components and we can actually write this in a more and we can actually write this in a more compact fashion by a matrix by a vector multiplication", "image_path": "img_data/video_87_chunk_29.jpg"}
{"video": "video_87", "start": "0:15:00", "end": "0:15:30", "timestamp": "0:15:00 - 0:15:30", "text": "compact fashion by a matrix by a vector multiplication a matrix by a vector multiplication sorry a vector by matrix sorry a vector by matrix multiplication the vector that we have is the current vector that we have is the current vector from above as we have as we had it and as we have as we had it and here we actually start introducing here we actually start introducing before we only had a single component x1 before we only had a single component x1 now we're going to have x2 x to the xd now we're going to have x2 x to the xd or all the way to xm components we're adding more and more xm components we're adding more and more columns into this column a vector effectively into this column a vector effectively creating a matrix", "image_path": "img_data/video_87_chunk_30.jpg"}
{"video": "video_87", "start": "0:15:30", "end": "0:16:00", "timestamp": "0:15:30 - 0:16:00", "text": "into this column a vector effectively creating a matrix and it is this matrix that we will be and it is this matrix that we will be calling the jacobian which is really a fundamental matrix of interest in back propagation and it of interest in back propagation and it also defines some of the conventions we also defines some of the conventions we will follow in terms of dimensionality and in terms of dimensionality and the dimensionality is actually important the dimensionality is actually important because it will allow us to check because it will allow us to check intermediate results intermediate results whether or not they are correct and make", "image_path": "img_data/video_87_chunk_31.jpg"}
{"video": "video_87", "start": "0:16:00", "end": "0:16:30", "timestamp": "0:16:00 - 0:16:30", "text": "intermediate results whether or not they are correct and make whether or not they are correct and make sense or not the jacobian matrix is sense or not the jacobian matrix is always output times input it actually refers this jacobian it actually refers this jacobian matrix to this vector function g2 that takes vector function g2 that takes vector inputs produce vector outputs inputs produce vector outputs and it's always by convention outputs and it's always by convention outputs with respect to images that one thing to with respect to images that one thing to remember very important the second thing to very important the second thing to remember is that because of that we can remember is that because of that we can actually write the elements actually write the elements or each element of the jacobian matrix i", "image_path": "img_data/video_87_chunk_32.jpg"}
{"video": "video_87", "start": "0:16:30", "end": "0:17:00", "timestamp": "0:16:30 - 0:17:00", "text": "actually write the elements or each element of the jacobian matrix i or each element of the jacobian matrix i and j in other words row i and column j and j in other words row i and column j as a partial derivative of the output i as a partial derivative of the output i with respect to the input j that's the with respect to the input j that's the second thing you need to remember and after that you need to remember and after that everything will actually flow everything will actually flow along and just to connect what we along and just to connect what we have just said before have just said before with respect to of this derivative with respect to of this derivative with this with respect to other words with this with respect to other words of a scalar with respect to a vector a scalar with respect to a vector we can actually see the jacobian matrix", "image_path": "img_data/video_87_chunk_33.jpg"}
{"video": "video_87", "start": "0:17:00", "end": "0:17:30", "timestamp": "0:17:00 - 0:17:30", "text": "a scalar with respect to a vector we can actually see the jacobian matrix we can actually see the jacobian matrix and see if you the convention and see if you the convention and we can now readily understand why this was a row vector understand why this was a row vector and in other words if you look at the and in other words if you look at the jacobian matrix this correspond this situation of scalar this correspond this situation of scalar with respect to a vector with respect to a vector corresponds to one of the rows of the corresponds to one of the rows of the jacobian let's take one case where we have a let's take one case where we have a scalar y1 with respect to vector x1 x2 and on with respect to vector x1 x2 and on xm this is really the first row here", "image_path": "img_data/video_87_chunk_34.jpg"}
{"video": "video_87", "start": "0:17:30", "end": "0:18:00", "timestamp": "0:17:30 - 0:18:00", "text": "xm this is really the first row here this is really the first row here that's why this is the derivative i can write it as this is the derivative i can write it as in general with respect of a scalar in general with respect of a scalar that's a direct convention with respect that's a direct convention with respect to it's electron consequence with it's electron consequence with respect to the convection of the respect to the convection of the jacobian matrix is going to be a row vector and if we take the other case where we have a vector with respect have a vector with respect to a scalar where we had let's say the", "image_path": "img_data/video_87_chunk_35.jpg"}
{"video": "video_87", "start": "0:18:00", "end": "0:18:30", "timestamp": "0:18:00 - 0:18:30", "text": "have a vector with respect to a scalar where we had let's say the to a scalar where we had let's say the derivative of let's say of y derivative of let's say of y being a vector with respect to a scalar being a vector with respect to a scalar let's say x1 this was effectively now a column vector and you can see it here in the jacobian matrix it is the case where we have it is the case where we have a vector let's say y1 all the way to a vector let's say y1 all the way to y n with respect to a scalar where let's", "image_path": "img_data/video_87_chunk_36.jpg"}
{"video": "video_87", "start": "0:18:30", "end": "0:19:00", "timestamp": "0:18:30 - 0:19:00", "text": "a vector let's say y1 all the way to y n with respect to a scalar where let's y n with respect to a scalar where let's take a take x one that doesn't change in take x one that doesn't change in this kind of column it's a scalar this kind of column it's a scalar this corresponds to a column this corresponds to a column vector that's what we will write here that's what we will write here just to remember these conventions which just to remember these conventions which is straightforward after we straightforward after we understand exactly how understand exactly how the jacobian matrix is structured the jacobian matrix is structured before i start the backwards pass is backwards pass the backwards pass is certainly an", "image_path": "img_data/video_87_chunk_37.jpg"}
{"video": "video_87", "start": "0:19:00", "end": "0:19:30", "timestamp": "0:19:00 - 0:19:30", "text": "backwards pass the backwards pass is certainly an exercise that involves starting from the exercise that involves starting from the top if you of this computational graph that we have just computational graph that we have just seen and we will be using i will call and we will be using i will call it the chain rule in a bit more it the chain rule in a bit more data flow oriented sort of data flow oriented sort of representation this is effectively one gate g this is effectively one gate g that is receiving from upstream that is receiving from upstream a derivative in this case be a gradient or could be a", "image_path": "img_data/video_87_chunk_38.jpg"}
{"video": "video_87", "start": "0:19:30", "end": "0:20:00", "timestamp": "0:19:30 - 0:20:00", "text": "a derivative in this case be a gradient or could be a in this case be a gradient or could be a scalar derivative it depends of course scalar derivative it depends of course on what really happened before on what really happened before and this is the derivative of and this is the derivative of the target function f with respect to the target function f with respect to z that is going to be obviously be z that is going to be obviously be calculated itself via number of stages but this is via number of stages but this is actually going to be given to us actually going to be given to us this is going to be arriving to our gate this is going to be arriving to our gate and our job here is to and our job here is to understand how this gradient understand how this gradient is flowing through the", "image_path": "img_data/video_87_chunk_39.jpg"}
{"video": "video_87", "start": "0:20:00", "end": "0:20:30", "timestamp": "0:20:00 - 0:20:30", "text": "understand how this gradient is flowing through the is flowing through the ports x and y those ports and x and y ports x and y those ports and x and y was effectively the ports that was effectively the ports that correspond to the variables x and y correspond to the variables x and y in the forward pass we go we went ahead in the forward pass we go we went ahead and calculated those and calculated those in our four pass equations and those in our four pass equations and those downstream derivatives as we will call downstream derivatives as we will call them those downstream derivatives are them those downstream derivatives are going to be written as derivative of f going to be written as derivative of f all the way to the top of the all the way to the top of the diagram with respect to x", "image_path": "img_data/video_87_chunk_40.jpg"}
{"video": "video_87", "start": "0:20:30", "end": "0:21:00", "timestamp": "0:20:30 - 0:21:00", "text": "all the way to the top of the diagram with respect to x of the diagram with respect to x and then obviously the derivative half and then obviously the derivative half with respect to y over here with respect to y over here and as it will turn out this is and as it will turn out this is everything that we need we have we everything that we need we have we will be using the term upstream gradient this option gradient is actually given and we will call these guys downstream", "image_path": "img_data/video_87_chunk_41.jpg"}
{"video": "video_87", "start": "0:21:00", "end": "0:21:30", "timestamp": "0:21:00 - 0:21:30", "text": "this option gradient is actually given and we will call these guys downstream gradients that obviously would be upstream that obviously would be upstream gradients for the subsequent gate of as we go from for the subsequent gate of as we go from top to bottom in that computational top to bottom in that computational graph and as it will turn out and as it will turn out dawson gradients are all we need to dawson gradients are all we need to calculate them are effectively the abstract gradient that effectively the abstract gradient that is actually given here is actually given here and what we call the local gradient and what we call the local gradient let's write down the expressions it let's write down the expressions it was going to be three", "image_path": "img_data/video_87_chunk_42.jpg"}
{"video": "video_87", "start": "0:21:30", "end": "0:22:00", "timestamp": "0:21:30 - 0:22:00", "text": "let's write down the expressions it was going to be three was going to be three really important expressions there really important expressions there will be the derivative of f with respect to x the derivative of f with respect to x is equal to the derivative of f with is equal to the derivative of f with respect to z which is what is coming respect to z which is what is coming from on top and the absolute gradient given to us times the absolute gradient given to us times what we will be called the local what we will be called the local gradient the derivative of z in other words with the derivative of z in other words with respect to x", "image_path": "img_data/video_87_chunk_43.jpg"}
{"video": "video_87", "start": "0:22:00", "end": "0:22:30", "timestamp": "0:22:00 - 0:22:30", "text": "respect to x all right this is the one equation and the second equation and the second equation is going to be the derivative of f is going to be the derivative of f with respect to y is equal the with respect to y is equal the derivative of f with respect to z again times with respect to z again times the times is actually not strictly the times is actually not strictly speaking correct but speaking correct but you can think about it this you can think about it this derivative of z with respect to y in this case these are the fundamental equations", "image_path": "img_data/video_87_chunk_44.jpg"}
{"video": "video_87", "start": "0:22:30", "end": "0:23:00", "timestamp": "0:22:30 - 0:23:00", "text": "y in this case these are the fundamental equations fundamental equations that we will be using for our that we will be using for our propagation and these are going to be the local and these are going to be the local gradients and they're called local because they only depend on the local knowledge of the gate on the local knowledge of the gate g usually these gates are very g usually these gates are very elementary and because of their elementary kind of and because of their elementary kind of", "image_path": "img_data/video_87_chunk_45.jpg"}
{"video": "video_87", "start": "0:23:00", "end": "0:23:30", "timestamp": "0:23:00 - 0:23:30", "text": "and because of their elementary kind of nature you will be able using nature you will be able using symbolic simple symbolic lookups symbolic simple symbolic lookups to understand how the to understand how the these derivatives can be these derivatives can be written something that will become written something that will become evident in this exercise as we go in this exercise as we go now back into that computational diagram back into that computational diagram this is the computational diagram that this is the computational diagram that we have and in this case we need to and in this case we need to sort of introduce certain variables we sort of introduce certain variables we will start at the from the top of the", "image_path": "img_data/video_87_chunk_46.jpg"}
{"video": "video_87", "start": "0:23:30", "end": "0:24:00", "timestamp": "0:23:30 - 0:24:00", "text": "sort of introduce certain variables we will start at the from the top of the will start at the from the top of the diagram which is actually diagram which is actually equation 8 here and we go all the way to equation 8 here and we go all the way to back into the variables x and y we go back into the variables x and y we go into this kind of direction into this kind of direction and let's write down now the first and let's write down now the first equation and the first equation we can actually and the first equation we can actually write on the backward variation side write on the backward variation side is really all we i care about is to is really all we i care about is to calculate this gradient the derivative of f this gradient the derivative of f with respect to x and divided by with respect to x and divided by with respect to y", "image_path": "img_data/video_87_chunk_47.jpg"}
{"video": "video_87", "start": "0:24:00", "end": "0:24:30", "timestamp": "0:24:00 - 0:24:30", "text": "with respect to x and divided by with respect to y we will start from here and we write we will start from here and we write the first equation which is the first equation which is the derivative of f and i will be the derivative of f and i will be writing these quantities on the sort of right left quantities on the sort of right left hand side of the arrows or whenever we have of the arrows or whenever we have sort of the empty space sort of the empty space in this kind of diagram as it is a bit in this kind of diagram as it is a bit congested i'm going to be calling congested i'm going to be calling this derivatives d for the -called", "image_path": "img_data/video_87_chunk_48.jpg"}
{"video": "video_87", "start": "0:24:30", "end": "0:25:00", "timestamp": "0:24:30 - 0:25:00", "text": "this derivatives d for the -called derivatives d for the -called derivative of f instead of actually derivative of f instead of actually writing the derivative of f with respect to f the derivative of f with respect to f we all know the derivative of f with we all know the derivative of f with respect to f this is effectively respect to f this is effectively and this is equal to 1.0 and this is equal to 1.0 this symbolic derivative of a this symbolic derivative of a variable with respect to itself variable with respect to itself is equal to 1.0 the we have our first if you the we have our first if you upstream gradient and this upstream gradient and this upstream gradient is now", "image_path": "img_data/video_87_chunk_49.jpg"}
{"video": "video_87", "start": "0:25:00", "end": "0:25:30", "timestamp": "0:25:00 - 0:25:30", "text": "upstream gradient and this upstream gradient is now received by this gate which is our first received by this gate which is our first gate which is multiplication gate which is multiplication gate let's first before we forget write let's first before we forget write down the first equation the derivativity the first equation the derivativity f is equal to 1.0 and we can say the we can actually have now this is the we can actually have now this is a multiplication gate where a multiplication gate where multiplication gate is really on multiplication gate is really on equation 8 the multiplication gate was", "image_path": "img_data/video_87_chunk_50.jpg"}
{"video": "video_87", "start": "0:25:30", "end": "0:26:00", "timestamp": "0:25:30 - 0:26:00", "text": "equation 8 the multiplication gate was the multiplication gate was right here just write down again the right here just write down again the expression f is equal to numerator f is equal to numerator times the inverse of the denominator this was a variable these are the this was a variable these are the variables that we have we actually variables that we have we actually looking as inputs in the forward propagation to as inputs in the forward propagation to this gate now there are the places we're going to there are the places we're going to estimate the downstream gradients", "image_path": "img_data/video_87_chunk_51.jpg"}
{"video": "video_87", "start": "0:26:00", "end": "0:26:30", "timestamp": "0:26:00 - 0:26:30", "text": "there are the places we're going to estimate the downstream gradients estimate the downstream gradients and in order for us to estimate the and in order for us to estimate the downstream gradients downstream gradients we will need to apply this we will need to apply this specific rule in this specific rule in this specific convention in terms of convention in terms of sort of calling what is abstinent what sort of calling what is abstinent what is downstream the next equation that we will then the next equation that we will then go down to downwards to back propagate to downwards to back propagate is the this gate we already have is the this gate we already have the downstream gradient from this gate", "image_path": "img_data/video_87_chunk_52.jpg"}
{"video": "video_87", "start": "0:26:30", "end": "0:27:00", "timestamp": "0:26:30 - 0:27:00", "text": "the downstream gradient from this gate downstream gradient from this gate which is the abstinent gradient of this which is the abstinent gradient of this gate d in the norm is going to be our upstream gradient is going to be our upstream gradient arriving over here let's now arriving over here let's now write down we are going to this write down we are going to this equation number seven equation number seven in the order presented to us in the order presented to us in the forward pass equation number seven equation number seven we will basically back we will basically back propagate", "image_path": "img_data/video_87_chunk_53.jpg"}
{"video": "video_87", "start": "0:27:00", "end": "0:27:30", "timestamp": "0:27:00 - 0:27:30", "text": "we will basically back propagate the equation in the norm is equal to one over the norm is equal to one over the norm this was equation number seven the norm this was equation number seven and we will actually and we will actually do exactly the same thing as we have do exactly the same thing as we have done before we have arrived we are receiving an we have arrived we are receiving an upstream gradient which is the d in this is our gate which is the d in this is our gate over here now this upstream gradient", "image_path": "img_data/video_87_chunk_54.jpg"}
{"video": "video_87", "start": "0:27:30", "end": "0:28:00", "timestamp": "0:27:30 - 0:28:00", "text": "over here now this upstream gradient now this upstream gradient d in the gnome was just being computed a few the gnome was just being computed a few minutes ago this is now minutes ago this is now vr and this is i guess just one port it has just received one just one port it has just received one port and it produces one port we have one it produces one port we have one downstream gradient and this downstream gradient and this downstream gradient we will call it gradient we will call it d", "image_path": "img_data/video_87_chunk_55.jpg"}
{"video": "video_87", "start": "0:28:00", "end": "0:28:30", "timestamp": "0:28:00 - 0:28:30", "text": "gradient we will call it d the norm because that is really the variable that it is really the variable that it is coming at the output of this a gate coming at the output of this a gate according to this computational graph according to this computational graph here all right this is a fairly all right this is a fairly straightforward now we will apply this rule the we will apply this rule the derivative of f with respect to denominator", "image_path": "img_data/video_87_chunk_56.jpg"}
{"video": "video_87", "start": "0:28:30", "end": "0:29:00", "timestamp": "0:28:30 - 0:29:00", "text": "denominator which we will call it a d denom is equal to the upstream gradient d in the norm times again the time is not strictly correct again the time is not strictly correct but it doesn't really matter now but it doesn't really matter now all right it is the derivative of all right it is the derivative of f with respect to sorry it's not", "image_path": "img_data/video_87_chunk_57.jpg"}
{"video": "video_87", "start": "0:29:00", "end": "0:29:30", "timestamp": "0:29:00 - 0:29:30", "text": "f with respect to sorry it's not with respect to sorry it's not derivative f it is the local derivative f it is the local derivative it is the derivative of it is the derivative of in the norm with respect to the no the derivative of if the known with the derivative of if the known with respect to the norm is we what we do is we go it's a", "image_path": "img_data/video_87_chunk_58.jpg"}
{"video": "video_87", "start": "0:29:30", "end": "0:30:00", "timestamp": "0:29:30 - 0:30:00", "text": "respect to the norm is we what we do is we go it's a is we what we do is we go it's a symbolic derivative obviously we are going to a derivative obviously we are going to a lookup table from derivative lookup table we know that the lookup table we know that the derivative of one over x is minus one derivative of one over x is minus one over x squared and therefore x squared and therefore we can write this as the d we can write this as the d in the norm which is d in the novice in the norm which is d in the novice in fact numerator", "image_path": "img_data/video_87_chunk_59.jpg"}
{"video": "video_87", "start": "0:30:00", "end": "0:30:30", "timestamp": "0:30:00 - 0:30:30", "text": "fact numerator again this is now completely a lookup we don't calculate anything we just look we don't calculate anything we just look up what we already have up what we already have calculated before and we calculated before and we apply the one formula over here which is formula over here which is the num squared both of these quantities the num squared both of these quantities are known to us because the num was in fact obviously", "image_path": "img_data/video_87_chunk_60.jpg"}
{"video": "video_87", "start": "0:30:30", "end": "0:31:00", "timestamp": "0:30:30 - 0:31:00", "text": "are known to us because the num was in fact obviously because the num was in fact obviously calculated in the forward pass it calculated in the forward pass it is actually present here in equation six is actually present here in equation six it was calculated in the forward pass it was calculated in the forward pass and therefore we are able to store it and therefore we are able to store it we have already stored it in memory from we have already stored it in memory from the forward pass we just the forward pass we just retrieve it and we immediately enable retrieve it and we immediately enable them to calculate and this is a in fact what we are", "image_path": "img_data/video_87_chunk_61.jpg"}
{"video": "video_87", "start": "0:31:00", "end": "0:31:30", "timestamp": "0:31:00 - 0:31:30", "text": "and this is a in fact what we are have just we just back have just we just back propagated that this specific gate on equation that this specific gate on equation seven maybe we have some space to propagate maybe we have some space to propagate now equation six which is this guy over here equation six which is this guy over here let's write down again back let's write down again back propagation of back probe of the equation the question 6 was", "image_path": "img_data/video_87_chunk_62.jpg"}
{"video": "video_87", "start": "0:31:30", "end": "0:32:00", "timestamp": "0:31:30 - 0:32:00", "text": "back probe of the equation the question 6 was denom is equal to sig x plus x p y s q r that was our equation y s q r that was our equation which of course this equation which of course this equation corresponds to yet another gate corresponds to yet another gate what was this actually gate this", "image_path": "img_data/video_87_chunk_63.jpg"}
{"video": "video_87", "start": "0:32:00", "end": "0:32:30", "timestamp": "0:32:00 - 0:32:30", "text": "corresponds to yet another gate what was this actually gate this what was this actually gate this gate was this gate is it is an gate was this gate is it is an adder we are receiving adder we are receiving the d ten ohm upstream gradient and now we ten ohm upstream gradient and now we have two ports the two parts is has dancing gradients will be has dancing gradients will be d x and the other one would be", "image_path": "img_data/video_87_chunk_64.jpg"}
{"video": "video_87", "start": "0:32:30", "end": "0:33:00", "timestamp": "0:32:30 - 0:33:00", "text": "has dancing gradients will be d x and the other one would be d x and the other one would be d x b y s q r these are the two downstream s q r these are the two downstream gradients we applied exactly the same we applied exactly the same [music] [music] template that we have applied before template that we have applied before what was actually going to happen is that the derivative of f with respect to sig x", "image_path": "img_data/video_87_chunk_65.jpg"}
{"video": "video_87", "start": "0:33:00", "end": "0:33:30", "timestamp": "0:33:00 - 0:33:30", "text": "to sig x we have called d sig x is equal to the upstream gradient sig x is equal to the upstream gradient d the norm times the local gradient and the local times the local gradient and the local gradient corresponds gradient corresponds depends only on the gate it is depends only on the gate it is effectively the derivative of effectively the derivative of a genome with respect to sig x", "image_path": "img_data/video_87_chunk_66.jpg"}
{"video": "video_87", "start": "0:33:30", "end": "0:34:00", "timestamp": "0:33:30 - 0:34:00", "text": "with respect to sig x well by the derivative of the number well by the derivative of the number with respect to sig x is with respect to sig x is the derivative of x with respect to the derivative of x with respect to sorry the derivative of the denominator sorry the derivative of the denominator with respect to sig x with respect to sig x is effectively 1.0 because this term is actually a zero because this term is actually a zero this effectively brings d this effectively brings d the norm as the answer the norm as the answer and the second derivative which i", "image_path": "img_data/video_87_chunk_67.jpg"}
{"video": "video_87", "start": "0:34:00", "end": "0:34:30", "timestamp": "0:34:00 - 0:34:30", "text": "the norm as the answer and the second derivative which i and the second derivative which i will call from the other port over here will call from the other port over here the d x b y s q r it's actually very the d x b y s q r it's actually very similar is going to be ultimately is going to be ultimately the upstream which is the upstream which is the norm times 1.0 again because at the time that we were taking the at the time that we were taking the derivative of this thing with respect to derivative of this thing with respect to x b y sqr this is 0 and this is 1.0", "image_path": "img_data/video_87_chunk_68.jpg"}
{"video": "video_87", "start": "0:34:30", "end": "0:35:00", "timestamp": "0:34:30 - 0:35:00", "text": "x b y sqr this is 0 and this is 1.0 y sqr this is 0 and this is 1.0 this is going to be again also this is going to be again also the norm which we know from previously which we know from previously we can proceed very similarly to we can proceed very similarly to backward by create all the remaining backward by create all the remaining gates over here something that we have done in the notes something that we have done in the notes and there's one tricky point in this diagram there's one tricky point in this diagram that we need to pay attention in this", "image_path": "img_data/video_87_chunk_69.jpg"}
{"video": "video_87", "start": "0:35:00", "end": "0:35:30", "timestamp": "0:35:00 - 0:35:30", "text": "there's one tricky point in this diagram that we need to pay attention in this that we need to pay attention in this kind of backwards pass and it is actually these two points pass and it is actually these two points over here and in these two points there are and in these two points there are these are called forks or effectively these are called forks or effectively we are actually copy gates we are actually copy gates they are they should be treated as gates they are they should be treated as gates in this fashion you have an upstream in this fashion you have an upstream gradient and you have another abstract and you have another abstract gradient coming in and there is only one gradient coming in and there is only one dancing gradient which is basically the dancing gradient which is basically the summation", "image_path": "img_data/video_87_chunk_70.jpg"}
{"video": "video_87", "start": "0:35:30", "end": "0:36:00", "timestamp": "0:35:30 - 0:36:00", "text": "dancing gradient which is basically the summation of the two upstream gradients of the two upstream gradients make sure that you have sort of make sure that you have sort of followed the equation followed the equation in the notes this equation in the notes this equation is involving the back propagation is involving the back propagation of sigma x over here gate of sigma x over here gate and the back propagation and the back propagation of the gate that we have here on the of the gate that we have here on the summer this gradient over here summer this gradient over here these two upstream gradients have to", "image_path": "img_data/video_87_chunk_71.jpg"}
{"video": "video_87", "start": "0:36:00", "end": "0:36:30", "timestamp": "0:36:00 - 0:36:30", "text": "summer this gradient over here these two upstream gradients have to these two upstream gradients have to be calculated first before we are able obviously to first before we are able obviously to back propagate through this gate finally after we actually go through these two points over here these two actually points over here these two actually gates we will be arriving at the desired we will be arriving at the desired result which is the derivative of f with which is the derivative of f with respect to x and also the derivative of f with and also the derivative of f with respect to y which is exactly what we wanted and this", "image_path": "img_data/video_87_chunk_72.jpg"}
{"video": "video_87", "start": "0:36:30", "end": "0:37:00", "timestamp": "0:36:30 - 0:37:00", "text": "respect to y which is exactly what we wanted and this which is exactly what we wanted and this is an example that serves at any you can have example that serves at any you can have any function even outside of the neural network in even outside of the neural network in space and the backward bycase is a space and the backward bycase is a general procedure that can be applied to any function that can be applied to any function whatsoever even those functions have whatsoever even those functions have nothing to do with neural networks nothing to do with neural networks in the next video we will be in the next video we will be dealing with some elementary neural dealing with some elementary neural networks just to reinforce the point in networks just to reinforce the point in our switch now to the neural networking our switch now to the neural networking space and start applying back propagation and start applying back propagation with vectors as", "image_path": "img_data/video_87_chunk_73.jpg"}
{"video": "video_87", "start": "0:37:00", "end": "0:37:07.766667", "timestamp": "0:37:00 - 0:37:07.766667", "text": "and start applying back propagation with vectors as those quantities are a bit more common those quantities are a bit more common in the neural network space thank in the neural network space thank you", "image_path": "img_data/video_87_chunk_74.jpg"}
{"video": "video_92", "start": "0:00:00", "end": "0:00:30", "timestamp": "0:00:00 - 0:00:30", "text": "[music] and more yes define the sample this is super [music] for this change there is a better year of san telmo", "image_path": "img_data/video_92_chunk_0.jpg"}
{"video": "video_92", "start": "0:00:30", "end": "0:01:00", "timestamp": "0:00:30 - 0:01:00", "text": "a better year of san telmo what is for me [music] [music] i did not put it there i am not", "image_path": "img_data/video_92_chunk_1.jpg"}
{"video": "video_92", "start": "0:01:00", "end": "0:01:30", "timestamp": "0:01:00 - 0:01:30", "text": "i am not [music] i am going nothing [", "image_path": "img_data/video_92_chunk_2.jpg"}
{"video": "video_92", "start": "0:01:30", "end": "0:02:00", "timestamp": "0:01:30 - 0:02:00", "text": "going nothing [ applause] [music] of power i already know who will seek to start in branch", "image_path": "img_data/video_92_chunk_3.jpg"}
{"video": "video_92", "start": "0:02:00", "end": "0:02:30", "timestamp": "0:02:00 - 0:02:30", "text": "who will seek to start in branch yes [music] to me and in family i do not yes m", "image_path": "img_data/video_92_chunk_4.jpg"}
{"video": "video_92", "start": "0:02:30", "end": "0:03:00", "timestamp": "0:02:30 - 0:03:00", "text": "m no longer to e i well [music] i am going nothing wow", "image_path": "img_data/video_92_chunk_5.jpg"}
{"video": "video_92", "start": "0:03:00", "end": "0:03:30", "timestamp": "0:03:00 - 0:03:30", "text": "i am going nothing wow i", "image_path": "img_data/video_92_chunk_6.jpg"}
{"video": "video_92", "start": "0:03:30", "end": "0:04:00", "timestamp": "0:03:30 - 0:04:00", "text": "i dream [music] going line filma mella herning rey and on this date", "image_path": "img_data/video_92_chunk_7.jpg"}
{"video": "video_92", "start": "0:04:00", "end": "0:04:30", "timestamp": "0:04:00 - 0:04:30", "text": "herning rey and on this date if my chilean front goes to more to myself that in first there are that to myself that in first there are that field reflectors of alvarez field reflectors of alvarez he and i in this house", "image_path": "img_data/video_92_chunk_8.jpg"}
{"video": "video_92", "start": "0:04:30", "end": "0:05:00", "timestamp": "0:04:30 - 0:05:00", "text": "in this house yes that the ends [music]", "image_path": "img_data/video_92_chunk_9.jpg"}
{"video": "video_92", "start": "0:05:00", "end": "0:05:30", "timestamp": "0:05:00 - 0:05:30", "text": "the ends [music] here yes and [music] by einstein", "image_path": "img_data/video_92_chunk_10.jpg"}
{"video": "video_92", "start": "0:05:30", "end": "0:06:00", "timestamp": "0:05:30 - 0:06:00", "text": "[music] by einstein 5 [music] [music] and marinsa and that no and marinsa and that no we must understand that they were no we must understand that they were he [music] [music] to the city well [music] [music] what they call of the city has to", "image_path": "img_data/video_92_chunk_11.jpg"}
{"video": "video_92", "start": "0:06:00", "end": "0:06:23.216167", "timestamp": "0:06:00 - 0:06:23.216167", "text": "[music] what they call of the city has to what they call of the city has to enter condiments", "image_path": "img_data/video_92_chunk_12.jpg"}
